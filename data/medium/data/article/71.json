{"sentences": [{"cd34": "Architecture of Giants: Data Stacks at Facebook, Netflix, Airbnb, and Pinterest"}, {"5c6f": "Here at Keen IO, we believe that companies who learn to wield event data will have a competitive advantage. That certainly seems to be the case at the world\u2019s leading tech companies. We continue to be amazed by the data engineering teams at Facebook, Amazon, Airbnb, Pinterest, and Netflix. Their work sets new standards for what software and businesses can know."}, {"1e69": "Because their products have massive adoption, these teams must continuously redefine what it means to do analytics at scale. They\u2019ve invested millions into their data architectures, and have data teams that outnumber the entire engineering departments at most companies."}, {"791e": "We built Keen so that most software engineering teams could leverage the latest large-scale event data technologies without having to set up everything from scratch. But, if you\u2019re curious about what it would be like to be a giant, continue on for a collection of architectures from the best of them."}, {"9fc0": "Netflix"}, {"9f06": "With 93 million MAU, Netflix has no shortage of interactions to capture. As their engineering team describes in the Evolution of the Netflix Data Pipeline, they capture roughly 500 billion events per day, which translates to roughly 1.3 PB per day. At peak hours, they\u2019ll record 8 million events per second. They employ over 100 people as data engineers or analysts."}, {"2522": "Here\u2019s a simplified view of their data architecture from the aforementioned post, showing Apache Kafka, Elastic Search, AWS S3, Apache Spark, Apache Hadoop, and EMR as major components."}, {"0b95": "Source: Evolution of Netflix Data\u00a0Pipeline"}, {"feef": "Facebook"}, {"b1b8": "With over 1B active users, Facebook has one of the largest data warehouses in the world, storing more than 300 petabytes. The data is used for a wide range of applications, from traditional batch processing to graph analytics, machine learning, and real-time interactive analytics."}, {"a3ac": "In order to do interactive querying at scale, Facebook engineering invented Presto, a custom distributed SQL query engine optimized for ad-hoc analysis. It\u2019s used by over a thousand employees, who run more than 30,000 queries daily across a variety of pluggable backend data stores like Hive, HBase, and Scribe."}, {"c4d7": "Airbnb"}, {"cb8b": "Airbnb supports over 100M users browsing over 2M listings, and their ability to intelligently make new travel suggestions to those users is critical to their growth."}, {"0a79": "At a meetup last year \u201cBuilding a World-Class Analytics Team\u201d, Elena Grewal, a Data Science Manager at Airbnb, mentioned that they had already scaled Airbnb\u2019s data team to 30+ engineers. That\u2019s a 5M+ annual investment on headcount alone."}, {"d65d": "In a great blog post \u201cData Infrastructure at Airbnb\u201d, AirbnbEng architects James Mayfield, Krishna Puttaswamy, Swaroop Jagadish, and Kevin Long describe the components that make up their stack, as well as how they maintain higher reliability levels for mission-critical data. They rely heavily on Hive and Apache Spark, and have also expanded Facebook\u2019s Presto."}, {"433f": "Pinterest"}, {"d801": "Pinterest serves over 100M MAU doing over 10B+ pageviews per month. As of 2015, they had scaled their data team to over 250 engineers. Their infrastructure relies heavily on Apache Kafka, Storm, Hadoop, HBase, and Redshift."}, {"4e1c": "Pinterest Data Architecture Overview"}, {"a857": "Not only does the Pinterest team need to keep track of enormous amounts of data related to Pinterest\u2019s customer base. Like any social platform, they also need to provide detailed analytics to their ad buyers. Tongbo Huang wrote \u201cBehind the Pins: Building Analytics at Pinterest\u201d about their work revamping their analytics stack to meet that need. Here\u2019s how they used Apache Kafka, AWS S3, and HBase to do it:"}, {"f03f": "Data Architecture for Pinterest Analytics for Businesses"}, {"cfd8": "User View of Pinterest Analytics for Businesses"}, {"e359": "Twitter / Crashlytics"}, {"83ee": "In Handling 5 Billions Sessions Per Day\u200a\u2014\u200ain Real Time, Ed Solovey describes some of the architecture built by the Crashlytics Answers team to handle billions of daily mobile device events."}, {"4f2b": "Event Reception"}, {"6c67": "Archival"}, {"bf62": "Batch Computation"}, {"26c1": "Speed Computation"}, {"6454": "Combined View"}, {"dbdb": "Data Architecture at Keen\u00a0IO"}, {"53a7": "As I mentioned before, we built Keen\u2019s APIs so that any developer could use world-class data architectures without having to staff a huge team and build a bunch of infrastructure. Thousands of engineering teams use Keen\u2019s APIs for capturing, analyzing, streaming, and embedding event data throughout their business, in realtime and in batch applications."}, {"4782": "Although a typical developer using Keen would never need to know what\u2019s happening behind the scenes when they send an event or run a query, here\u2019s what the architecture looks like that processes their requests:"}, {"50e6": "On the ingestion side, load balancers handle billions of incoming post requests as events stream in from apps, web sites, connected devices, servers, billing systems, etc. Events are validated, queued, and optionally enriched with additional metadata like IP-to-geo lookups. This all happens within seconds."}, {"6812": "Once safely stored in Apache Cassandra, event data is available for querying via a REST API. Our architecture (via technologies like Apache Storm, DynamoDB, Redis, and AWS lambda), supports various querying needs from real-time data exploration on the raw incoming data, to cached queries which can be instantly loaded in applications and customer-facing reports. Keen queries trillions of event properties per day and services thousands of customers building reporting, automation, and data exploration interfaces."}, {"fa50": "Thank You"}, {"db5a": "Thank you to the collaborative data engineering community who continue to not only invent new data technology, but to open source it and write about their learnings. Our work wouldn\u2019t be possible without the foundational work of so many engineering teams who have come before us. Nor would it be possible without those who continue to collaborate with us day in and day out. Comments and feedback welcome on this post."}, {"342f": "Special thanks to the authors and architects of the posts mentioned above: Steven Wu at Netflix, Martin Traverso at Facebook Presto, AirbnbEng, Pinterest Engineering, and Ed Solovey at Crashlytics Answers."}, {"e18c": "Thanks to editors Terry Horner, Dan Kador, Manu Mahajan, and Ryan Spraetz for help with this post."}], "content": "Architecture of Giants: Data Stacks at Facebook, Netflix, Airbnb, and Pinterest Here at Keen IO, we believe that companies who learn to wield event data will have a competitive advantage. That certainly seems to be the case at the world\u2019s leading tech companies. We continue to be amazed by the data engineering teams at Facebook, Amazon, Airbnb, Pinterest, and Netflix. Their work sets new standards for what software and businesses can know. Because their products have massive adoption, these teams must continuously redefine what it means to do analytics at scale. They\u2019ve invested millions into their data architectures, and have data teams that outnumber the entire engineering departments at most companies. We built Keen so that most software engineering teams could leverage the latest large-scale event data technologies without having to set up everything from scratch. But, if you\u2019re curious about what it would be like to be a giant, continue on for a collection of architectures from the best of them. Netflix With 93 million MAU, Netflix has no shortage of interactions to capture. As their engineering team describes in the Evolution of the Netflix Data Pipeline, they capture roughly 500 billion events per day, which translates to roughly 1.3 PB per day. At peak hours, they\u2019ll record 8 million events per second. They employ over 100 people as data engineers or analysts. Here\u2019s a simplified view of their data architecture from the aforementioned post, showing Apache Kafka, Elastic Search, AWS S3, Apache Spark, Apache Hadoop, and EMR as major components. Source: Evolution of Netflix Data\u00a0Pipeline Facebook With over 1B active users, Facebook has one of the largest data warehouses in the world, storing more than 300 petabytes. The data is used for a wide range of applications, from traditional batch processing to graph analytics, machine learning, and real-time interactive analytics. In order to do interactive querying at scale, Facebook engineering invented Presto, a custom distributed SQL query engine optimized for ad-hoc analysis. It\u2019s used by over a thousand employees, who run more than 30,000 queries daily across a variety of pluggable backend data stores like Hive, HBase, and Scribe. Airbnb Airbnb supports over 100M users browsing over 2M listings, and their ability to intelligently make new travel suggestions to those users is critical to their growth. At a meetup last year \u201cBuilding a World-Class Analytics Team\u201d, Elena Grewal, a Data Science Manager at Airbnb, mentioned that they had already scaled Airbnb\u2019s data team to 30+ engineers. That\u2019s a 5M+ annual investment on headcount alone. In a great blog post \u201cData Infrastructure at Airbnb\u201d, AirbnbEng architects James Mayfield, Krishna Puttaswamy, Swaroop Jagadish, and Kevin Long describe the components that make up their stack, as well as how they maintain higher reliability levels for mission-critical data. They rely heavily on Hive and Apache Spark, and have also expanded Facebook\u2019s Presto. Pinterest Pinterest serves over 100M MAU doing over 10B+ pageviews per month. As of 2015, they had scaled their data team to over 250 engineers. Their infrastructure relies heavily on Apache Kafka, Storm, Hadoop, HBase, and Redshift. Pinterest Data Architecture Overview Not only does the Pinterest team need to keep track of enormous amounts of data related to Pinterest\u2019s customer base. Like any social platform, they also need to provide detailed analytics to their ad buyers. Tongbo Huang wrote \u201cBehind the Pins: Building Analytics at Pinterest\u201d about their work revamping their analytics stack to meet that need. Here\u2019s how they used Apache Kafka, AWS S3, and HBase to do it: Data Architecture for Pinterest Analytics for Businesses User View of Pinterest Analytics for Businesses Twitter / Crashlytics In Handling 5 Billions Sessions Per Day\u200a\u2014\u200ain Real Time, Ed Solovey describes some of the architecture built by the Crashlytics Answers team to handle billions of daily mobile device events. Event Reception Archival Batch Computation Speed Computation Combined View Data Architecture at Keen\u00a0IO As I mentioned before, we built Keen\u2019s APIs so that any developer could use world-class data architectures without having to staff a huge team and build a bunch of infrastructure. Thousands of engineering teams use Keen\u2019s APIs for capturing, analyzing, streaming, and embedding event data throughout their business, in realtime and in batch applications. Although a typical developer using Keen would never need to know what\u2019s happening behind the scenes when they send an event or run a query, here\u2019s what the architecture looks like that processes their requests: On the ingestion side, load balancers handle billions of incoming post requests as events stream in from apps, web sites, connected devices, servers, billing systems, etc. Events are validated, queued, and optionally enriched with additional metadata like IP-to-geo lookups. This all happens within seconds. Once safely stored in Apache Cassandra, event data is available for querying via a REST API. Our architecture (via technologies like Apache Storm, DynamoDB, Redis, and AWS lambda), supports various querying needs from real-time data exploration on the raw incoming data, to cached queries which can be instantly loaded in applications and customer-facing reports. Keen queries trillions of event properties per day and services thousands of customers building reporting, automation, and data exploration interfaces. Thank You Thank you to the collaborative data engineering community who continue to not only invent new data technology, but to open source it and write about their learnings. Our work wouldn\u2019t be possible without the foundational work of so many engineering teams who have come before us. Nor would it be possible without those who continue to collaborate with us day in and day out. Comments and feedback welcome on this post. Special thanks to the authors and architects of the posts mentioned above: Steven Wu at Netflix, Martin Traverso at Facebook Presto, AirbnbEng, Pinterest Engineering, and Ed Solovey at Crashlytics Answers. Thanks to editors Terry Horner, Dan Kador, Manu Mahajan, and Ryan Spraetz for help with this post. ", "parent": "", "title": "Architecture of Giants: Data Stacks at Facebook, Netflix, Airbnb, and Pinterest", "child": "71_1\t71_2", "name": "71"}