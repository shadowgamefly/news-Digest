{"name": "1674", "parent": "", "content": "Deep Learning in 7 lines of\u00a0code The essence of machine learning is recognizing patterns within data. This boils down to 3 things: data, software and math. What can be done in seven lines of code you ask? A lot. Steve McQueen and Yul Brynner in \u201cThe Magnificent Seven\u201d\u00a0(1960) The way to reduce a deep learning problem to a few lines of code is to use layers of abstraction, otherwise known as \u2018frameworks\u2019. Today we\u2019ll use tensorflow and tflearn. Abstraction is an essential property of software: the app you are using to view this piece is an abstraction layer above some operating system that knows how to read files, display images, etc. and this is an abstraction above lower level functions. Ultimately there is CPU-level code that moves bits\u200a\u2014\u200athe \u2018bare metal\u2019. Software frameworks are abstraction layers. Our reduction is achieved by using tflearn, a layer above tensorflow, a layer above a Python. As always we\u2019ll use iPython notebook as a tool to facilitate our work. Let\u2019s start at the beginning In \u201cHow Neural Networks Work\u201d we built a neural network in Python (no frameworks), and we showed how machine learning could \u2018learn\u2019 from patterns of data, using a \u2018toy data\u2019 example. Recall the \u2018toy\u2019 data is purposefully simple so that we can intuitively grok the patterns within it. The notebook for this \u201czero abstraction\u201d code is here. Each mathematical operation within the model is detailed. 2-hidden layer\u00a0ANN Extending our model to use 2 hidden layers and Gradient Descent such as the one we built for analyzing text, we have ~80 lines of code, again sans frameworks. This is an example of \u201cDeep Learning, the \u201cdepth\u201d comes from the hidden layers. The definition of our model is relatively straight-forward, albeit laborious, most of the code is applied to training: This worked quite well\u200a\u2014\u200awe then abstracted it using a framework. Abstracting with Tensorflow In \u201cTensorflow demystified\u201d we built the same neural network, again we showed how machine learning could \u2018learn\u2019 from patterns of data. This simplified our code (same underlying mathematical structures), for example the handling of Gradient Descent and loss function is conveniently reduced to 2 lines of code. https://github.com/ugik/notebooks/blob/master/Tensorflow%20ANN.ipynb Our model\u2019s definition is also simplified, the math and common functions (eg. sigmoid) are encapsulated for us in the framework. You can imagine a complex neural network \u2018flow\u2019, like AlexNet, using tensorflow to simplify the definition and work for its mathematical \u2018flow\u2019. https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf Abstracting again Still unsatisfied with the amount of code and complexity involved, we abstract again using tflearn, which describes itself as: TFLearn: Deep learning library featuring a higher-level API for TensorFlow. By \u201chigher-level\u201d they mean higher abstraction level, which is what we\u2019re after. So we have our 7 lines of code for a multi-layer neural net. This is magnificent\u200a\u2014\u200a5 lines of code to define our neural net structure (input +2 hidden +output +regression), 2 lines to train it. Our notebook code is here. Let\u2019s go through this in detail, you\u2019ll notice that the data and learning intent is identical to our earlier example. Framework installation Make sure you have tensorflow 1.0.x installed, the tflearn framework will not work with tensorflow prior to version 1.0 '1.0.1' It may be helpful (on Linux with pip) to use: python -m pip install\u200a\u2014\u200aupgrade tensorflow tflearn Data Next we setup our data, same toy data as our tensorflow example. The training data was explained in detail there\u200a\u2014\u200ashould be self-explanatory. Notice we no longer need to carve out testing data, the tflearn framework can do this for us. Magnificent Seven Our deep-learning code: The first 5 lines define our neural \u2018net\u2019 with a sequence of tflearn functions: from tflearn.input_data to tflearn.fully_connected, to tflearn.regression. This \u2018flow\u2019 is identical to our tensorflow example: our input data has 5 features, we\u2019ll use 32 nodes in each hidden layer and our output has 2 classes. Next we instantiate a Deep Neural Network: tflearn.DNN with our network, with a tensorboard parameter to enable logging. And finally we fit our model with the training data. Notice the sweet interface for the training metrics. Change the n_epochs to see impact to accuracy. interactive metrics while training\u00a0model Training Step: 1999  | total loss: 0.01591 | time: 0.003s| Adam | epoch: 1000 | loss: 0.01591 - acc: 0.9997 -- iter: 16/22Training Step: 2000  | total loss: 0.01561 | time: 0.006s| Adam | epoch: 1000 | loss: 0.01561 - acc: 0.9997 -- iter: 22/22-- Predictions We now can use our model to predict output. Be sure to remove any test patterns from your training data (comment out lines containing the patterns you want to test), otherwise the model is cheating. [[0.004509848542511463, 0.9954901337623596]][[0.9810173511505127, 0.018982617184519768]] Our model correctly recognizes the [1, _, _, _, 1] pattern with output [1, 0] As a convenience when working iteratively with notebook, we reset our model\u2019s graph by adding 2 lines directly above our model code: By abstracting, we can focus on preparing our data and using our model to make predictions. Tensorboard Our tflearn framework automatically passes data to tensorboard: a visualization tool for tensorflow. Because we provided a log file with tflearn.DNN we can have a quick look. $ tensorboard\u200a\u2014\u200alogdir=tflearn_logs Starting TensorBoard b\u201941' on port 6006(You can navigate to http://127.0.1.1:6006) Here we can see a graph of our \u2018flow\u2019: tensorboard Graphs\u00a0view And our accuracy and loss function performance: tensorboard Scalars\u00a0view It\u2019s clear we don\u2019t need as many epochs in our training to achieve solid accuracy. Other examples Here\u2019s a tflearn setup for an LSTM RNN (Long-Short-Term-Memory Recurrent Neural-Net), often used to learn sequences of data with memory. Notice a different setup for the network and the tflearn.lstm, but mostly the same basic concept. https://github.com/tflearn/tflearn/blob/master/examples/nlp/lstm.py And here\u2019s a tflearn setup for a Convolutional Neural Network, often used for image recognition. Notice again all we\u2019re doing is providing the mathematical sequence for the network\u2019s mathematical equations, then feeding it data. https://github.com/tflearn/tflearn/blob/master/examples/images/convnet_mnist.py We started by learning from code without any frameworks, this showed us precisely what was going on. No \u2018black box\u2019. Once we have a solid understanding of the underlying code, we use frameworks to simplify our work, knowing that what\u2019s inside. Deep-learning frameworks simplify your work by encapsulating the underlying functions necessary. As the frameworks evolve and improve, we inherit those improvements automatically, consequentially we go from \u2018black box\u2019 to \u2018black boxes within a black box\u2019. \u201cSo let it be written, So let it be\u00a0done\u201d Yul Brenner \u201cThe King and I\u201d\u00a0(1954) ", "title": "Deep Learning in 7 lines of\u00a0code", "sentences": [{"8a22": "Deep Learning in 7 lines of\u00a0code"}, {"4dd5": "The essence of machine learning is recognizing patterns within data. This boils down to 3 things: data, software and math. What can be done in seven lines of code you ask? A lot."}, {"a594": "Steve McQueen and Yul Brynner in \u201cThe Magnificent Seven\u201d\u00a0(1960)"}, {"35b9": "The way to reduce a deep learning problem to a few lines of code is to use layers of abstraction, otherwise known as \u2018frameworks\u2019. Today we\u2019ll use tensorflow and tflearn."}, {"419d": "Abstraction is an essential property of software: the app you are using to view this piece is an abstraction layer above some operating system that knows how to read files, display images, etc. and this is an abstraction above lower level functions. Ultimately there is CPU-level code that moves bits\u200a\u2014\u200athe \u2018bare metal\u2019."}, {"4033": "Software frameworks are abstraction layers."}, {"dddf": "Our reduction is achieved by using tflearn, a layer above tensorflow, a layer above a Python. As always we\u2019ll use iPython notebook as a tool to facilitate our work."}, {"ba1a": "Let\u2019s start at the beginning"}, {"7db0": "In \u201cHow Neural Networks Work\u201d we built a neural network in Python (no frameworks), and we showed how machine learning could \u2018learn\u2019 from patterns of data, using a \u2018toy data\u2019 example. Recall the \u2018toy\u2019 data is purposefully simple so that we can intuitively grok the patterns within it."}, {"58b9": "The notebook for this \u201czero abstraction\u201d code is here. Each mathematical operation within the model is detailed."}, {"7966": "2-hidden layer\u00a0ANN"}, {"6f06": "Extending our model to use 2 hidden layers and Gradient Descent such as the one we built for analyzing text, we have ~80 lines of code, again sans frameworks. This is an example of \u201cDeep Learning, the \u201cdepth\u201d comes from the hidden layers."}, {"eb0e": "The definition of our model is relatively straight-forward, albeit laborious, most of the code is applied to training:"}, {"1745": "This worked quite well\u200a\u2014\u200awe then abstracted it using a framework."}, {"88fa": "Abstracting with Tensorflow"}, {"cf0d": "In \u201cTensorflow demystified\u201d we built the same neural network, again we showed how machine learning could \u2018learn\u2019 from patterns of data."}, {"b1b9": "This simplified our code (same underlying mathematical structures), for example the handling of Gradient Descent and loss function is conveniently reduced to 2 lines of code."}, {"3b9e": "https://github.com/ugik/notebooks/blob/master/Tensorflow%20ANN.ipynb"}, {"f522": "Our model\u2019s definition is also simplified, the math and common functions (eg. sigmoid) are encapsulated for us in the framework."}, {"5840": "You can imagine a complex neural network \u2018flow\u2019, like AlexNet, using tensorflow to simplify the definition and work for its mathematical \u2018flow\u2019."}, {"4772": "https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf"}, {"4ab9": "Abstracting again"}, {"3109": "Still unsatisfied with the amount of code and complexity involved, we abstract again using tflearn, which describes itself as:"}, {"bb53": "TFLearn: Deep learning library featuring a higher-level API for TensorFlow."}, {"b2ee": "By \u201chigher-level\u201d they mean higher abstraction level, which is what we\u2019re after. So we have our 7 lines of code for a multi-layer neural net."}, {"4c85": "This is magnificent\u200a\u2014\u200a5 lines of code to define our neural net structure (input +2 hidden +output +regression), 2 lines to train it."}, {"730b": "Our notebook code is here."}, {"c165": "Let\u2019s go through this in detail, you\u2019ll notice that the data and learning intent is identical to our earlier example."}, {"3ddc": "Framework installation"}, {"0932": "Make sure you have tensorflow 1.0.x installed, the tflearn framework will not work with tensorflow prior to version 1.0"}, {"89b1": "'1.0.1'"}, {"6496": "It may be helpful (on Linux with pip) to use:"}, {"7d32": "python -m pip install\u200a\u2014\u200aupgrade tensorflow tflearn"}, {"4290": "Data"}, {"43ae": "Next we setup our data, same toy data as our tensorflow example. The training data was explained in detail there\u200a\u2014\u200ashould be self-explanatory. Notice we no longer need to carve out testing data, the tflearn framework can do this for us."}, {"8f6d": "Magnificent Seven"}, {"40c8": "Our deep-learning code:"}, {"6df2": "The first 5 lines define our neural \u2018net\u2019 with a sequence of tflearn functions: from tflearn.input_data to tflearn.fully_connected, to tflearn.regression. This \u2018flow\u2019 is identical to our tensorflow example: our input data has 5 features, we\u2019ll use 32 nodes in each hidden layer and our output has 2 classes."}, {"4573": "Next we instantiate a Deep Neural Network: tflearn.DNN with our network, with a tensorboard parameter to enable logging."}, {"88f8": "And finally we fit our model with the training data. Notice the sweet interface for the training metrics. Change the n_epochs to see impact to accuracy."}, {"03af": "interactive metrics while training\u00a0model"}, {"dc61": "Training Step: 1999  | total loss: 0.01591 | time: 0.003s| Adam | epoch: 1000 | loss: 0.01591 - acc: 0.9997 -- iter: 16/22Training Step: 2000  | total loss: 0.01561 | time: 0.006s| Adam | epoch: 1000 | loss: 0.01561 - acc: 0.9997 -- iter: 22/22--"}, {"c844": "Predictions"}, {"d3cd": "We now can use our model to predict output. Be sure to remove any test patterns from your training data (comment out lines containing the patterns you want to test), otherwise the model is cheating."}, {"5475": "[[0.004509848542511463, 0.9954901337623596]][[0.9810173511505127, 0.018982617184519768]]"}, {"ebdd": "Our model correctly recognizes the [1, _, _, _, 1] pattern with output [1, 0]"}, {"b834": "As a convenience when working iteratively with notebook, we reset our model\u2019s graph by adding 2 lines directly above our model code:"}, {"61f3": "By abstracting, we can focus on preparing our data and using our model to make predictions."}, {"6ad6": "Tensorboard"}, {"b55d": "Our tflearn framework automatically passes data to tensorboard: a visualization tool for tensorflow. Because we provided a log file with tflearn.DNN we can have a quick look."}, {"59f6": "$ tensorboard\u200a\u2014\u200alogdir=tflearn_logs"}, {"8ed9": "Starting TensorBoard b\u201941' on port 6006(You can navigate to http://127.0.1.1:6006)"}, {"caf4": "Here we can see a graph of our \u2018flow\u2019:"}, {"2535": "tensorboard Graphs\u00a0view"}, {"3b64": "And our accuracy and loss function performance:"}, {"4da8": "tensorboard Scalars\u00a0view"}, {"6317": "It\u2019s clear we don\u2019t need as many epochs in our training to achieve solid accuracy."}, {"567e": "Other examples"}, {"b33d": "Here\u2019s a tflearn setup for an LSTM RNN (Long-Short-Term-Memory Recurrent Neural-Net), often used to learn sequences of data with memory. Notice a different setup for the network and the tflearn.lstm, but mostly the same basic concept."}, {"2a6b": "https://github.com/tflearn/tflearn/blob/master/examples/nlp/lstm.py"}, {"3a2e": "And here\u2019s a tflearn setup for a Convolutional Neural Network, often used for image recognition. Notice again all we\u2019re doing is providing the mathematical sequence for the network\u2019s mathematical equations, then feeding it data."}, {"44ad": "https://github.com/tflearn/tflearn/blob/master/examples/images/convnet_mnist.py"}, {"18b3": "We started by learning from code without any frameworks, this showed us precisely what was going on. No \u2018black box\u2019. Once we have a solid understanding of the underlying code, we use frameworks to simplify our work, knowing that what\u2019s inside."}, {"f864": "Deep-learning frameworks simplify your work by encapsulating the underlying functions necessary. As the frameworks evolve and improve, we inherit those improvements automatically, consequentially we go from \u2018black box\u2019 to \u2018black boxes within a black box\u2019."}, {"32d0": "\u201cSo let it be written, So let it be\u00a0done\u201d"}, {"7b1a": "Yul Brenner \u201cThe King and I\u201d\u00a0(1954)"}], "child": "1674_1\t1674_2"}