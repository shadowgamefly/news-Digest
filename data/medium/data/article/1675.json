{"name": "1675", "parent": "", "content": "A Peek at Trends in Machine\u00a0Learning Have you looked at Google Trends? It\u2019s pretty cool\u200a\u2014\u200ayou enter some keywords and see how Google Searches of that term vary through time. I thought\u200a\u2014\u200ahey, I happen to have this arxiv-sanity database of 28,303 (arxiv) Machine Learning papers over the last 5 years, so why not do something similar and take a look at how Machine Learning research has evolved over the last 5 years? The results are fairly fun, so I thought I\u2019d post. (Edit: machine learning is a large area. A good chunk of this post is about deep learning specifically, which is the subarea I am most familiar with.) The arxiv singularity Let\u2019s first look at the total number of submitted papers across the arxiv-sanity categories (cs.AI,cs.LG,cs.CV,cs.CL,cs.NE,stat.ML), over time. We get the following: Yes, March of 2017 saw almost 2,000 submissions in these areas. The peaks are likely due to conference deadlines (e.g. NIPS/ICML). Note that this is not directly a statement about the size of the area itself, since not everyone submits their paper to arxiv, and the fraction of people who do likely changes over time. But the point remains\u200a\u2014\u200athat\u2019s a lot of papers to be aware of, skim, or (gasp) read. This total number of papers will serve as the denominator. We can now look at what fraction of papers contain certain keywords of interest. Deep Learning Frameworks To warm up let\u2019s look at the Deep Learning frameworks that are in use. To compute this, we record the fraction of papers that mention the framework somewhere in the full text (anywhere\u200a\u2014\u200aincluding bibliography etc). For papers uploaded on March 2017, we get the following: % of papers \t framework \t has been around for (months)------------------------------------------------------------    9.1          tensorflow       16    7.1               caffe       37    4.6              theano       54    3.3               torch       37    2.5               keras       19    1.7          matconvnet       26    1.2             lasagne       23    0.5             chainer       16    0.3               mxnet       17    0.3                cntk       13    0.2             pytorch       1    0.1      deeplearning4j       14 That is, 10% of all papers submitted in March 2017 mention TensorFlow. Of course, not every paper declares the framework used, but if we assume that papers declare the framework with some fixed random probability independent of the framework, then it looks like about 40% of the community is currently using TensorFlow (or a bit more, if you count Keras with the TF backend). And here is the plot of how some of the more popular frameworks evolved over time: We can see that Theano has been around for a while but its growth has somewhat stalled. Caffe shot up quickly in 2014, but was overtaken by the TensorFlow singularity in the last few months. Torch (and the very recent PyTorch) are also climbing up, slow and steady. It will be fun to watch this develop in the next few months\u200a\u2014\u200amy own guess is that Caffe/Theano will go on a slow decline and TF growth will become a bit slower due to PyTorch. ConvNet Models For fun, how about if we look at common ConvNet models? Here, we can clearly see a huge spike up for ResNets, to the point that they occur in 9% of all papers last March: Also, who was talking about \u201cinception\u201d before the InceptionNet? Curious. Optimization algorithms In terms of optimization algorithms, it looks like Adam is on a roll, found in about 23% of papers! The actual fraction of use is hard to estimate; it\u2019s likely higher than 23% because some papers don\u2019t declare the optimization algorithm, and a good chunk of papers might not even be optimizing any neural network at all. It\u2019s then likely lower by about 5%, which is the \u201cbackground activity\u201d of \u201cAdam\u201d, likely a collision with author names, as the Adam optimization algorithm was only released on Dec 2014. Researchers I was also curious to plot the mentions of some of the most senior PIs in Deep Learning (this gives something similar to citation count, but 1) it is more robust across population of papers with a \u201c0/1\u201d count, and 2) it is normalized by the total size of the pie): A few things to note: \u201cbengio\u201d is mentioned in 35% of all submissions, but there are two Bengios: Samy and Yoshua, who add up on this plot. In particular, Geoff Hinton is mentioned in more than 30% of all new papers! That seems like a lot. Hot or Not\u00a0Keywords Finally, instead of manually going by categories of keywords, let\u2019s actively look at the keywords that are \u201chot\u201d (or not). Top hot\u00a0keywords There are many ways to define this, but for this experiment I look at each unigram or bigram in all the papers and record the ratio of its max use last year compared to its max use up to last year. The papers that excel at this metric are those that one year ago were niche, but this year appear with a much higher relative frequency. The top list (slightly edited out some duplicates) comes out as follows: 8.17394726486 resnet6.76767676768 tensorflow5.21818181818 gans5.0098386462 residual networks4.34787878788 adam2.95181818182 batch normalization2.61663993305 fcn2.47812783318 vgg162.03636363636 style transfer1.99958217686 gated1.99057177616 deep reinforcement1.98428686543 lstm1.93700787402 nmt1.90606060606 inception1.8962962963 siamese1.88976377953 character level1.87533998187 region proposal1.81670721817 distillation1.81400378481 tree search1.78578069795 torch1.77685950413 policy gradient1.77370153867 encoder decoder1.74685427385 gru1.72430399325 word2vec1.71884293052 relu activation1.71459655485 visual question1.70471560525 image generation For example, ResNet\u2019s ratio of 8.17 is because until 1 year ago it appeared in up to only 1.044% of all submissions (in Mar 2016), but last last month (Mar 2017) it appeared in 8.53% of submissions, so 8.53 / 1.044 ~= 8.17. So there you have it\u200a\u2014\u200athe core innovations that became all the rage over the last year are 1) ResNets, 2) GANs, 3) Adam, 4) BatchNorm. Use more of these to fit in with your friends. In terms of research interests, we see 1) style transfer, 2) deep RL, 3) Neural Machine Translation (\u201cnmt\u201d), and perhaps 4) image generation. And architecturally, it is hot to use 1) Fully Convolutional Nets (FCN), 2) LSTMs/GRUs, 3) Siamese nets, and 4) Encoder decoder nets. Top not\u00a0hot How about the reverse? What has seen many fewer submissions over the last year than has historically had a higher \u201cmind share\u201d? Here are a few: 0.0462375339982 fractal0.112222705524 learning bayesian0.123531424661 ibp0.138351983723 texture analysis0.152810895084 bayesian network0.170535340862 differential evolution0.227932960894 wavelet transform0.24482875551 dirichlet process I\u2019m not sure what \u201cfractal\u201d is referring to, but more generally it looks like bayesian nonparametrics are under attack. Conclusion Now is the time to submit paper on Fully Convolutional Encoder Decoder BatchNorm ResNet GAN applied to Style Transfer, optimized with Adam. Hey, that doesn\u2019t even sound too far-fetched. :) ", "title": "A Peek at Trends in Machine\u00a0Learning", "sentences": [{"bc37": "A Peek at Trends in Machine\u00a0Learning"}, {"534d": "Have you looked at Google Trends? It\u2019s pretty cool\u200a\u2014\u200ayou enter some keywords and see how Google Searches of that term vary through time. I thought\u200a\u2014\u200ahey, I happen to have this arxiv-sanity database of 28,303 (arxiv) Machine Learning papers over the last 5 years, so why not do something similar and take a look at how Machine Learning research has evolved over the last 5 years? The results are fairly fun, so I thought I\u2019d post."}, {"d29a": "(Edit: machine learning is a large area. A good chunk of this post is about deep learning specifically, which is the subarea I am most familiar with.)"}, {"9551": "The arxiv singularity"}, {"3f4e": "Let\u2019s first look at the total number of submitted papers across the arxiv-sanity categories (cs.AI,cs.LG,cs.CV,cs.CL,cs.NE,stat.ML), over time. We get the following:"}, {"d670": "Yes, March of 2017 saw almost 2,000 submissions in these areas. The peaks are likely due to conference deadlines (e.g. NIPS/ICML). Note that this is not directly a statement about the size of the area itself, since not everyone submits their paper to arxiv, and the fraction of people who do likely changes over time. But the point remains\u200a\u2014\u200athat\u2019s a lot of papers to be aware of, skim, or (gasp) read."}, {"b7f0": "This total number of papers will serve as the denominator. We can now look at what fraction of papers contain certain keywords of interest."}, {"95a9": "Deep Learning Frameworks"}, {"a2e5": "To warm up let\u2019s look at the Deep Learning frameworks that are in use. To compute this, we record the fraction of papers that mention the framework somewhere in the full text (anywhere\u200a\u2014\u200aincluding bibliography etc). For papers uploaded on March 2017, we get the following:"}, {"a491": "% of papers \t framework \t has been around for (months)------------------------------------------------------------    9.1          tensorflow       16    7.1               caffe       37    4.6              theano       54    3.3               torch       37    2.5               keras       19    1.7          matconvnet       26    1.2             lasagne       23    0.5             chainer       16    0.3               mxnet       17    0.3                cntk       13    0.2             pytorch       1    0.1      deeplearning4j       14"}, {"23ed": "That is, 10% of all papers submitted in March 2017 mention TensorFlow. Of course, not every paper declares the framework used, but if we assume that papers declare the framework with some fixed random probability independent of the framework, then it looks like about 40% of the community is currently using TensorFlow (or a bit more, if you count Keras with the TF backend). And here is the plot of how some of the more popular frameworks evolved over time:"}, {"aa22": "We can see that Theano has been around for a while but its growth has somewhat stalled. Caffe shot up quickly in 2014, but was overtaken by the TensorFlow singularity in the last few months. Torch (and the very recent PyTorch) are also climbing up, slow and steady. It will be fun to watch this develop in the next few months\u200a\u2014\u200amy own guess is that Caffe/Theano will go on a slow decline and TF growth will become a bit slower due to PyTorch."}, {"1f85": "ConvNet Models"}, {"ba81": "For fun, how about if we look at common ConvNet models? Here, we can clearly see a huge spike up for ResNets, to the point that they occur in 9% of all papers last March:"}, {"06eb": "Also, who was talking about \u201cinception\u201d before the InceptionNet? Curious."}, {"0fe9": "Optimization algorithms"}, {"c4bd": "In terms of optimization algorithms, it looks like Adam is on a roll, found in about 23% of papers! The actual fraction of use is hard to estimate; it\u2019s likely higher than 23% because some papers don\u2019t declare the optimization algorithm, and a good chunk of papers might not even be optimizing any neural network at all. It\u2019s then likely lower by about 5%, which is the \u201cbackground activity\u201d of \u201cAdam\u201d, likely a collision with author names, as the Adam optimization algorithm was only released on Dec 2014."}, {"0e60": "Researchers"}, {"d2e0": "I was also curious to plot the mentions of some of the most senior PIs in Deep Learning (this gives something similar to citation count, but 1) it is more robust across population of papers with a \u201c0/1\u201d count, and 2) it is normalized by the total size of the pie):"}, {"d2de": "A few things to note: \u201cbengio\u201d is mentioned in 35% of all submissions, but there are two Bengios: Samy and Yoshua, who add up on this plot. In particular, Geoff Hinton is mentioned in more than 30% of all new papers! That seems like a lot."}, {"5e6d": "Hot or Not\u00a0Keywords"}, {"47dd": "Finally, instead of manually going by categories of keywords, let\u2019s actively look at the keywords that are \u201chot\u201d (or not)."}, {"dbcc": "Top hot\u00a0keywords"}, {"031c": "There are many ways to define this, but for this experiment I look at each unigram or bigram in all the papers and record the ratio of its max use last year compared to its max use up to last year. The papers that excel at this metric are those that one year ago were niche, but this year appear with a much higher relative frequency. The top list (slightly edited out some duplicates) comes out as follows:"}, {"d80d": "8.17394726486 resnet6.76767676768 tensorflow5.21818181818 gans5.0098386462 residual networks4.34787878788 adam2.95181818182 batch normalization2.61663993305 fcn2.47812783318 vgg162.03636363636 style transfer1.99958217686 gated1.99057177616 deep reinforcement1.98428686543 lstm1.93700787402 nmt1.90606060606 inception1.8962962963 siamese1.88976377953 character level1.87533998187 region proposal1.81670721817 distillation1.81400378481 tree search1.78578069795 torch1.77685950413 policy gradient1.77370153867 encoder decoder1.74685427385 gru1.72430399325 word2vec1.71884293052 relu activation1.71459655485 visual question1.70471560525 image generation"}, {"23fb": "For example, ResNet\u2019s ratio of 8.17 is because until 1 year ago it appeared in up to only 1.044% of all submissions (in Mar 2016), but last last month (Mar 2017) it appeared in 8.53% of submissions, so 8.53 / 1.044 ~= 8.17. So there you have it\u200a\u2014\u200athe core innovations that became all the rage over the last year are 1) ResNets, 2) GANs, 3) Adam, 4) BatchNorm. Use more of these to fit in with your friends. In terms of research interests, we see 1) style transfer, 2) deep RL, 3) Neural Machine Translation (\u201cnmt\u201d), and perhaps 4) image generation. And architecturally, it is hot to use 1) Fully Convolutional Nets (FCN), 2) LSTMs/GRUs, 3) Siamese nets, and 4) Encoder decoder nets."}, {"46f1": "Top not\u00a0hot"}, {"52c8": "How about the reverse? What has seen many fewer submissions over the last year than has historically had a higher \u201cmind share\u201d? Here are a few:"}, {"577b": "0.0462375339982 fractal0.112222705524 learning bayesian0.123531424661 ibp0.138351983723 texture analysis0.152810895084 bayesian network0.170535340862 differential evolution0.227932960894 wavelet transform0.24482875551 dirichlet process"}, {"5615": "I\u2019m not sure what \u201cfractal\u201d is referring to, but more generally it looks like bayesian nonparametrics are under attack."}, {"f4ff": "Conclusion"}, {"2a07": "Now is the time to submit paper on Fully Convolutional Encoder Decoder BatchNorm ResNet GAN applied to Style Transfer, optimized with Adam. Hey, that doesn\u2019t even sound too far-fetched."}, {"c2fe": ":)"}], "child": "1675_1\t1675_2\t1675_3\t1675_4\t1675_5\t1675_6\t1675_7"}