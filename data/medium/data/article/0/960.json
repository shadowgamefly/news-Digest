{"parent": "", "title": "Tests and physical state: an\u00a0analogy", "author": "Diogo Lucas", "sentences": [{"d743": "Tests and physical state: an\u00a0analogy"}, {"e73d": "DISCLAIMER: this is an ultra geeky, trippy post. Well, trippier\u2026"}, {"a958": "TL;DR"}, {"2796": "Having tests that break all the time (vapor) is bad, but having tests that never break (solid) is usually wasteful. Tests should be fluid enough (liquid) to break on occasion, as a result of system evolution."}, {"9ef3": "Dynamical systems and Chaos\u00a0Theory"}, {"4972": "Mathematics, this distant cousin (or antecedent, take your pick) of IT, is a frequent provider of insights into ways to model software solutions. And sometimes, it can offer quite compelling analogies."}, {"926b": "A particularly interesting area in that space is called Dynamical Systems and has been the plaything for founders of our field, such as von Neumann (if you don\u2019t know him, check him out\u200a\u2014\u200adude was impressive). Oh, and it spawned the whole trendy line of study named Chaos Theory. Yeah, that talk about butterflies and stuff."}, {"0dc7": "The famous Lorenz attractor. Squint your eyes really hard and you may see a butterfly. Or have cornea\u00a0issues."}, {"8b68": "At the core of it (and pardon my oversimplification of the mathematics), is the idea that if you represent a system\u2019s state as M and define a function f(M) \u2192 M\u2019, that you can iterate over time. In other words, given a discrete state, there is a function that, applied to it, outputs the next step in system evolution."}, {"a501": "Many interesting implementations exist out there. Edward Lorenz, for example, created a virtual, simplified simulation of atmospheric events. And discovered that small precision errors could lead the system to a completely different state within a few interactions. The mind blowing conclusion is that a small error in measurement in a chaotic system can lead to a completely inability to predict behavior over a medium/large time window. This is something that leads many to claim precise weather forecast over more than a few days is a no-go."}, {"f717": "Another interesting case is John Conway\u2019s Game of Life, where a matrix of dots can be either dead (0) or alive (1) and may change their state based on neighboring cell statuses. The animation below shows a bit of that system into action."}, {"f5d0": "Gliders guns, gliders and all sorts of crazy patterns in this animation\u200a\u2014\u200afun\u00a0stuff!"}, {"50bf": "Dynamical systems and physical\u00a0state"}, {"68b8": "As dynamical systems go, folks came up with an interesting analogy in the physical state of matter:"}, {"cff0": "A game of life matrix reaching a stable equilibrium."}, {"e0bb": "A game of life matrix in the way of reaching cyclic equilibrium. Much to my frustration, animation stops before the cycle is\u00a0clear."}, {"8044": "Not much to see here,\u00a0right?"}, {"ceb8": "Tests and physical\u00a0state"}, {"6c55": "So yeah, this is where we come back to talk about software development. The very same analogy can be made when talking about automated tests. And similar conclusions can be reached\u2026"}, {"eed7": "Vapor tests"}, {"fc7b": "Ever had those cases where a test will fail on occasion, with a simple action such as re-running the build taking them back to green? Yeah, we all had it\u200a\u2014\u200aand those flaky tests are vapor. And quite nasty at that."}, {"3b3b": "The big problem behind these cases is that they encourage a dangerous, slippery slope: since test failure may not indicate a problem, people tend to be lenient about it. And this can easily drive teams to miss issues hidden by a broken test or, even worse, to start accumulating test failure, even for robust cases, in a classic broken window situation."}, {"b5dd": "Seriously, if there is one thing to keep from this post is that teams should adopt a zero-tolerance policy for flaky tests."}, {"5c46": "At the end of the day, most of these frailties come from bad practices such as:"}, {"4ce9": "Solid tests"}, {"6358": "\u201cWait, solid is good, right? There\u2019s even an acronym that we\u2019ve learned as a must-have!\u201d. Well, SOLID principles are great, but if a test literally never, ever breaks, then a question is begging to be asked\u200a\u2014\u200awhat value does it provide?"}, {"7109": "The major downside of a stagnant test is that it is a performance detractor: each second you add to test execution is an age in today\u2019s development sense of time and an encouragement to reduce its frequency."}, {"1c9d": "Solid tests may be born this way: if they are exercising code without really validating its quality, they may be just checking availability, instead of full functionality. Increasing your asserts makes these cases more relevant and dynamic."}, {"949b": "A more justifiable case is when a test freezes over time. This is usually an indication that their target (AKA subject under test\u200a\u2014\u200aSUT) has matured into a stable (perhaps even stagnant) state. In such cases, a good approach it to perform an architectural refactoring, moving the SUT into a separate component/microservice."}, {"a1dd": "Liquid tests"}, {"87e6": "This is the ideal, middle ground solution: eventual test breakage being an indicator that things are evolving and that yeah, eventually we screw up."}, {"bd76": "How do I detect these\u00a0cases?"}, {"8434": "A pretty good question by now would be \u201chey, my team does a great job at not breaking the build\u200a\u2014\u200ain this case, all of our tests would be considered solid\u201d. Fair statement, but the point is: this is not something you should assess on a build server, but on local dev boxes."}, {"95fe": "This will probably require some level of manual analysis (unfortunately a common need these days), but with the advent of tools such as industrial logic\u2019s TDD dashboard, one can hope to see a new generation of development metrics tools coming to our aid."}], "content": "Tests and physical state: an\u00a0analogy DISCLAIMER: this is an ultra geeky, trippy post. Well, trippier\u2026 TL;DR Having tests that break all the time (vapor) is bad, but having tests that never break (solid) is usually wasteful. Tests should be fluid enough (liquid) to break on occasion, as a result of system evolution. Dynamical systems and Chaos\u00a0Theory Mathematics, this distant cousin (or antecedent, take your pick) of IT, is a frequent provider of insights into ways to model software solutions. And sometimes, it can offer quite compelling analogies. A particularly interesting area in that space is called Dynamical Systems and has been the plaything for founders of our field, such as von Neumann (if you don\u2019t know him, check him out\u200a\u2014\u200adude was impressive). Oh, and it spawned the whole trendy line of study named Chaos Theory. Yeah, that talk about butterflies and stuff. The famous Lorenz attractor. Squint your eyes really hard and you may see a butterfly. Or have cornea\u00a0issues. At the core of it (and pardon my oversimplification of the mathematics), is the idea that if you represent a system\u2019s state as M and define a function f(M) \u2192 M\u2019, that you can iterate over time. In other words, given a discrete state, there is a function that, applied to it, outputs the next step in system evolution. Many interesting implementations exist out there. Edward Lorenz, for example, created a virtual, simplified simulation of atmospheric events. And discovered that small precision errors could lead the system to a completely different state within a few interactions. The mind blowing conclusion is that a small error in measurement in a chaotic system can lead to a completely inability to predict behavior over a medium/large time window. This is something that leads many to claim precise weather forecast over more than a few days is a no-go. Another interesting case is John Conway\u2019s Game of Life, where a matrix of dots can be either dead (0) or alive (1) and may change their state based on neighboring cell statuses. The animation below shows a bit of that system into action. Gliders guns, gliders and all sorts of crazy patterns in this animation\u200a\u2014\u200afun\u00a0stuff! Dynamical systems and physical\u00a0state As dynamical systems go, folks came up with an interesting analogy in the physical state of matter: A game of life matrix reaching a stable equilibrium. A game of life matrix in the way of reaching cyclic equilibrium. Much to my frustration, animation stops before the cycle is\u00a0clear. Not much to see here,\u00a0right? Tests and physical\u00a0state So yeah, this is where we come back to talk about software development. The very same analogy can be made when talking about automated tests. And similar conclusions can be reached\u2026 Vapor tests Ever had those cases where a test will fail on occasion, with a simple action such as re-running the build taking them back to green? Yeah, we all had it\u200a\u2014\u200aand those flaky tests are vapor. And quite nasty at that. The big problem behind these cases is that they encourage a dangerous, slippery slope: since test failure may not indicate a problem, people tend to be lenient about it. And this can easily drive teams to miss issues hidden by a broken test or, even worse, to start accumulating test failure, even for robust cases, in a classic broken window situation. Seriously, if there is one thing to keep from this post is that teams should adopt a zero-tolerance policy for flaky tests. At the end of the day, most of these frailties come from bad practices such as: Solid tests \u201cWait, solid is good, right? There\u2019s even an acronym that we\u2019ve learned as a must-have!\u201d. Well, SOLID principles are great, but if a test literally never, ever breaks, then a question is begging to be asked\u200a\u2014\u200awhat value does it provide? The major downside of a stagnant test is that it is a performance detractor: each second you add to test execution is an age in today\u2019s development sense of time and an encouragement to reduce its frequency. Solid tests may be born this way: if they are exercising code without really validating its quality, they may be just checking availability, instead of full functionality. Increasing your asserts makes these cases more relevant and dynamic. A more justifiable case is when a test freezes over time. This is usually an indication that their target (AKA subject under test\u200a\u2014\u200aSUT) has matured into a stable (perhaps even stagnant) state. In such cases, a good approach it to perform an architectural refactoring, moving the SUT into a separate component/microservice. Liquid tests This is the ideal, middle ground solution: eventual test breakage being an indicator that things are evolving and that yeah, eventually we screw up. How do I detect these\u00a0cases? A pretty good question by now would be \u201chey, my team does a great job at not breaking the build\u200a\u2014\u200ain this case, all of our tests would be considered solid\u201d. Fair statement, but the point is: this is not something you should assess on a build server, but on local dev boxes. This will probably require some level of manual analysis (unfortunately a common need these days), but with the advent of tools such as industrial logic\u2019s TDD dashboard, one can hope to see a new generation of development metrics tools coming to our aid. ", "name": "960", "child": "960_1\t960_2960_1\t960_2", "timestamp": "Oct 31, 2016"}