{"sentences": [{"0dcc": "How Machines Make Sense of Big Data: an Introduction to Clustering Algorithms"}, {"3329": "Take a look at the image below. It\u2019s a collection of bugs and creepy-crawlies of different shapes and sizes. Take a moment to categorize them by similarity into a number of groups."}, {"941a": "This isn\u2019t a trick question. Start with grouping the spiders together."}, {"2fa8": "Done? While there\u2019s not necessarily a \u201ccorrect\u201d answer here, it\u2019s most likely you split the bugs into four clusters. The spiders in one cluster, the pair of snails in another, the butterflies and moth into one, and the trio of wasps and bees into one more."}, {"3c8e": "That wasn\u2019t too bad, was it? You could probably do the same with twice as many bugs, right? If you had a bit of time to spare\u200a\u2014\u200aor a passion for entomology\u200a\u2014\u200ayou could probably even do the same with a hundred bugs."}, {"376c": "For a machine though, grouping ten objects into however many meaningful clusters is no small task, thanks to a mind-bending branch of maths called combinatorics, which tells us that are 115,975 different possible ways you could have grouped those ten insects together. Had there been twenty bugs, there would have been over fifty trillion possible ways of clustering them."}, {"2d04": "With a hundred bugs\u200a\u2014\u200athere\u2019d be many times more solutions than there are particles in the known universe. How many times more? By my calculation, approximately five hundred million billion billion times more. In fact, there are more than four million billion googol solutions (what\u2019s a googol?). For just a hundred objects."}, {"4152": "Almost all of those solutions would be meaningless\u200a\u2014\u200ayet from that unimaginable number of possible choices, you pretty quickly found one of the very few that clustered the bugs in a useful way."}, {"93b1": "Us humans take it for granted how good we are categorizing and making sense of large volumes of data pretty quickly. Whether it\u2019s a paragraph of text, or images on a screen, or a sequence of objects\u200a\u2014\u200ahumans are generally fairly efficient at making sense of whatever data the world throws at us."}, {"64d2": "Given that a key aspect of developing A.I. and Machine Learning is getting machines to quickly make sense of large sets of input data, what shortcuts are there available? Here, you can read about three clustering algorithms that machines can use to quickly make sense of large datasets. This is by no means an exhaustive list\u200a\u2014\u200athere are other algorithms out there\u200a\u2014\u200abut they represent a good place to start!"}, {"f64e": "You\u2019ll find for each a quick summary of when you might use them, a brief overview of how they work, and a more detailed, step-by-step worked example. I believe it helps to understand an algorithm by actually carrying out yourself. If you\u2019re really keen, you\u2019ll find the best way to do this is with pen and paper. Go ahead\u200a\u2014\u200anobody will judge!"}, {"2386": "Three suspiciously neat clusters, with K =\u00a03"}, {"c662": "K-means clustering"}, {"cf9b": "Use when:"}, {"bbf1": "\u2026you have an idea of how many groups you\u2019re expecting to find a priori."}, {"71dc": "How it\u00a0works:"}, {"ad35": "The algorithm randomly assigns each observation into one of k categories, then calculates the mean of each category. Next, it reassigns each observation to the category with the closest mean before recalculating the means. This step repeats over and over until no more reassignments are necessary."}, {"6699": "Worked Example:"}, {"cd33": "Take a group of 12 football (or \u2018soccer\u2019) players who have each scored a certain number of goals this season (say in the range 3\u201330). Let\u2019s divide them into separate clusters\u200a\u2014\u200asay three."}, {"597a": "Step 1 requires us to randomly split the players into three groups and calculate the means of each."}, {"8aeb": "Group 1Player A (5 goals), Player B (20 goals), Player C (11 goals) Group Mean = (5 + 20 + 11) / 3 = 12"}, {"b753": "Group 2Player D (5 goals), Player E (3 goals), Player F (19 goals)Group Mean = 9"}, {"37d3": "Group 3Player G (30 goals), Player H (3 goals), Player I (15 goals)Group Mean = 16"}, {"e597": "Step 2: For each player, reassign them to the group with the closest mean. E.g., Player A (5 goals) is assigned to Group 2 (mean = 9). Then recalculate the group means."}, {"7025": "Group 1 (Old Mean = 12)Player C (11 goals) New Mean = 11"}, {"eb82": "Group 2 (Old Mean = 9)Player A (5 goals), Player D (5 goals), Player E (3 goals), Player H (3 goals)New Mean = 4"}, {"7542": "Group 3 (Old Mean = 16)Player G (30 goals), Player I (15 goals), Player B (20 goals), Player F (19 goals)New Mean = 21"}, {"dbda": "Repeat Step 2 over and over until the group means no longer change. For this somewhat contrived example, this happens on the next iteration. Stop! You have now formed three clusters from the dataset!"}, {"b6c7": "Group 1 (Old Mean = 11)Player C (11 goals), Player I (15 goals)Final Mean = 13"}, {"d3b7": "Group 2 (Old Mean = 4)Player A (5 goals), Player D (5 goals), Player E (3 goals), Player H (3 goals)Final Mean = 4"}, {"d581": "Group 3 (Old Mean = 21)Player G (30 goals), Player B (20 goals), Player F (19 goals)Final Mean = 23"}, {"8b03": "With this example, the clusters could correspond to the players\u2019 positions on the field\u200a\u2014\u200asuch as defenders, midfielders and attackers. K-means works here because we could have reasonably expected the data to fall naturally into these three categories."}, {"300f": "In this way, given data on a range of performance statistics, a machine could do a reasonable job of estimating the positions of players from any team sport\u200a\u2014\u200auseful for sports analytics, and indeed any other purpose where classification of a dataset into predefined groups can provide relevant insights."}, {"27e9": "Finer details:"}, {"7aa0": "There are several variations on the algorithm described here. The initial method of \u2018seeding\u2019 the clusters can be done in one of several ways. Here, we randomly assigned every player into a group, then calculated the group means. This causes the initial group means to tend towards being similar to one another, which ensures greater repeatability."}, {"0e60": "An alternative is to seed the clusters with just one player each, then start assigning players to the nearest cluster. The returned clusters are more sensitive to the initial seeding step, reducing repeatability in highly variable datasets. However, this approach may reduce the number of iterations required to complete the algorithm, as the groups will take less time to diverge."}, {"bd3d": "An obvious limitation to K-means clustering is that you have to provide a priori assumptions about how many clusters you\u2019re expecting to find. There are methods to assess the fit of a particular set of clusters. For example, the Within-Cluster Sum-of-Squares is a measure of the variance within each cluster. The \u2018better\u2019 the clusters, the lower the overall WCSS."}, {"acbe": "Hierarchical clustering"}, {"1eb8": "Use when:"}, {"26af": "\u2026you wish to uncover the underlying relationships between your observations."}, {"a79b": "How it\u00a0works:"}, {"6625": "A distance matrix is computed, where the value of cell (i, j) is a distance metric between observations i and j. Then, pair the closest two observations and calculate their average. Form a new distance matrix, merging the paired observations into a single object. From this distance matrix, pair up the closest two observations and calculate their average. Repeat until all observations are grouped together."}, {"5028": "Worked example:"}, {"6bb0": "Here\u2019s a super-simplified dataset about a selection of whale and dolphin species. As a trained biologist, I can assure you we normally use much more detailed datasets for things like reconstructing phylogeny. For now though, we\u2019ll just look at the typical body lengths for these six species. We\u2019ll be using just two repeated steps."}, {"fa19": "Species          Initials  Length(m)Bottlenose Dolphin     BD        3.0Risso's Dolphin        RD        3.6Pilot Whale            PW        6.5Killer Whale           KW        7.5Humpback Whale         HW       15.0Fin Whale              FW       20.0"}, {"0611": "Step 1: compute a distance matrix between each species. Here, we\u2019ll use the Euclidean distance\u200a\u2014\u200ahow far apart are the data points? Read this exactly as you would a distance chart in a road atlas. The difference in length between any pair of species can be looked up by reading the value at the intersection of the relevant row and column."}, {"68e7": "    BD   RD   PW   KW   HWRD  0.6                    PW  3.5  2.9               KW  4.5  3.9  1.0          HW 12.0 11.4  8.5  7.5     FW 17.0 16.4 13.5 12.5  5.0"}, {"7109": "Step 2: Pair up the two closest species. Here, this will be the Bottlenose & Risso\u2019s Dolphins, with an average length of 3.3m."}, {"d409": "Repeat Step 1 by recalculating the distance matrix, but this time merge the Bottlenose & Risso\u2019s Dolphins into a single object with length 3.3m."}, {"4cbb": "    [BD, RD]   PW   KW   HWPW       3.2               KW       4.2   1.0          HW      11.7   8.5  7.5     FW      16.7  13.5 12.5  5.0"}, {"77af": "Next, repeat Step 2 with this new distance matrix. Here, the smallest distance is between the Pilot & Killer Whales, so we pair them up and take their average\u200a\u2014\u200awhich gives us 7.0m."}, {"047f": "Then, we repeat Step 1\u200a\u2014\u200arecalculate the distance matrix, but now we\u2019ve merged the Pilot & Killer Whales into a single object of length 7.0m."}, {"8e34": "         [BD, RD] [PW, KW]   HW[PW, KW]      3.7              HW           11.7      8.0     FW           16.7     13.0   5.0"}, {"da48": "Next, we repeat Step 2 with this distance matrix. The smallest distance (3.7m) is between the two merged objects\u200a\u2014\u200aso now we merge them into an even bigger object, and take the average (which is 5.2m)."}, {"d16c": "Then, we repeat Step 1 and compute a new distance matrix, having merged the Bottlenose & Risso\u2019s Dolphins with the Pilot & Killer Whales."}, {"0a32": "   [[BD, RD] , [PW, KW]]    HWHW                   9.8    FW                  14.8   5.0"}, {"cf09": "Next, we repeat Step 2. The smallest distance (5.0m) is between the Humpback & Fin Whales, so we merge them into a single object, and take the average (17.5m)."}, {"a88d": "Then, it\u2019s back to Step 1\u200a\u2014\u200acompute the distance matrix, having merged the Humpback & Fin Whales."}, {"c60d": "         [[BD, RD] , [PW, KW]][HW, FW]                  12.3"}, {"3ec3": "Finally, we repeat Step 2\u200a\u2014\u200athere is only one distance (12.3m) in this matrix, so we pair everything into one big object, and now we can stop! Let\u2019s look at the final merged object:"}, {"5548": "[[[BD, RD],[PW, KW]],[HW, FW]]"}, {"3fa2": "It has a nested structure (think JSON), which allows it to be drawn up as a tree-like graph, or dendrogram. It reads in much the same way a family tree might. The nearer two observations are on the tree, the more similar or closely-related they are taken to be."}, {"a48d": "A no-frills dendrogram generated at R-Fiddle.org"}, {"e2dc": "The structure of the dendrogram gives us insight into how our dataset is structured. In our example, we see two main branches, with Humpback Whale and Fin Whale on one side, and the Bottlenose Dolphin/Risso\u2019s Dolphin and Pilot Whale/Killer Whale on the other."}, {"5b08": "In evolutionary biology, much larger datasets with many more specimens and measurements are used in this way to infer taxonomic relationships between them. Outside of biology, hierarchical clustering has applications in Data Mining and Machine Learning contexts."}, {"a111": "The cool thing is that this approach requires no assumptions about the number of clusters you\u2019re looking for. You can split the returned dendrogram into clusters by \u201ccutting\u201d the tree at a given height. This height can be chosen in a number of ways, depending on the resolution at which you wish to cluster the data."}, {"c890": "For instance, looking at the dendrogram above, if we draw a horizontal line at height = 10, we\u2019d intersect the two main branches, splitting the dendrogram into two sub-graphs. If we cut at height = 2, we\u2019d be splitting the dendrogram into three clusters."}, {"acf8": "Finer details:"}, {"a2ce": "There are essentially three aspects in which hierarchical clustering algorithms can vary to the one given here."}, {"bce7": "Most fundamental is the approach\u200a\u2014\u200ahere, we have used an agglomerative process, whereby we start with individual data points and iteratively cluster them together until we\u2019re left with one large cluster. An alternative (but more computationally intensive) approach is to start with one giant cluster, and then proceed to divide the data into smaller and smaller clusters until you\u2019re left with isolated data points."}, {"acac": "There are also a range of methods that can be used to calculate the distance matrices. For many purposes, the Euclidean distance (think Pythagoras\u2019 Theorem) will suffice, but there are alternatives that may be more applicable in some circumstances."}, {"c65b": "Finally, the linkage criterion can also vary. Clusters are linked according to how close they are to one another, but the way in which we define \u2018close\u2019 is flexible. In the example above, we measured the distances between the means (or \u2018centroids\u2019) of each group and paired up the nearest groups. However, you may want to use a different definition."}, {"4ba3": "For example, each cluster is made up of several discrete points. We could define the distance between two clusters to be the minimum (or maximum) distance between any of their points\u200a\u2014\u200aas illustrated in the figure below. There are still other ways of defining the linkage criterion, which may be suitable in different contexts."}, {"170b": "Red/Blue: centroid linkage; Red/Green: minimum linkage; Green/Blue: maximum\u00a0linkage"}, {"b664": "Graph Community Detection"}, {"f884": "Use when:"}, {"7a65": "\u2026you have data that can be represented as a network, or \u2018graph\u2019."}, {"ed55": "How it\u00a0works:"}, {"6e96": "A graph community is very generally defined as a subset of vertices which are more connected to each other than with the rest of the network. Various algorithms exist to identify communities, based upon more specific definitions. Algorithms include, but are not limited to, Edge Betweenness, Modularity-Maximsation, Walktrap, Clique Percolation, Leading Eigenvector\u2026"}, {"f180": "Worked example:"}, {"6cc8": "Graph theory, or the mathematical study of networks, is a fascinating branch of mathematics that lets us model complex systems as an abstract collection of \u2018dots\u2019 (or vertices) connected by \u2018lines\u2019 (or edges)."}, {"a226": "Perhaps the most intuitive case-studies are social networks. Here, the vertices represent people, and edges connect vertices who are friends/followers. However, any system can be modelled as a network if you can justify a method to meaningfully connect different components. Among the more innovative applications of graph theory to clustering include feature extraction from image data, and analysing gene regulatory networks."}, {"30e4": "As an entry-level example, take a look at this quickly put-together graph. It shows the eight websites I most recently visited, linked according to whether their respective Wikipedia articles link out to one another. You could assemble this data manually, but for larger-scale projects, it\u2019s much quicker to write a Python script to do the same. Here\u2019s one I wrote earlier."}, {"83ee": "Graph plotted with \u2018igraph\u2019 package for R version\u00a03.3.3"}, {"3c6b": "The vertices are colored according to their community membership, and sized according to their centrality. See how Google and Twitter are the most central?"}, {"b37e": "Also, the clusters make pretty good sense in the real-world (always an important performance indicator). The yellow vertices are generally reference/look-up sites; the blue vertices are all used for online publishing (of articles, tweets, or code); and the red vertices include YouTube, which was of course founded by former PayPal employees. Not bad deductions for a machine!"}, {"4c1c": "Aside from being a useful way to visualize large systems, the real power of networks comes from their mathematical analysis. Let\u2019s start by translating our nice picture of the network into a more mathematical format. Below is the adjacency matrix of the network."}, {"eb66": "         GH Gl  M  P  Q  T  W  YGitHub    0  1  0  0  0  1  0  0  Google    1  0  1  1  1  1  1  1Medium    0  1  0  0  0  1  0  0PayPal    0  1  0  0  0  1  0  1Quora     0  1  0  0  0  1  1  0Twitter   1  1  1  1  1  0  0  1Wikipedia 0  1  0  0  1  0  0  0YouTube   0  1  0  1  0  1  0  0"}, {"7ad5": "The value at the intersection of each row and column records whether there is an edge between that pair of vertices. For instance, there is an edge between Medium and Twitter (surprise, surprise!), so the value where their rows/columns intersect is 1. Similarly, there is no edge between Medium and PayPal, so the intersection of their rows/columns returns 0."}, {"fd3d": "Encoded within the adjacency matrix are all the properties of this network\u200a\u2014\u200ait gives us the key to start unlocking all manner of valuable insights. For a start, summing any column (or row) gives you the degree of each vertex\u200a\u2014\u200ai.e., how many others it is connected to. This is commonly denoted with the letter k."}, {"7634": "Likewise, summing the degrees of every vertex and dividing by two gives you L, the number of edges (or \u2018links\u2019) in the network. The number of rows/columns gives us N, the number of vertices (or \u2018nodes\u2019) in the network."}, {"bc6b": "Knowing just k, L, N and the value of each cell in the adjacency matrix A lets us calculate the modularity of any given clustering of the network."}, {"facf": "Say we\u2019ve clustered the network into a number of communities. We can use the modularity score to assess the \u2018quality\u2019 of this clustering. A higher score will show we\u2019ve split the network into \u2018accurate\u2019 communities, whereas a low score suggests our clusters are more random than insightful. The image below illustrates this."}, {"b803": "Modularity serves as a measure of the \u2018quality\u2019 of a partition."}, {"e812": "Modularity can be calculated using the formula below:"}, {"4d1a": "That\u2019s a fair amount of math, but we can break it down bit by bit and it\u2019ll make more sense."}, {"72c4": "M is of course what we\u2019re calculating\u200a\u2014\u200amodularity."}, {"3b54": "1/2L tells us to divide everything that follows by 2L, i.e., twice the number of edges in the network. So far, so good."}, {"31c7": "The \u03a3 symbol tells us we\u2019re summing up everything to the right, and lets us iterate over every row and column in the adjacency matrix A. For those unfamiliar with sum notation, the i, j = 1 and the N work much like nested for-loops in programming. In Python, you\u2019d write it as follows:"}, {"7b23": "sum = 0"}, {"3514": "for i in range(1,N):    for j in range(1,N):        ans = #stuff with i and j as indices        sum += ans"}, {"6114": "So what is #stuff with i and j in more detail?"}, {"1f91": "Well, the bit in brackets tells us to subtract ( k_i k_j ) / 2L from A_ij."}, {"4555": "A_ij is simply the value in the adjacency matrix at row i, column j."}, {"1ca6": "The values of k_i and k_j are the degrees of each vertex\u200a\u2014\u200afound by adding up the entries in row i and column j respectively. Multiplying these together and dividing by 2L gives us the expected number of edges between vertices i and j if the network were randomly shuffled up."}, {"f5d3": "Overall, the term in the brackets reveals the difference between the network\u2019s real structure and the expected structure it would have if randomly reassembled. Playing around with the values shows that it returns its highest value when A_ij = 1, and ( k_i k_j ) / 2L is low. This means we see a higher value if there is an \u2018unexpected\u2019 edge between vertices i and j."}, {"c769": "Finally, we multiply the bracketed term by whatever the last few symbols refer to."}, {"3f93": "The \ud835\udeffc_i, c_j is the fancy-sounding but totally harmless Kronecker-delta function. Here it is, explained in Python:"}, {"1363": "def Kronecker_Delta(ci, cj):    if ci == cj:        return 1    else:        return 0"}, {"333d": "Kronecker_Delta(\"A\",\"A\")    #returns 1Kronecker_Delta(\"A\",\"B\")    #returns 0"}, {"e32d": "Yes\u200a\u2014\u200ait really is that simple. The Kronecker-delta function takes two arguments, and returns 1 if they are identical, otherwise, zero."}, {"25d5": "This means that if vertices i and j have been put in the same cluster, then \ud835\udeffc_i, c_j = 1. Otherwise, if they are in different clusters, the function returns zero."}, {"dc84": "As we are multiplying the bracketed term by this Kronecker-delta function, we find that for the nested sum \u03a3, the outcome is highest when there are lots of \u2018unexpected\u2019 edges connecting vertices assigned to the same cluster. As such, modularity is a measure of how well-clustered the graph is into separate communities."}, {"7643": "Dividing by 2L bounds the upper value of modularity at 1. Modularity scores near to or below zero indicate the current clustering of the network is really no use. The higher the modularity, the better the clustering of the network into separate communities. By maximising modularity, we can find the best way of clustering the network."}, {"4f28": "Notice that we have to pre-define how the graph is clustered to find out how \u2018good\u2019 that clustering actually is. Unfortunately, employing brute force to try out every possible way of clustering the graph to find which has the highest modularity score would be computationally impossible beyond a very limited sample size."}, {"c430": "Combinatorics tells us that for a network of just eight vertices, there are 4140 different ways of clustering them. A network twice the size would have over ten billion possible ways of clustering the vertices. Doubling the network again (to a very modest 32 vertices) would give 128 septillion possible ways, and a network of eighty vertices would be cluster-able in more ways than there are atoms in the observable universe."}, {"9fe2": "Instead, we have to turn to a heuristic method that does a reasonably good job at estimating the clusters that will produce the highest modularity score, without trying out every single possibility. This is an algorithm called Fast-Greedy Modularity-Maximization, and it\u2019s somewhat analogous to the agglomerative hierarchical clustering algorithm describe above. Instead of merging according to distance, \u2018Mod-Max\u2019 merges communities according to changes in modularity. Here\u2019s how it goes:"}, {"db81": "Begin by initially assigning every vertex to its own community, and calculating the modularity of the whole network, M."}, {"80a1": "Step 1 requires that for each community pair linked by at least a single edge, the algorithm calculates the resultant change in modularity \u0394M if the two communities were merged into one."}, {"09cd": "Step 2 then takes the pair of communities that produce the biggest increase in \u0394M, which are then merged. Calculate the new modularity M for this clustering, and keep a record of it."}, {"8228": "Repeat steps 1 and 2\u200a\u2014\u200aeach time merging the pair of communities for which doing so produces the biggest gain in \u0394M, then recording the new clustering pattern and its associated modularity score M."}, {"80a6": "Stop when all the vertices are grouped into one giant cluster. Now the algorithm checks the records it kept as it went along, and identifies the clustering pattern that returned the highest value of M. This is the returned community structure."}, {"3665": "Finer details:"}, {"37cf": "Whew! That was computationally intensive, at least for us humans. Graph theory is a rich source of computationally challenging, often NP-hard problems\u200a\u2014\u200ayet it also has incredible potential to provide valuable insights into complex systems and datasets. Just ask Larry Page, whose eponymous PageRank algorithm\u200a\u2014\u200awhich helped propel Google from start-up to basically world domination in less than a generation\u200a\u2014\u200awas based entirely in graph theory."}, {"8eec": "Community detection is a major focus of current research in graph theory, and there are plenty of alternatives to Modularity-Maximization, which while useful, does have some drawbacks."}, {"7379": "For a start, its agglomerative approach often sees small, well-defined communities swallowed up into larger ones. This is known as the resolution limit\u200a\u2014\u200athe algorithm will not find communities below a certain size. Another challenge is that rather than having one distinct, easy-to-reach global peak, the Mod-Max approach actually tends to produce a wide \u2018plateau\u2019 of many similar high modularity scores\u200a\u2014\u200amaking it somewhat difficult to truly identify the absolute maximum score."}, {"3c5a": "Other algorithms use different ways to define and approach community detection. Edge-Betweenness is a divisive algorithm, starting with all vertices grouped in one giant cluster. It proceeds to iteratively remove the least \u2018important\u2019 edges in the network, until all vertices are left isolated. This produces a hierarchical structure, with similar vertices closer together in the hierarchy."}, {"dd08": "Another algorithm is Clique Percolation, which takes into account possible overlap between graph communities. Yet another set of algorithms are based on random-walks across the graph, and then there are spectral clustering methods which start delving into the eigendecomposition of the adjacency matrix and other matrices derived therefrom. These ideas are used in feature extraction in, for example, areas such as Computer Vision."}, {"b391": "It\u2019d be well beyond the scope of this article to give each algorithm its own in-depth worked example. Suffice to say that this is an active area of research, providing powerful methods to make sense of data that even a generation ago would have been extremely difficult to process."}, {"6ec6": "Conclusion"}, {"4fee": "Hopefully this article has informed and inspired you to better understand how machines can make sense of Big Data! The future is a rapidly changing place, and many of those changes will be driven by what technology becomes capable of in the next generation or two."}, {"e236": "As outlined in the introduction, Machine Learning is an extraordinarily ambitious field of research, in which massively complex problems require solving in as accurate and as efficient a way possible. Tasks that come naturally to us humans require innovative solutions when taken on by machines."}, {"49ba": "There\u2019s still plenty of progress to be made, and whoever contributes the next breakthrough idea will no doubt be generously rewarded. Maybe someone reading this article will be behind the next powerful algorithm? All great ideas have to start somewhere!"}], "content": "How Machines Make Sense of Big Data: an Introduction to Clustering Algorithms Take a look at the image below. It\u2019s a collection of bugs and creepy-crawlies of different shapes and sizes. Take a moment to categorize them by similarity into a number of groups. This isn\u2019t a trick question. Start with grouping the spiders together. Done? While there\u2019s not necessarily a \u201ccorrect\u201d answer here, it\u2019s most likely you split the bugs into four clusters. The spiders in one cluster, the pair of snails in another, the butterflies and moth into one, and the trio of wasps and bees into one more. That wasn\u2019t too bad, was it? You could probably do the same with twice as many bugs, right? If you had a bit of time to spare\u200a\u2014\u200aor a passion for entomology\u200a\u2014\u200ayou could probably even do the same with a hundred bugs. For a machine though, grouping ten objects into however many meaningful clusters is no small task, thanks to a mind-bending branch of maths called combinatorics, which tells us that are 115,975 different possible ways you could have grouped those ten insects together. Had there been twenty bugs, there would have been over fifty trillion possible ways of clustering them. With a hundred bugs\u200a\u2014\u200athere\u2019d be many times more solutions than there are particles in the known universe. How many times more? By my calculation, approximately five hundred million billion billion times more. In fact, there are more than four million billion googol solutions (what\u2019s a googol?). For just a hundred objects. Almost all of those solutions would be meaningless\u200a\u2014\u200ayet from that unimaginable number of possible choices, you pretty quickly found one of the very few that clustered the bugs in a useful way. Us humans take it for granted how good we are categorizing and making sense of large volumes of data pretty quickly. Whether it\u2019s a paragraph of text, or images on a screen, or a sequence of objects\u200a\u2014\u200ahumans are generally fairly efficient at making sense of whatever data the world throws at us. Given that a key aspect of developing A.I. and Machine Learning is getting machines to quickly make sense of large sets of input data, what shortcuts are there available? Here, you can read about three clustering algorithms that machines can use to quickly make sense of large datasets. This is by no means an exhaustive list\u200a\u2014\u200athere are other algorithms out there\u200a\u2014\u200abut they represent a good place to start! You\u2019ll find for each a quick summary of when you might use them, a brief overview of how they work, and a more detailed, step-by-step worked example. I believe it helps to understand an algorithm by actually carrying out yourself. If you\u2019re really keen, you\u2019ll find the best way to do this is with pen and paper. Go ahead\u200a\u2014\u200anobody will judge! Three suspiciously neat clusters, with K =\u00a03 K-means clustering Use when: \u2026you have an idea of how many groups you\u2019re expecting to find a priori. How it\u00a0works: The algorithm randomly assigns each observation into one of k categories, then calculates the mean of each category. Next, it reassigns each observation to the category with the closest mean before recalculating the means. This step repeats over and over until no more reassignments are necessary. Worked Example: Take a group of 12 football (or \u2018soccer\u2019) players who have each scored a certain number of goals this season (say in the range 3\u201330). Let\u2019s divide them into separate clusters\u200a\u2014\u200asay three. Step 1 requires us to randomly split the players into three groups and calculate the means of each. Group 1Player A (5 goals), Player B (20 goals), Player C (11 goals) Group Mean = (5 + 20 + 11) / 3 = 12 Group 2Player D (5 goals), Player E (3 goals), Player F (19 goals)Group Mean = 9 Group 3Player G (30 goals), Player H (3 goals), Player I (15 goals)Group Mean = 16 Step 2: For each player, reassign them to the group with the closest mean. E.g., Player A (5 goals) is assigned to Group 2 (mean = 9). Then recalculate the group means. Group 1 (Old Mean = 12)Player C (11 goals) New Mean = 11 Group 2 (Old Mean = 9)Player A (5 goals), Player D (5 goals), Player E (3 goals), Player H (3 goals)New Mean = 4 Group 3 (Old Mean = 16)Player G (30 goals), Player I (15 goals), Player B (20 goals), Player F (19 goals)New Mean = 21 Repeat Step 2 over and over until the group means no longer change. For this somewhat contrived example, this happens on the next iteration. Stop! You have now formed three clusters from the dataset! Group 1 (Old Mean = 11)Player C (11 goals), Player I (15 goals)Final Mean = 13 Group 2 (Old Mean = 4)Player A (5 goals), Player D (5 goals), Player E (3 goals), Player H (3 goals)Final Mean = 4 Group 3 (Old Mean = 21)Player G (30 goals), Player B (20 goals), Player F (19 goals)Final Mean = 23 With this example, the clusters could correspond to the players\u2019 positions on the field\u200a\u2014\u200asuch as defenders, midfielders and attackers. K-means works here because we could have reasonably expected the data to fall naturally into these three categories. In this way, given data on a range of performance statistics, a machine could do a reasonable job of estimating the positions of players from any team sport\u200a\u2014\u200auseful for sports analytics, and indeed any other purpose where classification of a dataset into predefined groups can provide relevant insights. Finer details: There are several variations on the algorithm described here. The initial method of \u2018seeding\u2019 the clusters can be done in one of several ways. Here, we randomly assigned every player into a group, then calculated the group means. This causes the initial group means to tend towards being similar to one another, which ensures greater repeatability. An alternative is to seed the clusters with just one player each, then start assigning players to the nearest cluster. The returned clusters are more sensitive to the initial seeding step, reducing repeatability in highly variable datasets. However, this approach may reduce the number of iterations required to complete the algorithm, as the groups will take less time to diverge. An obvious limitation to K-means clustering is that you have to provide a priori assumptions about how many clusters you\u2019re expecting to find. There are methods to assess the fit of a particular set of clusters. For example, the Within-Cluster Sum-of-Squares is a measure of the variance within each cluster. The \u2018better\u2019 the clusters, the lower the overall WCSS. Hierarchical clustering Use when: \u2026you wish to uncover the underlying relationships between your observations. How it\u00a0works: A distance matrix is computed, where the value of cell (i, j) is a distance metric between observations i and j. Then, pair the closest two observations and calculate their average. Form a new distance matrix, merging the paired observations into a single object. From this distance matrix, pair up the closest two observations and calculate their average. Repeat until all observations are grouped together. Worked example: Here\u2019s a super-simplified dataset about a selection of whale and dolphin species. As a trained biologist, I can assure you we normally use much more detailed datasets for things like reconstructing phylogeny. For now though, we\u2019ll just look at the typical body lengths for these six species. We\u2019ll be using just two repeated steps. Species          Initials  Length(m)Bottlenose Dolphin     BD        3.0Risso's Dolphin        RD        3.6Pilot Whale            PW        6.5Killer Whale           KW        7.5Humpback Whale         HW       15.0Fin Whale              FW       20.0 Step 1: compute a distance matrix between each species. Here, we\u2019ll use the Euclidean distance\u200a\u2014\u200ahow far apart are the data points? Read this exactly as you would a distance chart in a road atlas. The difference in length between any pair of species can be looked up by reading the value at the intersection of the relevant row and column.     BD   RD   PW   KW   HWRD  0.6                    PW  3.5  2.9               KW  4.5  3.9  1.0          HW 12.0 11.4  8.5  7.5     FW 17.0 16.4 13.5 12.5  5.0 Step 2: Pair up the two closest species. Here, this will be the Bottlenose & Risso\u2019s Dolphins, with an average length of 3.3m. Repeat Step 1 by recalculating the distance matrix, but this time merge the Bottlenose & Risso\u2019s Dolphins into a single object with length 3.3m.     [BD, RD]   PW   KW   HWPW       3.2               KW       4.2   1.0          HW      11.7   8.5  7.5     FW      16.7  13.5 12.5  5.0 Next, repeat Step 2 with this new distance matrix. Here, the smallest distance is between the Pilot & Killer Whales, so we pair them up and take their average\u200a\u2014\u200awhich gives us 7.0m. Then, we repeat Step 1\u200a\u2014\u200arecalculate the distance matrix, but now we\u2019ve merged the Pilot & Killer Whales into a single object of length 7.0m.          [BD, RD] [PW, KW]   HW[PW, KW]      3.7              HW           11.7      8.0     FW           16.7     13.0   5.0 Next, we repeat Step 2 with this distance matrix. The smallest distance (3.7m) is between the two merged objects\u200a\u2014\u200aso now we merge them into an even bigger object, and take the average (which is 5.2m). Then, we repeat Step 1 and compute a new distance matrix, having merged the Bottlenose & Risso\u2019s Dolphins with the Pilot & Killer Whales.    [[BD, RD] , [PW, KW]]    HWHW                   9.8    FW                  14.8   5.0 Next, we repeat Step 2. The smallest distance (5.0m) is between the Humpback & Fin Whales, so we merge them into a single object, and take the average (17.5m). Then, it\u2019s back to Step 1\u200a\u2014\u200acompute the distance matrix, having merged the Humpback & Fin Whales.          [[BD, RD] , [PW, KW]][HW, FW]                  12.3 Finally, we repeat Step 2\u200a\u2014\u200athere is only one distance (12.3m) in this matrix, so we pair everything into one big object, and now we can stop! Let\u2019s look at the final merged object: [[[BD, RD],[PW, KW]],[HW, FW]] It has a nested structure (think JSON), which allows it to be drawn up as a tree-like graph, or dendrogram. It reads in much the same way a family tree might. The nearer two observations are on the tree, the more similar or closely-related they are taken to be. A no-frills dendrogram generated at R-Fiddle.org The structure of the dendrogram gives us insight into how our dataset is structured. In our example, we see two main branches, with Humpback Whale and Fin Whale on one side, and the Bottlenose Dolphin/Risso\u2019s Dolphin and Pilot Whale/Killer Whale on the other. In evolutionary biology, much larger datasets with many more specimens and measurements are used in this way to infer taxonomic relationships between them. Outside of biology, hierarchical clustering has applications in Data Mining and Machine Learning contexts. The cool thing is that this approach requires no assumptions about the number of clusters you\u2019re looking for. You can split the returned dendrogram into clusters by \u201ccutting\u201d the tree at a given height. This height can be chosen in a number of ways, depending on the resolution at which you wish to cluster the data. For instance, looking at the dendrogram above, if we draw a horizontal line at height = 10, we\u2019d intersect the two main branches, splitting the dendrogram into two sub-graphs. If we cut at height = 2, we\u2019d be splitting the dendrogram into three clusters. Finer details: There are essentially three aspects in which hierarchical clustering algorithms can vary to the one given here. Most fundamental is the approach\u200a\u2014\u200ahere, we have used an agglomerative process, whereby we start with individual data points and iteratively cluster them together until we\u2019re left with one large cluster. An alternative (but more computationally intensive) approach is to start with one giant cluster, and then proceed to divide the data into smaller and smaller clusters until you\u2019re left with isolated data points. There are also a range of methods that can be used to calculate the distance matrices. For many purposes, the Euclidean distance (think Pythagoras\u2019 Theorem) will suffice, but there are alternatives that may be more applicable in some circumstances. Finally, the linkage criterion can also vary. Clusters are linked according to how close they are to one another, but the way in which we define \u2018close\u2019 is flexible. In the example above, we measured the distances between the means (or \u2018centroids\u2019) of each group and paired up the nearest groups. However, you may want to use a different definition. For example, each cluster is made up of several discrete points. We could define the distance between two clusters to be the minimum (or maximum) distance between any of their points\u200a\u2014\u200aas illustrated in the figure below. There are still other ways of defining the linkage criterion, which may be suitable in different contexts. Red/Blue: centroid linkage; Red/Green: minimum linkage; Green/Blue: maximum\u00a0linkage Graph Community Detection Use when: \u2026you have data that can be represented as a network, or \u2018graph\u2019. How it\u00a0works: A graph community is very generally defined as a subset of vertices which are more connected to each other than with the rest of the network. Various algorithms exist to identify communities, based upon more specific definitions. Algorithms include, but are not limited to, Edge Betweenness, Modularity-Maximsation, Walktrap, Clique Percolation, Leading Eigenvector\u2026 Worked example: Graph theory, or the mathematical study of networks, is a fascinating branch of mathematics that lets us model complex systems as an abstract collection of \u2018dots\u2019 (or vertices) connected by \u2018lines\u2019 (or edges). Perhaps the most intuitive case-studies are social networks. Here, the vertices represent people, and edges connect vertices who are friends/followers. However, any system can be modelled as a network if you can justify a method to meaningfully connect different components. Among the more innovative applications of graph theory to clustering include feature extraction from image data, and analysing gene regulatory networks. As an entry-level example, take a look at this quickly put-together graph. It shows the eight websites I most recently visited, linked according to whether their respective Wikipedia articles link out to one another. You could assemble this data manually, but for larger-scale projects, it\u2019s much quicker to write a Python script to do the same. Here\u2019s one I wrote earlier. Graph plotted with \u2018igraph\u2019 package for R version\u00a03.3.3 The vertices are colored according to their community membership, and sized according to their centrality. See how Google and Twitter are the most central? Also, the clusters make pretty good sense in the real-world (always an important performance indicator). The yellow vertices are generally reference/look-up sites; the blue vertices are all used for online publishing (of articles, tweets, or code); and the red vertices include YouTube, which was of course founded by former PayPal employees. Not bad deductions for a machine! Aside from being a useful way to visualize large systems, the real power of networks comes from their mathematical analysis. Let\u2019s start by translating our nice picture of the network into a more mathematical format. Below is the adjacency matrix of the network.          GH Gl  M  P  Q  T  W  YGitHub    0  1  0  0  0  1  0  0  Google    1  0  1  1  1  1  1  1Medium    0  1  0  0  0  1  0  0PayPal    0  1  0  0  0  1  0  1Quora     0  1  0  0  0  1  1  0Twitter   1  1  1  1  1  0  0  1Wikipedia 0  1  0  0  1  0  0  0YouTube   0  1  0  1  0  1  0  0 The value at the intersection of each row and column records whether there is an edge between that pair of vertices. For instance, there is an edge between Medium and Twitter (surprise, surprise!), so the value where their rows/columns intersect is 1. Similarly, there is no edge between Medium and PayPal, so the intersection of their rows/columns returns 0. Encoded within the adjacency matrix are all the properties of this network\u200a\u2014\u200ait gives us the key to start unlocking all manner of valuable insights. For a start, summing any column (or row) gives you the degree of each vertex\u200a\u2014\u200ai.e., how many others it is connected to. This is commonly denoted with the letter k. Likewise, summing the degrees of every vertex and dividing by two gives you L, the number of edges (or \u2018links\u2019) in the network. The number of rows/columns gives us N, the number of vertices (or \u2018nodes\u2019) in the network. Knowing just k, L, N and the value of each cell in the adjacency matrix A lets us calculate the modularity of any given clustering of the network. Say we\u2019ve clustered the network into a number of communities. We can use the modularity score to assess the \u2018quality\u2019 of this clustering. A higher score will show we\u2019ve split the network into \u2018accurate\u2019 communities, whereas a low score suggests our clusters are more random than insightful. The image below illustrates this. Modularity serves as a measure of the \u2018quality\u2019 of a partition. Modularity can be calculated using the formula below: That\u2019s a fair amount of math, but we can break it down bit by bit and it\u2019ll make more sense. M is of course what we\u2019re calculating\u200a\u2014\u200amodularity. 1/2L tells us to divide everything that follows by 2L, i.e., twice the number of edges in the network. So far, so good. The \u03a3 symbol tells us we\u2019re summing up everything to the right, and lets us iterate over every row and column in the adjacency matrix A. For those unfamiliar with sum notation, the i, j = 1 and the N work much like nested for-loops in programming. In Python, you\u2019d write it as follows: sum = 0 for i in range(1,N):    for j in range(1,N):        ans = #stuff with i and j as indices        sum += ans So what is #stuff with i and j in more detail? Well, the bit in brackets tells us to subtract ( k_i k_j ) / 2L from A_ij. A_ij is simply the value in the adjacency matrix at row i, column j. The values of k_i and k_j are the degrees of each vertex\u200a\u2014\u200afound by adding up the entries in row i and column j respectively. Multiplying these together and dividing by 2L gives us the expected number of edges between vertices i and j if the network were randomly shuffled up. Overall, the term in the brackets reveals the difference between the network\u2019s real structure and the expected structure it would have if randomly reassembled. Playing around with the values shows that it returns its highest value when A_ij = 1, and ( k_i k_j ) / 2L is low. This means we see a higher value if there is an \u2018unexpected\u2019 edge between vertices i and j. Finally, we multiply the bracketed term by whatever the last few symbols refer to. The \ud835\udeffc_i, c_j is the fancy-sounding but totally harmless Kronecker-delta function. Here it is, explained in Python: def Kronecker_Delta(ci, cj):    if ci == cj:        return 1    else:        return 0 Kronecker_Delta(\"A\",\"A\")    #returns 1Kronecker_Delta(\"A\",\"B\")    #returns 0 Yes\u200a\u2014\u200ait really is that simple. The Kronecker-delta function takes two arguments, and returns 1 if they are identical, otherwise, zero. This means that if vertices i and j have been put in the same cluster, then \ud835\udeffc_i, c_j = 1. Otherwise, if they are in different clusters, the function returns zero. As we are multiplying the bracketed term by this Kronecker-delta function, we find that for the nested sum \u03a3, the outcome is highest when there are lots of \u2018unexpected\u2019 edges connecting vertices assigned to the same cluster. As such, modularity is a measure of how well-clustered the graph is into separate communities. Dividing by 2L bounds the upper value of modularity at 1. Modularity scores near to or below zero indicate the current clustering of the network is really no use. The higher the modularity, the better the clustering of the network into separate communities. By maximising modularity, we can find the best way of clustering the network. Notice that we have to pre-define how the graph is clustered to find out how \u2018good\u2019 that clustering actually is. Unfortunately, employing brute force to try out every possible way of clustering the graph to find which has the highest modularity score would be computationally impossible beyond a very limited sample size. Combinatorics tells us that for a network of just eight vertices, there are 4140 different ways of clustering them. A network twice the size would have over ten billion possible ways of clustering the vertices. Doubling the network again (to a very modest 32 vertices) would give 128 septillion possible ways, and a network of eighty vertices would be cluster-able in more ways than there are atoms in the observable universe. Instead, we have to turn to a heuristic method that does a reasonably good job at estimating the clusters that will produce the highest modularity score, without trying out every single possibility. This is an algorithm called Fast-Greedy Modularity-Maximization, and it\u2019s somewhat analogous to the agglomerative hierarchical clustering algorithm describe above. Instead of merging according to distance, \u2018Mod-Max\u2019 merges communities according to changes in modularity. Here\u2019s how it goes: Begin by initially assigning every vertex to its own community, and calculating the modularity of the whole network, M. Step 1 requires that for each community pair linked by at least a single edge, the algorithm calculates the resultant change in modularity \u0394M if the two communities were merged into one. Step 2 then takes the pair of communities that produce the biggest increase in \u0394M, which are then merged. Calculate the new modularity M for this clustering, and keep a record of it. Repeat steps 1 and 2\u200a\u2014\u200aeach time merging the pair of communities for which doing so produces the biggest gain in \u0394M, then recording the new clustering pattern and its associated modularity score M. Stop when all the vertices are grouped into one giant cluster. Now the algorithm checks the records it kept as it went along, and identifies the clustering pattern that returned the highest value of M. This is the returned community structure. Finer details: Whew! That was computationally intensive, at least for us humans. Graph theory is a rich source of computationally challenging, often NP-hard problems\u200a\u2014\u200ayet it also has incredible potential to provide valuable insights into complex systems and datasets. Just ask Larry Page, whose eponymous PageRank algorithm\u200a\u2014\u200awhich helped propel Google from start-up to basically world domination in less than a generation\u200a\u2014\u200awas based entirely in graph theory. Community detection is a major focus of current research in graph theory, and there are plenty of alternatives to Modularity-Maximization, which while useful, does have some drawbacks. For a start, its agglomerative approach often sees small, well-defined communities swallowed up into larger ones. This is known as the resolution limit\u200a\u2014\u200athe algorithm will not find communities below a certain size. Another challenge is that rather than having one distinct, easy-to-reach global peak, the Mod-Max approach actually tends to produce a wide \u2018plateau\u2019 of many similar high modularity scores\u200a\u2014\u200amaking it somewhat difficult to truly identify the absolute maximum score. Other algorithms use different ways to define and approach community detection. Edge-Betweenness is a divisive algorithm, starting with all vertices grouped in one giant cluster. It proceeds to iteratively remove the least \u2018important\u2019 edges in the network, until all vertices are left isolated. This produces a hierarchical structure, with similar vertices closer together in the hierarchy. Another algorithm is Clique Percolation, which takes into account possible overlap between graph communities. Yet another set of algorithms are based on random-walks across the graph, and then there are spectral clustering methods which start delving into the eigendecomposition of the adjacency matrix and other matrices derived therefrom. These ideas are used in feature extraction in, for example, areas such as Computer Vision. It\u2019d be well beyond the scope of this article to give each algorithm its own in-depth worked example. Suffice to say that this is an active area of research, providing powerful methods to make sense of data that even a generation ago would have been extremely difficult to process. Conclusion Hopefully this article has informed and inspired you to better understand how machines can make sense of Big Data! The future is a rapidly changing place, and many of those changes will be driven by what technology becomes capable of in the next generation or two. As outlined in the introduction, Machine Learning is an extraordinarily ambitious field of research, in which massively complex problems require solving in as accurate and as efficient a way possible. Tasks that come naturally to us humans require innovative solutions when taken on by machines. There\u2019s still plenty of progress to be made, and whoever contributes the next breakthrough idea will no doubt be generously rewarded. Maybe someone reading this article will be behind the next powerful algorithm? All great ideas have to start somewhere! ", "parent": "", "title": "How Machines Make Sense of Big Data: an Introduction to Clustering Algorithms", "child": "72_1\t72_2\t72_3\t72_4\t72_5\t72_6\t72_7\t72_8\t72_9\t72_10\t72_11\t72_12\t72_13\t72_14\t72_15\t72_16\t72_17\t72_18", "name": "72"}