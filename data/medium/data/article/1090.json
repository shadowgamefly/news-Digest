{"name": "1090", "parent": "", "content": "Query Relaxation In the previous post, we discussed query expansion as a way to increase recall. In this post we\u2019ll discuss the other major technique for increasing recall: query relaxation. Query relaxation feels like the opposite of query expansion. Instead of adding tokens to the query, we remove them. Ignoring tokens makes the query less restrictive and thus increases recall. An effective query relaxation strategy removes only tokens that aren\u2019t necessary to communicate the searcher\u2019s intent. Let\u2019s consider four approaches to query relaxation, in increasing order of complexity: stop words, specificity, syntactic analysis, and semantic analysis. Stop Words The simplest form of query relaxation is ignoring stop words. Stop words are words like the and of: they generally don\u2019t contribute meaning to the query; hence, removing them preserves the intent while increasing recall. But sometimes stop words matter. There\u2019s a difference between king hill and king of the hill. And there\u2019s even a post-punk band named The The. These edge cases notwithstanding, stop words are usually safe to ignore. Most open-source and commercial search engines come with a list of default stop words and offer the option of ignoring them during query processing (e.g. Lucene\u2019s StopFilter). In addition, Google has published lists of stop words in 29 languages. Specificity Query tokens vary in their specificity. For example, in the search query black hdmi cable, the token hdmi is more specific than cable, which is in turn more specific than black. Specificity generally indicates how essential each query token is to communicating the searcher\u2019s intent. Using specificity, we can determine that it\u2019s more more reasonable to relax the query black hdmi cable to hdmi cable than to black cable. Inverse Document Frequency We can measure token specificity using inverse document frequency (idf). The inverse (idf is actually the logarithm of the inverse) means that rare tokens\u200a\u2014\u200athat is, tokens that occur in fewer documents\u200a\u2014\u200ahave a higher idf than those that occur more frequently. Using idf to measure token specificity is a generalization of stop words, since stop words are very common words and thus have low idf. Information retrieval has used idf for decades, ever since Karen Sp\u00e4rck Jones\u2019s seminal 1972 paper on \u201cA statistical interpretation of term specificity and its application in retrieval\u201d. It\u2019s often combined with term frequency (tf) to obtain tf-idf, a function that assigns weights to query tokens for scoring document relevance. For example, Lucene implements a TFIDFSimilarity class for scoring. But be careful about edge cases. Unique tokens, such as proper names or misspelled words, have very high idf but don\u2019t necessarily represent a corresponding share of query intent. Tokens that aren\u2019t in the corpus have undefined idf\u200a\u2014\u200athough that can be fixed with smoothing, e.g., adding 1 before taking the logarithm. Nonetheless, idf is a useful signal of token specificity. Lexical Databases A completely different approach to measuring specificity is to use a lexical database (also called a \u201cknowledge graph\u201d) like WordNet that arranges concepts into semantic hierarchies. This approach is useful for comparing tokens with a hierarchical relationship, e.g., dog is more specific than animal. It\u2019s less useful for tokens without a hierarchical relationship, e.g., black and hdmi. A lexical database also enables a more nuanced form of query relaxation. Instead of ignoring a token, we can replace it with a more general term, also known as a hypernym. We can also use a lexical database for query expansion. Syntactic Analysis Another approach to query relaxation to use a query\u2019s syntactic structure to determine which tokens are optional. A large fraction of search queries are noun phrases. A noun phrase serves the place of a noun\u200a\u2014\u200athat is, it represents a thing or set of things. A noun phrase can be a solitary noun, e.g., cat, or it can a complex phrase like the best cat in the whole wide world. We can analyze search queries using a part-of-speech tagger, such as NLTK, which in turn allows us to parse the overall syntactic structure of the query. If the query is a noun phrase, parsing allows us to identify its head noun, as well as any adjectives and phrases modifying it. A reasonable query relaxation strategy preserves the head noun and removes one or more of its modifiers. For example, the most important word in the best cat in the whole wide world is the head noun, cat. But this strategy can break down. For example, if the query is free shipping, the adjective free is at least as important to the meaning as the head noun shipping. Syntax does not always dictate semantics. Still, emphasizing the head noun and the modifiers closest to it usually works in practice. Semantic Analysis The most sophisticated approach to query relaxation goes beyond token frequency and syntax and considers semantics\u200a\u2014\u200athat is, what the tokens mean, particularly in relation to one another. For example, we can relax the query polo shirt to polo, since shirt is implied. In contrast, relaxing dress shirt to dress completely changes the query\u2019s meaning. Syntax isn\u2019t helpful: in both cases, we\u2019re replacing a noun phrase with a token that isn\u2019t even the head noun. And shirt is no more specific than polo or dress. So we need to understand the semantics to relax this query successfully. We can use the Word2vec model to embed words and phrases into a vector space that captures their semantics. This embedding allows us to recognize how much the query tokens overlap with one another in meaning, which in turn helps us estimate the consequence of ignoring a token. Word2vec also allows us to compute the similarity between a token and a phrase containing that token. If they\u2019re similar, then it\u2019s probably safe to relax the query by replacing the phrase with the token. Relax but be Careful Like query expansion, query relaxation aims to increase recall while minimizing the loss of precision. If we already have a reasonable quantity and quality of results, then query relaxation probably isn\u2019t worth the risk. Query relaxation is most useful for queries that return few or no results, since those are the queries for which we have the least to lose and the most to gain. But remember that more results doesn\u2019t necessarily mean better results. Use query relaxation, but use it with care. Previous: Query Expansion ", "title": "Query Relaxation", "sentences": [{"f173": "Query Relaxation"}, {"ec8c": "In the previous post, we discussed query expansion as a way to increase recall. In this post we\u2019ll discuss the other major technique for increasing recall: query relaxation."}, {"bb7b": "Query relaxation feels like the opposite of query expansion. Instead of adding tokens to the query, we remove them. Ignoring tokens makes the query less restrictive and thus increases recall. An effective query relaxation strategy removes only tokens that aren\u2019t necessary to communicate the searcher\u2019s intent."}, {"edd9": "Let\u2019s consider four approaches to query relaxation, in increasing order of complexity: stop words, specificity, syntactic analysis, and semantic analysis."}, {"39ff": "Stop Words"}, {"a5b8": "The simplest form of query relaxation is ignoring stop words. Stop words are words like the and of: they generally don\u2019t contribute meaning to the query; hence, removing them preserves the intent while increasing recall."}, {"1881": "But sometimes stop words matter. There\u2019s a difference between king hill and king of the hill. And there\u2019s even a post-punk band named The The. These edge cases notwithstanding, stop words are usually safe to ignore."}, {"87a3": "Most open-source and commercial search engines come with a list of default stop words and offer the option of ignoring them during query processing (e.g. Lucene\u2019s StopFilter). In addition, Google has published lists of stop words in 29 languages."}, {"2d1f": "Specificity"}, {"5ceb": "Query tokens vary in their specificity. For example, in the search query black hdmi cable, the token hdmi is more specific than cable, which is in turn more specific than black. Specificity generally indicates how essential each query token is to communicating the searcher\u2019s intent. Using specificity, we can determine that it\u2019s more more reasonable to relax the query black hdmi cable to hdmi cable than to black cable."}, {"18f0": "Inverse Document Frequency"}, {"51c2": "We can measure token specificity using inverse document frequency (idf). The inverse (idf is actually the logarithm of the inverse) means that rare tokens\u200a\u2014\u200athat is, tokens that occur in fewer documents\u200a\u2014\u200ahave a higher idf than those that occur more frequently. Using idf to measure token specificity is a generalization of stop words, since stop words are very common words and thus have low idf."}, {"9bc8": "Information retrieval has used idf for decades, ever since Karen Sp\u00e4rck Jones\u2019s seminal 1972 paper on \u201cA statistical interpretation of term specificity and its application in retrieval\u201d. It\u2019s often combined with term frequency (tf) to obtain tf-idf, a function that assigns weights to query tokens for scoring document relevance. For example, Lucene implements a TFIDFSimilarity class for scoring."}, {"c131": "But be careful about edge cases. Unique tokens, such as proper names or misspelled words, have very high idf but don\u2019t necessarily represent a corresponding share of query intent. Tokens that aren\u2019t in the corpus have undefined idf\u200a\u2014\u200athough that can be fixed with smoothing, e.g., adding 1 before taking the logarithm. Nonetheless, idf is a useful signal of token specificity."}, {"a6f1": "Lexical Databases"}, {"7c03": "A completely different approach to measuring specificity is to use a lexical database (also called a \u201cknowledge graph\u201d) like WordNet that arranges concepts into semantic hierarchies. This approach is useful for comparing tokens with a hierarchical relationship, e.g., dog is more specific than animal. It\u2019s less useful for tokens without a hierarchical relationship, e.g., black and hdmi."}, {"5f39": "A lexical database also enables a more nuanced form of query relaxation. Instead of ignoring a token, we can replace it with a more general term, also known as a hypernym. We can also use a lexical database for query expansion."}, {"654b": "Syntactic Analysis"}, {"2fbc": "Another approach to query relaxation to use a query\u2019s syntactic structure to determine which tokens are optional."}, {"a1d1": "A large fraction of search queries are noun phrases. A noun phrase serves the place of a noun\u200a\u2014\u200athat is, it represents a thing or set of things. A noun phrase can be a solitary noun, e.g., cat, or it can a complex phrase like the best cat in the whole wide world."}, {"9f65": "We can analyze search queries using a part-of-speech tagger, such as NLTK, which in turn allows us to parse the overall syntactic structure of the query. If the query is a noun phrase, parsing allows us to identify its head noun, as well as any adjectives and phrases modifying it."}, {"553f": "A reasonable query relaxation strategy preserves the head noun and removes one or more of its modifiers. For example, the most important word in the best cat in the whole wide world is the head noun, cat."}, {"bfe5": "But this strategy can break down. For example, if the query is free shipping, the adjective free is at least as important to the meaning as the head noun shipping. Syntax does not always dictate semantics. Still, emphasizing the head noun and the modifiers closest to it usually works in practice."}, {"6b7f": "Semantic Analysis"}, {"9417": "The most sophisticated approach to query relaxation goes beyond token frequency and syntax and considers semantics\u200a\u2014\u200athat is, what the tokens mean, particularly in relation to one another."}, {"3381": "For example, we can relax the query polo shirt to polo, since shirt is implied. In contrast, relaxing dress shirt to dress completely changes the query\u2019s meaning. Syntax isn\u2019t helpful: in both cases, we\u2019re replacing a noun phrase with a token that isn\u2019t even the head noun. And shirt is no more specific than polo or dress. So we need to understand the semantics to relax this query successfully."}, {"b19b": "We can use the Word2vec model to embed words and phrases into a vector space that captures their semantics. This embedding allows us to recognize how much the query tokens overlap with one another in meaning, which in turn helps us estimate the consequence of ignoring a token. Word2vec also allows us to compute the similarity between a token and a phrase containing that token. If they\u2019re similar, then it\u2019s probably safe to relax the query by replacing the phrase with the token."}, {"24b4": "Relax but be Careful"}, {"be8a": "Like query expansion, query relaxation aims to increase recall while minimizing the loss of precision. If we already have a reasonable quantity and quality of results, then query relaxation probably isn\u2019t worth the risk."}, {"2435": "Query relaxation is most useful for queries that return few or no results, since those are the queries for which we have the least to lose and the most to gain. But remember that more results doesn\u2019t necessarily mean better results. Use query relaxation, but use it with care."}, {"b86b": "Previous: Query Expansion"}], "child": "1090_1\t1090_2"}