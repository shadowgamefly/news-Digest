{"name": "642", "parent": "", "content": "Stop Pretending to Be A Data Scientist: Pseudoscientific Split Testing is a\u00a0Waste Every would-be digital marketer goes through an evolution of understanding about the idea of split testing. The best of them usually get to the point where they realize they need to focus their tests on one variable and that they need to reach a certain sample size before they choose a winner. Little guy doesn\u2019t even know what a p-value is\u00a0yet Then, they begin to take statistical significance into account. They learn about sample sizes and confirmation bias. Maybe they even resort to Bayesian analyses of their data. He was testing for statistical significance before it was\u00a0cool. But even then, there are some blind spots. False positives, regression to the mean, sample pollution. Plenty of room for error. The Pitfalls of Split\u00a0Testing \u201cWhat if I told you most winning A/B test results are illusory?\u201d As Quick Sprout data scientist Will Kurt wrote back when he was the lead data scientist at KISSmetrics: A/B tests are designed to imitate scientific experiments, but most marketers running A/B Tests do not live in a world that is anything like a lab at a university. Not only do we not live in a university lab, but in most cases, we don\u2019t even have the tools to create an environment that comes anywhere close. Running a test in AdWords is like trying to run a clinical trial in a Chuck E. Cheese. You have no control over who comes in, where they sit, or whether or not they get in the ball pit. You\u2019re Doing It\u00a0Wrong Data Science Dr. Martin Goodson. Source:\u00a0Twitter When he was research lead at Qubit, Dr. Martin Goodson (currently Chief Scientist at Evolution AI) wrote a white paper called \u201cMost Winning A/B Test Results Are Illusory\u201d. In the paper, Goodson explains how \u201cbadly performed A/B tests can produce winning results which are more likely to be false than true.\u201d If you don\u2019t read the white paper, you might think, \u201cOh, that doesn\u2019t apply to me. I test until I reach statistical significance!\u201d There\u2019s more to it than that. Read the paper. (Also, the American Statistical Association released this \u201cStatement on Statistical Significance and p-Values\u201d in March of 2016. You ought to read that too.) \u201cStatistical significance. You keep using that word. I don\u2019t think it means what you think it\u00a0means.\u201d The Catch-22 of Split\u00a0Testing Even ignoring problems with so-called statistical significance, there are two problems digital marketers of smaller accounts run into when trying to run a good split test. One is the sample size. It\u2019s too small. \u201c\u2026\u201d The second is the test duration. It\u2019s too long. \u201cThat\u2019s what she\u00a0said!\u201d For accounts that aren\u2019t reaching a certain number of conversions each month, the need to run longer tests (to reach the desired sample size) leads to a greater likelihood of sample pollution as users delete their cookies and third-party split testing solutions can\u2019t keep treatment groups separated. \u201cIf you don\u2019t know about sample pollution, stop testing.\u201d\u200a\u2014\u200aBart\u00a0Schutz And when you\u2019re testing ad copy or creative, the problem is even worse. For example, on AdWords, there is no way to ensure that a user only sees one variation of the ad. Sample pollution is almost guaranteed. One person could see both ads multiple times. He could see one on a Monday morning, one on a Tuesday afternoon. One on Christmas Eve while he\u2019s at home opening a gift (it\u2019s a sweater), one two days after January 1 at the mall (he\u2019s taking the sweater back), and three while he\u2019s looking at his phone during dinner as he assures his soon-to-be ex-girlfriend that he loved the sweater she got him. What\u2019s the point of attempting a rigorous scientific experiment under those conditions? Does adding a p-value requirement significantly increase the ability to predict success? I doubt it. So\u2026? Where does that leave us? Is this what existential angst feels like? Uli is a nihilist. He doesn\u2019t care about ad optimization. What do you do? Just leave your ads running forever, damn optimization? Go with your gut and just make adjustments whenever the mood strikes you? How do we live in this scary new world without the false sense of security derived from bogus data science? Now, now. Calm down! You just need to make a realistic assessment of your current approach, and make adjustments to ensure that you\u2019re not wasting your time or your (client\u2019s) money. Here\u2019s how: The Highly Unscientific Kick Ass Methodology Since becoming disillusioned with the attempt to run scientific studies in the uncontrolled environment that is AdWords (and Facebook, and Bing, and every other online advertising platform developed so far), I\u2019ve begun implementing my highly unscientific Kick Ass methodology (using this AdWords script). I require an average of 1,000 impressions per ad, and a two percentage point difference in the metric I\u2019m testing (conversion rate, click-through rate, etc.) to choose a winner. If neither ad pulls away enough to meet that threshold after reaching 3000 impressions, I say it\u2019s too close to ever get a definitive answer. I go ahead and pick the \u201cwinner\u201d and try to create an ad that\u2019s substantially better. Cage fighting for\u00a0ads My theory is this: if one ad isn\u2019t really kicking the ass of the other one, any difference between them is insignificant. With all the variables we can\u2019t control, who knows if a 0.5% improvement is just because some people saw the same ad multiple times, or on a certain day, or when the weather was just so, etc.? This might not be an acceptable way to run a medical study, but it works for optimizing ad copy. Your Thoughts? My goal is to reduce wasted effort and ad spend. If I can improve results with better testing I want to do it, but most digital advertising platforms simply don\u2019t allow for the sort of controlled environment needed for a good test. That being the case, are we better off simplifying our testing methodologies to leave more time for other account management activities? What do you think? Is an attempt at rigorous split testing worth it or should we settle for a simpler approach? If you found this post interesting (or infuriating), click the heart below, because that will keep the conversation going. ", "title": "Stop Pretending to Be A Data Scientist: Pseudoscientific Split Testing is a\u00a0Waste", "sentences": [{"c98d": "Stop Pretending to Be A Data Scientist: Pseudoscientific Split Testing is a\u00a0Waste"}, {"6a52": "Every would-be digital marketer goes through an evolution of understanding about the idea of split testing. The best of them usually get to the point where they realize they need to focus their tests on one variable and that they need to reach a certain sample size before they choose a winner."}, {"dd44": "Little guy doesn\u2019t even know what a p-value is\u00a0yet"}, {"a93e": "Then, they begin to take statistical significance into account. They learn about sample sizes and confirmation bias. Maybe they even resort to Bayesian analyses of their data."}, {"08a7": "He was testing for statistical significance before it was\u00a0cool."}, {"322b": "But even then, there are some blind spots. False positives, regression to the mean, sample pollution. Plenty of room for error."}, {"f569": "The Pitfalls of Split\u00a0Testing"}, {"3ebf": "\u201cWhat if I told you most winning A/B test results are illusory?\u201d"}, {"913f": "As Quick Sprout data scientist Will Kurt wrote back when he was the lead data scientist at KISSmetrics:"}, {"f779": "A/B tests are designed to imitate scientific experiments, but most marketers running A/B Tests do not live in a world that is anything like a lab at a university."}, {"c4d4": "Not only do we not live in a university lab, but in most cases, we don\u2019t even have the tools to create an environment that comes anywhere close."}, {"2571": "Running a test in AdWords is like trying to run a clinical trial in a Chuck E. Cheese. You have no control over who comes in, where they sit, or whether or not they get in the ball pit."}, {"2bc7": "You\u2019re Doing It\u00a0Wrong"}, {"a7d6": "Data Science Dr. Martin Goodson. Source:\u00a0Twitter"}, {"1347": "When he was research lead at Qubit, Dr. Martin Goodson (currently Chief Scientist at Evolution AI) wrote a white paper called \u201cMost Winning A/B Test Results Are Illusory\u201d. In the paper, Goodson explains how \u201cbadly performed A/B tests can produce winning results which are more likely to be false than true.\u201d"}, {"1d7b": "If you don\u2019t read the white paper, you might think, \u201cOh, that doesn\u2019t apply to me. I test until I reach statistical significance!\u201d There\u2019s more to it than that. Read the paper."}, {"9fd7": "(Also, the American Statistical Association released this \u201cStatement on Statistical Significance and p-Values\u201d in March of 2016. You ought to read that too.)"}, {"c7c5": "\u201cStatistical significance. You keep using that word. I don\u2019t think it means what you think it\u00a0means.\u201d"}, {"f34d": "The Catch-22 of Split\u00a0Testing"}, {"3ec3": "Even ignoring problems with so-called statistical significance, there are two problems digital marketers of smaller accounts run into when trying to run a good split test."}, {"c533": "One is the sample size. It\u2019s too small."}, {"d659": "\u201c\u2026\u201d"}, {"0561": "The second is the test duration. It\u2019s too long."}, {"9bd0": "\u201cThat\u2019s what she\u00a0said!\u201d"}, {"5042": "For accounts that aren\u2019t reaching a certain number of conversions each month, the need to run longer tests (to reach the desired sample size) leads to a greater likelihood of sample pollution as users delete their cookies and third-party split testing solutions can\u2019t keep treatment groups separated."}, {"52aa": "\u201cIf you don\u2019t know about sample pollution, stop testing.\u201d\u200a\u2014\u200aBart\u00a0Schutz"}, {"afa6": "And when you\u2019re testing ad copy or creative, the problem is even worse. For example, on AdWords, there is no way to ensure that a user only sees one variation of the ad. Sample pollution is almost guaranteed."}, {"8172": "One person could see both ads multiple times. He could see one on a Monday morning, one on a Tuesday afternoon. One on Christmas Eve while he\u2019s at home opening a gift (it\u2019s a sweater), one two days after January 1 at the mall (he\u2019s taking the sweater back), and three while he\u2019s looking at his phone during dinner as he assures his soon-to-be ex-girlfriend that he loved the sweater she got him."}, {"2a68": "What\u2019s the point of attempting a rigorous scientific experiment under those conditions? Does adding a p-value requirement significantly increase the ability to predict success?"}, {"5552": "I doubt it."}, {"e253": "So\u2026?"}, {"0a1f": "Where does that leave us? Is this what existential angst feels like?"}, {"dbd3": "Uli is a nihilist. He doesn\u2019t care about ad optimization."}, {"7629": "What do you do? Just leave your ads running forever, damn optimization? Go with your gut and just make adjustments whenever the mood strikes you? How do we live in this scary new world without the false sense of security derived from bogus data science?"}, {"1268": "Now, now. Calm down! You just need to make a realistic assessment of your current approach, and make adjustments to ensure that you\u2019re not wasting your time or your (client\u2019s) money."}, {"cc70": "Here\u2019s how:"}, {"0c7c": "The Highly Unscientific Kick Ass Methodology"}, {"79a8": "Since becoming disillusioned with the attempt to run scientific studies in the uncontrolled environment that is AdWords (and Facebook, and Bing, and every other online advertising platform developed so far), I\u2019ve begun implementing my highly unscientific Kick Ass methodology (using this AdWords script)."}, {"a341": "I require an average of 1,000 impressions per ad, and a two percentage point difference in the metric I\u2019m testing (conversion rate, click-through rate, etc.) to choose a winner."}, {"4836": "If neither ad pulls away enough to meet that threshold after reaching 3000 impressions, I say it\u2019s too close to ever get a definitive answer. I go ahead and pick the \u201cwinner\u201d and try to create an ad that\u2019s substantially better."}, {"0e43": "Cage fighting for\u00a0ads"}, {"e393": "My theory is this: if one ad isn\u2019t really kicking the ass of the other one, any difference between them is insignificant. With all the variables we can\u2019t control, who knows if a 0.5% improvement is just because some people saw the same ad multiple times, or on a certain day, or when the weather was just so, etc.?"}, {"ea02": "This might not be an acceptable way to run a medical study, but it works for optimizing ad copy."}, {"2b9f": "Your Thoughts?"}, {"3c24": "My goal is to reduce wasted effort and ad spend. If I can improve results with better testing I want to do it, but most digital advertising platforms simply don\u2019t allow for the sort of controlled environment needed for a good test."}, {"4a16": "That being the case, are we better off simplifying our testing methodologies to leave more time for other account management activities?"}, {"6fcc": "What do you think? Is an attempt at rigorous split testing worth it or should we settle for a simpler approach?"}, {"e8ad": "If you found this post interesting (or infuriating), click the heart below, because that will keep the conversation going."}], "child": "642_1\t642_2"}