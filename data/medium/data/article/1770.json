{"name": "1770", "parent": "", "content": "How do you deal with a problem like \u201cfake\u00a0news?\u201d CC BY-SA 2.0-licensed photo by Marco Pak\u00f6eningrat. Last month, Adam Mosseri, a VP at Facebook in charge of changes to the News Feed, announced steps Facebook is taking to prevent the spread of fake news and hoaxes on the platform. The solution\u200a\u2014\u200aa mix of user engagement, third-party fact checkers, user-experience design, and disrupting financial incentives\u200a\u2014\u200ais interesting because of its multi-modal approach. Facebook should be lauded for engaging with the problem head-on, but there are still significant weaknesses in the steps that have been suggested by the company. As this issue continues to unfold, and as more countries deal with the prospect of fake news and hoaxes influencing their own political process, we should take stock of the potential fallout of Facebook\u2019s new approach. Facebook will rely on users to report fake news despite evidence that suggests users have a difficult time assessing or identifying fake news. Teens seem to be especially vulnerable to fake news. A recent study by researchers at Stanford found that middle and high school students have a difficult time detecting fake news from real news, or detecting bias in tweets and Facebook statuses. In Pew\u2019s recent study on fake news, they found that individuals are often confident in their ability to identify fake news (around 2/3 of those surveyed), but we don\u2019t know if that confidence translates to being able to identify fake news. The same study found that a fourth of respondents admitted to sharing fake news on at least one occasion. If Facebook\u2019s solution is to rely, in part, on users to report fake news through a system of flagging, users need to be trained in this type of identification. Facebook\u2019s fake news software may have unintended consequences for the news media industry and reporting. In addition to relying on reports from users, Facebook is deploying their own software to help them identify fake news through patterns in user behavior. Reports of how Facebook\u2019s software would determine fake news have been unclear. Following an interview with Mosseri, Recode reported that Facebook would use data available to them to determine if a story had been read, and then shared, as an indicator of truthfulness. The presumption Mosseri and his Facebook colleagues are working under is, per Peter Kafka at Recode, \u201cIf I click on/read a story and then don\u2019t share it with my friends\u2026that\u2019s a sign it may be \u2018problematic content.\u2019\u201d The obvious problem with this measurement of truthfulness is that there are a great many variables contributing to why a user would read content and not share it. And the logic of clickbait\u200a\u2014\u200acontent that is highly personal or includes calls to action is more likely to be shared\u200a\u2014\u200amakes Facebook\u2019s changes worrisome because they could lead to the production of more share-worthy news and less hard-hitting investigative pieces exploring the often dry and less shareable nuances of domestic and foreign policies. As we\u2019ve seen in the past, because Facebook has become so central to the news media industry, changes like these can have drastic effects on newsroom operations and reporting, affecting variables like referral traffic that have become so powerful in generating ad-based revenues. Third-party fact checkers will need to find a sustainable business model to continue this work long-term. The third-party fact checkers or \u201cpartners\u201d (in Facebook\u2019s language) will not be receiving any sort of compensation from Facebook for contributing labor to this cause. Currently this list includes ABC news, The Associated Press, Politifact, FactCheck, and Snopes, which have agreed to vet potentially fake news that is identified by users or Facebook\u2019s software (Facebook puts these stories into a list that is available to the fact-checking organizations on the back-end for verification). ABC News has said their team will include about a half dozen journalists who were fact checking during the election and who will be redirected by the organization to this work. Though this system could have indirect financial benefits to news organizations\u200a\u2014\u200ae.g., fake news gets less traction leaving more room for real news\u200a\u2014\u200ait\u2019s still a fairly significant investment to make. (And it presumes that people trust fact checkers themselves, which isn\u2019t always the case.) It also relies on the notion that the flow of reports from users and software will be manageable, when content moderation is often an incredibly large undertaking. In a 2014 story for Wired by Adrien Chen, the potential number of moderators for social media sites (often working in areas like the Philippines) was estimated in the hundreds of thousands. It\u2019s unclear how this model will be translated internationally as well. U.S.-based fact checkers may not have the context, knowledge, or bandwidth, to address fake news and hoax claims now entering into the online ecosystems of Germany, France, Myanmar, Brazil, India, and other countries. Facebook will continue to restrict ads on fake news, disrupting the financial incentives for producers. This is something that Facebook and other platforms, like Google, had already committed to doing over the past two months. For Facebook, this currently means that they will not \u201cintegrate or display ads in apps or sites containing content that is illegal, misleading, or deceptive.\u201d However, until Facebook changes its own financial model, which prioritizes content that is easily shared, there is little hope for disrupting the current norms affecting the production of fake news or misleading content. While these policies do inhibit fake news producers from generating money on their own site, Facebook still benefits from the increased traffic and sharing on the News Feed. It\u2019s unclear how Facebook will reduce their own reliance on easily shareable content, which has influenced the spread of fake or misleading news. This proposal rests on the assumption that \u201cfake news\u201d will be easy to spot and define. At its base, \u201cfake news,\u201d the concept and practice, is clickbait, and has been, in part, manufactured by the incentives baked into how organizations and individuals gain attention over social media networks (through clicks, shares, and likes). It\u2019s a term that has come to refer to a wide-range of media practices that build upon clickbait logics. In more black-and-white cases, fake news refers to intentionally made-up stories hosted on hastily built websites (i.e. The Denver Guardian or The Baltimore Gazette). These types of fake news websites, built by teenagers in Macedonia or by citizens in the U.S. to make money (or just lulz) off the circulation of outrage, fear, or anxiety. But most fake news is very gray. It consists of misleading headlines, deceptive edits, consensus-based truthmaking in communities like reddit or 8chan (i.e. pizzagate), or by the absorption of fake news by political figures, like Donald Trump, who have the power to make fake news, newsworthy. Facebook and other platforms may, in addition, have to consider how they would actively prevent this misleading news, as well as hate speech, from trending over their networks soon, at least in certain areas of the world. Legislation recently proposed by Thomas Oppermann, of the German Social Democratic Party in Germany, would require Facebook to have a German-based team dedicated to fielding reports of fake news and hate speech, and would require Facebook to remove fake news items or be subject to a $522,575 fine for each post that is not removed. How countries seek to limit the spread of hate speech and misinformation will continue to be an important counterweight to efforts proposed by companies based in the U.S. (subject to much less stringent media laws); companies will seek to maintain their market share within jurisdictions attempting to extend the media regulations and protections of the print, broadcast, and cable eras to newly dominant information and news distributors like Facebook. Robyn Caplan is a Research Analyst at Data & Society and a PhD Candidate at Rutgers University\u2019s School of Communication and Information. Points/spheres: In \u201cHow do you deal with a problem like \u2018fake news?\u2019\u201d Robyn Caplan weighs in on the potential\u200a\u2014\u200aand pitfalls\u200a\u2014\u200aof efforts to curb Facebook\u2019s fake news problem. This piece is part of a batch of new additions to an ongoing Points series on media, accountability, and the public sphere. See also: ", "title": "How do you deal with a problem like \u201cfake\u00a0news?\u201d", "sentences": [{"793f": "How do you deal with a problem like \u201cfake\u00a0news?\u201d"}, {"34b8": "CC BY-SA 2.0-licensed photo by Marco Pak\u00f6eningrat."}, {"043d": "Last month, Adam Mosseri, a VP at Facebook in charge of changes to the News Feed, announced steps Facebook is taking to prevent the spread of fake news and hoaxes on the platform. The solution\u200a\u2014\u200aa mix of user engagement, third-party fact checkers, user-experience design, and disrupting financial incentives\u200a\u2014\u200ais interesting because of its multi-modal approach. Facebook should be lauded for engaging with the problem head-on, but there are still significant weaknesses in the steps that have been suggested by the company."}, {"0cc4": "As this issue continues to unfold, and as more countries deal with the prospect of fake news and hoaxes influencing their own political process, we should take stock of the potential fallout of Facebook\u2019s new approach."}, {"25cc": "Facebook will rely on users to report fake news despite evidence that suggests users have a difficult time assessing or identifying fake news. Teens seem to be especially vulnerable to fake news. A recent study by researchers at Stanford found that middle and high school students have a difficult time detecting fake news from real news, or detecting bias in tweets and Facebook statuses. In Pew\u2019s recent study on fake news, they found that individuals are often confident in their ability to identify fake news (around 2/3 of those surveyed), but we don\u2019t know if that confidence translates to being able to identify fake news. The same study found that a fourth of respondents admitted to sharing fake news on at least one occasion. If Facebook\u2019s solution is to rely, in part, on users to report fake news through a system of flagging, users need to be trained in this type of identification."}, {"a463": "Facebook\u2019s fake news software may have unintended consequences for the news media industry and reporting. In addition to relying on reports from users, Facebook is deploying their own software to help them identify fake news through patterns in user behavior. Reports of how Facebook\u2019s software would determine fake news have been unclear. Following an interview with Mosseri, Recode reported that Facebook would use data available to them to determine if a story had been read, and then shared, as an indicator of truthfulness. The presumption Mosseri and his Facebook colleagues are working under is, per Peter Kafka at Recode, \u201cIf I click on/read a story and then don\u2019t share it with my friends\u2026that\u2019s a sign it may be \u2018problematic content.\u2019\u201d The obvious problem with this measurement of truthfulness is that there are a great many variables contributing to why a user would read content and not share it."}, {"038c": "And the logic of clickbait\u200a\u2014\u200acontent that is highly personal or includes calls to action is more likely to be shared\u200a\u2014\u200amakes Facebook\u2019s changes worrisome because they could lead to the production of more share-worthy news and less hard-hitting investigative pieces exploring the often dry and less shareable nuances of domestic and foreign policies. As we\u2019ve seen in the past, because Facebook has become so central to the news media industry, changes like these can have drastic effects on newsroom operations and reporting, affecting variables like referral traffic that have become so powerful in generating ad-based revenues."}, {"60b1": "Third-party fact checkers will need to find a sustainable business model to continue this work long-term. The third-party fact checkers or \u201cpartners\u201d (in Facebook\u2019s language) will not be receiving any sort of compensation from Facebook for contributing labor to this cause. Currently this list includes ABC news, The Associated Press, Politifact, FactCheck, and Snopes, which have agreed to vet potentially fake news that is identified by users or Facebook\u2019s software (Facebook puts these stories into a list that is available to the fact-checking organizations on the back-end for verification). ABC News has said their team will include about a half dozen journalists who were fact checking during the election and who will be redirected by the organization to this work."}, {"0ea1": "Though this system could have indirect financial benefits to news organizations\u200a\u2014\u200ae.g., fake news gets less traction leaving more room for real news\u200a\u2014\u200ait\u2019s still a fairly significant investment to make. (And it presumes that people trust fact checkers themselves, which isn\u2019t always the case.) It also relies on the notion that the flow of reports from users and software will be manageable, when content moderation is often an incredibly large undertaking. In a 2014 story for Wired by Adrien Chen, the potential number of moderators for social media sites (often working in areas like the Philippines) was estimated in the hundreds of thousands. It\u2019s unclear how this model will be translated internationally as well. U.S.-based fact checkers may not have the context, knowledge, or bandwidth, to address fake news and hoax claims now entering into the online ecosystems of Germany, France, Myanmar, Brazil, India, and other countries."}, {"5cba": "Facebook will continue to restrict ads on fake news, disrupting the financial incentives for producers. This is something that Facebook and other platforms, like Google, had already committed to doing over the past two months. For Facebook, this currently means that they will not \u201cintegrate or display ads in apps or sites containing content that is illegal, misleading, or deceptive.\u201d However, until Facebook changes its own financial model, which prioritizes content that is easily shared, there is little hope for disrupting the current norms affecting the production of fake news or misleading content. While these policies do inhibit fake news producers from generating money on their own site, Facebook still benefits from the increased traffic and sharing on the News Feed. It\u2019s unclear how Facebook will reduce their own reliance on easily shareable content, which has influenced the spread of fake or misleading news."}, {"34e5": "This proposal rests on the assumption that \u201cfake news\u201d will be easy to spot and define. At its base, \u201cfake news,\u201d the concept and practice, is clickbait, and has been, in part, manufactured by the incentives baked into how organizations and individuals gain attention over social media networks (through clicks, shares, and likes). It\u2019s a term that has come to refer to a wide-range of media practices that build upon clickbait logics. In more black-and-white cases, fake news refers to intentionally made-up stories hosted on hastily built websites (i.e. The Denver Guardian or The Baltimore Gazette). These types of fake news websites, built by teenagers in Macedonia or by citizens in the U.S. to make money (or just lulz) off the circulation of outrage, fear, or anxiety. But most fake news is very gray. It consists of misleading headlines, deceptive edits, consensus-based truthmaking in communities like reddit or 8chan (i.e. pizzagate), or by the absorption of fake news by political figures, like Donald Trump, who have the power to make fake news, newsworthy."}, {"9e64": "Facebook and other platforms may, in addition, have to consider how they would actively prevent this misleading news, as well as hate speech, from trending over their networks soon, at least in certain areas of the world. Legislation recently proposed by Thomas Oppermann, of the German Social Democratic Party in Germany, would require Facebook to have a German-based team dedicated to fielding reports of fake news and hate speech, and would require Facebook to remove fake news items or be subject to a $522,575 fine for each post that is not removed."}, {"f6c1": "How countries seek to limit the spread of hate speech and misinformation will continue to be an important counterweight to efforts proposed by companies based in the U.S. (subject to much less stringent media laws); companies will seek to maintain their market share within jurisdictions attempting to extend the media regulations and protections of the print, broadcast, and cable eras to newly dominant information and news distributors like Facebook."}, {"5fec": "Robyn Caplan is a Research Analyst at Data & Society and a PhD Candidate at Rutgers University\u2019s School of Communication and Information."}, {"b1b4": "Points/spheres: In \u201cHow do you deal with a problem like \u2018fake news?\u2019\u201d Robyn Caplan weighs in on the potential\u200a\u2014\u200aand pitfalls\u200a\u2014\u200aof efforts to curb Facebook\u2019s fake news problem. This piece is part of a batch of new additions to an ongoing Points series on media, accountability, and the public sphere. See also:"}], "child": "1770_1\t1770_2\t1770_3\t1770_4\t1770_51770_1\t1770_2\t1770_3\t1770_4\t1770_51770_1\t1770_2\t1770_3\t1770_4\t1770_5"}