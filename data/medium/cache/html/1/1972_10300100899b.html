<!DOCTYPE html><html xmlns:cc="http://creativecommons.org/ns#"><head prefix="og: http://ogp.me/ns# fb: http://ogp.me/ns/fb# medium-com: http://ogp.me/ns/fb/medium-com#"><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><meta name="viewport" content="width=device-width, initial-scale=1"><title>Rohan &amp; Lenny #3: Recurrent Neural Networks &amp; LSTMs</title><link rel="canonical" href="https://ayearofai.com/rohan-lenny-3-recurrent-neural-networks-10300100899b"><meta name="title" content="Rohan &amp; Lenny #3: Recurrent Neural Networks &amp; LSTMs"><meta name="referrer" content="unsafe-url"><meta name="description" content="It seems like most of our posts on this blog start with “We’re back!”, so… you know the drill. It’s been a while since our last post — just over 5 months — but it certainly doesn’t feel that way…"><meta property="og:title" content="Rohan &amp; Lenny #3: Recurrent Neural Networks &amp; LSTMs"><meta property="og:url" content="https://ayearofai.com/rohan-lenny-3-recurrent-neural-networks-10300100899b"><meta property="og:image" content="https://cdn-images-1.medium.com/max/1200/1*khIKl9t4XmZGSsKhW_Yg2w.png"><meta property="fb:app_id" content="542599432471018"><meta property="og:description" content="The ultimate guide to machine learning’s favorite child."><meta name="twitter:description" content="The ultimate guide to machine learning’s favorite child."><meta name="twitter:image:src" content="https://cdn-images-1.medium.com/max/1200/1*khIKl9t4XmZGSsKhW_Yg2w.png"><link rel="publisher" href="https://plus.google.com/103654360130207659246"><link rel="author" href="https://ayearofai.com/@mckapur"><meta property="author" content="Rohan Kapur"><meta property="og:type" content="article"><meta name="twitter:card" content="summary_large_image"><meta property="article:publisher" content="https://www.facebook.com/mckapur"><meta property="article:author" content="1004843339565980"><meta property="fb:smart_publish:robots" content="noauto"><meta name="robots" content="index, follow"><meta property="article:published_time" content="2017-04-13T08:27:31.153Z"><meta name="twitter:creator" content="@mckapur"><meta name="twitter:site" content="@mckapur"><meta property="og:site_name" content="A Year of Artificial Intelligence"><meta name="twitter:label1" value="Reading time"><meta name="twitter:data1" value="84 min read"><script type="application/ld+json">{"@context":"http://schema.org","@type":"NewsArticle","image":{"@type":"ImageObject","width":1920,"height":870,"url":"https://cdn-images-1.medium.com/max/1920/1*khIKl9t4XmZGSsKhW_Yg2w.png"},"datePublished":"2017-04-13T08:27:31.153Z","dateModified":"2017-04-17T00:31:23.311Z","headline":"Rohan & Lenny #3: Recurrent Neural Networks & LSTMs","name":"Rohan & Lenny #3: Recurrent Neural Networks & LSTMs","keywords":["Machine Learning","Artificial Intelligence","Data Science","Deep Learning","Algorithms"],"author":{"@type":"Person","name":"Rohan Kapur","url":"https://ayearofai.com/@mckapur"},"creator":["Rohan Kapur"],"publisher":{"@type":"Organization","name":"A Year of Artificial Intelligence","url":"https://ayearofai.com","logo":{"@type":"ImageObject","width":90,"height":60,"url":"https://cdn-images-1.medium.com/max/90/1*NZsNSuNxe_O2YW1ybboOvA.jpeg"}},"mainEntityOfPage":"https://ayearofai.com/rohan-lenny-3-recurrent-neural-networks-10300100899b"}</script><meta name="twitter:app:name:iphone" content="Medium"><meta name="twitter:app:id:iphone" content="828256236"><meta name="twitter:app:url:iphone" content="medium://p/10300100899b"><meta property="al:ios:app_name" content="Medium"><meta property="al:ios:app_store_id" content="828256236"><meta property="al:android:package" content="com.medium.reader"><meta property="al:android:app_name" content="Medium"><meta property="al:ios:url" content="medium://p/10300100899b"><meta property="al:android:url" content="medium://p/10300100899b"><meta property="al:web:url" content="https://ayearofai.com/rohan-lenny-3-recurrent-neural-networks-10300100899b"><link rel="search" type="application/opensearchdescription+xml" title="Medium" href="/osd.xml" /><link rel="alternate" href="android-app://com.medium.reader/https/medium.com/p/10300100899b" /><meta name="theme-color" content="#000000"><link rel="stylesheet" href="https://cdn-static-1.medium.com/_/fp/css/main-base.g2e09PlZK_j3Lw2AgNmbfQ.css"><script>if (window.top !== window.self) window.top.location = window.self.location.href;var OB_startTime = new Date().getTime(); var OB_loadErrors = []; function _onerror(e) { OB_loadErrors.push(e) }; if (document.addEventListener) document.addEventListener("error", _onerror, true); else if (document.attachEvent) document.attachEvent("onerror", _onerror); function _asyncScript(u) {var d = document, f = d.getElementsByTagName("script")[0], s = d.createElement("script"); s.type = "text/javascript"; s.async = true; s.src = u; f.parentNode.insertBefore(s, f);}function _asyncStyles(u) {var d = document, f = d.getElementsByTagName("script")[0], s = d.createElement("link"); s.rel = "stylesheet"; s.href = u; f.parentNode.insertBefore(s, f); return s}var _gaq = _gaq || [];_gaq.push(["_setAccount", "UA-24232453-2"]); _gaq.push(["_setDomainName", window.location.hostname]); _gaq.push(["_setAllowLinker", true]); _gaq.push(["_trackPageview"]);_asyncScript(("https:" == document.location.protocol ? "https://ssl" : "http://www") + ".google-analytics.com/ga.js");(new Image()).src = "/_/stat?event=pixel.load&origin=" + encodeURIComponent(location.origin);</script><script>(function () {var height = window.innerHeight || document.documentElement.clientHeight || document.body.clientHeight; var width = window.innerWidth || document.documentElement.clientWidth || document.body.clientWidth; document.write("<style>section.section-image--fullBleed.is-backgrounded {padding-top: " + Math.round(1.1 * height) + "px;}section.section-image--fullScreen.is-backgrounded, section.section-image--coverFade.is-backgrounded {min-height: " + height + "px; padding-top: " + Math.round(0.5 * height) + "px;}.u-sizeViewHeight100 {height: " + height + "px !important;}.u-sizeViewHeight110 {height: " + Math.round(1.1 * height) + "px !important;}.u-sizeViewHeightMin100 {min-height: " + height + "px !important;}.u-sizeViewHeightMax100 {max-height: " + height + "px !important;}section.section-image--coverFade {height: " + height + "px;}.section-aspectRatioViewportPlaceholder, .section-aspectRatioViewportCropPlaceholder {max-height: " + height + "px;}.section-aspectRatioViewportBottomSpacer, .section-aspectRatioViewportBottomPlaceholder {max-height: " + Math.round(0.5 * height) + "px;}.zoomable:before {top: " + (-1 * height) + "px; left: " + (-1 * width) + "px; padding: " + height + "px " + width + "px;}</style>");})()</script><!--[if lt IE 9]><script charset="UTF-8" src="https://cdn-static-1.medium.com/_/fp/js/shiv.RI2ePTZ5gFmMgLzG5bEVAA.js"></script><![endif]--><link rel="icon" href="https://cdn-static-1.medium.com/_/fp/icons/favicon-medium.TAS6uQ-Y7kcKgi0xjcYHXw.ico" class="js-favicon"><link rel="apple-touch-icon" sizes="152x152" href="https://cdn-images-1.medium.com/fit/c/152/152/1*NZsNSuNxe_O2YW1ybboOvA.jpeg"><link rel="apple-touch-icon" sizes="120x120" href="https://cdn-images-1.medium.com/fit/c/120/120/1*NZsNSuNxe_O2YW1ybboOvA.jpeg"><link rel="apple-touch-icon" sizes="76x76" href="https://cdn-images-1.medium.com/fit/c/76/76/1*NZsNSuNxe_O2YW1ybboOvA.jpeg"><link rel="apple-touch-icon" sizes="60x60" href="https://cdn-images-1.medium.com/fit/c/60/60/1*NZsNSuNxe_O2YW1ybboOvA.jpeg"><link rel="mask-icon" href="https://cdn-static-1.medium.com/_/fp/icons/favicon.KjTfUJo7yJH_fCoUzzH3cg.svg" color="#171717"></head><body itemscope class=" postShowScreen is-noJs"><script>document.body.className = document.body.className.replace(/(^|\s)is-noJs(\s|$)/, "$1is-js$2")</script><div class="site-main" id="container"><div class="butterBar butterBar--error"></div><div class="surface"><div id="prerendered" class="screenContent"><canvas class="canvas-renderer"></canvas><div class="container u-maxWidth740 u-xs-margin0 notesPositionContainer js-notesPositionContainer"></div><div class="metabar u-clearfix js-metabar u-textColorTransparentWhiteDarker u-tintBgColor u-tintSpectrum"><div class="metabar-inner u-marginAuto u-maxWidth1000 u-paddingLeft20 u-paddingRight20 js-metabarMiddle"><div class="metabar-block metabar-block--left u-floatLeft u-height65 u-xs-height56"><div class="u-alignMiddle u-inlineBlock u-verticalAlignTop u-height65 u-xs-height56 u-marginRight18"><div class="u-alignBlock"><a class="js-logCollection" href="https://ayearofai.com?source=logo-lo_ced97fb2944f---bb87da25612c"><img height="36" width="54" class="u-paddingTop5" src="https://cdn-images-1.medium.com/letterbox/54/36/50/50/1*NZsNSuNxe_O2YW1ybboOvA.jpeg?source=logoAvatar-lo_ced97fb2944f---bb87da25612c" alt="A Year of Artificial Intelligence" /></a></div></div><div class="u-alignMiddle u-inlineBlock u-verticalAlignTop u-height65 u-xs-height56 u-xs-hide"><div class="u-alignBlock"><div class="buttonSet u-lineHeightInherit u-marginLeft0"><div class="buttonSet-inner"><button class="button button--primary u-paddingLeft10 u-paddingRight10 u-height19 u-lineHeight13 u-verticalAlignMiddle u-fontSize12 u-uiTextMedium button--small u-noUserSelect button--withChrome u-accentColor--buttonNormal js-relationshipButton is-smallPill"  data-action="sign-in-prompt" data-sign-in-action="toggle-follow-collection" data-requires-token="true" data-redirect="https://medium.com/_/subscribe/collection/a-year-of-artificial-intelligence" data-collection-id="bb87da25612c"><span class="button-label  js-buttonLabel">Follow</span></button></div><a class="button button--light button--chromeless is-touchIconBlackPulse u-baseColor--buttonLight button--withIcon button--withSvgIcon"  href="https://twitter.com/mckapur" title="Visit “A Year of Artificial Intelligence” on Twitter" aria-label="Visit “A Year of Artificial Intelligence” on Twitter" rel="me" target="_blank"><span class="button-defaultState"><span class="svgIcon svgIcon--twitterFilled svgIcon--25px"><svg class="svgIcon-use" width="25" height="25" viewBox="0 0 25 25" ><path d="M21.725 5.338c-.744.47-1.605.804-2.513 1.006a3.978 3.978 0 0 0-2.942-1.293c-2.22 0-4.02 1.81-4.02 4.02 0 .32.034.63.07.94-3.31-.18-6.27-1.78-8.255-4.23a4.544 4.544 0 0 0-.574 2.01c.04 1.43.74 2.66 1.8 3.38-.63-.01-1.25-.19-1.79-.5v.08c0 1.93 1.38 3.56 3.23 3.95-.34.07-.7.12-1.07.14-.25-.02-.5-.04-.72-.07.49 1.58 1.97 2.74 3.74 2.8a8.49 8.49 0 0 1-5.02 1.72c-.3-.03-.62-.04-.93-.07A11.447 11.447 0 0 0 8.88 21c7.386 0 11.43-6.13 11.414-11.414.015-.21.01-.38 0-.578a7.604 7.604 0 0 0 2.01-2.08 7.27 7.27 0 0 1-2.297.645 3.856 3.856 0 0 0 1.72-2.23"/></svg></span></span></a><a class="button button--light button--chromeless is-touchIconBlackPulse u-baseColor--buttonLight button--withIcon button--withSvgIcon u-paddingLeft0"  href="//facebook.com/mckapur" title="Visit “A Year of Artificial Intelligence” on Facebook" aria-label="Visit “A Year of Artificial Intelligence” on Facebook" rel="me" target="_blank"><span class="button-defaultState"><span class="svgIcon svgIcon--facebookFilled svgIcon--25px"><svg class="svgIcon-use" width="25" height="25" viewBox="0 0 25 25" ><path d="M21 12.646C21 7.65 16.97 3.6 12 3.6s-9 4.05-9 9.046a9.026 9.026 0 0 0 7.59 8.924v-6.376H8.395V12.64h2.193v-1.88c0-2.186 1.328-3.375 3.267-3.375.93 0 1.728.07 1.96.1V9.77H14.47c-1.055 0-1.26.503-1.26 1.242v1.63h2.517l-.33 2.554H13.21V21.6c4.398-.597 7.79-4.373 7.79-8.954"/></svg></span></span></a></div></div></div></div><div class="metabar-block u-floatRight u-xs-absolute u-xs-textAlignRight u-xs-right0 u-xs-marginRight20 u-height65 u-xs-height56"><div class="u-alignMiddle u-inlineBlock u-verticalAlignTop u-height65 u-xs-height56"><div class="u-alignBlock"><div class="buttonSet u-lineHeightInherit"><a class="button button--primary button--light button--chromeless u-accentColor--buttonNormal is-inSiteNavBar u-lineHeight30 u-height32"  href="https://medium.com/m/signin?redirect=https%3A%2F%2Fayearofai.com%2Frohan-lenny-3-recurrent-neural-networks-10300100899b" data-action="sign-in-prompt" data-redirect="https://ayearofai.com/rohan-lenny-3-recurrent-neural-networks-10300100899b" data-action-source="nav_signup">Sign in / Sign up</a></div></div></div></div></div><div class="metabar-inner u-marginAuto u-maxWidth1000 js-metabarBottom"><nav role="navigation" class="metabar-block metabar-block--below u-overflowHiddenY u-height40"><ul class="u-textAlignLeft u-noWrap u-overflowX js-collectionNavItems u-sm-paddingLeft20 u-sm-paddingRight20 u-paddingBottom100"><li class="metabar-navItem js-collectionNavItem u-uiTextMedium u-fontSize14 u-inlineBlock u-textUppercase u-letterSpacing003 u-textColorNormal u-xs-paddingRight12 u-xs-marginRight0 u-paddingTop5 u-xs-paddingTop10"><a class="link link--darken u-accentColor--textDarken js-homeNav u-baseColor--link"  href="https://ayearofai.com">Home</a></li><li class="metabar-navItem js-collectionNavItem  u-uiTextMedium u-fontSize14 u-inlineBlock u-textUppercase u-letterSpacing003 u-textColorNormal u-xs-paddingRight12 u-xs-marginRight0 u-paddingTop5 u-xs-paddingTop10"><a class="link link--darken u-accentColor--textDarken link--noUnderline js-navItemLink u-baseColor--link"  href="https://ayearofai.com/tagged/algorithms">Algorithms</a></li><li class="metabar-navItem js-collectionNavItem  u-uiTextMedium u-fontSize14 u-inlineBlock u-textUppercase u-letterSpacing003 u-textColorNormal u-xs-paddingRight12 u-xs-marginRight0 u-paddingTop5 u-xs-paddingTop10"><a class="link link--darken u-accentColor--textDarken link--noUnderline js-navItemLink u-baseColor--link"  href="https://ayearofai.com/tagged/today-i-learned">Today I Learned</a></li><li class="metabar-navItem js-collectionNavItem  u-uiTextMedium u-fontSize14 u-inlineBlock u-textUppercase u-letterSpacing003 u-textColorNormal u-xs-paddingRight12 u-xs-marginRight0 u-paddingTop5 u-xs-paddingTop10"><a class="link link--darken u-accentColor--textDarken link--noUnderline js-navItemLink u-baseColor--link"  href="https://ayearofai.com/tagged/case-studies">Case Studies</a></li><li class="metabar-navItem js-collectionNavItem  u-uiTextMedium u-fontSize14 u-inlineBlock u-textUppercase u-letterSpacing003 u-textColorNormal u-xs-paddingRight12 u-xs-marginRight0 u-paddingTop5 u-xs-paddingTop10"><a class="link link--darken u-accentColor--textDarken link--noUnderline js-navItemLink u-baseColor--link"  href="https://ayearofai.com/tagged/philosophical">Philosophical</a></li><li class="metabar-navItem js-collectionNavItem  u-uiTextMedium u-fontSize14 u-inlineBlock u-textUppercase u-letterSpacing003 u-textColorNormal u-xs-paddingRight12 u-xs-marginRight0 u-paddingTop5 u-xs-paddingTop10"><a class="link link--darken u-accentColor--textDarken link--noUnderline js-navItemLink u-baseColor--link"  href="https://ayearofai.com/tagged/meta">Meta</a></li><li class="metabar-navItem js-collectionNavItem u-uiTextMedium u-fontSize14 u-inlineBlock u-textUppercase u-letterSpacing003 u-textColorNormal u-xs-paddingRight12 u-xs-marginRight0 u-paddingTop5 u-xs-paddingTop10"><a class="button button--chromeless u-baseColor--buttonNormal button--withIcon button--withSvgIcon u-top1"  href="https://ayearofai.com/search" title="Search" aria-label="Search"><span class="button-defaultState"><span class="svgIcon svgIcon--search svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"  viewBox="0 0 25 25"><path d="M20.067 18.933l-4.157-4.157a6 6 0 1 0-.884.884l4.157 4.157a.624.624 0 1 0 .884-.884zM6.5 11c0-2.62 2.13-4.75 4.75-4.75S16 8.38 16 11s-2.13 4.75-4.75 4.75S6.5 13.62 6.5 11z"/></svg></span></span></a></li></ul></nav></div></div><div class="metabar metabar--spacer js-metabarSpacer u-tintBgColor  u-height105 u-xs-height95"></div><main role="main"><article class=" u-sizeViewHeightMin100 u-overflowHidden postArticle postArticle--full is-withAccentColors u-marginBottom40"  lang="en"><header class="container u-maxWidth740"><div class="postMetaHeader u-paddingBottom10 row"><div class="col u-size12of12 js-postMetaLockup"><div class="postMetaLockup postMetaLockup--authorWithBio u-flex js-postMetaLockup"><div class="u-flex0"><a class="link avatar u-baseColor--link"  href="https://ayearofai.com/@mckapur?source=post_header_lockup" data-action="show-user-card" data-action-source="post_header_lockup" data-action-value="cb55958ea3bb" data-action-type="hover" data-user-id="cb55958ea3bb" data-collection-slug="a-year-of-artificial-intelligence" dir="auto"><img  src="https://cdn-images-1.medium.com/fit/c/60/60/1*pGDfwp8yLzgCeG1RkPUxgQ.jpeg" class="avatar-image avatar-image--small" alt="Go to the profile of Rohan Kapur"></a></div><div class="u-flex1 u-paddingLeft15 u-overflowHidden"><a class="link link link--darken link--darker u-baseColor--link"  href="https://ayearofai.com/@mckapur?source=post_header_lockup" data-action="show-user-card" data-action-source="post_header_lockup" data-action-value="cb55958ea3bb" data-action-type="hover" data-user-id="cb55958ea3bb" data-collection-slug="a-year-of-artificial-intelligence" dir="auto">Rohan Kapur</a><span class="followState js-followState buttonSet-inner" data-user-id="cb55958ea3bb"><button class="button u-paddingLeft10 u-paddingRight10 u-height19 u-lineHeight13 u-verticalAlignMiddle u-fontSize12 u-uiTextMedium u-noUserSelect button--withChrome u-baseColor--buttonNormal button--withHover button--unblock js-unblockButton u-marginLeft10 u-marginTopNegative2 u-xs-hide"  data-action="sign-in-prompt" data-sign-in-action="toggle-block-user" data-requires-token="true" data-action-source="post_header_lockup"><span class="button-label  button-defaultState">Blocked</span><span class="button-label button-hoverState">Unblock</span></button><button class="button button--primary u-paddingLeft10 u-paddingRight10 u-height19 u-lineHeight13 u-verticalAlignMiddle u-fontSize12 u-uiTextMedium u-noUserSelect button--withChrome u-accentColor--buttonNormal button--follow js-followButton u-marginLeft10 u-marginTopNegative2 u-xs-hide"  data-action="sign-in-prompt" data-sign-in-action="toggle-subscribe-user" data-requires-token="true" data-redirect="https://medium.com/_/subscribe/user/cb55958ea3bb" data-action-source="post_header_lockup_follow"><span class="button-label  button-defaultState js-buttonLabel">Follow</span><span class="button-label button-activeState">Following</span></button></span><div class="postMetaInline u-noWrapWithEllipsis u-xs-normalWrap u-xs-lineClamp2">rohankapur.com</div><div class="postMetaInline js-testPostMetaInlineSupplemental"><time datetime="2017-04-13T08:27:31.153Z">Apr 13</time><span class="middotDivider u-fontSize12"></span><span class="readingTime" title="84 min read"></span></div></div></div></div></div></header><div class="postArticle-content js-postField js-notesSource js-trackedPost"  data-post-id="10300100899b" data-source="post_page" data-collection-id="bb87da25612c" data-tracking-context="postPage"><section name="20d1" class="section section--body section--first"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--fullWidth"><figure name="be8e" id="be8e" class="graf graf--figure graf--layoutFillWidth graf--leading"><div class="aspectRatioPlaceholder is-locked"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 45.4%;"></div><img class="graf-image" data-image-id="1*khIKl9t4XmZGSsKhW_Yg2w.png" data-width="2000" data-height="907" src="https://cdn-images-1.medium.com/max/2000/1*khIKl9t4XmZGSsKhW_Yg2w.png"></div><figcaption class="imageCaption">Sequences upon sequences upon sequences. Sequen-ception.</figcaption></figure></div><div class="section-inner sectionLayout--insetColumn"><h1 name="84b9" id="84b9" class="graf graf--h3 graf-after--figure graf--title">Rohan &amp; Lenny #3: Recurrent Neural Networks &amp; LSTMs</h1><h2 name="9485" id="9485" class="graf graf--h4 graf-after--h3 graf--trailing graf--subtitle">The ultimate guide to machine learning’s favorite child.</h2></div></div></section><section name="4110" class="section section--body"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><blockquote name="4569" id="4569" class="graf graf--pullquote graf--leading graf--trailing">This is the third group (<a href="https://medium.com/@lennykhazan" data-href="https://medium.com/@lennykhazan" data-anchor-type="2" data-user-id="de8e2540b759" data-action-value="de8e2540b759" data-action="show-user-card" data-action-type="hover" class="markup--user markup--pullquote-user" target="_blank">Lenny</a> and <a href="https://medium.com/@mckapur" data-href="https://medium.com/@mckapur" data-anchor-type="2" data-user-id="cb55958ea3bb" data-action-value="cb55958ea3bb" data-action="show-user-card" data-action-type="hover" class="markup--user markup--pullquote-user" target="_blank">Rohan</a>) entry in our <a href="https://medium.com/a-year-of-artificial-intelligence" data-href="https://medium.com/a-year-of-artificial-intelligence" class="markup--anchor markup--pullquote-anchor" target="_blank">journey</a> to extend our knowledge of artificial intelligence and convey that knowledge in a simple, fun, and accessible manner. Learn more about our motives in this <a href="https://medium.com/a-year-of-artificial-intelligence/0-2016-is-the-year-i-venture-into-artificial-intelligence-d702d65eb919#.bfjoaqxu5" data-href="https://medium.com/a-year-of-artificial-intelligence/0-2016-is-the-year-i-venture-into-artificial-intelligence-d702d65eb919#.bfjoaqxu5" class="markup--anchor markup--pullquote-anchor" target="_blank">introduction</a> post.</blockquote></div></div></section><section name="6590" class="section section--body"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><p name="a16a" id="a16a" class="graf graf--p graf--hasDropCapModel graf--hasDropCap graf--leading"><span class="graf-dropCap">It</span> seems like most of our posts on this blog start with “We’re back!”, so… you know the drill. It’s been a while since our last post — just over 5 months — but it certainly doesn’t feel that way. Whether our articles are more spaced out than we’d like them to be, well, we haven’t actually discussed that yet. But I, Rohan, would definitely like to get into a more frequent routine. Since November, we’ve been grinding on school (basically, getting it over and done with), banging out <a href="http://getcontra.com" data-href="http://getcontra.com" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Contra</a> v2, and lazing around more than we should. End of senior year is a fun time.</p><figure name="ac01" id="ac01" class="graf graf--figure graf--layoutOutsetLeft graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 525px; max-height: 358px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 68.2%;"></div><img class="graf-image" data-image-id="1*N5l-Uj5yUVPIDwIuGUvnfg.jpeg" data-width="570" data-height="389" data-action="zoom" data-action-value="1*N5l-Uj5yUVPIDwIuGUvnfg.jpeg" src="https://cdn-images-1.medium.com/max/600/1*N5l-Uj5yUVPIDwIuGUvnfg.jpeg"></div></figure><p name="93ae" id="93ae" class="graf graf--p graf-after--figure">It’s 2017. We started A <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">Year</em></strong><em class="markup--em markup--p-em"> </em>Of AI in 2016. Last year. Don’t panic, though. If you’ve read our <a href="https://ayearofai.com/the-goal-of-our-blog-c104d7b6377a#.75o5qyayi" data-href="https://ayearofai.com/the-goal-of-our-blog-c104d7b6377a#.75o5qyayi" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">letter</a>, you’ll know that, despite our name and inception date, we’re not going anywhere anytime soon. There’s a good chance we’ll move off Medium, but we’re still both obsessed with AI and writing these posts to hopefully make other people obsessed, as well.</p><p name="f6d1" id="f6d1" class="graf graf--p graf-after--p">I wrote the first article on this blog just over a year ago, and mentioned that my goal for the year was to be accepted into Stanford University as an undergrad student. A few months ago, I achieved this goal. At Stanford, I’ll probably be studying <a href="https://symsys.stanford.edu/" data-href="https://symsys.stanford.edu/" class="markup--anchor markup--p-anchor" rel="noopener nofollow noopener" target="_blank">Symbolic Systems</a>, which is a program that explores both the humanities and STEM to inform an understanding of artificial intelligence and the nature of minds. Needless to say, A Year of AI will continue to document the new things I learn 😀.</p><p name="b842" id="b842" class="graf graf--p graf-after--p">Anyways, you can find plenty of articles on recurrent neural networks (RNNs) online. My <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/" data-href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">favorite one</a>, personally, is from Andrej Karpathy’s blog. I read it about 1.5 years ago when I was learning about RNNs. We definitely think there’s space to simplify the topic even more, though. As usual, that’s our aim for the article — to teach you RNNs in a fun, simple manner. We’re also importantly doing this for completion purposes; we want people to hop onto A Year of AI and be able to work their way up all the way from logistic regression to neural machine translation (don’t worry, you’ll find out what means soon enough), and thus recurrent neural networks is a vital addition. After this, we want to look at and summarize/simplify a bunch of new super interesting research papers, and for most of them RNNs are a key ingredient. Finally, we think this article contains so much meat and ties together content unlike any other RNN tutorial on the interwebs.</p><p name="60e2" id="60e2" class="graf graf--p graf-after--p">Before we get started, you should try to familiarize yourself with “vanilla” neural networks. If you need a refresher, check out our <a href="https://ayearofai.com/rohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d#.quwnoqtot" data-href="https://ayearofai.com/rohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d#.quwnoqtot" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">neural networks and backpropogation mega-post</a> from earlier this year. This is so you know the basics of machine learning, linear algebra, neural network architecture, cost functions, optimization methods, training/test sets, activation functions/what they do, softmax, etc. Reading our article on <a href="https://ayearofai.com/rohan-lenny-2-convolutional-neural-networks-5f4cd480a60b#.ebwc0kf4z" data-href="https://ayearofai.com/rohan-lenny-2-convolutional-neural-networks-5f4cd480a60b#.ebwc0kf4z" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">convolutional neural networks</a> may also make you more comfortable entering this post, especially because we often reference CNNs. Checking out <a href="https://ayearofai.com/rohan-4-the-vanishing-gradient-problem-ec68f76ffb9b#.58nevwsoa" data-href="https://ayearofai.com/rohan-4-the-vanishing-gradient-problem-ec68f76ffb9b#.58nevwsoa" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">this</a> article I wrote on vanishing gradients will help later on, as well.</p><p name="81ec" id="81ec" class="graf graf--p graf-after--p">Rule of thumb: the more you know, the better!</p><h3 name="3288" id="3288" class="graf graf--h3 graf-after--p">Table of Contents</h3><p name="003f" id="003f" class="graf graf--p graf-after--h3">I can’t link to each section, but here’s what we cover in this article (save the intro and conclusion):</p><ol class="postList"><li name="8cfa" id="8cfa" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">What can RNNs do? </strong>Where we look at… what RNNs can do!</li><li name="7f98" id="7f98" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Why? </strong>Where we talk about the gap that RNNs fill in machine learning’s suite of algorithms.</li><li name="de84" id="de84" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Show me. </strong>Where we visualize RNNs for the first time.</li><li name="f2f4" id="f2f4" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Formalism. </strong>Where we walk through how an RNN mathematically works with proper notation.</li><li name="c163" id="c163" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">An example? Okay! </strong>Where we walk through, qualitatively, a simple application of RNNs and how the RNN operates in this application, including techniques we can use.</li><li name="394e" id="394e" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Training (or, why vanilla RNNs suck.) </strong>Where we talk about how to train RNNs, and why vanilla RNNs are bad at learning.</li><li name="5756" id="5756" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Fixing the problem with LSTMs (Part I). </strong>Where we introduce the solution to vanilla RNNs’ inability to learn: LSTMs.</li><li name="654d" id="654d" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Fixing the problem with LSTMs (Part II). </strong>Where we analyze on a close, technical level, the reasons LSTMs don’t suffer from vanishing gradients as much (and why they still do, to an extent). Then we conclude LSTMs with final thoughts on and facts about them.</li><li name="2dfc" id="2dfc" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Yay RNNs! </strong>Where <em class="markup--em markup--li-em">you </em>get to see neat little things RNNs have done!</li><li name="e56c" id="e56c" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">In Practice. </strong>Where we look at more technical and important applications and case studies of RNNs, including other variations of RNNs, especially as relevant in hot/recent research papers.</li><li name="7bc0" id="7bc0" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Building a Vanilla Recurrent Neural Network. </strong>Where you get to code your very first RNN! Woohoo!</li></ol><h3 name="02a8" id="02a8" class="graf graf--h3 graf-after--li">What can RNNs do?</h3><p name="1c1d" id="1c1d" class="graf graf--p graf-after--h3">There are a number of very important tasks that ANNs and CNNs cannot solve, that RNNs are used for instead. Tasks like: image captioning, language translation, sentiment classification, predictive typing, video classification, natural language processing, speech recognition, and a <em class="markup--em markup--p-em">lot </em>more interesting things that have been presented in recent research papers (for example… <a href="https://arxiv.org/pdf/1606.04474.pdf" data-href="https://arxiv.org/pdf/1606.04474.pdf" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">learning to learn by gradient descent by gradient descent</a>!).</p><figure name="1d0f" id="1d0f" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 393px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 56.10000000000001%;"></div><img class="graf-image" data-image-id="1*X5dk-xGw2yNYsEB3QvHWIA.png" data-width="713" data-height="400" data-action="zoom" data-action-value="1*X5dk-xGw2yNYsEB3QvHWIA.png" src="https://cdn-images-1.medium.com/max/800/1*X5dk-xGw2yNYsEB3QvHWIA.png"></div><figcaption class="imageCaption">Image captioning, taken from CS231n slides: <a href="http://cs231n.stanford.edu/slides/2016/winter1516_lecture10.pdf" data-href="http://cs231n.stanford.edu/slides/2016/winter1516_lecture10.pdf" class="markup--anchor markup--figure-anchor" rel="nofollow noopener noopener noopener" target="_blank">http://cs231n.stanford.edu/slides/2016/winter1516_lecture10.pdf</a></figcaption></figure><p name="02ef" id="02ef" class="graf graf--p graf-after--figure">RNNs are very powerful. Y’know how regular neural networks have been proved to be “universal function approximators” ? If you didn’t:</p><blockquote name="8206" id="8206" class="graf graf--blockquote graf-after--p">In the mathematical theory of artificial neural networks, the universal approximation theorem states that a feed-forward network with a single hidden layer containing a finite number of neurons can approximate continuous functions on compact subsets of R^n, under mild assumptions on the activation function.</blockquote><p name="314e" id="314e" class="graf graf--p graf-after--blockquote">That’s pretty confusing. Basically, what this states is that an artificial neural network can compute any function. Even if someone gives you an extremely wiggly, complex looking function, it’s guaranteed that there exists a neural network that can produce (or at least extremely closely approximate) it. The proof itself is very complex, but <a href="http://neuralnetworksanddeeplearning.com/chap4.html" data-href="http://neuralnetworksanddeeplearning.com/chap4.html" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">this</a> is a brilliant article offering a visual approach as to why<em class="markup--em markup--p-em"> </em>it’s true.</p><p name="a351" id="a351" class="graf graf--p graf-after--p">So, that’s great. ANNs are universal function approximators. RNNs take it a step further, though; <a href="http://stats.stackexchange.com/a/221142/98975" data-href="http://stats.stackexchange.com/a/221142/98975" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">they can compute/describe <em class="markup--em markup--p-em">programs</em></a>. In fact, some RNNs with proper weights and architecture qualify as Turing Complete:</p><blockquote name="8f10" id="8f10" class="graf graf--blockquote graf-after--p">A Turing Complete system means a system in which a program can be written that will find an answer (although with no guarantees regarding runtime or memory).</blockquote><blockquote name="c6a2" id="c6a2" class="graf graf--blockquote graf-after--blockquote">So, if somebody says “my new thing is Turing Complete” that means in principle (although <strong class="markup--strong markup--blockquote-strong">often not in practice</strong>) it could be used to solve any computation problem.</blockquote><blockquote name="1429" id="1429" class="graf graf--blockquote graf-after--blockquote">— <a href="http://stackoverflow.com/a/7320/1260708" data-href="http://stackoverflow.com/a/7320/1260708" class="markup--anchor markup--blockquote-anchor" rel="nofollow noopener noopener" target="_blank">http://stackoverflow.com/a/7320/1260708</a></blockquote><p name="2ba5" id="2ba5" class="graf graf--p graf-after--blockquote">That’s cool, isn’t it? Now, this is all theoretical, and in practice means less than you think, so don’t get too hyped. Hopefully, though, this gives some more insight into why RNNs are super important for future developments in machine learning — and why you should read on.</p><p name="662c" id="662c" class="graf graf--p graf-after--p">At this point, if you weren’t previously hooked on learning what the heck these things are, you should be now. (If you still aren’t, just bare with me. Things will get spicy soon.) So, let’s dive in.</p><h3 name="72f6" id="72f6" class="graf graf--h3 graf-after--p">Why?</h3><p name="54dc" id="54dc" class="graf graf--p graf-after--h3">We took a bit of a detour to talk about how great RNNs are, but haven’t focused on <em class="markup--em markup--p-em">why</em> ANNs can’t perform well in the tasks that RNNs can.</p><blockquote name="b88b" id="b88b" class="graf graf--blockquote graf-after--p">Why do we need another neural network model? Why do we need recurrent neural networks when we already have the beloved ANNs (and CNNs) in all their glory?</blockquote><p name="95c9" id="95c9" class="graf graf--p graf-after--blockquote">It boils down to a few things:</p><ul class="postList"><li name="3d40" id="3d40" class="graf graf--li graf-after--p">ANNs can’t deal with sequential or “temporal” data</li><li name="2a22" id="2a22" class="graf graf--li graf-after--li">ANNs lack memory</li><li name="0d57" id="0d57" class="graf graf--li graf-after--li">ANNs have a fixed architecture</li><li name="fbb9" id="fbb9" class="graf graf--li graf-after--li">RNNs are more “biologically realistic” because of the recurrent connectivity found in the visual cortex of the brain</li></ul><p name="341a" id="341a" class="graf graf--p graf-after--li">Let’s address the first three points individually. The first issue refers to the fact that ANNs have a <em class="markup--em markup--p-em">fixed input size </em>and a <em class="markup--em markup--p-em">fixed output size</em>. ANNs have an elaborate list of hyperparameters, and this notably includes the number of neurons in the input layer and output layer. But what if we wanted input data and/or output data of <em class="markup--em markup--p-em">variable</em> size, instead of something that needs to have its size as a preset constant? RNNs allow us to do that. In this aspect, they offer more flexibility than ANNs.</p><figure name="605a" id="605a" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 500px; max-height: 309px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 61.8%;"></div><img class="graf-image" data-image-id="1*BQ0SxdqC9Pl_3ZQtd3e45A.png" data-width="500" data-height="309" src="https://cdn-images-1.medium.com/max/800/1*BQ0SxdqC9Pl_3ZQtd3e45A.png"></div><figcaption class="imageCaption">We might choose this architecture for our ANN, with 4 inputs and 1 output. But that’s it — we can’t input a vector with 5 values, for example. <a href="https://qph.ec.quoracdn.net/main-qimg-050d12c5da82f4f97fdd942d7777b8e4" data-href="https://qph.ec.quoracdn.net/main-qimg-050d12c5da82f4f97fdd942d7777b8e4" class="markup--anchor markup--figure-anchor" rel="nofollow noopener noopener" target="_blank">https://qph.ec.quoracdn.net/main-qimg-050d12c5da82f4f97fdd942d7777b8e4</a>.</figcaption></figure><p name="1c8e" id="1c8e" class="graf graf--p graf-after--figure">I’ll give you a couple examples of why this matters.</p><p name="e228" id="e228" class="graf graf--p graf-after--p">It’s unclear how we could use an ANN by itself to perform a task like image captioning, because the network would need to output a sentence — a list of words in a specific order — which is a<em class="markup--em markup--p-em"> sequence</em>.<em class="markup--em markup--p-em"> </em>It would be a sequence of vectors, because each word would need to be represented numerically. In machine learning and data science, we represent words numerically as vectors; these are called word embeddings. An ANN can only output a <em class="markup--em markup--p-em">single </em>word/label, like in image classification where we treat the output as the label with the highest value in the final vector that is a softmax probability distribution over all classes. The only way to make sentences work with ANNs would be to have billions of output neurons that each map to a single possible sentence in the permutation of all [sensible] sentences that can be formed by the vocabulary we have. And that doesn’t sound like a good idea.</p><figure name="8f28" id="8f28" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 305px; max-height: 182px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 59.699999999999996%;"></div><img class="graf-image" data-image-id="1*GFVoFpD6cdCY_PGqnjhOlQ.png" data-width="305" data-height="182" src="https://cdn-images-1.medium.com/max/800/1*GFVoFpD6cdCY_PGqnjhOlQ.png"></div><figcaption class="imageCaption">A reminder of what the output of an ANN looks like — a probability distribution over classes — and how we convert that into a single final result (one-hot encoding): by taking the label with the greatest probability and making it 1, with the rest 0.</figcaption></figure><p name="f8ec" id="f8ec" class="graf graf--p graf-after--figure">Wow, that was a lot of words. Nevertheless, I hope it’s clear that, with ANNs, there’s no feasible way to output a sequence.</p><p name="f308" id="f308" class="graf graf--p graf-after--p">Now, what about <em class="markup--em markup--p-em">inputting </em>a sequence into an ANN? In other words, “temporal” data: data that varies over time, and is thus a sequence. Take the example of sentiment classification where we input a sentence (sequence of words = sequence of vectors = sequence of set of values where each value goes into an individual neuron) and want to output its sentiment: positive or negative. The output part seems easy, because it’s just one neuron that’s either rounded to 1 (positive) or 0 (negative). And, for the input, you might be thinking: couldn’t we input each “set of values” separately? Input the first word, wait for the neural net to fully feed forward and produce an output, then input the next word, etc. etc.</p><p name="8029" id="8029" class="graf graf--p graf-after--p">Let’s take the case of this utterly false, and most certainly negative sentence, to evaluate:</p><figure name="6e25" id="6e25" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 625px; max-height: 139px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 22.2%;"></div><img class="graf-image" data-image-id="1*yq_zmka1ssikrmD9GkWmnw.png" data-width="625" data-height="139" src="https://cdn-images-1.medium.com/max/800/1*yq_zmka1ssikrmD9GkWmnw.png"></div><figcaption class="imageCaption">This is just an alternative fact, believe me! Lenny is actually a <em class="markup--em markup--figure-em">great</em><strong class="markup--strong markup--figure-strong"><em class="markup--em markup--figure-em"> </em></strong><em class="markup--em markup--figure-em">coder. The best I know of. The best.</em></figcaption></figure><p name="322d" id="322d" class="graf graf--p graf-after--figure">We’d input “Lenny” first, then “Khazan”, then “is”, etc. But, at each feedforward iteration, the output would be completely useless. Why? Because the output would be dependent on <em class="markup--em markup--p-em">only </em>that word. We’d be finding the sentiment of a single word, which is useless, because we want the sentiment of the entire sentence. Sentiment analysis only makes sense when all the words come together, dependent on each other, to form a sentence.</p><p name="13ff" id="13ff" class="graf graf--p graf-after--p">Think of it this way — this means you’re essentially running a neural network a bunch of times, just with new data at each separate iteration. Those run-throughs aren’t linked in any way; they’re independent. Once you feedforward and fully run the neural network, it forgets everything it just did. This sentence only makes sense and can only be interpretable because it’s a collection of words put together in a specific order to form <em class="markup--em markup--p-em">meaning</em>. The relevance of each word is dependent on the words that precede it: the context. This is why RNNs are being used heavily in NLP; they retain context by having <em class="markup--em markup--p-em">memory</em>. ANNs have no memory.</p><p name="e71f" id="e71f" class="graf graf--p graf-after--p">I like this quote from another article on RNNs:</p><blockquote name="9cd3" id="9cd3" class="graf graf--blockquote graf-after--p">Humans don’t start their thinking from scratch every second. As you read this essay, you understand each word based on your understanding of previous words. You don’t throw everything away and start thinking from scratch again. Your thoughts have persistence.</blockquote><blockquote name="b760" id="b760" class="graf graf--blockquote graf-after--blockquote">— <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" data-href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" class="markup--anchor markup--blockquote-anchor" rel="nofollow noopener noopener" target="_blank">http://colah.github.io/posts/2015–08-Understanding-LSTMs/</a></blockquote><p name="e0e4" id="e0e4" class="graf graf--p graf-after--blockquote">(Furthermore, take the case where we had sequential data in <em class="markup--em markup--p-em">both </em>the input and the output. Translating one language to another is a good example of this. Clearly, ANNs aren’t the answer.)</p><p name="fa2f" id="fa2f" class="graf graf--p graf-after--p">RNNs don’t just need memory; they need <em class="markup--em markup--p-em">long term</em> memory. Let’s take the example of predictive typing. Let’s say we typed the following sentence in an SMS message to 911, and the operating system needs to fill in the blank:</p><figure name="2c67" id="2c67" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 543px; max-height: 90px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 16.6%;"></div><img class="graf-image" data-image-id="1*TugitPvwm_IZqAdAPR-7UA.png" data-width="543" data-height="90" src="https://cdn-images-1.medium.com/max/800/1*TugitPvwm_IZqAdAPR-7UA.png"></div></figure><figure name="3077" id="3077" class="graf graf--figure graf-after--figure"><div class="aspectRatioPlaceholder is-locked" style="max-width: 250px; max-height: 250px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 100%;"></div><img class="graf-image" data-image-id="1*7l0aNgpXDXZnY-P9C8K4IA.jpeg" data-width="250" data-height="250" src="https://cdn-images-1.medium.com/max/800/1*7l0aNgpXDXZnY-P9C8K4IA.jpeg"></div><figcaption class="imageCaption">The face of a criminal?</figcaption></figure><p name="325c" id="325c" class="graf graf--p graf-after--figure">Here, if the RNN wasn’t able to look back much (ie. before “should”), then many different options could arise:</p><figure name="da28" id="da28" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 552px; max-height: 266px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 48.199999999999996%;"></div><img class="graf-image" data-image-id="1*aMJu60wscb9m4A-Zn_xyqQ.png" data-width="552" data-height="266" src="https://cdn-images-1.medium.com/max/800/1*aMJu60wscb9m4A-Zn_xyqQ.png"></div><figcaption class="imageCaption">Lenny in the military? Make it into a TV show! I’d watch it.</figcaption></figure><p name="930e" id="930e" class="graf graf--p graf-after--figure">The word “sent” would indicate to the RNN that a location needs to be outputted. However, if the RNN was able to retain information from all the way back, such as the word “criminal”, then it would be much more confident that:</p><figure name="0990" id="0990" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 557px; max-height: 102px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 18.3%;"></div><img class="graf-image" data-image-id="1*4CZskdiGqIx29BQylqnYuA.png" data-width="557" data-height="102" src="https://cdn-images-1.medium.com/max/800/1*4CZskdiGqIx29BQylqnYuA.png"></div></figure><p name="c9f1" id="c9f1" class="graf graf--p graf-after--figure">The probability of outputting “jail” drastically increases when it sees the word “criminal” is present. That’s why context matters, be it predictive typing, image captioning, machine translation, etc. The output or outputs of a recurrent neural network will always be functionally dependent on (meaning, a function of) information from the very beginning, but how much it chooses to “forget” or “retain” (that is, varying degrees of influence from earlier information) depends on the weights that it learns from the training data.</p><p name="eb76" id="eb76" class="graf graf--p graf-after--p">As it turns out, RNNs — especially deep ones — are rarely good at retaining much information, due to an issue called the vanishing gradient problem. That’s where we turn to other variants of RNNs such as LSTMs and GRUs. But, more on that later.</p><p name="c00c" id="c00c" class="graf graf--p graf-after--p">To address the third point, one more constraint with ANNs is that they have a fixed number of computation/processing steps (because, once again, the number of hidden layers is a hyperparameter). With RNNs, we can have much more dynamic processing since we operate over vectors. Each neuron in an RNN is almost like an entire layer in an ANN; this will make more sense as we bring up an illustration for you. <strong class="markup--strong markup--p-strong">Exciting stuff.</strong></p><h3 name="4abf" id="4abf" class="graf graf--h3 graf-after--p">Show me.</h3><p name="0160" id="0160" class="graf graf--p graf-after--h3">OK, that’s enough teasing. Three sections into the article, and you’re yet to see what an RNN looks like, or appreciate how it really works. Everything comes in due time, though!</p><p name="d391" id="d391" class="graf graf--p graf-after--p">The first thing I’m going to do is show you what a normal ANN diagram looks like:</p><figure name="ce30" id="ce30" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 310px; max-height: 220px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 71%;"></div><img class="graf-image" data-image-id="1*GapzcZDrwnVbflhlRoWZ9g.png" data-width="310" data-height="220" src="https://cdn-images-1.medium.com/max/800/1*GapzcZDrwnVbflhlRoWZ9g.png"></div></figure><p name="deda" id="deda" class="graf graf--p graf-after--figure">Each neuron stores a single scalar value. Thus, each layer can be considered a vector.</p><p name="f077" id="f077" class="graf graf--p graf-after--p">Now I’m going to show you what this ANN looks like in our RNN visual notation:</p><figure name="c5e9" id="c5e9" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 300px; max-height: 65px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 21.7%;"></div><img class="graf-image" data-image-id="1*ntKLnv52DCUnkseNcm91iQ.png" data-width="300" data-height="65" src="https://cdn-images-1.medium.com/max/800/1*ntKLnv52DCUnkseNcm91iQ.png"></div></figure><p name="f1fc" id="f1fc" class="graf graf--p graf-after--figure">The two diagrams above represent the same thing. The latter, obviously, looks more succinct than the former. That’s because, with our RNN visual notation, each neuron (inputs, hidden(s), and outputs) contains a <strong class="markup--strong markup--p-strong">vector </strong>of information. The term “cell” is also used, and is interchangeable with neuron. (I’ll use the latter instead of the former.) Red is the input neuron, blue is the hidden neuron, and green is the output neuron. Therefore, an entire ANN layer is encapsulated into one neuron with our RNN illustration. All operations in RNNs, like the mapping from one neuron’s state to another, are over entire vectors, compared to individual scalars that are summed up with ANNs.</p><p name="abee" id="abee" class="graf graf--p graf-after--p">Let’s flip it the other way:</p><figure name="535f" id="535f" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 50px; max-height: 231px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 462%;"></div><img class="graf-image" data-image-id="1*HewvhrMCcdy-oQcpeeNJ9w.png" data-width="50" data-height="231" src="https://cdn-images-1.medium.com/max/800/1*HewvhrMCcdy-oQcpeeNJ9w.png"></div></figure><p name="8a15" id="8a15" class="graf graf--p graf-after--figure">This is in fact a type of recurrent neural network — a <strong class="markup--strong markup--p-strong">one to one</strong> recurrent net, because it maps one input to one output. A one to one recurrent net is equivalent to an artificial neural net.</p><p name="b960" id="b960" class="graf graf--p graf-after--p">We can have a one to <em class="markup--em markup--p-em">many</em> recurrent net, where one input is mapped to multiple outputs. An example of this would be image captioning — the input would be the image in some processed form (usually the result of a CNN analyzing the image), and the output would be a sequence of words. Such an RNN may look like this:</p><figure name="7fff" id="7fff" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 190px; max-height: 230px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 121.10000000000001%;"></div><img class="graf-image" data-image-id="1*-Jv3TxauJBwBgWwjoe_UkA.png" data-width="190" data-height="230" src="https://cdn-images-1.medium.com/max/800/1*-Jv3TxauJBwBgWwjoe_UkA.png"></div><figcaption class="imageCaption">Changed the shades of the green nodes… hope that’s OK!</figcaption></figure><p name="ea3c" id="ea3c" class="graf graf--p graf-after--figure">This may be confusing at first, so I’m going to make sure I walk slowly through it. On the x-axis we have time, and on the y-axis we have depth/layers:</p><figure name="c4eb" id="c4eb" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 323px; max-height: 287px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 88.9%;"></div><img class="graf-image" data-image-id="1*OEyIsiEi5SJ9l3GrB5DpuA.png" data-width="323" data-height="287" src="https://cdn-images-1.medium.com/max/800/1*OEyIsiEi5SJ9l3GrB5DpuA.png"></div></figure><p name="22b2" id="22b2" class="graf graf--p graf-after--figure">When I refer to “time” on the x-axis, I’m referring to the order at which these operations occur. Time could also be literal for temporal data, where the input is a sequence. When I say “depth” on the y-axis, I’m referring to the mapping from the input layer, to the hidden layer(s), to the output layer, where layer number and thus depth increases.</p><p name="f94b" id="f94b" class="graf graf--p graf-after--p">It may look like we have seven neurons now, but we still have three: one input neuron, one hidden neuron, and one output neuron. The difference is that these neurons now experience multiple “timesteps” where they take on different values, which are, again, vectors. The input neuron in our example above doesn’t, because it’s not representing sequential data (one to many), but for other architectures it could.</p><p name="e200" id="e200" class="graf graf--p graf-after--p">The hidden neuron will take on the vector value <strong class="markup--strong markup--p-strong">h_1</strong> first, then <strong class="markup--strong markup--p-strong">h_2</strong>, and finally <strong class="markup--strong markup--p-strong">h_3</strong>. At each timestep, the hidden neuron’s vector <strong class="markup--strong markup--p-strong">h_t </strong>is a function of the vector at the previous timestep <strong class="markup--strong markup--p-strong">h_t-1</strong>, except for <strong class="markup--strong markup--p-strong">h_1 </strong>which is dependent <em class="markup--em markup--p-em">only</em> on the input <strong class="markup--strong markup--p-strong">x_1</strong>. In the diagram above, each hidden vector then gives rise to an output <strong class="markup--strong markup--p-strong">y_t</strong>, and this is how we map one input to multiple outputs. You can visualize these functional dependencies with the arrows, which illustrates flow of information in the network.</p><p name="64eb" id="64eb" class="graf graf--p graf-after--p">As we progress on the x-axis, the current timestep increases. As we progress on the y-axis, the neuron in question changes. Each point on this graph thus represents one neuron — be it input, hidden, or output — at some timestep, being fed information from a neuron (be it itself or another) at the previous timestep.</p><p name="5402" id="5402" class="graf graf--p graf-after--p">The RNN would execute like so:</p><ol class="postList"><li name="3623" id="3623" class="graf graf--li graf-after--p">Input <strong class="markup--strong markup--li-strong">x_1</strong></li><li name="1923" id="1923" class="graf graf--li graf-after--li">Compute <strong class="markup--strong markup--li-strong">h_1</strong> based on <strong class="markup--strong markup--li-strong">x_1</strong> (the arrow implies functional dependency)</li><li name="00ff" id="00ff" class="graf graf--li graf-after--li">Compute <strong class="markup--strong markup--li-strong">h_2</strong> based on <strong class="markup--strong markup--li-strong">h_1</strong></li><li name="ca4c" id="ca4c" class="graf graf--li graf-after--li">Compute <strong class="markup--strong markup--li-strong">h_3</strong> based on <strong class="markup--strong markup--li-strong">h_2</strong></li><li name="9fac" id="9fac" class="graf graf--li graf-after--li">Compute <strong class="markup--strong markup--li-strong">y_1</strong> based on <strong class="markup--strong markup--li-strong">h_1</strong></li><li name="649b" id="649b" class="graf graf--li graf-after--li">Compute <strong class="markup--strong markup--li-strong">y_2</strong> based on <strong class="markup--strong markup--li-strong">h_2</strong></li><li name="ba63" id="ba63" class="graf graf--li graf-after--li">Compute <strong class="markup--strong markup--li-strong">y_3</strong> based on <strong class="markup--strong markup--li-strong">h_3</strong></li></ol><p name="bba5" id="bba5" class="graf graf--p graf-after--li">You could compute <strong class="markup--strong markup--p-strong">y_t </strong>either immediately after <strong class="markup--strong markup--p-strong">h_t </strong>has been computed, or, like above, compute all outputs once all hidden states have been computed. I’m not entirely sure which is more common in practice.</p><p name="6c68" id="6c68" class="graf graf--p graf-after--p">This allows for more complex and interesting networks than ANNs because we can have as many timesteps as we want.</p><p name="e78f" id="e78f" class="graf graf--p graf-after--p">The value of the output neuron at each timestep represents a word in the sentence, in the order the sentence will be constructed. The caption this RNN produces is hence 3 words long. (It’s actually 2, because the RNN would need to output a period or &lt;END&gt; marker at the final timestep, but we’ll get into that later.)</p><p name="a009" id="a009" class="graf graf--p graf-after--p">In case you don’t understand yet exactly <em class="markup--em markup--p-em">why </em>RNNs work, I’ll walk through how these functional dependencies come to fruition when you apply it to a one to many scenario such as image captioning.</p><figure name="2856" id="2856" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 422px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 60.199999999999996%;"></div><img class="graf-image" data-image-id="1*iBtLegQFwfsqWZpTVAjrEw.jpeg" data-width="1298" data-height="782" data-action="zoom" data-action-value="1*iBtLegQFwfsqWZpTVAjrEw.jpeg" src="https://cdn-images-1.medium.com/max/800/1*iBtLegQFwfsqWZpTVAjrEw.jpeg"></div><figcaption class="imageCaption">Lenny and I on student scholarship at WWDC 2013. Good times!</figcaption></figure><p name="6b74" id="6b74" class="graf graf--p graf-after--figure">When you combine an RNN and CNN, you — in practice — get an “LCRN”. The architecture for LCRNs are more complex than what I’m going to present in the next paragraph; rather, I’m going to simplify it to convey my point. We’ll actually get fully into how they work later.</p><p name="e029" id="e029" class="graf graf--p graf-after--p">Imagine an RNN tries to caption this image. An accurate result might be:</p><blockquote name="f14e" id="f14e" class="graf graf--pullquote graf-after--p">Two people happily posing for a photo inside a building.</blockquote><p name="482a" id="482a" class="graf graf--p graf-after--pullquote">The input to the RNN would be the output of a CNN that processes this image. (However, to be pedantic, it would be the output of the CNN without a classification/softmax layer — that is, pulled from the final fully connected layer.) The CNN might pick up on the fact that there are two primary human face-like objects present in the image, which, paired with what the RNN has learned via training, may induce the first hidden state<strong class="markup--strong markup--p-strong">¹</strong> of the recurrent neural network to be one where the most likely candidate word is “two”.</p><p name="f5ec" id="f5ec" class="graf graf--p graf-after--p">Pro-tip<strong class="markup--strong markup--p-strong">¹</strong>: The term “hidden state” refers to the vector of a hidden neuron at a given timestep. “First hidden state” refers to the hidden state at timestep 1.</p><p name="10db" id="10db" class="graf graf--p graf-after--p">The first output, which represents the word “two”, was functionally dependent on the first hidden state, which in itself was a function of the input to the RNN. Thus, “two” was ultimately determined from the information that the CNN gave us and the experience/weights of the RNN. Now, the second word, “people”, is functionally dependent on the second hidden state. However, note that the second hidden state is just a function of the <em class="markup--em markup--p-em">first </em>hidden state. This means that the word “people” was the most likely candidate given the hidden state where “two” was likely. In other words, the RNN recognized that, given the word “two”, the word “people” should be next, based on the RNN’s experience from training and the initial image [analysis] we inputted.</p><p name="b004" id="b004" class="graf graf--p graf-after--p">The same will occur for every following word; the nth word will be based on the nth hidden state, which, ultimately, is a function of every hidden state before it, and thus could be interpreted purely as an extremely complex and layered function of the input. The weights do the heavy lifting by making sense of all this information and deducing an output from it.</p><p name="d51d" id="d51d" class="graf graf--p graf-after--p">To put it bluntly, you can boil down what the RNN is “thinking” to this:</p><blockquote name="4a94" id="4a94" class="graf graf--blockquote graf-after--p">Based on what I’ve seen from the input, based on the current timestep I’m at, and based on what I know from all my training, I need to output: <strong class="markup--strong markup--blockquote-strong">“x”.</strong></blockquote><p name="a918" id="a918" class="graf graf--p graf-after--blockquote">Thus, each outputted word is dependent on the words before it, all the way back to the input image data. However, this relationship is indirect. It’s indirect because the outputs are only dependent on the hidden states, <em class="markup--em markup--p-em">not </em>on each other (ie. the RNN doesn’t deduce “people” from “two”, it deduces “people”, partly, from the information — the hidden state — that <em class="markup--em markup--p-em">gave rise</em> to “two”). In LCRNs, though, this is explicit instead of implicit; we “sample” the output of one timestep by taking it and literally feeding it back as input into the next timestep. In a sense, LCRNs can hence be interpreted as having many to many architecture.</p><p name="cdc3" id="cdc3" class="graf graf--p graf-after--p">The exact quantitative relationships depend on the RNN’s weights. But, generally, this is the concept of memory in play. Creating a coherent sentence as we go along is only really possible if we can recall what we said before. And RNNs are able to do exactly that; they remember what they said before and figure out, based on their image captioning expertise, what from this is useful to continue accurately speaking.</p><blockquote name="0d89" id="0d89" class="graf graf--blockquote graf-after--p">Yep, I went to France for a holiday. And I actually learned to speak some &lt;wait, shit, what was the language again? oh yea, “France”…&gt; French!</blockquote><p name="5857" id="5857" class="graf graf--p graf-after--blockquote">Obviously, an RNN needs to be trained and have proper weights for this to all function properly. RNNs aren’t magic; they only work because trained networks identified and learned patterns in data during training time that they now look for during prediction.</p><p name="34be" id="34be" class="graf graf--p graf-after--p">Perhaps this was a bit over-explaining on my part, but hopefully I nailed down some important and core ideas about how RNNs function.</p><p name="05f6" id="05f6" class="graf graf--p graf-after--p">So far we’ve looked at one to one and one to many recurrent networks. We can also have <em class="markup--em markup--p-em">many to one</em>:</p><figure name="6c8d" id="6c8d" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 190px; max-height: 228px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 120%;"></div><img class="graf-image" data-image-id="1*KjYQyc-JD_zs5ERQypM9EA.png" data-width="190" data-height="228" src="https://cdn-images-1.medium.com/max/800/1*KjYQyc-JD_zs5ERQypM9EA.png"></div></figure><p name="da0a" id="da0a" class="graf graf--p graf-after--figure">With many to one (and many to many), the input is in the form of a sequence, and so the hidden states are functionally dependent on <em class="markup--em markup--p-em">both </em>the input at that timestep and the previous hidden state. This is different to one to many, where the hidden state after <strong class="markup--strong markup--p-strong">h_1 </strong>is only dependent on the previous hidden state. That’s why, in the image above, the second hidden state has two arrows directed at it.</p><p name="4715" id="4715" class="graf graf--p graf-after--p">Only one output exists in many to one architecture. An example application is sentiment classification, where the input is a sentence (sequence of words) and the output is a probability indicating that the inputted sentence was positive.</p><p name="a55f" id="a55f" class="graf graf--p graf-after--p">The final type of recurrent net is many to many, where both the input and output are sequential:</p><figure name="86b5" id="86b5" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 310px; max-height: 214px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 69%;"></div><img class="graf-image" data-image-id="1*MPpLCBI1J6r6VmsDm7G4_g.png" data-width="310" data-height="214" src="https://cdn-images-1.medium.com/max/800/1*MPpLCBI1J6r6VmsDm7G4_g.png"></div></figure><p name="7038" id="7038" class="graf graf--p graf-after--figure">A use case would be machine translation where a sequence of words in one language needs to be translated to a sequence of words in another.</p><p name="2538" id="2538" class="graf graf--p graf-after--p">We can also go deeper and have multiple hidden layers, and/or a greater number of timesteps:</p><figure name="7896" id="7896" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 313px; max-height: 305px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 97.39999999999999%;"></div><img class="graf-image" data-image-id="1*vfUWsAgW-c5hUntaPhb1aQ.png" data-width="313" data-height="305" src="https://cdn-images-1.medium.com/max/800/1*vfUWsAgW-c5hUntaPhb1aQ.png"></div><figcaption class="imageCaption">We’re getting deeper and deeper!</figcaption></figure><p name="2703" id="2703" class="graf graf--p graf-after--figure">Really, this could be considered as <em class="markup--em markup--p-em">multiple RNNs</em>. Technically, you can consider each “hidden layer” as an RNN itself, given each neuron operates on vectors and updates through time; in ANN context, that volume of operations would be considered an entire network. So this is like stacking RNNs on top of each other. However, in this article I’ll refer to it as multiple hidden layers; different papers and lecturers may take different approaches.</p><p name="a0dc" id="a0dc" class="graf graf--p graf-after--p">When we have many timesteps (usually hundreds) and multiple hidden layers, the architecture of the network becomes much more complex and interesting. One feature of this RNN, in particular, is that all the outputs, including the first, depend on not just the input up to that timestep, but all of the inputs. (You can see this because the green neuron is only introduced <em class="markup--em markup--p-em">after</em> the final input timestep.) If this RNN was to translate English to Chinese, the first word of translated Chinese isn’t just dependent on the first word of the inputted English; it’s dependent on the <em class="markup--em markup--p-em">entire </em>sentence.</p><p name="98d9" id="98d9" class="graf graf--p graf-after--p">One way to demonstrate why this matters is to use Google Translate:</p><figure name="af3c" id="af3c" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 680px; max-height: 145px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 21.3%;"></div><img class="graf-image" data-image-id="1*mptNrzbgaDT3YuQL-tpEOw.png" data-width="680" data-height="145" src="https://cdn-images-1.medium.com/max/800/1*mptNrzbgaDT3YuQL-tpEOw.png"></div><figcaption class="imageCaption">One of my favorite <strong class="markup--strong markup--figure-strong">Green Day</strong> lyrics, from the song “Fashion Victim” on WARNING:. Side-note: Based on my experience with Google Translate in Chinese class over the last 8 years, this translation is <em class="markup--em markup--figure-em">probably </em>off.</figcaption></figure><p name="3229" id="3229" class="graf graf--p graf-after--figure">Now I’ll input “He’s a victim” and “of his own time” separately. You’ll notice that when you join the two translated outputs, this won’t be equal to the corresponding phrase in the first translation:</p><figure name="aef0" id="aef0" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 276px; max-height: 245px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 88.8%;"></div><img class="graf-image" data-image-id="1*lO5oCsSXTy4Loic3SASMlw.png" data-width="276" data-height="245" src="https://cdn-images-1.medium.com/max/800/1*lO5oCsSXTy4Loic3SASMlw.png"></div><figcaption class="imageCaption">What happens if we break up the English into different parts, translate, and join together the translated Chinese parts?</figcaption></figure><figure name="44aa" id="44aa" class="graf graf--figure graf-after--figure"><div class="aspectRatioPlaceholder is-locked" style="max-width: 551px; max-height: 110px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 20%;"></div><img class="graf-image" data-image-id="1*oJRFlstyAI-MkBguW6otfA.png" data-width="551" data-height="110" src="https://cdn-images-1.medium.com/max/800/1*oJRFlstyAI-MkBguW6otfA.png"></div><figcaption class="imageCaption"><strong class="markup--strong markup--figure-strong">They’re not equal.</strong></figcaption></figure><p name="a661" id="a661" class="graf graf--p graf-after--figure">What gives? Well, the way sentences are constructed in languages can differ in varying scenarios. Some words in English may also map to multiple different words in Chinese, depending on how it’s used. It all depends on the <strong class="markup--strong markup--p-strong">context </strong>and the entire sentence as a whole — the meaning you’re trying to convey. This is the exact approach a human translator would take.</p><p name="fad3" id="fad3" class="graf graf--p graf-after--p">Another type of many to many architecture exists where each neuron has a state at every timestep, in a “synchronized” fashion. Here, each output is only dependent on the inputs that were fed in during or before it. Because of this, synchronized many to many probably wouldn’t be suitable for translation.</p><figure name="5a39" id="5a39" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 190px; max-height: 231px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 121.6%;"></div><img class="graf-image" data-image-id="1*84IkP_dqLUfImZ5SyZLwjA.png" data-width="190" data-height="231" src="https://cdn-images-1.medium.com/max/800/1*84IkP_dqLUfImZ5SyZLwjA.png"></div></figure><p name="cef1" id="cef1" class="graf graf--p graf-after--figure">An application for this could be video classification where each frame needs to be mapped to some sort of class or label. Interesting note — an RNN is better at this task than CNNs are because what’s going on in a scene is much easier to understand if you’ve watched the video up to that point and thus can contextualize it. <strong class="markup--strong markup--p-strong">That’s what humans do!</strong></p><p name="f0a6" id="f0a6" class="graf graf--p graf-after--p">Quick note: we can “wrap” the RNN into a much more succinct form, where we collapse the depth and time properties, like so:</p><figure name="e5dd" id="e5dd" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 190px; max-height: 250px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 131.6%;"></div><img class="graf-image" data-image-id="1*7POP9GXAsRlbRRrsrhr-jA.png" data-width="190" data-height="250" src="https://cdn-images-1.medium.com/max/800/1*7POP9GXAsRlbRRrsrhr-jA.png"></div></figure><p name="fc78" id="fc78" class="graf graf--p graf-after--figure">This notation demonstrates that RNNs take input, process that input through multiple timesteps and hidden layers, and produce output. The arrow both leaving and entering the RNN conveys that an RNN hidden state is functionally dependent on the hidden state at the preceding timestep; it’s sort of like a loop that feeds itself.</p><p name="3ba7" id="3ba7" class="graf graf--p graf-after--p">When you ever read about “unrolling” an RNN into a feedforward network that looks like it’s in the same collapsed format as the diagram above, this means we expand it to show all timesteps and hidden layers like we did before.</p><p name="0d76" id="0d76" class="graf graf--p graf-after--p">Another quick note: when somebody or a research paper mentions that they are using “512 RNN units”, this translates to: “1 RNN neuron that outputs a 512-wide vector”; that is, a vector with 512 values. At first, I thought this meant that maybe at each timestep there were 512 separate neurons somehow working in conjunction, but nope, it’s luckily much simpler than that… albeit strangely worded.</p><p name="2ae1" id="2ae1" class="graf graf--p graf-after--p">Furthermore, one “RNN unit” usually refers to an RNN with one hidden layer; thus, instead of defining RNN as something that is multilayer inherently, we often see people use the phrase like: “stacking RNNs on top of each other”. Each RNN will have its on weights, but connecting them gives rise to an overarching multilayer RNN. In this article, we treat recurrent neural networks as a model that can have variable timesteps <strong class="markup--strong markup--p-strong">t </strong>and fixed layers <strong class="markup--strong markup--p-strong">ℓ</strong>, just make sure you understand that this is not always the case. Our formalism, especially for weights, will slightly differ.</p><h3 name="e5b5" id="e5b5" class="graf graf--h3 graf-after--p">Formalism</h3><p name="a93d" id="a93d" class="graf graf--p graf-after--h3">So, now, let’s walk through the formal mathematical notation involved in RNNs.</p><p name="ad6a" id="ad6a" class="graf graf--p graf-after--p">If an input or output neuron has a value at timestep <strong class="markup--strong markup--p-strong">t</strong>, we denote the vector as:</p><figure name="a4f8" id="a4f8" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 141px; max-height: 77px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 54.6%;"></div><img class="graf-image" data-image-id="1*_D9bOLepOSSbC2zgK7wreQ.png" data-width="141" data-height="77" src="https://cdn-images-1.medium.com/max/800/1*_D9bOLepOSSbC2zgK7wreQ.png"></div></figure><p name="a6fd" id="a6fd" class="graf graf--p graf-after--figure">For the hidden neurons it’s a bit different; since we can have multiple hidden layers, we denote the hidden state vector at timestep <strong class="markup--strong markup--p-strong">t </strong>and hidden layer <strong class="markup--strong markup--p-strong">ℓ </strong>as:</p><figure name="7987" id="7987" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 157px; max-height: 41px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 26.1%;"></div><img class="graf-image" data-image-id="1*QJFWCdOVxGAge0ZT17hw1g.png" data-width="157" data-height="41" src="https://cdn-images-1.medium.com/max/800/1*QJFWCdOVxGAge0ZT17hw1g.png"></div></figure><p name="7ba4" id="7ba4" class="graf graf--p graf-after--figure">The input is obviously some preset values that we know. The outputs and hidden states are<em class="markup--em markup--p-em"> not</em>; they are calculated.</p><p name="982f" id="982f" class="graf graf--p graf-after--p">Let’s start with hidden states. First, we’ll revisit the most complex recurrent net we came across earlier — the many to many architecture:</p><figure name="44bd" id="44bd" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 250px; max-height: 206px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 82.39999999999999%;"></div><img class="graf-image" data-image-id="1*TJxcM6GI8dMHEq6sK3Ky8Q.png" data-width="250" data-height="206" src="https://cdn-images-1.medium.com/max/800/1*TJxcM6GI8dMHEq6sK3Ky8Q.png"></div><figcaption class="imageCaption">Many to many, non-synchronized.</figcaption></figure><p name="5f3c" id="5f3c" class="graf graf--p graf-after--figure">This RNN has: sequential input, sequential output, multiple timesteps, and multiple hidden layers. The formula we derive for this RNN should generalize for all others.</p><p name="af44" id="af44" class="graf graf--p graf-after--p">First, let’s list out the possible functional dependencies for a given hidden state, based on the arrows and flow of information in the diagram:</p><ul class="postList"><li name="5021" id="5021" class="graf graf--li graf-after--p">An input</li><li name="8970" id="8970" class="graf graf--li graf-after--li">Hidden state at the previous timestep, same layer</li><li name="429d" id="429d" class="graf graf--li graf-after--li">Hidden state at the current timestep, previous layer</li></ul><p name="dc21" id="dc21" class="graf graf--p graf-after--li">A hidden state can have two functional dependencies at max. Just by looking at the diagram, the only impossible combination is to be dependent on both the input and a hidden state at the current timestep but previous layer. This is because the only hidden states that are dependent on input exist in the first hidden layer, where no such previous layer exists.</p><p name="b721" id="b721" class="graf graf--p graf-after--p">If this is all difficult to follow, make sure once again to look at and trace back the arrows in the RNN that illustrate flow of information throughout the network.</p><p name="5a0a" id="5a0a" class="graf graf--p graf-after--p">Because of the impossible combination, we define two separate equations: an equation for the hidden state at hidden layer 1, and for layers after 1.</p><figure name="8095" id="8095" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 400px; max-height: 107px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 26.8%;"></div><img class="graf-image" data-image-id="1*n5QR9Q9ZGnWFRf7pROtliA.png" data-width="400" data-height="107" src="https://cdn-images-1.medium.com/max/800/1*n5QR9Q9ZGnWFRf7pROtliA.png"></div></figure><p name="7d12" id="7d12" class="graf graf--p graf-after--figure">This probably looks a bit confusing; let me break it down for you. The function <strong class="markup--strong markup--p-strong">ƒw </strong>computes the numeric hidden state vector for timestep <strong class="markup--strong markup--p-strong">t </strong>and layer <strong class="markup--strong markup--p-strong">ℓ</strong>; it contains the “activation function” you’re used to hearing about with ANNs. <strong class="markup--strong markup--p-strong">W </strong>are the weights of the recurrent net, and thus <strong class="markup--strong markup--p-strong">ƒ </strong>is conditioned on <strong class="markup--strong markup--p-strong">W</strong>. We haven’t exactly defined <strong class="markup--strong markup--p-strong">ƒ</strong> just yet, but what’s important to note is the two parameters it takes. Once you do, this notation simply states what we have stated before in plain English:</p><blockquote name="a42e" id="a42e" class="graf graf--blockquote graf-after--p">Where <strong class="markup--strong markup--blockquote-strong">ℓ </strong>= 1, the hidden state at time <strong class="markup--strong markup--blockquote-strong">t </strong>and layer <strong class="markup--strong markup--blockquote-strong">ℓ </strong>is a function of the hidden state vector at time <strong class="markup--strong markup--blockquote-strong">t-1 </strong>and layer <strong class="markup--strong markup--blockquote-strong">ℓ </strong>as well as the input vector at time <strong class="markup--strong markup--blockquote-strong">t</strong>. Where <strong class="markup--strong markup--blockquote-strong">ℓ &gt; 1</strong>, this hidden state is a function of the hidden state vector at time <strong class="markup--strong markup--blockquote-strong">t-1 </strong>and layer <strong class="markup--strong markup--blockquote-strong">ℓ </strong>as well as the hidden state vector at time <strong class="markup--strong markup--blockquote-strong">t</strong>, layer <strong class="markup--strong markup--blockquote-strong">ℓ-1</strong>.</blockquote><p name="7713" id="7713" class="graf graf--p graf-after--blockquote">You might notice that we have a couple issues:</p><ul class="postList"><li name="69f7" id="69f7" class="graf graf--li graf-after--p">When <strong class="markup--strong markup--li-strong">t = 1</strong> — that is, when each neuron is at the initial timestep — then no previous timestep exists. However, we still attempt to pass <strong class="markup--strong markup--li-strong">h_0 </strong>as a parameter to <strong class="markup--strong markup--li-strong">ƒw</strong>.</li><li name="7b09" id="7b09" class="graf graf--li graf-after--li">If no input exists at time <strong class="markup--strong markup--li-strong">t</strong> — thus, <strong class="markup--strong markup--li-strong">x_t </strong>does not exist — then we still attempt to pass <strong class="markup--strong markup--li-strong">x_t </strong>as a parameter.</li></ul><p name="2062" id="2062" class="graf graf--p graf-after--li">Our respective solutions follow:</p><ul class="postList"><li name="c5c1" id="c5c1" class="graf graf--li graf-after--p">Define <strong class="markup--strong markup--li-strong">h_0 </strong>for any layer as 0</li><li name="68f6" id="68f6" class="graf graf--li graf-after--li">Consider <strong class="markup--strong markup--li-strong">x_t </strong>where no input exists at timestep <strong class="markup--strong markup--li-strong">t</strong> as 0</li></ul><p name="0ed3" id="0ed3" class="graf graf--p graf-after--li">If these are 0, then the invalid functional dependency stops existing, and our formal notation still holds up.</p><p name="cf77" id="cf77" class="graf graf--p graf-after--p">We actually have five different types of weight matrices:</p><figure name="61fa" id="61fa" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 390px; max-height: 233px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 59.699999999999996%;"></div><img class="graf-image" data-image-id="1*r09EQtFlEA1kIiOJD2aZ6g.png" data-width="390" data-height="233" src="https://cdn-images-1.medium.com/max/800/1*r09EQtFlEA1kIiOJD2aZ6g.png"></div></figure><p name="b66b" id="b66b" class="graf graf--p graf-after--figure">Pro-tip: The indices for each weight matrix tell you what they are used for in the recurrent net. <strong class="markup--strong markup--p-strong">W_xh </strong>maps an input vector <strong class="markup--strong markup--p-strong">x </strong>to a hidden state vector <strong class="markup--strong markup--p-strong">h</strong>. <strong class="markup--strong markup--p-strong">W_hht </strong>maps a hidden state vector <strong class="markup--strong markup--p-strong">h</strong> to another hidden state vector <strong class="markup--strong markup--p-strong">h </strong>along the time axis, ie. from <strong class="markup--strong markup--p-strong">h_t-1 </strong>to <strong class="markup--strong markup--p-strong">h_t</strong>. On the other hand, <strong class="markup--strong markup--p-strong">W_hhd </strong>maps a hidden state vector <strong class="markup--strong markup--p-strong">h </strong>to another hidden state vector <strong class="markup--strong markup--p-strong">h </strong>along the depth axis, ie. from <strong class="markup--strong markup--p-strong">h^(ℓ-1)_t </strong>to <strong class="markup--strong markup--p-strong">h^ℓ_t</strong>. <strong class="markup--strong markup--p-strong">W_hy </strong>maps a hidden state vector <strong class="markup--strong markup--p-strong">h </strong>to an output vector <strong class="markup--strong markup--p-strong">y</strong>.</p><p name="283a" id="283a" class="graf graf--p graf-after--p">Like with ANNs, we also learn and add a constant bias vector, denoted <strong class="markup--strong markup--p-strong">b_h</strong>, that can vertically shift what we pass to the activation function. We can also shift our outputs with <strong class="markup--strong markup--p-strong">b_y</strong>. More about bias units <a href="https://ayearofai.com/rohan-5-what-are-bias-units-828d942b4f52" data-href="https://ayearofai.com/rohan-5-what-are-bias-units-828d942b4f52" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">here</a>.</p><p name="246f" id="246f" class="graf graf--p graf-after--p">For both <strong class="markup--strong markup--p-strong">b_h </strong>and <strong class="markup--strong markup--p-strong">W_hht/W_hhd</strong>, we actually have multiple weight matrices depending on the value of <strong class="markup--strong markup--p-strong">ℓ</strong>, as indicated by the superscript. This is because each hidden layer can have a different set of weights (the network would be extremely uninteresting if this wasn’t the case), including the bias vector. However, inside a single hidden layer, all timesteps share the same weight matrix. This is important because the number of timesteps is a variable; we may train on sequences with up to 20 values, but in practice output sequences with up to 30 values — 10 extra timesteps. If each timestep had an independent weight to learn, those last 10 timesteps wouldn’t have anything to use.</p><p name="6da9" id="6da9" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">W_hy </strong>is just one matrix because only the final layer gives rise to the outputs denoted <strong class="markup--strong markup--p-strong">y</strong>. At the final hidden layer <strong class="markup--strong markup--p-strong">ℓ</strong>, we could suggest that <strong class="markup--strong markup--p-strong">W_hhd </strong>will not exist because <strong class="markup--strong markup--p-strong">W_hy </strong>will be in its place.</p><p name="e3ed" id="e3ed" class="graf graf--p graf-after--p">Now we’ll define the function <strong class="markup--strong markup--p-strong">ƒw</strong>:</p><figure name="d692" id="d692" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 404px; max-height: 226px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 55.900000000000006%;"></div><img class="graf-image" data-image-id="1*9YGuqXdNiknmZR2HScMDKw.png" data-width="404" data-height="226" src="https://cdn-images-1.medium.com/max/800/1*9YGuqXdNiknmZR2HScMDKw.png"></div></figure><p name="e06a" id="e06a" class="graf graf--p graf-after--figure">The function is very similar to the ANN hidden function you’ve seen before; it applies the correct weights to the corresponding parameters, adds the bias, and passes this weighted sum through an activation or “squashing” function to introduce non-linearities. The key difference, though, is that this is not a weighted sum<em class="markup--em markup--p-em"> </em>but rather a weighted sum <em class="markup--em markup--p-em">vector</em>; any <strong class="markup--strong markup--p-strong">W ⋅ h</strong>, along with the bias,<strong class="markup--strong markup--p-strong"> </strong>will have the dimensions of a vector. The <strong class="markup--strong markup--p-strong">tanh </strong>function will thus simply output a vector where each value is the tanh<strong class="markup--strong markup--p-strong"> </strong>of what it was in the inputted vector (sort of like an element-wise tanh). Remember, this contrasts ANNs because RNNs operate over vectors versus scalars.</p><p name="d904" id="d904" class="graf graf--p graf-after--p">If you’ve followed our blog so far, you most likely know about two activation functions: sigmoid and ReLU. tanh is another such function. We mostly use the tanh function with RNNs. This is, I think, mostly because of their role in LSTMs (a variant of RNNs that are used more than RNNs — more on that later), the fact that they produce gradients with a greater range, and that their second derivative don’t die off as quickly.</p><p name="25ba" id="25ba" class="graf graf--p graf-after--p">Similar to sigmoid, the tanh function has two horizontal asymptotes and a smooth S-shape. The main difference is that the tanh function asymptotes at y = -1 instead of y = 0, intercepting the y-axis at y = 0 instead of y = 0.5. Thus, the tanh function has a greater range than the sigmoid.</p><figure name="0f69" id="0f69" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 324px; max-height: 166px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 51.2%;"></div><img class="graf-image" data-image-id="1*NPI9iLLVlYLQ2gu9A9xp0A.png" data-width="324" data-height="166" src="https://cdn-images-1.medium.com/max/800/1*NPI9iLLVlYLQ2gu9A9xp0A.png"></div></figure><p name="9d21" id="9d21" class="graf graf--p graf-after--figure">If interested, the tanh equation follows (though I won’t walk you through it):</p><figure name="a172" id="a172" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 241px; max-height: 84px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 34.9%;"></div><img class="graf-image" data-image-id="1*w7LV9vY1hCAXcLk2K_peEg.png" data-width="241" data-height="84" src="https://cdn-images-1.medium.com/max/800/1*w7LV9vY1hCAXcLk2K_peEg.png"></div></figure><p name="85eb" id="85eb" class="graf graf--p graf-after--figure">The final equation is mapping a hidden state to an output.</p><figure name="f531" id="f531" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 177px; max-height: 44px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 24.9%;"></div><img class="graf-image" data-image-id="1*n7simJp73WxCRx_Bz4dXwg.png" data-width="177" data-height="44" src="https://cdn-images-1.medium.com/max/800/1*n7simJp73WxCRx_Bz4dXwg.png"></div></figure><p name="93b1" id="93b1" class="graf graf--p graf-after--figure">This is <em class="markup--em markup--p-em">one</em> such possible equation. Depending on the context, we might also remove the bias vector, apply a non-linearity like sigmoid/softmax (for example if the output needs to be a probability distribution), etc.</p><p name="3584" id="3584" class="graf graf--p graf-after--p">And that’s how we express recurrent nets, mathematically!</p><p name="2650" id="2650" class="graf graf--p graf-after--p">Quick note: Notation may and will differ between various lectures, research paper, articles, etc. For example — some research papers may start indexing at 0 instead of 1. More drastically, most RNN notation is <em class="markup--em markup--p-em">much </em>more general than mine to promote simplicity, ie. doesn’t cover edge cases like I did or obfuscates certain indices like <strong class="markup--strong markup--p-strong">ℓ </strong>with hidden to hidden weight matrices. So, just keep note that specifics don’t always transfer over and avoid being confused by this. The reason I was meticulous about notation in this article is that I wanted to ensure you understood exactly how RNNs work, fueled by my frustration with the very same problem ~1.5 years ago.</p><h3 name="ed09" id="ed09" class="graf graf--h3 graf-after--p">An example? Okay!</h3><p name="e53b" id="e53b" class="graf graf--p graf-after--h3">Let’s take a look at a quick example of an RNN in action. I’m going to adapt a super dumbed down one from Andrej Karpathy’s Stanford CS231n <a href="https://www.youtube.com/watch?v=cO0a0QYmFm8&amp;index=10&amp;list=PLlJy-eBtNFt6EuMxFYRiNRS07MCWN5UIA" data-href="https://www.youtube.com/watch?v=cO0a0QYmFm8&amp;index=10&amp;list=PLlJy-eBtNFt6EuMxFYRiNRS07MCWN5UIA" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">RNN lecture</a>, where a one to many “character level language model” single layer recurrent neural network needs to output “hello”. We’ll kick it of by giving the RNN the letter “h” , such that it needs to complete the word by outputting the other four letters.</p><p name="1901" id="1901" class="graf graf--p graf-after--p">Sidenote: this model nicknamed “char-rnn” — remember it for later, where we get to code our own!</p><p name="fc9c" id="fc9c" class="graf graf--p graf-after--p">The neural network has the vocabulary: h, e, l , o. That is, it only knows these four characters; exactly enough to produce the word “hello”. We will input the first character, “h”, and from there expect the output at the following timesteps to be: “e”, “l”, “l”, and “o” respectively, to form:</p><blockquote name="56af" id="56af" class="graf graf--pullquote graf-after--p">hello</blockquote><p name="9f5f" id="9f5f" class="graf graf--p graf-after--pullquote">We can represent input and output via one hot encoding, where each character is a vector with a 1 at the corresponding character position and otherwise all 0s. For example, since our vocabulary is [h, e, l, o], we can represent characters using a vector with four values, where a 1 in the first, second, third, and fourth position would represent “h”, “e”, “l”, and “o” respectively.</p><figure name="28cd" id="28cd" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 440px; max-height: 192px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 43.6%;"></div><img class="graf-image" data-image-id="1*pgWSPyximAFHqZtUkiLeKg.png" data-width="440" data-height="192" src="https://cdn-images-1.medium.com/max/800/1*pgWSPyximAFHqZtUkiLeKg.png"></div><figcaption class="imageCaption">This is called “one-hot encoding”, because only one of the values in the vector is equal to 1 and thus on (or “hot”).</figcaption></figure><p name="c682" id="c682" class="graf graf--p graf-after--figure">This is what we’d expect with a trained RNN:</p><figure name="0f80" id="0f80" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 250px; max-height: 227px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 90.8%;"></div><img class="graf-image" data-image-id="1*mmuQb8msqqQLtz580_lpvw.png" data-width="250" data-height="227" src="https://cdn-images-1.medium.com/max/800/1*mmuQb8msqqQLtz580_lpvw.png"></div></figure><p name="17a3" id="17a3" class="graf graf--p graf-after--figure">As you can see, we input the first letter and the word is completed. We don’t know exactly what the hidden states will be — that’s why they’re hidden!</p><p name="04ad" id="04ad" class="graf graf--p graf-after--p">One interesting technique would be to sample the output at each timestep and feed it into the next as input:</p><figure name="9eb9" id="9eb9" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 250px; max-height: 226px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 90.4%;"></div><img class="graf-image" data-image-id="1*KyVSttLGcexQWDLvSWD0Lg.png" data-width="250" data-height="226" src="https://cdn-images-1.medium.com/max/800/1*KyVSttLGcexQWDLvSWD0Lg.png"></div></figure><p name="08da" id="08da" class="graf graf--p graf-after--figure">When we “sample” from a distribution, we select a random character probabilistically following the distribution. For example, in the diagram above, the character with the highest likeliness is “e” at the first timestep’s output. Let’s say this likeliness is, concretely, 0.9. Now, when we sample into the next timestep’s input, there’s a 90% chance we select “e”; <em class="markup--em markup--p-em">most</em> of the time we will pick the most likely character, but not every time. This adds a level of randomness so you don’t end up in a loop where you keep sampling the same letter or sequence of letters over and over again.</p><p name="0880" id="0880" class="graf graf--p graf-after--p">As mentioned earlier, this is used pretty heavily with LCRNs. It’s even more effective than only relying on the memory of the RNN to output the correct letter at the future timesteps. In a sense, this makes the recurrent net many to many. (Though, not really, because we still only have one preset input.)</p><p name="9cac" id="9cac" class="graf graf--p graf-after--p">However, to be clear, this does not mean that the RNN can <em class="markup--em markup--p-em">only </em>rely on these sampled inputs. For example, at timestep 3 the input is “l” and the expected output is also “l”. However, at timestep 4, the input is again “l” but the output is now “o”, to complete the word. Memory is still needed to make a distinction like this.</p><p name="db16" id="db16" class="graf graf--p graf-after--p">In numerical form, it would look something like this:</p><figure name="5acf" id="5acf" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 280px; max-height: 371px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 132.5%;"></div><img class="graf-image" data-image-id="1*AguGRuRZg6e6RZ7Ctvn2Ww.png" data-width="280" data-height="371" src="https://cdn-images-1.medium.com/max/800/1*AguGRuRZg6e6RZ7Ctvn2Ww.png"></div></figure><p name="c815" id="c815" class="graf graf--p graf-after--figure">Of course, we won’t get a one-hot vector output during prediction mode; rather, we will get a probability distribution over each letter (so we’d apply softmax to the output), and will sample from this distribution to get a single character output.</p><figure name="afba" id="afba" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 300px; max-height: 574px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 191.3%;"></div><img class="graf-image" data-image-id="1*6067M6Oqz2zNoyyyQC1Suw.png" data-width="300" data-height="574" src="https://cdn-images-1.medium.com/max/800/1*6067M6Oqz2zNoyyyQC1Suw.png"></div></figure><p name="9419" id="9419" class="graf graf--p graf-after--figure">Each hidden state would contain a similar sort of vector, though not necessarily something we could interpret like we can for the output.</p><p name="58bf" id="58bf" class="graf graf--p graf-after--p">The RNN is saying: given “h”, “e” is most likely to be the next character. Given “he”, “l” is the next likely character. With “hel”, “l” should be next, and with “hell”, the final character should be “o”.</p><p name="b7ec" id="b7ec" class="graf graf--p graf-after--p">But, if the neural network wasn’t trained on the word “hello”, and thus didn’t have optimal weights (ie. just randomly initialized weights), then we’d have garble like “hleol” coming out.</p><p name="399a" id="399a" class="graf graf--p graf-after--p">One more important thing to note: <strong class="markup--strong markup--p-strong">start and end tokens</strong>. They signify when input begins and when output ends. For example, when the final character is outputted (“o”), we can sample this back as input and expect that the “&lt;END&gt;” token (however we choose to represent it — could also use a period) will be outputted at the next timestep; this is the RNN telling us that it has completed the word and its processing as a whole. The use case isn’t as obvious in this fabricated example, because we know when “hello” has been completed, but consider a real-life scenario where we don’t: image captioning. In image captioning, the caption could be 1, 2, 3, or n words long, given a reasonable upper limit of n. The end token tells us when the caption has been completed, so we can halt the RNN and complete the prediction loop (which would keep going forever if we were using <code class="markup--code markup--p-code">while</code> or stop after the upper limit/max possible preset constant value of n is reached).</p><p name="a127" id="a127" class="graf graf--p graf-after--p">Start tokens are more used for generating content from complete scratch. For example, imagine an RNN read and learned from a bunch of Shakespeare. (This is an actual funny application of character level language models that Karpathy implemented, and we’ll see it in action on a later section.) Now, based on what the RNN learned, we want it to create a brand new Shakespearean sonnet! Feeding in a “&lt;START&gt;” token enables it to kick this process off and begin writing without us giving the network some arbitrary pre-determined initial word or character.</p><p name="85be" id="85be" class="graf graf--p graf-after--p">I’ve also noticed that another potential use case of start tokens is when we have some other sort of <em class="markup--em markup--p-em">initial </em>input, like CNN produced image data with image captioning, that doesn’t “fit” what we’ll normally use for input at timesteps after <strong class="markup--strong markup--p-strong">t=1 </strong>(the word outputted at the previous timestep via sampling). As a result, we feed this data directly to the first hidden state and set the input as “&lt;START&gt;” instead.</p><p name="2aca" id="2aca" class="graf graf--p graf-after--p">Now, just to be clear, the RNN doesn’t magically output these end tokens and recognize the start tokens. We have to add them, along with start tokens, to the training data and vocabulary such that they can be outputted by the recurrent net during prediction time.</p><p name="1c00" id="1c00" class="graf graf--p graf-after--p">This is how we can get RNNs to “write”! More on some examples of text RNNs have actually generated, Shakespeare most certainly included, in a later section.</p><h3 name="6b00" id="6b00" class="graf graf--h3 graf-after--p">Training (or, why vanilla RNNs suck.)</h3><p name="58b5" id="58b5" class="graf graf--p graf-after--h3">For a recurrent net to be useful, it needs to learn proper weights via training. That’s no surprise.</p><p name="8a33" id="8a33" class="graf graf--p graf-after--p">Recall this snippet from earlier:</p><blockquote name="f72f" id="f72f" class="graf graf--blockquote graf-after--p">But, if the neural network wasn’t trained on the word “hello”, and thus didn’t have optimal weights (ie. just randomly initialized weights), then we’d have garble like “hleol” coming out.</blockquote><p name="f9d1" id="f9d1" class="graf graf--p graf-after--blockquote">This is, of course, because we initialize the <strong class="markup--strong markup--p-strong">W </strong>weights randomly at first, so random stuff will come out.</p><p name="c7a6" id="c7a6" class="graf graf--p graf-after--p">But, through multiple iterations of training with a first-order optimization algorithm like gradient descent, we perturb the weights such that the probability of each correct character being outputted at their respective timestep increases. The actual output would be “hello” in one-hot encoding form, and we’d compute the discrepancy between this output and what the recurrent net predicts (we’d get the error at each timestep and then add this up) as the total error to then calculate the gradient/update value.</p><p name="2ded" id="2ded" class="graf graf--p graf-after--p">So, each output contributes to the error somehow. If the error is an addition of the outputs, then, if we had something like <strong class="markup--strong markup--p-strong">Y</strong> outputs, we’d need to backpropagate them individually and add these up. This is because derivatives are distributed evenly when we’re differentiating a sum:</p><figure name="6145" id="6145" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 651px; max-height: 79px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 12.1%;"></div><img class="graf-image" data-image-id="1*d5mzuu-EmcZz0IFukW6XsQ.png" data-width="651" data-height="79" src="https://cdn-images-1.medium.com/max/800/1*d5mzuu-EmcZz0IFukW6XsQ.png"></div><figcaption class="imageCaption">For any arbitrary weight <strong class="markup--strong markup--figure-strong">W</strong>.</figcaption></figure><p name="bb6e" id="bb6e" class="graf graf--p graf-after--figure">But, you should know that, with artificial neural networks, calculating these gradients isn’t that easy. We have so many weights contributing to the output, and thus need to figure out exactly how much these weights contribute, and by how much we modify them to decrease overall error. To do this, we use the backpropagation algorithm; this algorithm propagates the error between the predicted output of a recurrent net and the actual output in the dataset all the way back to the beginning of the network. Using the chain rule from differential calculus, backprop helps us calculate the gradients of the output error w.r.t. each individual weight (sort of like the error of each individual weight).</p><p name="c4f4" id="c4f4" class="graf graf--p graf-after--p">Once we have those gradients, we have to use an optimization algorithm to calculate the update values and make the updates. We can use the vanilla gradient descent algorithm to do this, but there are many other possible, better variants as well; learn about them by reading <a href="http://sebastianruder.com/optimizing-gradient-descent/" data-href="http://sebastianruder.com/optimizing-gradient-descent/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">this article</a>, if you want. (I think we’re long overdue for our own mega-post on optimization!)</p><p name="0be4" id="0be4" class="graf graf--p graf-after--p">Backpropagation with RNNs is called “Backpropagation Through Time” (short for BPTT), since it operates on sequences in time. But don’t be fooled — there’s not much difference between normal backprop and BPTT; when it comes down to it, BPTT is just backprop, but on RNNs! Remember that when you “unroll” an RNN, it essentially becomes a feedforward network; not an ANN, but a feedforward network in the sense that we can visualize where all the information is flowing and observe the activations at each neuron and timestep, all the way from the input to the final output. Like ANNs, RNNs have functional dependencies that link the entire network together; it’s just that RNNs operate over vectors instead (yay for matrix calculus?) and extend in depth <em class="markup--em markup--p-em">as well as </em>time. There’s more work to do to compute the gradients, but it’s no surprise that backprop works pretty much the same way for recurrent nets that it would for normal ones. Because of this, I’m not going to walk through all the math and show the derivatives etc. Read our backprop mega-post for all that jazz.</p><p name="224b" id="224b" class="graf graf--p graf-after--p">One thing to note is that, since we have multiple timesteps in our RNN, each timestep in a single layer will want to change the weight in a different way and have different gradients. However, remember that each hidden layer uses only one weight matrix because the number of timesteps is a variable. Thus, we just average or sum the weight updates between these timesteps and apply this as an update to the <strong class="markup--strong markup--p-strong">W_hh </strong>for that entire layer. Also, a general practice is to train on shorter sequences first and then gradually increase sequence size as we train on more and more data.</p><p name="1fee" id="1fee" class="graf graf--p graf-after--p">Now, if you haven’t already, make sure to read this article that I wrote on vanishing and exploding gradients before proceeding:</p><div name="c672" id="c672" class="graf graf--mixtapeEmbed graf-after--p"><a href="https://ayearofai.com/rohan-4-the-vanishing-gradient-problem-ec68f76ffb9b" data-href="https://ayearofai.com/rohan-4-the-vanishing-gradient-problem-ec68f76ffb9b" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://ayearofai.com/rohan-4-the-vanishing-gradient-problem-ec68f76ffb9b"><strong class="markup--strong markup--mixtapeEmbed-strong">Rohan #4: The vanishing gradient problem</strong><br><em class="markup--em markup--mixtapeEmbed-em">Oh no — an obstacle to deep learning!</em>ayearofai.com</a><a href="https://ayearofai.com/rohan-4-the-vanishing-gradient-problem-ec68f76ffb9b" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="bb894bd0a8e1cfee65aea593ec3751b3" data-thumbnail-img-id="1*m2gDBT_nc-iE7R4AM3sHBQ.jpeg" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/1*m2gDBT_nc-iE7R4AM3sHBQ.jpeg);"></a></div><p name="be11" id="be11" class="graf graf--p graf-after--mixtapeEmbed">You may be thinking: how does this issue apply to RNNs? Well, RNNs are very deep models; on top of often having multiple hidden layers, each hidden layer in practice can have <em class="markup--em markup--p-em">hundreds</em> of timesteps. That’s like an ANN with hundreds of entire hidden layers! That’s <em class="markup--em markup--p-em">deep</em>.<em class="markup--em markup--p-em"> </em>(Well, it’s more <em class="markup--em markup--p-em">long </em>because we’re dealing with the time axis here, but you know what I mean.) tanh derivatives are very similar to sigmoid derivatives in range, so the problem of vanishing gradients is thus even more drastic with RNNs than with ANNs, and training them becomes almost impossible.</p><p name="4878" id="4878" class="graf graf--p graf-after--p">Imagine trying to propagate the error to the 1st timestep in an RNN with <strong class="markup--strong markup--p-strong">k</strong> timesteps. The derivative would look something like this:</p><figure name="44b3" id="44b3" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 346px; max-height: 79px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 22.8%;"></div><img class="graf-image" data-image-id="1*gKbRtQfPwGK2d7jnKZdv5w.png" data-width="346" data-height="79" src="https://cdn-images-1.medium.com/max/800/1*gKbRtQfPwGK2d7jnKZdv5w.png"></div></figure><p name="0e72" id="0e72" class="graf graf--p graf-after--figure">With a tanh activation function, that’s freaking crazy. Then, for getting the derivative of the error with respect to a weight matrix <strong class="markup--strong markup--p-strong">W_hh</strong>, we’d add — or, as mentioned before, we could average as well — each of these hidden state error gradients, then multiplied by the derivative of the hidden state with respect to the weight, such that we can backprop from the error to the weight:</p><figure name="0639" id="0639" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 72px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 10.299999999999999%;"></div><img class="graf-image" data-image-id="1*jf52uXcsAX6Nn8ghLYoJWQ.png" data-width="773" data-height="80" data-action="zoom" data-action-value="1*jf52uXcsAX6Nn8ghLYoJWQ.png" src="https://cdn-images-1.medium.com/max/800/1*jf52uXcsAX6Nn8ghLYoJWQ.png"></div><figcaption class="imageCaption">Assuming our sequence is of length <strong class="markup--strong markup--figure-strong">k</strong>.</figcaption></figure><p name="9baa" id="9baa" class="graf graf--p graf-after--figure">So we’d be effectively adding together a bunch of terms that have vanished — the exception being very late gradients with a small number of terms — and so <strong class="markup--strong markup--p-strong">dJ/dWhh </strong>would only capture gradient signals from the last few timesteps. (Or, for exploding gradients, it would become infinity).</p><p name="fd7e" id="fd7e" class="graf graf--p graf-after--p">But, you might be asking, instead of tanh — which is bounded between -1 and 1, and has a similar problem to sigmoid where the peak of the derivative is smaller than 1 — why don’t we just use ReLUs? Don’t ReLUs, or perhaps leaky ReLUs, solve the vanishing gradient problem?</p><p name="6a76" id="6a76" class="graf graf--p graf-after--p">Well, not entirely; it’s not enough to solve the problem. With RNNs, the problem really lies in the architecture. Even though we could use ReLU to ensure many of the values in the gradient computation are not between -1, 0, and 1 such that they vanish — or vice-versa, explode — we do still indeed have a lot of other variables other than the activation function derivative in the gradient computation such as the weights; you can revisit the mega-post on backprop we wrote to confirm this. Since weights are also normally randomly initialized in the range -1 to 1, and RNNs are like super deep ANNs, these weights keep multiplying on top of each other and potentially cause the gradients to vanish.</p><p name="4faf" id="4faf" class="graf graf--p graf-after--p">This is more my suspicion though — I’m yet to confirm this is the case by testing. I was curious so I asked this exact question on Quora:</p><figure name="6eaa" id="6eaa" class="graf graf--figure graf--iframe graf-after--p"><div class="aspectRatioPlaceholder is-locked"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 100%;"></div><div class="iframeContainer"><IFRAME data-width="560" data-height="560" width="560" height="560" src="/media/aa3fb6f891aba6d55742cf7dabf3f7f7?postId=10300100899b" data-media-id="aa3fb6f891aba6d55742cf7dabf3f7f7" data-thumbnail="https://i.embed.ly/1/image?url=https%3A%2F%2Fwww.quora.com%2Fstatic%2Fimages%2Flogo%2Fwordmark_default.png&amp;key=4fce0568f2ce49e8b54624ef71a8a5bd" allowfullscreen frameborder="0"></IFRAME></div></div></figure><p name="e537" id="e537" class="graf graf--p graf-after--figure">From this, something interesting I learned is that: since ReLUs are <em class="markup--em markup--p-em">un</em>bounded (it’s not restricted to be between -1 and 1 or 0 and 1) unlike sigmoid/tanh, and RNNs are very deep, the activations, especially later ones, can become too big. This is because hidden states have a multiplicative relationship; one hidden state is a multiple of the previous ones, where that multiple specifically is a weight. If we use ReLU, then the hidden state isn’t limited by any range, and we could have a bunch of numbers bigger than 1 multiplying by each other.</p><p name="b4fc" id="b4fc" class="graf graf--p graf-after--p">It ends up being sort of like the exploding gradient problem, but with the values inside the neurons, not gradients. This is also what <em class="markup--em markup--p-em">then</em> causes the gradients to explode: large activations → large gradients → large change in weights → even bigger activations, because updating the weights in the wrong direction ever so slightly can cause the entire network to explode. This makes learning unstable:</p><blockquote name="2c04" id="2c04" class="graf graf--blockquote graf-after--p">This means that the computation within the RNN can potentially blow up to infinity without sensible weights. This makes learning VERY unstable because a slight shift in the weights in the wrong direction during backprop can blow up the activations during the forward pass. So that’s why you see most people using sigmoid/tanh units, despite the vanishing gradient descent problem.</blockquote><p name="4f2a" id="4f2a" class="graf graf--p graf-after--blockquote">Also well said:</p><blockquote name="e86a" id="e86a" class="graf graf--blockquote graf-after--p">With RNN’s, the problem is that you are repeatedly applying your RNN to itself, which tends to [mostly] cause exponential blowup or [rarely, but sometimes] shrinkage.</blockquote><p name="4cb6" id="4cb6" class="graf graf--p graf-after--blockquote">Other issues with ReLU functions are discussed in the article I wrote, and they similarly apply to RNNs. Generally speaking, though, they just don’t work that well, especially compared to other options we have. Making RNNs perform well with ReLUs is actually a pretty hot topic of research right now, but until someone figures out something genius, RNNs are a lost cause.</p><p name="4a29" id="4a29" class="graf graf--p graf-after--p">And that’s why <strong class="markup--strong markup--p-strong">vanilla RNNs suck</strong>. Seriously. In practice, nobody uses them. Even if you didn’t fully grasp this section on how the vanishing and exploding gradient/activation problem is applicable to them, it doesn’t matter anyways. Because, everything you’ve read up to this point so far… throw it all away. Forget about it.</p><p name="044a" id="044a" class="graf graf--p graf-after--p">Just kidding. Don’t do that.</p><h3 name="1d2b" id="1d2b" class="graf graf--h3 graf-after--p">Fixing the problem with LSTMs (Part I)</h3><p name="daad" id="daad" class="graf graf--p graf-after--h3">You shouldn’t do that because RNNs actually <em class="markup--em markup--p-em">aren’t </em>a lost cause. They’re far from it. We just need to make a few… modifications.</p><p name="9491" id="9491" class="graf graf--p graf-after--p">Enter the LSTM.</p><figure name="32fa" id="32fa" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 298px; max-height: 170px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 56.99999999999999%;"></div><img class="graf-image" data-image-id="1*JuC5afKk7QIntsvyEn-IFA.png" data-width="298" data-height="170" src="https://cdn-images-1.medium.com/max/800/1*JuC5afKk7QIntsvyEn-IFA.png"></div></figure><p name="731a" id="731a" class="graf graf--p graf-after--figure">Makes sense, no?</p><figure name="ea5b" id="ea5b" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 350px; max-height: 357px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 102%;"></div><img class="graf-image" data-image-id="1*tB6QdzunJV08wyep0ZhVMA.png" data-width="350" data-height="357" src="https://cdn-images-1.medium.com/max/800/1*tB6QdzunJV08wyep0ZhVMA.png"></div></figure><p name="03ff" id="03ff" class="graf graf--p graf-after--figure">How about this?</p><figure name="0c36" id="0c36" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 400px; max-height: 314px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 78.5%;"></div><img class="graf-image" data-image-id="1*Oin8uuuQzp_wqtHAX1yyjQ.png" data-width="400" data-height="314" src="https://cdn-images-1.medium.com/max/800/1*Oin8uuuQzp_wqtHAX1yyjQ.png"></div></figure><p name="0d8c" id="0d8c" class="graf graf--p graf-after--figure">OK. Clearly something’s not registering here. But that’s fine; LSTM diagrams are frikin’ difficult for beginners to grasp. I too remember when I first searched up “LSTM” on Google to encounter something similar to the works of art above. I reacted like this:</p><figure name="16e7" id="16e7" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 195px; max-height: 229px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 117.39999999999999%;"></div><img class="graf-image" data-image-id="1*S7ABwK33X7no_MP3epry6A.gif" data-width="195" data-height="229" src="https://cdn-images-1.medium.com/max/800/1*S7ABwK33X7no_MP3epry6A.gif"></div><figcaption class="imageCaption">MRW first Google Image-ing LSTMs.</figcaption></figure><p name="7cd4" id="7cd4" class="graf graf--p graf-after--figure">In this section, I’m going to embark on a mission to design the first simple, comprehensible, and beautiful LSTM diagram. Wish me luck, because I’ll probably fail.</p><p name="b288" id="b288" class="graf graf--p graf-after--p graf--trailing">With that being said, let’s dive into <strong class="markup--strong markup--p-strong">Long Short-Term Memory networks</strong>. (Yes, that’s what LSTM stands for.)</p></div></div></section><section name="da40" class="section section--body"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><p name="bb27" id="bb27" class="graf graf--p graf--leading">With RNNs, the real “substance” of the model were the hidden neurons; these were the units that did processing on the input, through time, to produce the outputs. Specifically, at each timestep, a hidden neuron embodies a hidden state that is computed by feeding the weighted sum vector of the input and/or previous hidden states with an added bias vector through a tanh squashing function. We can have multiple hidden neurons, each of which when unrolled forms an entire hidden “layer”.</p><p name="66c2" id="66c2" class="graf graf--p graf-after--p">If you need a refresher on this, look through the “Formalism” section once again.</p><p name="cd18" id="cd18" class="graf graf--p graf-after--p">With LSTMs, we still have hidden states, but they’re computed through a much more complex mechanism: LSTM cells. Instead of computing each hidden state as a direct function of inputs and other hidden states, we compute it as a function of the LSTM cell’s value (the “cell state”) at that timestep. Each cell state is in turn functionally dependent on the previous<em class="markup--em markup--p-em"> </em>cell state and any available input or previous hidden states. That’s right — hidden states are computed from cell states, and cell states are (in part) computed from older and/or shallower hidden states.</p><p name="095f" id="095f" class="graf graf--p graf-after--p">The cell state at a specific timestep <strong class="markup--strong markup--p-strong">t </strong>is denoted <strong class="markup--strong markup--p-strong">c_t</strong>. Like a hidden state, a cell state is just a vector.</p><figure name="e92a" id="e92a" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 500px; max-height: 245px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 49%;"></div><img class="graf-image" data-image-id="1*sr8XQzvr-WTNSbgWwI9qbQ.png" data-width="500" data-height="245" src="https://cdn-images-1.medium.com/max/800/1*sr8XQzvr-WTNSbgWwI9qbQ.png"></div><figcaption class="imageCaption">For simplicity’s sake, I’ve obfuscated layer index <strong class="markup--strong markup--figure-strong">ℓ</strong>.</figcaption></figure><p name="68f5" id="68f5" class="graf graf--p graf-after--figure">If the diagram above seems a bit trippy, let me break it down for you.</p><p name="8bd2" id="8bd2" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">c_t</strong>, as highlighted by the three arrows pointing towards it, has multiple potential functional dependencies. Four to be exact, though only a maximum of three can exist at once. These are:</p><ul class="postList"><li name="4f8f" id="4f8f" class="graf graf--li graf-after--p">The previous hidden state in time: <strong class="markup--strong markup--li-strong">h_t-1</strong>. Again, if <strong class="markup--strong markup--li-strong">t = 1</strong>, then this won’t exist. If it does, this would be the first arrow pointing into the left side of <strong class="markup--strong markup--li-strong">c_t</strong>.</li><li name="f283" id="f283" class="graf graf--li graf-after--li">The previous cell state: <strong class="markup--strong markup--li-strong">c_t-1</strong>. If <strong class="markup--strong markup--li-strong">t = 1</strong>, the dependency obviously won’t exist. This refers to the second arrow pointing into the left side of <strong class="markup--strong markup--li-strong">c_t</strong>.</li><li name="7237" id="7237" class="graf graf--li graf-after--li">Input at the current timestep: <strong class="markup--strong markup--li-strong">x_t</strong>. There may very well be no input available, for example if we are at a hidden layer <strong class="markup--strong markup--li-strong">ℓ &gt; 1</strong>. So this dependency doesn’t always exist. When it does, it’s the arrow pointing into the bottom of <strong class="markup--strong markup--li-strong">c_t</strong>.</li><li name="83f3" id="83f3" class="graf graf--li graf-after--li">The previous hidden state in depth: <strong class="markup--strong markup--li-strong">h^(ℓ-1)_t</strong>. This applies for any hidden layer <strong class="markup--strong markup--li-strong">ℓ &gt; 1</strong>. In such case, it would — like the input <strong class="markup--strong markup--li-strong">x_t</strong> — be the arrow pointing into the bottom.</li></ul><p name="2a71" id="2a71" class="graf graf--p graf-after--li">Only three can exist at once because the last two are mutually exclusive.</p><p name="5160" id="5160" class="graf graf--p graf-after--p">From there, we pass information to the next cell state <strong class="markup--strong markup--p-strong">c_t+1</strong> and compute <strong class="markup--strong markup--p-strong">h_t</strong>. As you can hopefully see, <strong class="markup--strong markup--p-strong">h_t</strong> then goes on to also influence <strong class="markup--strong markup--p-strong">c_t+1 </strong>(as indicated by the horizontal arrow), along with higher level cell states or final outputs (the vertical arrow).</p><p name="0f0f" id="0f0f" class="graf graf--p graf-after--p">Right now the cells are a black box… literally; we know what is inputted to them and what they output, but we don’t know their internal process. So… what’s inside these cells? What do they do? What are the exact computations involved? How have the equations changed?</p><p name="ee47" id="ee47" class="graf graf--p graf-after--p">To help answer the question, I want you to imagine something: a factory. Inside this factory we have workers who perform their own tasks. Those tasks are, specifically, operating on some sort of product that runs down a <strong class="markup--strong markup--p-strong">conveyer belt</strong>. Think of, hell, I don’t know — chicken nuggets! The first worker cuts an appropriately sized piece, the second worker applies egg wash, the third worker adds breadcrumbs, the fourth worker chucks it in the fryer, etc.</p><figure name="5a97" id="5a97" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 490px; max-height: 600px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 122.39999999999999%;"></div><img class="graf-image" data-image-id="1*4avrG18SFOMJGI4CpDIsoA.png" data-width="490" data-height="600" src="https://cdn-images-1.medium.com/max/800/1*4avrG18SFOMJGI4CpDIsoA.png"></div><figcaption class="imageCaption">I’m not sure what product this conveyer belt carries, but it certainly doesn’t look appetizing (or like chicken nuggets).</figcaption></figure><p name="02ea" id="02ea" class="graf graf--p graf-after--figure">You’re thinking: “OK Rohan, but how does this relate to LSTMs?”. Good question.</p><p name="9004" id="9004" class="graf graf--p graf-after--p">Basically, think of the conveyer belt as the cell state, the chicken-nugget-in-progress as information flowing through the cell, and the workers as operations we apply to this information. The final product is the finished chicken nugget — or, the cell state value.</p><figure name="b54e" id="b54e" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 459px; max-height: 95px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 20.7%;"></div><img class="graf-image" data-image-id="1*qNUGFMhlnl0-mNLIVvyGAg.png" data-width="459" data-height="95" src="https://cdn-images-1.medium.com/max/800/1*qNUGFMhlnl0-mNLIVvyGAg.png"></div><figcaption class="imageCaption">Chicken. Nugget.</figcaption></figure><p name="bca7" id="bca7" class="graf graf--p graf-after--figure">The reason we use the analogy of a conveyer belt is because information can flow through a cell super super easily. It’s theoretically possible for information to speed past a single cell state without being modified at all. In fact, I think the term ‘modified’ is a really strong one here. With vanilla RNNs, each hidden state takes all the information it has from before and fully <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">transforms</em></strong> it by applying a function over it. LSTM cells instead take information and make minor <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">modifications</em></strong> (like additions or multiplications) to it while it flows through.</p><figure name="eca6" id="eca6" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 330px; max-height: 241px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 73%;"></div><img class="graf-image" data-image-id="1*I_nQdhxdoDa7KrZBTFeHSQ.png" data-width="330" data-height="241" src="https://cdn-images-1.medium.com/max/800/1*I_nQdhxdoDa7KrZBTFeHSQ.png"></div><figcaption class="imageCaption">Ew. Vanilla RNNs.</figcaption></figure><p name="72ea" id="72ea" class="graf graf--p graf-after--figure">Vanilla RNNs look something like this. And it’s why the vanishing gradient problem exists; during backprop, gradients cannot flow back past these transformations easily, because the tanh derivatives and weights chain up and multiply together and tend to zero. We then add up or average all these gradients that are basically zero, and we get zero.</p><figure name="9b8e" id="9b8e" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 450px; max-height: 276px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 61.3%;"></div><img class="graf-image" data-image-id="1*360GYNV8kyF5ATWefrSasA.png" data-width="450" data-height="276" src="https://cdn-images-1.medium.com/max/800/1*360GYNV8kyF5ATWefrSasA.png"></div><figcaption class="imageCaption">LSTMs 💦 💦 💦</figcaption></figure><p name="0a23" id="0a23" class="graf graf--p graf-after--figure">This is an extreme a simplification — and I’ll go on to fill in the blanks later — but it’s sort of what an LSTM looks like. The previous timestep’s cell state value flows through and instead of transforming the information, we tweak it by <em class="markup--em markup--p-em">adding </em>(another vector) to it. The added term is some function <strong class="markup--strong markup--p-strong">ƒw </strong>of previous information, but this is <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">not</em></strong> the same function as with vanilla RNNs — it’s heavily changed to make sense in this context (more on that soon), do more interesting things, and also reduce the vanishing gradient problem.</p><p name="ba34" id="ba34" class="graf graf--p graf-after--p">Another neat way to think of it is like a live circuit: there are two paths where information, like current, can flow through. After the information passes through<strong class="markup--strong markup--p-strong"> ƒw</strong>, it’s added to the information flowing towards <strong class="markup--strong markup--p-strong">c_t</strong>. Thus, in equation form it could look something like this:</p><figure name="ea17" id="ea17" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 208px; max-height: 39px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 18.8%;"></div><img class="graf-image" data-image-id="1*qGqSrpJmO5h6ZGeIT7RK3w.png" data-width="208" data-height="39" src="https://cdn-images-1.medium.com/max/800/1*qGqSrpJmO5h6ZGeIT7RK3w.png"></div><figcaption class="imageCaption">Again… sort of. We’ll get into the <strong class="markup--strong markup--figure-strong">actual</strong><em class="markup--em markup--figure-em"> equations soon. This is a good proxy to convey my point.</em></figcaption></figure><p name="c99b" id="c99b" class="graf graf--p graf-after--figure">With a bit of substitution, we can expand this to:</p><figure name="2268" id="2268" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 444px; max-height: 39px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 8.799999999999999%;"></div><img class="graf-image" data-image-id="1*OZs7rDSty0VhDhzTLH4Dgg.png" data-width="444" data-height="39" src="https://cdn-images-1.medium.com/max/800/1*OZs7rDSty0VhDhzTLH4Dgg.png"></div><figcaption class="imageCaption">Technically, this could expand even more, if you did some sort of recursive substitution to fully simplify the unrolled recurrence formula. You could express <strong class="markup--strong markup--figure-strong">c_t </strong>for some large value of <strong class="markup--strong markup--figure-strong">t </strong>as a really really really really long function of, ultimately, <strong class="markup--strong markup--figure-strong">c_1</strong>.</figcaption></figure><p name="48be" id="48be" class="graf graf--p graf-after--figure">Why is this better? Well, if you have basic differentiation knowledge, you’ll know that addition distributes gradients equally. When we take the derivative of this whole expression, it’ll become a long addition of the derivatives of individual terms. As Andrej Karpathy puts it, this additive interaction creates “gradient super-highways”, where gradients can flow back super easily.</p><figure name="740d" id="740d" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 224px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 31.900000000000002%;"></div><img class="graf-image" data-image-id="1*n26drGfEkc-Xnmqc2Lw7cw.png" data-width="2161" data-height="690" data-action="zoom" data-action-value="1*n26drGfEkc-Xnmqc2Lw7cw.png" src="https://cdn-images-1.medium.com/max/800/1*n26drGfEkc-Xnmqc2Lw7cw.png"></div><figcaption class="imageCaption">Look — it’s a long conveyer belt! (In a sense, we can use this conveyor belt analogy for the <strong class="markup--strong markup--figure-strong"><em class="markup--em markup--figure-em">whole unrolled LSTM</em></strong> as well. Each cell state is a subsection of the conveyer belt.)</figcaption></figure><figure name="2d87" id="2d87" class="graf graf--figure graf-after--figure"><div class="aspectRatioPlaceholder is-locked" style="max-width: 610px; max-height: 193px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 31.6%;"></div><img class="graf-image" data-image-id="1*szBIWNdr0O0doBI8rfGjzw.png" data-width="610" data-height="193" src="https://cdn-images-1.medium.com/max/800/1*szBIWNdr0O0doBI8rfGjzw.png"></div><figcaption class="imageCaption">Look — it’s an outdated machine learning algorithm!</figcaption></figure><p name="68da" id="68da" class="graf graf--p graf-after--figure">In the former, gradients are always added together, never multiplied. In the latter, gradients are always multiplied. Thus, in the former, when we inject a gradient at the end, it’ll easily flow back all the way to the beginning. Contributions by the <strong class="markup--strong markup--p-strong">ƒw </strong>function will be made to this gradient flowing on the bottom conveyer belt as well.</p><p name="779d" id="779d" class="graf graf--p graf-after--p">This is what gradient flow would look like:</p><figure name="8074" id="8074" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 248px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 35.4%;"></div><img class="graf-image" data-image-id="1*dqOCXyepO590ORWV3VgBKw.png" data-width="740" data-height="262" data-action="zoom" data-action-value="1*dqOCXyepO590ORWV3VgBKw.png" src="https://cdn-images-1.medium.com/max/800/1*dqOCXyepO590ORWV3VgBKw.png"></div></figure><p name="7ea0" id="7ea0" class="graf graf--p graf-after--figure">Before, we discussed that when multiplicative interaction exists between gradients, the gradients either vanish (if they are mostly &lt; 1, as is usually the case for us) or explode (if they are mostly &gt; 1). Here’s some real calculus to demonstrate this:</p><figure name="6f86" id="6f86" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 287px; max-height: 288px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 100.29999999999998%;"></div><img class="graf-image" data-image-id="1*09oGK1btsVezIoMyBAwqbw.png" data-width="287" data-height="288" src="https://cdn-images-1.medium.com/max/800/1*09oGK1btsVezIoMyBAwqbw.png"></div><figcaption class="imageCaption">Former is akin to RNNs. Latter is akin to LSTMs.</figcaption></figure><p name="0175" id="0175" class="graf graf--p graf-after--figure">Imagine <strong class="markup--strong markup--p-strong">f </strong>being any sort of function, like our <strong class="markup--strong markup--p-strong">ƒw</strong>. When we apply a function to itself repeatedly, the chain rule shows that the overall derivative is the multiplication of multiple different derivative terms. But, when we add functions together, the derivative is simply the addition of the individual derivatives. This won’t vanish or explode quickly, so our LSTMs won’t vanish or explode quickly. Yay!</p><p name="4e50" id="4e50" class="graf graf--p graf-after--p">Furthermore, if some of our gradients vanish — for whatever reason — then it should still be OK. It won’t be optimal, but since our gradient terms add together, if some of them vanish it doesn’t mean the whole thing will vanish (versus if they were multiplied together). Look: 2 + 0 = 2 but 2 × 0 = 0.</p><figure name="bbb5" id="bbb5" class="graf graf--figure graf-after--p graf--trailing"><div class="aspectRatioPlaceholder is-locked" style="max-width: 466px; max-height: 240px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 51.5%;"></div><img class="graf-image" data-image-id="1*K7rYONPTfcpCb0xTvO3ydw.jpeg" data-width="466" data-height="240" src="https://cdn-images-1.medium.com/max/800/1*K7rYONPTfcpCb0xTvO3ydw.jpeg"></div><figcaption class="imageCaption">A gradient super highway? Sounds good to me! <a href="http://www.dyoung.com/assets/images/Articles%20images/article4_PPH.jpg" data-href="http://www.dyoung.com/assets/images/Articles%20images/article4_PPH.jpg" class="markup--anchor markup--figure-anchor" rel="nofollow noopener noopener" target="_blank">http://www.dyoung.com/assets/images/Articles%20images/article4_PPH.jpg</a></figcaption></figure></div></div></section><section name="5df4" class="section section--body"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><p name="a58a" id="a58a" class="graf graf--p graf--leading">So far, we haven’t <em class="markup--em markup--p-em">really </em>explored LSTMs. We’ve more setup a foundation for them. And there’s one glaring issue with our foundation: if we just keep adding information to cell state, it could just grow and grow and grow, and essentially act as a counter that only increments. This is not very useful, and could regularly lead to explosion. We want more fine and rich control over memory. Well, worry not, because this is exactly what LSTMs are capable of doing.</p><p name="7ba6" id="7ba6" class="graf graf--p graf-after--p">LSTM cells handle memory in a very intelligent way, enabling them to learn long-term dependencies and perform well. How, exactly? Well, the cell is sort of like an internal memory state that allows for context; it “forgets”, a.k.a. resets, information it doesn’t find useful from the previous cell state, “writes” in new information it <em class="markup--em markup--p-em">does</em> find useful from the current input and/or previous hidden state, and similarly only “reads” out part of its information — the good stuff — in the computation of <strong class="markup--strong markup--p-strong">h_t</strong>. This respectively corresponds to the concepts of: resetting memory, writing to memory, and reading from memory. Very similar to how a modern computer system works, and we often describe an LSTM cell as a “memory cell”.</p><p name="6706" id="6706" class="graf graf--p graf-after--p">The “writing to memory” part is additive — it’s what I showed you in the initial diagrams. Information flows through and we add stuff we think is relevant to it. The “resetting memory” part is multiplicative, and occurs before writing to memory; when information from the previous cell state initially flows in, we multiply it by a vector with values between 0 and 1 to reset or retain parts of it we find useless and useful respectively. The “reading from memory” part is also multiplicative with a similar 0–1 range vector, but it doesn’t modify the information flowing through the cell states. Rather, it modifies the information flowing into the hidden states and thus decides what the hidden state is influenced by.</p><p name="6813" id="6813" class="graf graf--p graf-after--p">Both of these multiplications are <em class="markup--em markup--p-em">element wise</em>, like so:</p><figure name="2255" id="2255" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 167px; max-height: 82px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 49.1%;"></div><img class="graf-image" data-image-id="1*YuIuYxt0oYEvGMoTz_J59g.png" data-width="167" data-height="82" src="https://cdn-images-1.medium.com/max/800/1*YuIuYxt0oYEvGMoTz_J59g.png"></div></figure><p name="c869" id="c869" class="graf graf--p graf-after--figure">In this equation, when <strong class="markup--strong markup--p-strong">a = 0 </strong>the information of <strong class="markup--strong markup--p-strong">c </strong>is lost. This is what resetting does, and retaining is the vice versa. I also imagine that values such as 0.5 could be used to diminish the importance of certain information, but not completely wipe it out.</p><p name="c920" id="c920" class="graf graf--p graf-after--p">Our (unfinished) cell state computational graph now looks like this:</p><figure name="f4a0" id="f4a0" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 419px; max-height: 158px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 37.7%;"></div><img class="graf-image" data-image-id="1*_mbUA8vdaTbYreXpdPJccA.png" data-width="419" data-height="158" src="https://cdn-images-1.medium.com/max/800/1*_mbUA8vdaTbYreXpdPJccA.png"></div><figcaption class="imageCaption">This is closer to what an LSTM looks like, though we’re not exactly there yet.</figcaption></figure><p name="ba90" id="ba90" class="graf graf--p graf-after--figure">Sidenote: don’t be scared whenever you see the word “multiplicative” and don’t immediately think of “vanishing” or “exploding”. It depends on the context. Here, as I’ll show mathematically in a bit, it’s fine.</p><p name="9705" id="9705" class="graf graf--p graf-after--p">This concept in general is known as <strong class="markup--strong markup--p-strong">gating</strong>, because we “gate” what can flow in and out of the LSTM cell. What we actually multiply and add by to reset, write, and read are known as the “gates”. There are four such gates:</p><ul class="postList"><li name="dafe" id="dafe" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">f</strong>: <em class="markup--em markup--li-em">forget gate. </em>This is the “reset” tool that wipes out, diminishes, or retains information from the previous cell state. It’s the first interaction we make, and it’s multiplicative. That is, we multiply it with the cell state. The sigmoid function is used to compute the forget gate such that its values can be in the range 0 to 1. When a value is 1, we “remember” something, and when it is 0 we “forget”. We might choose to forget, for example, when see a period or some sort of end of sentence marker. This is counterintuitive… I guess it should really be called the “<em class="markup--em markup--li-em">remember gate</em>”!</li><li name="aeb2" id="aeb2" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">g</strong>:<strong class="markup--strong markup--li-strong"><em class="markup--em markup--li-em"> </em></strong><em class="markup--em markup--li-em">?.</em><strong class="markup--strong markup--li-strong"><em class="markup--em markup--li-em"> </em></strong>This gate doesn’t really have a name, but it’s partly responsible for the “write” process. It stores a value between -1 and 1 that represents how much we want to add to the cell state by, and represents the input to the cell state. It’s computed with the tanh function. We apply a bounded function to it such that the cell state acts as a stable counter, and it also introduces more complexity. (And it works well.)</li><li name="0612" id="0612" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">i</strong>:<strong class="markup--strong markup--li-strong"><em class="markup--em markup--li-em"> </em></strong><em class="markup--em markup--li-em">input gate.</em> This is the other gate responsible for the “write” process. It controls how much of <strong class="markup--strong markup--li-strong"><em class="markup--em markup--li-em">g</em></strong> we “let in”, and is thus between 0 and 1, computed with sigmoid. It’s similar to the forget gate in this sense, in that it blocks input like the forget gate blocks the incoming cell state. We multiply <strong class="markup--strong markup--li-strong"><em class="markup--em markup--li-em">i</em> </strong>by <strong class="markup--strong markup--li-strong"><em class="markup--em markup--li-em">g </em></strong>and add this to the cell state.<strong class="markup--strong markup--li-strong"> </strong>Since <strong class="markup--strong markup--li-strong"><em class="markup--em markup--li-em">i </em></strong>is in the range 0 to 1, and <strong class="markup--strong markup--li-strong"><em class="markup--em markup--li-em">g</em></strong> is in the range -1 to 1, we add a value between -1 and 1 to the cell state. Intuitively, this sort of acts as decrementing or incrementing the counter.</li><li name="57eb" id="57eb" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">o</strong>:<em class="markup--em markup--li-em"> output gate</em>.<strong class="markup--strong markup--li-strong"><em class="markup--em markup--li-em"> </em></strong>This is also passed through sigmoid, and is a number between 0 and 1 that modulates which aspects the hidden state can draw from the cell state. It enables the “read from memory” operation. It multiplies with the tanh<strong class="markup--strong markup--li-strong"><em class="markup--em markup--li-em"> </em></strong>of the cell state to compute the hidden state. So, I didn’t bring this up before, but the cell state leaks into a tanh before <strong class="markup--strong markup--li-strong">h_t</strong> is computed.</li></ul><p name="5cb0" id="5cb0" class="graf graf--p graf-after--li">Here’s our updated computational graph for the cell state:</p><figure name="d768" id="d768" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 533px; max-height: 251px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 47.099999999999994%;"></div><img class="graf-image" data-image-id="1*3xq3p-nVgxQXXPSXueVWdw.png" data-width="533" data-height="251" src="https://cdn-images-1.medium.com/max/800/1*3xq3p-nVgxQXXPSXueVWdw.png"></div></figure><p name="01f6" id="01f6" class="graf graf--p graf-after--figure">Looks like I’m starting to create a complex diagram of my own. Damn. 😞 I guess LSTMs and immediately interpretable diagrams just weren’t meant to be!</p><p name="7eb5" id="7eb5" class="graf graf--p graf-after--p">Basically, <strong class="markup--strong markup--p-strong">f </strong>interacts with the cell state through a multiplication. <strong class="markup--strong markup--p-strong">i </strong>interacts with <strong class="markup--strong markup--p-strong">g </strong>through a multiplication as well, the result of which interacts with the cell state through an addition. Finally, the cell state leaks into a tanh (that’s the shape of the tanh function in the circle), the result of which then interacts with <strong class="markup--strong markup--p-strong">o </strong>through multiplication to compute <strong class="markup--strong markup--p-strong">h_t</strong>. This does not disrupt the cell state, which flows to the next timestep. <strong class="markup--strong markup--p-strong">h_t </strong>then flows forward (and it could flow upward as well).</p><p name="e614" id="e614" class="graf graf--p graf-after--p">Here’s the equation form:</p><figure name="94fe" id="94fe" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 222px; max-height: 84px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 37.8%;"></div><img class="graf-image" data-image-id="1*B9Qd1pW1kYM_zcg0IhPfUA.png" data-width="222" data-height="84" src="https://cdn-images-1.medium.com/max/800/1*B9Qd1pW1kYM_zcg0IhPfUA.png"></div><figcaption class="imageCaption">Each gate should actually be indexed by timestep <strong class="markup--strong markup--figure-strong">t </strong>— we’ll see why soon.</figcaption></figure><p name="2518" id="2518" class="graf graf--p graf-after--figure">As you can see, our cell state has no activation function; the activation function is simply the identity function! Yet, the cell state usually doesn’t explode — it stays stable by “forgetting” and “writing”, and does interesting things with this gating to promote context, fine control over memory, and long-term dependency learning.</p><p name="b30a" id="b30a" class="graf graf--p graf-after--p">So, how are the gates calculated? Well, all of these gates have their own learnable weights and are functions of the last timestep’s <em class="markup--em markup--p-em">hidden state </em>flowing in and any current timestep inputs, not the cell state (contrary to what I may have implied earlier with the gradient flow diagrams). This should make sense when you think about it; I mean, firstly, the <strong class="markup--strong markup--p-strong">g </strong>and <strong class="markup--strong markup--p-strong">i </strong>gates literally represent input, so they better be functionally dependent on hidden states and input data! On an intuitive level, the gates help us modify the cell state, and we modify the cell state based on our current context. External stimulus that provide context should be used to compute these gates, and since context = input + hidden states our gates are functionally dependent on input and hidden states.</p><p name="27fb" id="27fb" class="graf graf--p graf-after--p">Since every gate has a different value at each timestep, we index by timestep <strong class="markup--strong markup--p-strong">t </strong>just like for hidden states, cell states, or something similar.</p><p name="6fb5" id="6fb5" class="graf graf--p graf-after--p">We could generalize for multiple hidden layers as well:</p><figure name="53d0" id="53d0" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 290px; max-height: 41px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 14.099999999999998%;"></div><img class="graf-image" data-image-id="1*BWz6E_IFi6UTLkSNSoq1Yg.png" data-width="290" data-height="41" src="https://cdn-images-1.medium.com/max/800/1*BWz6E_IFi6UTLkSNSoq1Yg.png"></div></figure><p name="1ae0" id="1ae0" class="graf graf--p graf-after--figure">But, for simplicity’s sake, let’s assume we are at the first hidden layer, or that there is only one hidden layer in the LSTM. This way, we can obfuscate the <strong class="markup--strong markup--p-strong">ℓ </strong>term and ignore influence from hidden states in the previous depth. We’ll also forget about edge cases and assume input exists at the current timestep. In practice, we obviously can’t make said assumptions, but for the sake of demonstrating the equations it becomes too tedious otherwise.</p><p name="c042" id="c042" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Sidenote: we make this assumption for the rest of the discussion on LSTMs in this article.</strong></p><figure name="5a86" id="5a86" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 384px; max-height: 180px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 46.9%;"></div><img class="graf-image" data-image-id="1*hP6I692iv7oc6AkcWINDaw.png" data-width="384" data-height="180" src="https://cdn-images-1.medium.com/max/800/1*hP6I692iv7oc6AkcWINDaw.png"></div></figure><p name="033c" id="033c" class="graf graf--p graf-after--figure">Like with the RNN hidden state, the index of each weight matrix is descriptive; for example, <strong class="markup--strong markup--p-strong">W_xf </strong>are the weights that map input <strong class="markup--strong markup--p-strong">x </strong>to the forget gate <strong class="markup--strong markup--p-strong">f</strong>. Each gate has weight matrices that map input and hidden states to itself, including biases.</p><p name="6989" id="6989" class="graf graf--p graf-after--p">And this is the beauty of LSTMs; the whole thing is end-to-end differentiable. These gates can <em class="markup--em markup--p-em">learn </em>when to allow data to flow and what data should flow depending on the context it sees (the input and the hidden states). It learns this based on patterns it sees while training. In this sense, it’s sort of like how a CNN learns feature detectors for images, but the patterns are way more complex and less human interpretable with LSTMs. This is why they perform so well.</p><figure name="1be4" id="1be4" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 551px; max-height: 321px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 58.3%;"></div><img class="graf-image" data-image-id="1*VJL6ONtLK77GpO2XmFCH7g.png" data-width="551" data-height="321" src="https://cdn-images-1.medium.com/max/800/1*VJL6ONtLK77GpO2XmFCH7g.png"></div><figcaption class="imageCaption"><strong class="markup--strong markup--figure-strong">😨 </strong>😱 😰 : perhaps your immediate reaction.</figcaption></figure><p name="b263" id="b263" class="graf graf--p graf-after--figure">Okay, this looks scarier, but it’s actually not much different to what we had before, especially once you look past the intimidating web of arrows. One notable change is that we’re showing the previous hidden state in time and the current input flowing in. This diagram makes the assumption that we’re in the first layer and at some timestep &gt; 1 where input exists. We then show how the <strong class="markup--strong markup--p-strong">f</strong>, <strong class="markup--strong markup--p-strong">i</strong>, <strong class="markup--strong markup--p-strong">g, </strong>and <strong class="markup--strong markup--p-strong">o</strong> gates are computed from this information — the hidden state and inputs are fed into an activation function like sigmoid (or, for <strong class="markup--strong markup--p-strong">g</strong>, a tanh; you can tell because it’s double the height of the others) — and it’s expressed through the web of arrows. It’s implied that we weight the two terms entering our activation functions, adding them up with a bias vector, but it’s not necessarily explicit in the diagram.</p><p name="b56c" id="b56c" class="graf graf--p graf-after--p">Let’s embed this into our overall LSTM diagram for a single timestep:</p><figure name="689a" id="689a" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 317px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 45.300000000000004%;"></div><img class="graf-image" data-image-id="1*0h88NXeFxkb-xD1rBq4lgA.png" data-width="770" data-height="349" data-action="zoom" data-action-value="1*0h88NXeFxkb-xD1rBq4lgA.png" src="https://cdn-images-1.medium.com/max/800/1*0h88NXeFxkb-xD1rBq4lgA.png"></div></figure><p name="147c" id="147c" class="graf graf--p graf-after--figure">Now let’s zoom out and view our entire unrolled single layer, three timestep LSTM:</p></div><div class="section-inner sectionLayout--fullWidth"><figure name="4127" id="4127" class="graf graf--figure graf--layoutFillWidth graf-after--p"><div class="aspectRatioPlaceholder is-locked"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 28.499999999999996%;"></div><img class="graf-image" data-image-id="1*-lhIk-yAsXk88gcPvEeIRQ.png" data-width="4225" data-height="1204" src="https://cdn-images-1.medium.com/max/2000/1*-lhIk-yAsXk88gcPvEeIRQ.png"></div></figure></div><div class="section-inner sectionLayout--insetColumn"><p name="c9d0" id="c9d0" class="graf graf--p graf-after--figure">It’s beautiful, isn’t it? The full screen width size just adds to the effect! <a href="https://drive.google.com/file/d/0BwbWRPtraa2zQUsydXRKSkd3YUU/view?usp=sharing" data-href="https://drive.google.com/file/d/0BwbWRPtraa2zQUsydXRKSkd3YUU/view?usp=sharing" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Here’s</a> a link to the full res version.</p><p name="c678" id="c678" class="graf graf--p graf-after--p">The only thing that would look more beautiful would be multiple LSTM cells that stack on top of each other (multiple hidden layers)! 😍</p><h3 name="e117" id="e117" class="graf graf--h3 graf-after--p">Fixing the problem with LSTMs (Part II)</h3><p name="f89a" id="f89a" class="graf graf--p graf-after--h3">You’ve come a long way, young padawan. But there’s still a bit left to go. Part I focused on the motivation for LSTMs, how they work, and a bit on why they reduce the vanishing gradient problem. Now, having a full understanding of LSTMs, Part II will hone in on the latter part—analyzing on a more close, technical level why our gradients stop vanishing as quickly. You won’t find a lot of this information online easily; I had to search and ask left and right to find an explanation better and more comprehensive than what you’ll find in other current tutorials.</p><p name="a182" id="a182" class="graf graf--p graf-after--p">Firstly, truncated BPTT is often used with LSTMs; it’s a method to speed up training. In particular, note that if we input a sequence of length 1000 into an LSTM, and want to train it, it’s equivalent to training a 1000 layer neural network. Doing forward and backwards passes into this is very memory and time consuming, especially while backpropagating the error when we need to compute a derivative like this:</p><figure name="7285" id="7285" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 87px; max-height: 79px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 90.8%;"></div><img class="graf-image" data-image-id="1*UqC4IRIfcDfoiwD8zvqW2A.png" data-width="87" data-height="79" src="https://cdn-images-1.medium.com/max/800/1*UqC4IRIfcDfoiwD8zvqW2A.png"></div></figure><p name="06fe" id="06fe" class="graf graf--p graf-after--figure">…which would include a <em class="markup--em markup--p-em">lot</em> of terms.</p><p name="792f" id="792f" class="graf graf--p graf-after--p">When we backprop the error, and add all the gradients up, this is what we get:</p><figure name="b325" id="b325" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 675px; max-height: 79px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 11.700000000000001%;"></div><img class="graf-image" data-image-id="1*ucOvP6wOs9MHzH8WKiykbQ.png" data-width="675" data-height="79" src="https://cdn-images-1.medium.com/max/800/1*ucOvP6wOs9MHzH8WKiykbQ.png"></div></figure><p name="9c5f" id="9c5f" class="graf graf--p graf-after--figure">Truncated BPTT does two things:</p><ul class="postList"><li name="5bfb" id="5bfb" class="graf graf--li graf-after--p">Instead of doing a forward pass on the whole sequence and then doing a backwards pass, we process the sequence timestep by timestep and do a backwards pass “every so often”. That is — we compute <strong class="markup--strong markup--li-strong">h_1 </strong>and <strong class="markup--strong markup--li-strong">c_1</strong>, then <strong class="markup--strong markup--li-strong">h_2 </strong>and <strong class="markup--strong markup--li-strong">c_2</strong>, then <strong class="markup--strong markup--li-strong">h_3</strong> and <strong class="markup--strong markup--li-strong">c_3</strong>, and then at some point in time, quantified by <strong class="markup--strong markup--li-strong">k1, </strong>we do a backwards pass. Every <strong class="markup--strong markup--li-strong">k1 </strong>timesteps, we perform BPTT; if <strong class="markup--strong markup--li-strong">k1 = 10</strong>, for example, then once we compute <strong class="markup--strong markup--li-strong">h_10 </strong>and <strong class="markup--strong markup--li-strong">c_10 </strong>we perform BPTT. Same for <strong class="markup--strong markup--li-strong">h_20 </strong>and <strong class="markup--strong markup--li-strong">c_20</strong>, and so on so forth. When we perform the backwards pass, our error <strong class="markup--strong markup--li-strong">J </strong>won’t be the same as if we did a full forwards pass and full backwards pass, since we haven’t observed all the outputs yet—we wouldn’t have even computed all the potential outputs yet! Instead, the error describes what we’ve observed and computed so far, because we process the sequence timestep by timestep. Intuitively, it’s like we train on a small subset of the training sequence, and this subset increases in length each time, which enables us to continue learning long-term dependencies. We could denote the error at timestep <strong class="markup--strong markup--li-strong">t</strong> —where <strong class="markup--strong markup--li-strong">t </strong>is a multiple of <strong class="markup--strong markup--li-strong">k1</strong> — with truncated backprop as<strong class="markup--strong markup--li-strong"> J^t</strong>. So:</li></ul><figure name="dd7e" id="dd7e" class="graf graf--figure graf-after--li"><div class="aspectRatioPlaceholder is-locked" style="max-width: 445px; max-height: 80px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 18%;"></div><img class="graf-image" data-image-id="1*Br6EoWvUmGTNoX3NqkZVpA.png" data-width="445" data-height="80" src="https://cdn-images-1.medium.com/max/800/1*Br6EoWvUmGTNoX3NqkZVpA.png"></div></figure><p name="eb5a" id="eb5a" class="graf graf--p graf-after--figure">For example, if <strong class="markup--strong markup--p-strong">t = 20</strong> and <strong class="markup--strong markup--p-strong">k1 = 10</strong>, our <em class="markup--em markup--p-em">second</em> (because 20 ÷ 10 = 2) round of BPTT would be:</p><figure name="6f7d" id="6f7d" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 667px; max-height: 80px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 12%;"></div><img class="graf-image" data-image-id="1*pg91-TmNosH9B7Py0wjCdQ.png" data-width="667" data-height="80" src="https://cdn-images-1.medium.com/max/800/1*pg91-TmNosH9B7Py0wjCdQ.png"></div></figure><ul class="postList"><li name="f441" id="f441" class="graf graf--li graf-after--figure">On top of this, instead of backpropagating from <strong class="markup--strong markup--li-strong">J^t </strong>all the way to the first timestep <strong class="markup--strong markup--li-strong">c_1</strong>, we set a cut-off point. This cut-off point, quantified by <strong class="markup--strong markup--li-strong">k2</strong>, is the timestep at which our cell states stop contributing to the overall gradient. For example, if <strong class="markup--strong markup--li-strong">k2 = 10</strong>, and we’re backpropagating at <strong class="markup--strong markup--li-strong">t = 20</strong>, then <strong class="markup--strong markup--li-strong">c_10 </strong>is the final cell state to contribute to the overall gradient. Everything before <strong class="markup--strong markup--li-strong">c_10 </strong>will have no say. This is designed such that we avoid computing derivatives between cell states far apart in time, which would include a huge number of terms (as mentioned earlier). The equation is now:</li></ul><figure name="73ca" id="73ca" class="graf graf--figure graf-after--li"><div class="aspectRatioPlaceholder is-locked" style="max-width: 515px; max-height: 80px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 15.5%;"></div><img class="graf-image" data-image-id="1*GmP4nvsdBTyyRwo7ffRW2g.png" data-width="515" data-height="80" src="https://cdn-images-1.medium.com/max/800/1*GmP4nvsdBTyyRwo7ffRW2g.png"></div></figure><p name="956c" id="956c" class="graf graf--p graf-after--figure">So, with <strong class="markup--strong markup--p-strong">t = 20</strong>, <strong class="markup--strong markup--p-strong">k2 = 10</strong>, and <strong class="markup--strong markup--p-strong">k1 = 10</strong>, our second round of BPTT would follow:</p><figure name="12a1" id="12a1" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 674px; max-height: 80px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 11.899999999999999%;"></div><img class="graf-image" data-image-id="1*WyHlRZljjmHEaFKsGg0JQg.png" data-width="674" data-height="80" src="https://cdn-images-1.medium.com/max/800/1*WyHlRZljjmHEaFKsGg0JQg.png"></div></figure><p name="a297" id="a297" class="graf graf--p graf-after--figure">Both <strong class="markup--strong markup--p-strong">k1</strong> and <strong class="markup--strong markup--p-strong">k2</strong> are hyperparameters. <strong class="markup--strong markup--p-strong">k1 </strong>does not have to equal <strong class="markup--strong markup--p-strong">k2</strong>.</p><p name="93ed" id="93ed" class="graf graf--p graf-after--p">These two techniques combined enables truncated BPTT to not lose the ability to learn long term dependencies. Here’s a formal definition:</p><blockquote name="72ee" id="72ee" class="graf graf--blockquote graf-after--p">[Truncated BPTT] processes the sequence one timestep at a time, and every <strong class="markup--strong markup--blockquote-strong">k1</strong> timesteps, it runs BPTT for <strong class="markup--strong markup--blockquote-strong">k2</strong> timesteps, so a parameter update can be cheap if <strong class="markup--strong markup--blockquote-strong">k2</strong> is small. Consequently, its hidden states have been exposed to many timesteps and so may contain useful information about the far past, which would be opportunistically exploited.</blockquote><blockquote name="ae91" id="ae91" class="graf graf--blockquote graf-after--blockquote">— <a href="http://www.cs.utoronto.ca/~ilya/pubs/ilya_sutskever_phd_thesis.pdf" data-href="http://www.cs.utoronto.ca/~ilya/pubs/ilya_sutskever_phd_thesis.pdf" class="markup--anchor markup--blockquote-anchor" rel="noopener" target="_blank">“Training Recurrent Neural Networks”</a>, 2.8.6, Page 23</blockquote><p name="28a4" id="28a4" class="graf graf--p graf-after--blockquote">The same paper gives nice pseudocode for truncated BPTT:</p><figure name="f5d9" id="f5d9" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 413px; max-height: 113px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 27.400000000000002%;"></div><img class="graf-image" data-image-id="1*0SnUb2iYt1RNa7JsGG-7gQ.png" data-width="413" data-height="113" src="https://cdn-images-1.medium.com/max/800/1*0SnUb2iYt1RNa7JsGG-7gQ.png"></div></figure><p name="399f" id="399f" class="graf graf--p graf-after--figure">The rest of the math in this section will not be in the context of using truncated backprop, because it’s a technique vs. something rooted in the mathematical foundation of LSTMs.</p><p name="d546" id="d546" class="graf graf--p graf-after--p">Moving on — before, we saw this diagram:</p><figure name="dc69" id="dc69" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 224px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 31.900000000000002%;"></div><img class="graf-image" data-image-id="1*n26drGfEkc-Xnmqc2Lw7cw.png" data-width="2161" data-height="690" data-action="zoom" data-action-value="1*n26drGfEkc-Xnmqc2Lw7cw.png" src="https://cdn-images-1.medium.com/max/800/1*n26drGfEkc-Xnmqc2Lw7cw.png"></div></figure><p name="48f4" id="48f4" class="graf graf--p graf-after--figure">In this context, <strong class="markup--strong markup--p-strong">ƒw = i ⊙ g</strong>, because it’s the value we’re adding to the cell state.</p><p name="db0e" id="db0e" class="graf graf--p graf-after--p">But this diagram is a bit of a lie. Why? It ignores forget gates. So, does the presence of forget gates affect the vanishing gradient problem? Quite significantly, actually. How? Let’s bring up our cell state equation to see:</p><figure name="4484" id="4484" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 236px; max-height: 39px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 16.5%;"></div><img class="graf-image" data-image-id="1*i6rbrX0k9mKLXewD4korCw.png" data-width="236" data-height="39" src="https://cdn-images-1.medium.com/max/800/1*i6rbrX0k9mKLXewD4korCw.png"></div></figure><p name="09ae" id="09ae" class="graf graf--p graf-after--figure">With the forget gate, we now include a multiplicative interaction. Our new diagram will look like this:</p><figure name="931c" id="931c" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 283px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 40.5%;"></div><img class="graf-image" data-image-id="1*UVx1vL6ADQGTBeKSaWX7bw.png" data-width="2436" data-height="986" data-action="zoom" data-action-value="1*UVx1vL6ADQGTBeKSaWX7bw.png" src="https://cdn-images-1.medium.com/max/800/1*UVx1vL6ADQGTBeKSaWX7bw.png"></div><figcaption class="imageCaption">Do not confuse forget gate <strong class="markup--strong markup--figure-strong">ƒ</strong> with function <strong class="markup--strong markup--figure-strong">ƒw</strong> in this diagram. I know, it’s confusing… 😢</figcaption></figure><p name="bf2c" id="bf2c" class="graf graf--p graf-after--figure">When our gradients flow back, they will be affected by this multiplicative interaction. So, let’s compute the new derivative:</p><figure name="0504" id="0504" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 113px; max-height: 79px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 69.89999999999999%;"></div><img class="graf-image" data-image-id="1*60XFfJvc0t9a0ekdMTAp_Q.png" data-width="113" data-height="79" src="https://cdn-images-1.medium.com/max/800/1*60XFfJvc0t9a0ekdMTAp_Q.png"></div></figure><p name="f174" id="f174" class="graf graf--p graf-after--figure">This seems super neat, actually. <em class="markup--em markup--p-em">Obviously</em> the gradient will be <strong class="markup--strong markup--p-strong">f</strong>, because <strong class="markup--strong markup--p-strong">f </strong>acts as a blocker and controls how much <strong class="markup--strong markup--p-strong">c_t-1 </strong>influences <strong class="markup--strong markup--p-strong">c_t</strong>; it’s the gate that you can fully or partially open and close that lets information from <strong class="markup--strong markup--p-strong">c_t-1 </strong>flow through! It’s just intuitive that it would propagate back perfectly.</p><p name="d859" id="d859" class="graf graf--p graf-after--p">But, if you’ve payed close attention so far, you might be asking: “<em class="markup--em markup--p-em">wait, what happened to </em><strong class="markup--strong markup--p-strong">ƒw</strong><em class="markup--em markup--p-em">’s contribution to the gradient?”</em><strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em"> </em></strong>If you’re a hardcore mathematician, you might also be worried that we’re content with leaving the gradient as just <strong class="markup--strong markup--p-strong">f</strong>. This is because the gates <strong class="markup--strong markup--p-strong">f</strong>,<strong class="markup--strong markup--p-strong"> i</strong>, and <strong class="markup--strong markup--p-strong">g </strong>are all functions of <strong class="markup--strong markup--p-strong">c_t-1</strong>; they are functions of <strong class="markup--strong markup--p-strong">h_t-1</strong>, which is, in turn, a function of <strong class="markup--strong markup--p-strong">c_t-1</strong>! The diagram shows this visually, as well. It seems we’re failing to apply calculus properly. We’d need to backprop through <strong class="markup--strong markup--p-strong">f</strong> and through <strong class="markup--strong markup--p-strong">i ⊙ g </strong>to complete the derivative.</p><p name="3f0c" id="3f0c" class="graf graf--p graf-after--p">Let’s walk through the differentiation to show why you’re actually not wrong<strong class="markup--strong markup--p-strong">,<em class="markup--em markup--p-em"> </em></strong>but neither am I:</p><figure name="f0c6" id="f0c6" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 380px; max-height: 162px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 42.6%;"></div><img class="graf-image" data-image-id="1*x1mvDnbZOmZ1CnHJo23tZg.png" data-width="380" data-height="162" src="https://cdn-images-1.medium.com/max/800/1*x1mvDnbZOmZ1CnHJo23tZg.png"></div></figure><p name="80e0" id="80e0" class="graf graf--p graf-after--figure">Now, with the first derivative, we need to apply product rule. Why? Because we’re differentiating the product of two functions of <strong class="markup--strong markup--p-strong">c_t-1</strong>. The former being the forget gate, and the latter being just <strong class="markup--strong markup--p-strong">c_t-1</strong>. Let’s do it:</p><figure name="b39b" id="b39b" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 271px; max-height: 124px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 45.800000000000004%;"></div><img class="graf-image" data-image-id="1*TwjnkG6vtuIke1pkAzTzzA.png" data-width="271" data-height="124" src="https://cdn-images-1.medium.com/max/800/1*TwjnkG6vtuIke1pkAzTzzA.png"></div></figure><p name="00fd" id="00fd" class="graf graf--p graf-after--figure">Then, from product rule:</p><figure name="416d" id="416d" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 381px; max-height: 164px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 43%;"></div><img class="graf-image" data-image-id="1*cgq4UnWxun6rQ6H00gDzWg.png" data-width="381" data-height="164" src="https://cdn-images-1.medium.com/max/800/1*cgq4UnWxun6rQ6H00gDzWg.png"></div></figure><p name="7dd1" id="7dd1" class="graf graf--p graf-after--figure">That’s the first derivative done. We purposely choose not to compute the derivative of the forget gate with respect to the previous cell state on previous. You’ll see why in a bit.</p><p name="e8f2" id="e8f2" class="graf graf--p graf-after--p">Now let’s tackle the second one:</p><figure name="be20" id="be20" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 150px; max-height: 77px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 51.300000000000004%;"></div><img class="graf-image" data-image-id="1*1T0iaNg6vY4pEOz-ybkjuw.png" data-width="150" data-height="77" src="https://cdn-images-1.medium.com/max/800/1*1T0iaNg6vY4pEOz-ybkjuw.png"></div></figure><p name="10b0" id="10b0" class="graf graf--p graf-after--figure">You’ll notice that it’s also two functions of <strong class="markup--strong markup--p-strong">c_t-1 </strong>multiplied together, so we use the product rule again:</p><figure name="f85e" id="f85e" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 320px; max-height: 124px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 38.800000000000004%;"></div><img class="graf-image" data-image-id="1*tVfrKCc1T7eRgypliDe19A.png" data-width="320" data-height="124" src="https://cdn-images-1.medium.com/max/800/1*tVfrKCc1T7eRgypliDe19A.png"></div></figure><p name="618f" id="618f" class="graf graf--p graf-after--figure">So:</p><figure name="c8bf" id="c8bf" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 361px; max-height: 164px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 45.4%;"></div><img class="graf-image" data-image-id="1*s2nnva6Yhb2AuZDYYALbEA.png" data-width="361" data-height="164" src="https://cdn-images-1.medium.com/max/800/1*s2nnva6Yhb2AuZDYYALbEA.png"></div><figcaption class="imageCaption">Once again, we purposely do not simplify the gate derivative terms.</figcaption></figure><p name="41e0" id="41e0" class="graf graf--p graf-after--figure">Thus, our overall derivative becomes:</p><figure name="7b20" id="7b20" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 461px; max-height: 79px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 17.1%;"></div><img class="graf-image" data-image-id="1*o0dzU_s9WxoTYfOkQo1a0A.png" data-width="461" data-height="79" src="https://cdn-images-1.medium.com/max/800/1*o0dzU_s9WxoTYfOkQo1a0A.png"></div><figcaption class="imageCaption">Notice that the first term in this derivative is our forget gate.</figcaption></figure><p name="9307" id="9307" class="graf graf--p graf-after--figure">Pay attention to the caption of the diagram.</p><p name="4712" id="4712" class="graf graf--p graf-after--p">This is actually our <em class="markup--em markup--p-em">real </em>derivative. Modern LSTM implementations just use an auto differentiation library to compute derivatives, so they’ll probably come up with this. However, <em class="markup--em markup--p-em">effectively</em>, our gradient is just the forget gate, because the other three terms tend towards zero. Yup — they vanish. Why?</p><p name="b5a4" id="b5a4" class="graf graf--p graf-after--p">When we backprop error in LSTMs, we backprop through cell states to propagate the error from the outputs to the cell state we want. For example, if we want to backprop the error from the output at time <strong class="markup--strong markup--p-strong">t</strong> down <strong class="markup--strong markup--p-strong">k </strong>timesteps, then we need to compute the derivative of the cell state at time <strong class="markup--strong markup--p-strong">t </strong>to the cell state at time <strong class="markup--strong markup--p-strong">t-k</strong>. Look what happens when we do that:</p><figure name="db33" id="db33" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 642px; max-height: 79px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 12.3%;"></div><img class="graf-image" data-image-id="1*dBFbl6NCqp94Lnyb0taFWg.png" data-width="642" data-height="79" src="https://cdn-images-1.medium.com/max/800/1*dBFbl6NCqp94Lnyb0taFWg.png"></div></figure><p name="57e3" id="57e3" class="graf graf--p graf-after--figure">We didn’t simplify the gate w.r.t. cell state derivatives for a reason; as we backpropagate through time, they begin to vanish. Thus, whatever they multiplied with is killed off from making contributions to the gradient, too. So, effectively:</p><figure name="08ea" id="08ea" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 458px; max-height: 71px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 15.5%;"></div><img class="graf-image" data-image-id="1*Q9BJ7JxQ08YvfBW_OsJabw.png" data-width="458" data-height="71" src="https://cdn-images-1.medium.com/max/800/1*Q9BJ7JxQ08YvfBW_OsJabw.png"></div></figure><p name="e607" id="e607" class="graf graf--p graf-after--figure">The rationale behind this is pretty simple, and we don’t need math for it; these gates are the outputs of non-linearities eg. sigmoid and tanh. If we were to get the derivative of them in getting our cell state derivative, then this derivative would contain the derivatives of sigmoid/tanh in them. But, just because we don’t <em class="markup--em markup--p-em">need</em> to use math to show this, doesn’t mean we don’t <em class="markup--em markup--p-em">want </em>to 😏 :</p><figure name="0dbb" id="0dbb" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 541px; max-height: 79px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 14.6%;"></div><img class="graf-image" data-image-id="1*-mUDovQ8ovejmWPNoFSI1g.png" data-width="541" data-height="79" src="https://cdn-images-1.medium.com/max/800/1*-mUDovQ8ovejmWPNoFSI1g.png"></div><figcaption class="imageCaption">I obfuscated the input to the sigmoid function for the input gate, just for simplicity.</figcaption></figure><p name="f6be" id="f6be" class="graf graf--p graf-after--figure">Recall from our vanishing gradient article that the max output of sigmoid’s first order derivative is 0.25, and it’s something similar for tanh. This becomes textbook vanishing gradient problem. As we backprop through more and more cell states, the gradient terms become longer and longer, and this will definitely vanish. When they don’t vanish, they’ll be super minor contributions, so we can just leave them out for brevity.</p><blockquote name="95ec" id="95ec" class="graf graf--blockquote graf-after--p">Sidenote: one person reached out to me unsure of why gradients with long terms — aka, that are equal to the product of a lot of terms — usually vanishes/explodes. Here’s what I said in response:</blockquote><blockquote name="eabd" id="eabd" class="graf graf--blockquote graf--startsWithDoubleQuote graf-after--blockquote">“If you have long gradient terms, you probably have the vanishing gradient problem, unless you can guarantee those terms are around 1 each. If they’re not, it’ll explode or vanish. And, given the nature of the problems where this is an issue, it’s very unlikely they’ll be around 1 each. Especially if they are the output some non-linear function like sigmoid/tanh or their derivatives.</blockquote><blockquote name="944e" id="944e" class="graf graf--blockquote graf-after--blockquote">For example, let’s say the gradient term = k_1 × k_2 × k_3 × … × k_100. 100 terms in this product.</blockquote><blockquote name="1583" id="1583" class="graf graf--blockquote graf-after--blockquote">If each of these terms is, let’s say, around 0.5, then you have 0.5¹⁰⁰ = some absurdly low number. If you have each term be arond 1.5, then you have 1.5¹⁰⁰ which is some absurdly high number.<br><br>When we introduce tanh/sigmoid and/or their derivatives in these huge products, you can guarantee that they’ll saturate and die off. As mentioned, the max for sigmoid’s first order derivative is 0.25, so just imagine something like 0.25¹⁰⁰.</blockquote><p name="59c9" id="59c9" class="graf graf--p graf-after--blockquote">Ultimately, the reason I obfuscate these terms that vanish in the derivative is because I would like to show the effect of the forget gate on gradient flow now. If I included the other terms, the same implications would be present, but the math would just take longer to type out and render.</p><p name="e361" id="e361" class="graf graf--p graf-after--p">Because <strong class="markup--strong markup--p-strong">ƒw = i ⊙ g</strong>, we can redraw our diagram showing that <strong class="markup--strong markup--p-strong">ƒw </strong>won’t make any contributions to the gradient flow back. Again — <strong class="markup--strong markup--p-strong">ƒw</strong> does, but it’s effectively negligible, so we can just exclude it from our updated gradient flow diagram, which follows:</p><figure name="3217" id="3217" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 290px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 41.4%;"></div><img class="graf-image" data-image-id="1*RsHULZCgY6p-5bKkE99Q-Q.png" data-width="1095" data-height="453" data-action="zoom" data-action-value="1*RsHULZCgY6p-5bKkE99Q-Q.png" src="https://cdn-images-1.medium.com/max/800/1*RsHULZCgY6p-5bKkE99Q-Q.png"></div></figure><p name="8811" id="8811" class="graf graf--p graf-after--figure">But wait! This doesn’t look good; the gradients have to multiply by this <strong class="markup--strong markup--p-strong">f_t</strong> gate at each timestep. Before, they didn’t have to multiply by anything (or, in other words, they multiplied by 1) and flowed past super easily.</p><p name="9feb" id="9feb" class="graf graf--p graf-after--p">Machine learning researchers coined a name for the type of function we had before we introduced the forget gate where the derivative of one cell state w.r.t. the previous is <strong class="markup--strong markup--p-strong">1.0</strong>: “Constant Error Carousel” (CEC). With our new function, the derivative is equal to <strong class="markup--strong markup--p-strong">f</strong>. You’ll see this referred to as a “linear carousel” in papers.</p><p name="fdbb" id="fdbb" class="graf graf--p graf-after--p">Before we introduced a forget gate — where all we had was the additive interaction from <strong class="markup--strong markup--p-strong">ƒw </strong>— our cell state function was a CEC:</p><figure name="2bf9" id="2bf9" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 208px; max-height: 84px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 40.400000000000006%;"></div><img class="graf-image" data-image-id="1*9O__qOVOK1wxJDFy6m4YAQ.png" data-width="208" data-height="84" src="https://cdn-images-1.medium.com/max/800/1*9O__qOVOK1wxJDFy6m4YAQ.png"></div><figcaption class="imageCaption">A CEC — same as before, but no forget gate.</figcaption></figure><p name="61b2" id="61b2" class="graf graf--p graf-after--figure">The derivative of this cell state w.r.t. the previous one, again as long as we don’t backprop through the <strong class="markup--strong markup--p-strong">i </strong>and <strong class="markup--strong markup--p-strong">g</strong> gates, is just 1. That’s why gradients flow back super comfortably, without vanishing at all. Basically, for a CEC to exist in this context, the coefficient of <strong class="markup--strong markup--p-strong">c_t-1 </strong>needs to be 1.</p><p name="7ab2" id="7ab2" class="graf graf--p graf-after--p">Once we introduced this multiplicative interaction (for good reason), we got a linear carousel; the coefficient of <strong class="markup--strong markup--p-strong">c_t-1 </strong>is <strong class="markup--strong markup--p-strong">f</strong>. So, in our case, when <strong class="markup--strong markup--p-strong">f = 1</strong> (when we’re not going to forget) our function becomes a CEC, and our gradients will pretty much never vanish. If it’s close to 0, though, the gradient term will immediately die. Gradients will stay on the carousel for a while until the forget gate is triggered; the effect on the gradient is like a step function, in that it’s constant with a value of 1 and then drops off to zero/dies when we have <strong class="markup--strong markup--p-strong">f ≈ 0</strong>.</p><p name="3337" id="3337" class="graf graf--p graf-after--p">Intuitively, this seems problematic. Let’s do some math to investigate:</p><figure name="0955" id="0955" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 372px; max-height: 164px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 44.1%;"></div><img class="graf-image" data-image-id="1*UbqEhAyW7bMv_tDf-cvuWg.png" data-width="372" data-height="164" src="https://cdn-images-1.medium.com/max/800/1*UbqEhAyW7bMv_tDf-cvuWg.png"></div></figure><p name="2abc" id="2abc" class="graf graf--p graf-after--figure">The derivative of a cell state to the previous is <strong class="markup--strong markup--p-strong">f_t</strong>. The derivative of a cell state to two prior cell states is <strong class="markup--strong markup--p-strong">f_t ⊙ f_t-1</strong>. Thus:</p><figure name="1596" id="1596" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 603px; max-height: 83px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 13.8%;"></div><img class="graf-image" data-image-id="1*p9OndETS7tR-zUU-1TuaTw.png" data-width="603" data-height="83" src="https://cdn-images-1.medium.com/max/800/1*p9OndETS7tR-zUU-1TuaTw.png"></div></figure><p name="8238" id="8238" class="graf graf--p graf-after--figure">As we backpropagate through time, these forget gates keep chaining up and multiplying together to form the overall gradient term.</p><p name="6f12" id="6f12" class="graf graf--p graf-after--p">Now, imagine an LSTM with 100 timesteps. If we wanted to get the derivative of the error w.r.t. a weight like <strong class="markup--strong markup--p-strong">W_xi</strong>, to optimize it, remember that with BPTT we add up or average all the gradients from the different timesteps:</p><figure name="d867" id="d867" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 692px; max-height: 101px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 14.6%;"></div><img class="graf-image" data-image-id="1*rhb_2DO5MulJvzV9QxasMg.png" data-width="692" data-height="101" src="https://cdn-images-1.medium.com/max/800/1*rhb_2DO5MulJvzV9QxasMg.png"></div></figure><p name="a73a" id="a73a" class="graf graf--p graf-after--figure">OK. Now let’s look at an early (in time) term, like the gradient propagated from the error to the third cell:</p><figure name="9f34" id="9f34" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 106px; max-height: 79px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 74.5%;"></div><img class="graf-image" data-image-id="1*fAaYlJPgsPjRGJGoyc8XCw.png" data-width="106" data-height="79" src="https://cdn-images-1.medium.com/max/800/1*fAaYlJPgsPjRGJGoyc8XCw.png"></div></figure><p name="9972" id="9972" class="graf graf--p graf-after--figure">Remember that <strong class="markup--strong markup--p-strong">J </strong>is an addition of errors from<strong class="markup--strong markup--p-strong"> Y </strong>individual outputs, so we backpropagate through each of the outputs first:</p><figure name="7c21" id="7c21" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 609px; max-height: 164px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 26.900000000000002%;"></div><img class="graf-image" data-image-id="1*vsybuvtlGl-cQqUI1Pqbag.png" data-width="609" data-height="164" src="https://cdn-images-1.medium.com/max/800/1*vsybuvtlGl-cQqUI1Pqbag.png"></div></figure><p name="4355" id="4355" class="graf graf--p graf-after--figure">The first few terms, where we backprop <strong class="markup--strong markup--p-strong">y_k </strong>to <strong class="markup--strong markup--p-strong">c_3 </strong>where <strong class="markup--strong markup--p-strong">k &lt; 3</strong>, would just be equal to zero because <strong class="markup--strong markup--p-strong">c_3 </strong>only exists after these outputs have been computed.</p><p name="e686" id="e686" class="graf graf--p graf-after--p">Let’s assume that <strong class="markup--strong markup--p-strong">Y = 100 </strong>and continue with our assumption that <strong class="markup--strong markup--p-strong">t = 100 </strong>(so each timestep gives rise to an output), for simplicity. With this, let’s now look at the last term in this sum.</p><figure name="53d1" id="53d1" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 501px; max-height: 79px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 15.8%;"></div><img class="graf-image" data-image-id="1*JJIQxpb1mHjDn5KaoOKQkA.png" data-width="501" data-height="79" src="https://cdn-images-1.medium.com/max/800/1*JJIQxpb1mHjDn5KaoOKQkA.png"></div></figure><p name="6208" id="6208" class="graf graf--p graf-after--figure">That’s a lot of forget gates chained together. <strong class="markup--strong markup--p-strong">If one of these forget gates is [approximately] zero, the whole gradient dies</strong>. If these also tend to be a small number between 0 and 1, the whole thing will vanish, and <strong class="markup--strong markup--p-strong">c_3 </strong>won’t make any contributions to the gradient here.</p><p name="fb13" id="fb13" class="graf graf--p graf-after--p">This isn’t <em class="markup--em markup--p-em">intrinsically </em>an issue though! Because, when a forget gate is zero, it means that cell is no longer making any contributions past that point. If <strong class="markup--strong markup--p-strong">f_4 </strong>is zero, then any <strong class="markup--strong markup--p-strong">y</strong> outputs at/past timestep 4 won’t be influenced by <strong class="markup--strong markup--p-strong">c_3 </strong>(as well as <strong class="markup--strong markup--p-strong">c_2 </strong>and <strong class="markup--strong markup--p-strong">c_1</strong>) because we “erased” it from memory. Therefore that particular gradient should be zero. If <strong class="markup--strong markup--p-strong">y_80</strong> is zero, then any outputs at/past timestep 80 won’t be influenced by <strong class="markup--strong markup--p-strong">c_1, c_2, … , c_79</strong>. Same story here. If these forget gates are between 0 and 1, then the influence of our cell decays over time anyways, and our gradients will be very small, so they’ll reflect that. <a href="https://pdfs.semanticscholar.org/1154/0131eae85b2e11d53df7f1360eeb6476e7f4.pdf" data-href="https://pdfs.semanticscholar.org/1154/0131eae85b2e11d53df7f1360eeb6476e7f4.pdf" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">Gers 1999</a> calls this “releasing resources”.</p><p name="b6f8" id="b6f8" class="graf graf--p graf-after--p">Cell <strong class="markup--strong markup--p-strong">c_3 </strong>will still contribute to the overall gradient, though. For example, take this term:</p><figure name="0851" id="0851" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 474px; max-height: 79px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 16.7%;"></div><img class="graf-image" data-image-id="1*KZjK3wcZpYG_qnjkMB1HkA.png" data-width="474" data-height="79" src="https://cdn-images-1.medium.com/max/800/1*KZjK3wcZpYG_qnjkMB1HkA.png"></div></figure><p name="8736" id="8736" class="graf graf--p graf-after--figure">Here, we’re looking at <strong class="markup--strong markup--p-strong">y_12 </strong>instead of <strong class="markup--strong markup--p-strong">y_100</strong>. Chances are that, if you have a sequence of length 100, your 100th cell state isn’t drawing from your 3rd; the forget gate would have been triggered at some point by then. However, the 12th cell state probably will still be drawing from the ones before it.</p><p name="5cd8" id="5cd8" class="graf graf--p graf-after--p">If we decide not to forget in the first 12 timesteps, ie. <strong class="markup--strong markup--p-strong">f_1 … f_12 </strong>are each not far from 1, then <strong class="markup--strong markup--p-strong">c_3 </strong>would have more influence over <strong class="markup--strong markup--p-strong">y_12 </strong>and the error that stems from <strong class="markup--strong markup--p-strong">y_12</strong>. Thus, the gradient would not vanish and <strong class="markup--strong markup--p-strong">c_3</strong> still contributes to update <strong class="markup--strong markup--p-strong">W_xi</strong>, it just doesn’t contribute a gradient where it’s not warranted to (that is, where it doesn’t actually contribute to any activation, because it’s been forgotten). To summarize: one activated forget gate will indeed kill off gradient flow to cell(s), but that is a good thing because the network is learning that that gradient from the future has no benefit and is completely irrelevant to those particular cell(s), since those cells have been forgotten by then. In practice, different cells learn different ranges of context, some short, some long. This is a <em class="markup--em markup--p-em">benefit </em>for LSTMs.</p><p name="7b24" id="7b24" class="graf graf--p graf-after--p">So, given a gradient between two cell states in time, when all of these forget gates are [approximately] equal to 1, the gradient signal will remain stable, because we’re multiplying by 1 at each timestep — effectively, not multiplying by anything at all. In such a case, our gradient flow diagram would look like this:</p><figure name="9b4b" id="9b4b" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 222px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 31.7%;"></div><img class="graf-image" data-image-id="1*rBJm9F6zz8drQnDlWd7wvQ.png" data-width="2190" data-height="694" data-action="zoom" data-action-value="1*rBJm9F6zz8drQnDlWd7wvQ.png" src="https://cdn-images-1.medium.com/max/800/1*rBJm9F6zz8drQnDlWd7wvQ.png"></div><figcaption class="imageCaption">It’s… it’s beautiful!</figcaption></figure><p name="f12e" id="f12e" class="graf graf--p graf-after--figure">The gradient will have literally zero interactions or disturbances, and will just flow through like it’s driving 150 mph on an empty countryside America highway. The beauty of CECs is that they’re <em class="markup--em markup--p-em">always </em>like this.</p><p name="87cf" id="87cf" class="graf graf--p graf-after--p">But, let’s get back to reality. LSTMs aren’t CECs. One disadvantage of these forget gates chaining together is that it could block learning. That is, when we set out to train our LSTM, the forget gates have not been learned; we have to learn them while we learn everything else. So, if they all start around 0, no gradients will flow through our cell states when we perform BPTT, and learning won’t happen at all.</p><p name="3db3" id="3db3" class="graf graf--p graf-after--p">The obvious solution is to set the forget gate bias to a very large value when training, so it starts at 1 instead of 0 (because <strong class="markup--strong markup--p-strong">y = 1</strong> is to the far right of the sigmoid function, so adding to the input will ensure ~1 will be the output). In early stages of training, the forget gates equalling/approximating 1 will result in learning not being blocked. So many papers do this and mention it explicitly such that this forget gate bias could even be considered a hyperparameter.</p><p name="d763" id="d763" class="graf graf--p graf-after--p graf--trailing">By introducing forget gates, we stray from CECs and thus the guarantee that our gradients will never ever vanish. But, again, we do it for good reason. And when gradients vanish it’s because we chose to forget that cell — so it’s not necessarily a bad thing. We just need to make sure the forget gates don’t block learning in initial stages of training.</p></div></div></section><section name="ccfc" class="section section--body"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><p name="e1c7" id="e1c7" class="graf graf--p graf--leading">We can try computing some more derivatives, just for fun! Let’s sub in real values for the timesteps, backprop across more than one timestep, and do it for a gate this time.</p><p name="62f7" id="62f7" class="graf graf--p graf-after--p">We’ll expand <strong class="markup--strong markup--p-strong">c_4 </strong>and express it in terms of our gates only. In the process, each <strong class="markup--strong markup--p-strong">c_t, </strong>except <strong class="markup--strong markup--p-strong">c_1</strong>, will collapse into a few interactions between the <strong class="markup--strong markup--p-strong">f</strong>, <strong class="markup--strong markup--p-strong">i</strong>,<strong class="markup--strong markup--p-strong"> </strong>and<strong class="markup--strong markup--p-strong"> g </strong>gate:</p><figure name="26eb" id="26eb" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 669px; max-height: 166px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 24.8%;"></div><img class="graf-image" data-image-id="1*2LZxa4YAMGCJqOrirI1-_w.png" data-width="669" data-height="166" src="https://cdn-images-1.medium.com/max/800/1*2LZxa4YAMGCJqOrirI1-_w.png"></div></figure><p name="5092" id="5092" class="graf graf--p graf-after--figure">Now, let’s get the derivative of <strong class="markup--strong markup--p-strong">c_4</strong> with respect to one of the earliest possible gates, like <strong class="markup--strong markup--p-strong">g_2</strong>. In the expression above, this turns out to just be the coefficient of <strong class="markup--strong markup--p-strong">g_2</strong>:</p><figure name="c936" id="c936" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 205px; max-height: 79px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 38.5%;"></div><img class="graf-image" data-image-id="1*rHDurdaN9SKnChWfyVk38w.png" data-width="205" data-height="79" src="https://cdn-images-1.medium.com/max/800/1*rHDurdaN9SKnChWfyVk38w.png"></div></figure><p name="708a" id="708a" class="graf graf--p graf-after--figure">We experience the same neatness here as with the cell state backprop! It makes complete sense that the gradient would be <strong class="markup--strong markup--p-strong">i_2 ⊙</strong> <strong class="markup--strong markup--p-strong">f_3 ⊙ f_4</strong>, since <strong class="markup--strong markup--p-strong">i_2 </strong>controls what influence <strong class="markup--strong markup--p-strong">g_2 </strong>has over <strong class="markup--strong markup--p-strong">c_2</strong>, <strong class="markup--strong markup--p-strong">f_3 </strong>controls what influence <strong class="markup--strong markup--p-strong">c_2 </strong>has on <strong class="markup--strong markup--p-strong">c_3</strong>, and <strong class="markup--strong markup--p-strong">f_4 </strong>controls what influence <strong class="markup--strong markup--p-strong">c_3 </strong>has over <strong class="markup--strong markup--p-strong">c_4</strong>. Notice the chaining up of the forget gates 👻; everything about the carousels I just talked about — and what they imply about vanishing gradients — applies here.</p><p name="6229" id="6229" class="graf graf--p graf-after--p">I’ll leave it up to you to derive something similar for the other gates.</p><p name="f15d" id="f15d" class="graf graf--p graf-after--p">And that’s it! That’s why LSTMs rock their socks off when it comes to keeping their gradients in check,.</p><p name="627a" id="627a" class="graf graf--p graf-after--p">Here’s a neat GIF showing a visual representation of the gradients that exist at each timestep, starting from timestep 128 and going all the way to the first, during backprop. More noise represents greater values:</p></div><div class="section-inner sectionLayout--outsetColumn"><figure name="2414" id="2414" class="graf graf--figure graf--iframe graf--layoutOutsetCenter graf-after--p"><div class="aspectRatioPlaceholder is-locked"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 56.2%;"></div><div class="iframeContainer"><IFRAME data-width="854" data-height="480" width="980" height="551" src="/media/c239248e2e0b9a4aadc7b43d8c08ca12?postId=10300100899b" data-media-id="c239248e2e0b9a4aadc7b43d8c08ca12" data-thumbnail="https://i.embed.ly/1/image?url=http%3A%2F%2Fi.imgur.com%2FvaNahKEh.jpg&amp;key=4fce0568f2ce49e8b54624ef71a8a5bd" allowfullscreen frameborder="0"></IFRAME></div></div><figcaption class="imageCaption">Super highway indeed. <a href="http://imgur.com/gallery/vaNahKE" data-href="http://imgur.com/gallery/vaNahKE" class="markup--anchor markup--figure-anchor" rel="noopener" target="_blank">imgur.com/gallery/vaNahKE</a>.</figcaption></figure></div><div class="section-inner sectionLayout--insetColumn"><p name="868a" id="868a" class="graf graf--p graf-after--figure">As you can see, the vanilla RNN’s gradients die off way quicker than the LSTM’s. The RNN is almost immediate in comparison. LSTMs seem like a super highway indeed, although it does seem that they do vanish. In this diagram, it can be suggested that the gradients perhaps die for the LSTM eventually because we chose to forget early cell states; again, this depends on the application at hand, and is <em class="markup--em markup--p-em">learnable</em>. (I’m not sure if this GIF uses truncated backprop, so that could be another thing. In general, I don’t know the context of this GIF.) Also, part of the gradient signal definitely vanishes—it’s the signals that pass through the <strong class="markup--strong markup--p-strong">f/i/g </strong>gates that we looked at earlier and obfuscated from the cell state→cell state derivative. We showed they would vanish because of tanh/sigmoid derivatives; initially, these signals will make a fairly significant contribution, but over time they’ll get smaller and smaller. That’s the explanation for this GIF.</p><p name="2b66" id="2b66" class="graf graf--p graf-after--p">Exploding gradients is still an issue, though. Recall that when we have a bunch of gradient terms added together, if some of them vanish it doesn’t mean the whole thing will vanish (versus if they were multiplied together). However, if some of the gradients explode, the whole thing explodes; x + 0 = x, but x + <strong class="markup--strong markup--p-strong">∞</strong> = <strong class="markup--strong markup--p-strong">∞</strong>. If cell states become unstable and grow too much in some rare scenario, then our gradients could explode. In such a case we’d need to implement gradient clipping, which is where we choose some arbitrary threshold that gradients cannot be larger than; so, <code class="markup--code markup--p-code">grad = min(grad, clip_threshold)</code>. This would enable the LSTM to deal with such cases without essentially collapsing. Many successful LSTM applications use gradient clipping.</p><p name="69b6" id="69b6" class="graf graf--p graf-after--p graf--trailing">Usually, though, exploding gradients are avoided because sooner or later the forget gate in the carousel is triggered and we reset the memory.</p></div></div></section><section name="70a1" class="section section--body section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><p name="3679" id="3679" class="graf graf--p graf--leading">There are variants of LSTMs. People have tried modifying the model, like computing the hidden state without using tanh activation (so <strong class="markup--strong markup--p-strong">h_t = o ⊙ c_t</strong>) or ditching the <strong class="markup--strong markup--p-strong">i </strong>input gate and only using <strong class="markup--strong markup--p-strong">g</strong>, since that would still satisfy the -1 to 1 range. The results didn’t change by much.</p><p name="dc3e" id="dc3e" class="graf graf--p graf-after--p">In fact, some researchers even applied evolutionary algorithms to spawn and test a bunch of variants on the standard LSTM equations. Most of the good ones just worked roughly the same.</p><p name="c7f8" id="c7f8" class="graf graf--p graf-after--p">This highlights an issue with LSTMs — they are definitely fairly handwavy. We use them because their architecture allows us to prevent gradients from vanishing such that we can learn long-term dependencies, but there’s not much theoretical or empirical backing for them. ANNs and RNNs make sense in that they’re biologically inspired and that they’re essentially just deep composite functions that have parameters we can optimize. LSTMs stray so far from statistical methods and introduce complex concepts/architectures that work but aren’t necessarily justified from the get-go. Fully understanding why LSTMs work so well and coming up with better/simpler architectures is a hot topic of research right now.</p><p name="4bca" id="4bca" class="graf graf--p graf-after--p">There are also other variants of RNNs, similar to LSTMs, like <a href="https://en.wikipedia.org/wiki/Gated_recurrent_unit" data-href="https://en.wikipedia.org/wiki/Gated_recurrent_unit" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">GRUs</a> (Gated Recurrent Units). LSTM is still the king, but GRUs in particular have grown in popularity and are seen in many recent, well-respected research papers. It’s a must learn next to LSTMs, but this article would get too bloated with it. TL;DR: GRUs have a less complex architecture than LSTMs but achieves similar results, and they can control the flow of information without requiring a memory cell. And, they’re fairly new. (See, told you “coming up with better/simpler architectures is a hot topic of research right now” is true!)</p><h3 name="c57a" id="c57a" class="graf graf--h3 graf-after--p">Yay RNNs!</h3><p name="8a09" id="8a09" class="graf graf--p graf-after--h3">Wanna see a couple cool things? Practical applications were talked about in the first section, and the next section will walk through more technical applications of RNNs ie. in recent research papers. This section, instead, contains some fun things RNNs have done or produced that’ll ease yourself from the hell that was actually understanding them. As simply as we try to convey things on this blog, LSTMs be LSTMs.</p><p name="7337" id="7337" class="graf graf--p graf-after--p">Sidenote: now, don’t be frightened by “RNNs”. <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">Do </em></strong>be frightened by “vanilla RNNs”, in most contexts, but we use RNN as an umbrella term, and it most often refers to an LSTM or a similar variant like GRU.</p><p name="f30f" id="f30f" class="graf graf--p graf-after--p">Many if not all of these are taken from Andrej Karpathy’s <a href="https://www.youtube.com/watch?v=cO0a0QYmFm8&amp;index=10&amp;list=PLlJy-eBtNFt6EuMxFYRiNRS07MCWN5UIA" data-href="https://www.youtube.com/watch?v=cO0a0QYmFm8&amp;index=10&amp;list=PLlJy-eBtNFt6EuMxFYRiNRS07MCWN5UIA" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">CS231n lecture</a>, or his blog post on the same subject:</p><div name="d19b" id="d19b" class="graf graf--mixtapeEmbed graf-after--p"><a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/" data-href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/" class="markup--anchor markup--mixtapeEmbed-anchor" title="http://karpathy.github.io/2015/05/21/rnn-effectiveness/"><strong class="markup--strong markup--mixtapeEmbed-strong">The Unreasonable Effectiveness of Recurrent Neural Networks</strong><br><em class="markup--em markup--mixtapeEmbed-em">Musings of a Computer Scientist.</em>karpathy.github.io</a><a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="3019eda2afc61d3398ef9b0a1762edf9" data-thumbnail-img-id="0*7sIxt7RqO7deGldw." style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*7sIxt7RqO7deGldw.);"></a></div><p name="83d4" id="83d4" class="graf graf--p graf-after--mixtapeEmbed">You should <strong class="markup--strong markup--p-strong">most certainly</strong> visit either his blog post or lecture for info on exactly how these experiments were conducted, and for more interpretation on their results. Taking a look at the ‘Visualizing the predictions and the “neuron” firings in the RNN’ section would also be helpful to gain more insight and intuition on how RNNs work and learn over time.</p><p name="c051" id="c051" class="graf graf--p graf-after--p">A recurrent neural network generated this body of text, after it “read” a bunch of Shakespeare:</p><figure name="802f" id="802f" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 574px; max-height: 619px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 107.80000000000001%;"></div><img class="graf-image" data-image-id="1*BkvFHx8nYL1-NHmzZ_BbCQ.png" data-width="574" data-height="619" src="https://cdn-images-1.medium.com/max/800/1*BkvFHx8nYL1-NHmzZ_BbCQ.png"></div></figure><p name="62d6" id="62d6" class="graf graf--p graf-after--figure">Similarly, Karpathy gave an LSTM a lot of Paul Graham’s startup advice and life wisdom to read, and it produced this:</p><blockquote name="3654" id="3654" class="graf graf--pullquote graf--startsWithDoubleQuote graf-after--p">“The surprised in investors weren’t going to raise money. I’m not the company with the time there are all interesting quickly, don’t have to get off the same programmers. There’s a super-angel round fundraising, why do you can do. If you have a different physical investment are become in people who reduced in a startup with the way to argument the acquirer could see them just that you’re also the founders will part of users’ affords that and an alternation to the idea. [2] Don’t work at first member to see the way kids will seem in advance of a bad successful startup. And if you have to act the big company too.”</blockquote><p name="ee9f" id="ee9f" class="graf graf--p graf-after--pullquote">A lot of relevant terminology, but it doesn’t really… come together 😖.</p><p name="1fc3" id="1fc3" class="graf graf--p graf-after--p">An LSTM can even generate valid XML, after reading Wikipedia!:</p><pre name="89df" id="89df" class="graf graf--pre graf-after--p"><code class="markup--code markup--pre-code">&lt;page&gt;<br>  &lt;title&gt;Antichrist&lt;/title&gt;<br>  &lt;id&gt;865&lt;/id&gt;<br>  &lt;revision&gt;<br>    &lt;id&gt;15900676&lt;/id&gt;<br>    &lt;timestamp&gt;2002-08-03T18:14:12Z&lt;/timestamp&gt;<br>    &lt;contributor&gt;<br>      &lt;username&gt;Paris&lt;/username&gt;<br>      &lt;id&gt;23&lt;/id&gt;<br>    &lt;/contributor&gt;<br>    &lt;minor /&gt;<br>    &lt;comment&gt;Automated conversion&lt;/comment&gt;<br>    &lt;text xml:space=&quot;preserve&quot;&gt;#REDIRECT [[Christianity]]&lt;/text&gt;<br>  &lt;/revision&gt;<br>&lt;/page&gt;</code></pre><p name="7056" id="7056" class="graf graf--p graf-after--pre">After a similar LSTM read the raw Latex version of an advanced algebra textbook, it came up with this — put frankly — fancy looking bogus. Let’s be real, you could definitely believe this was actual math 😜:</p><figure name="221d" id="221d" class="graf graf--figure graf--iframe graf-after--p"><div class="aspectRatioPlaceholder is-locked"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 130%;"></div><div class="iframeContainer"><IFRAME data-width="600" data-height="780" width="600" height="780" src="/media/32e75fb2f7f388775689a155c5c27d86?postId=10300100899b" data-media-id="32e75fb2f7f388775689a155c5c27d86" allowfullscreen frameborder="0"></IFRAME></div></div></figure><p name="e129" id="e129" class="graf graf--p graf-after--figure">An LSTM also read the Linux source code, and tried to write some code of its own:</p><pre name="ce46" id="ce46" class="graf graf--pre graf-after--p"><code class="markup--code markup--pre-code u-paddingRight0 u-marginRight0"><em class="markup--em markup--pre-em">/*<br> * Increment the size file of the new incorrect UI_FILTER group information<br> * of the size generatively.<br> */</em><br><strong class="markup--strong markup--pre-strong">static</strong> <strong class="markup--strong markup--pre-strong">int</strong> <strong class="markup--strong markup--pre-strong">indicate_policy</strong>(<strong class="markup--strong markup--pre-strong">void</strong>)<br>{<br>  <strong class="markup--strong markup--pre-strong">int</strong> error;<br>  <strong class="markup--strong markup--pre-strong">if</strong> (fd <strong class="markup--strong markup--pre-strong">==</strong> MARN_EPT) {<br>    <em class="markup--em markup--pre-em">/*<br>     * The kernel blank will coeld it to userspace.<br>     */</em><br>    <strong class="markup--strong markup--pre-strong">if</strong> (ss<strong class="markup--strong markup--pre-strong">-&gt;</strong>segment <strong class="markup--strong markup--pre-strong">&lt;</strong> mem_total)<br>      unblock_graph_and_set_blocked();<br>    <strong class="markup--strong markup--pre-strong">else</strong><br>      ret <strong class="markup--strong markup--pre-strong">=</strong> 1;<br>    <strong class="markup--strong markup--pre-strong">goto</strong> bail;<br>  }<br>  segaddr <strong class="markup--strong markup--pre-strong">=</strong> in_SB(in.addr);<br>  selector <strong class="markup--strong markup--pre-strong">=</strong> seg <strong class="markup--strong markup--pre-strong">/</strong> 16;<br>  setup_works <strong class="markup--strong markup--pre-strong">=</strong> true;<br>  <strong class="markup--strong markup--pre-strong">for</strong> (i <strong class="markup--strong markup--pre-strong">=</strong> 0; i <strong class="markup--strong markup--pre-strong">&lt;</strong> blocks; i<strong class="markup--strong markup--pre-strong">++</strong>) {<br>    seq <strong class="markup--strong markup--pre-strong">=</strong> buf[i<strong class="markup--strong markup--pre-strong">++</strong>];<br>    bpf <strong class="markup--strong markup--pre-strong">=</strong> bd<strong class="markup--strong markup--pre-strong">-&gt;</strong>bd.next <strong class="markup--strong markup--pre-strong">+</strong> i <strong class="markup--strong markup--pre-strong">*</strong> search;<br>    <strong class="markup--strong markup--pre-strong">if</strong> (fd) {<br>      current <strong class="markup--strong markup--pre-strong">=</strong> blocked;<br>    }<br>  }<br>  rw<strong class="markup--strong markup--pre-strong">-&gt;</strong>name <strong class="markup--strong markup--pre-strong">=</strong> &quot;Getjbbregs&quot;;<br>  bprm_self_clearl(<strong class="markup--strong markup--pre-strong">&amp;</strong>iv<strong class="markup--strong markup--pre-strong">-&gt;</strong>version);<br>  regs<strong class="markup--strong markup--pre-strong">-&gt;</strong>new <strong class="markup--strong markup--pre-strong">=</strong> blocks[(BPF_STATS <strong class="markup--strong markup--pre-strong">&lt;&lt;</strong> info<strong class="markup--strong markup--pre-strong">-&gt;</strong>historidac)] <strong class="markup--strong markup--pre-strong">|</strong> PFMR_CLOBATHINC_SECONDS <strong class="markup--strong markup--pre-strong">&lt;&lt;</strong> 12;<br>  <strong class="markup--strong markup--pre-strong">return</strong> segtable;<br>}</code></pre><p name="da41" id="da41" class="graf graf--p graf-after--pre"><strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">SUPERINTELLIGENCE MUCH‽ SELF-RECURSIVE IMPROVEMENT MUCH‽ THE END OF THE UNIVERSE MUCH‽</em></strong></p><p name="7094" id="7094" class="graf graf--p graf-after--p">Nope. Just some code doesn’t compile or make any sense. It even has its own bogus comments!</p><p name="b3fc" id="b3fc" class="graf graf--p graf-after--p">Generating music? Easy! A fun watch:</p><figure name="eae8" id="eae8" class="graf graf--figure graf--iframe graf-after--p"><div class="aspectRatioPlaceholder is-locked"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 56.2%;"></div><div class="iframeContainer"><IFRAME data-width="854" data-height="480" width="700" height="393" src="/media/8de70b7fa3e5cb979099278112052953?postId=10300100899b" data-media-id="8de70b7fa3e5cb979099278112052953" data-thumbnail="https://i.embed.ly/1/image?url=https%3A%2F%2Fi.ytimg.com%2Fvi%2FA2gyidoFsoI%2Fhqdefault.jpg&amp;key=4fce0568f2ce49e8b54624ef71a8a5bd" allowfullscreen frameborder="0"></IFRAME></div></div></figure><p name="c80a" id="c80a" class="graf graf--p graf-after--figure">A more informative watch:</p><figure name="565f" id="565f" class="graf graf--figure graf--iframe graf-after--p"><div class="aspectRatioPlaceholder is-locked"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 56.2%;"></div><div class="iframeContainer"><IFRAME data-width="854" data-height="480" width="700" height="393" src="/media/3f992ef4ac506dafa8d2d8badfc31dc2?postId=10300100899b" data-media-id="3f992ef4ac506dafa8d2d8badfc31dc2" data-thumbnail="https://i.embed.ly/1/image?url=https%3A%2F%2Fi.ytimg.com%2Fvi%2FaSr8_QQYpYM%2Fhqdefault.jpg&amp;key=4fce0568f2ce49e8b54624ef71a8a5bd" allowfullscreen frameborder="0"></IFRAME></div></div></figure><p name="6518" id="6518" class="graf graf--p graf-after--figure">Something even cooler and… creepier (seriously, the results after the first couple iterations of training are so unsettling):</p><figure name="7c5c" id="7c5c" class="graf graf--figure graf--iframe graf-after--p"><div class="aspectRatioPlaceholder is-locked"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 56.2%;"></div><div class="iframeContainer"><IFRAME data-width="854" data-height="480" width="700" height="393" src="/media/b5f70a5d514e61ad4646217c70974843?postId=10300100899b" data-media-id="b5f70a5d514e61ad4646217c70974843" data-thumbnail="https://i.embed.ly/1/image?url=https%3A%2F%2Fi.ytimg.com%2Fvi%2FNG-LATBZNBs%2Fhqdefault.jpg&amp;key=4fce0568f2ce49e8b54624ef71a8a5bd" allowfullscreen frameborder="0"></IFRAME></div></div></figure><h3 name="7eb1" id="7eb1" class="graf graf--h3 graf-after--figure">In Practice</h3><p name="a21c" id="a21c" class="graf graf--p graf-after--h3">So we’ve seen how RNNs work in theory; now where do they fit in in practice?</p><p name="e928" id="e928" class="graf graf--p graf-after--p">As it turns out, recurrent neural networks can do a whole lot. I’ll try to cover a few of the important, significant, and interesting uses that have cropped up over the last few years.</p><h4 name="e30a" id="e30a" class="graf graf--h4 graf-after--p">Bidirectional Recurrent Neural Networks</h4><p name="5985" id="5985" class="graf graf--p graf-after--h4"><strong class="markup--strong markup--p-strong">The Problem</strong>: Giving the network access to a sequence of vectors is fine and dandy, but what if we want our output at time <strong class="markup--strong markup--p-strong">t</strong> to be conditioned on an input vector that comes at a later timestep? Take the example of speech recognition, where our input vectors are some kind of audio features at time <strong class="markup--strong markup--p-strong">t</strong> and the output is the predicted <a href="https://en.wikipedia.org/wiki/Phoneme" data-href="https://en.wikipedia.org/wiki/Phoneme" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">phoneme</a><em class="markup--em markup--p-em"> </em>at that time. In our traditional RNN architecture, the output at time <strong class="markup--strong markup--p-strong">t</strong> is conditioned <em class="markup--em markup--p-em">only</em> on input vectors <strong class="markup--strong markup--p-strong">1..t</strong>, but as it turns out future information might be useful too. The sounds at time step <strong class="markup--strong markup--p-strong">t+1 </strong>(and maybe <strong class="markup--strong markup--p-strong">t+2</strong>, <strong class="markup--strong markup--p-strong">t+3</strong>, …) are likely part of the same phoneme, and therefore could help us make more accurate predictions. But our network won’t have access to them until we already output a prediction at time <strong class="markup--strong markup--p-strong">t</strong>. That’s bad.</p><p name="60ef" id="60ef" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">The Solution</strong>: We essentially “double up” each RNN neuron into two independent neurons — a “forward” neuron and a “backward” neuron. The forward neuron is the same as a regular RNN neuron, which gets inputs <strong class="markup--strong markup--p-strong">0..T</strong> sequentially, updating its internal state and outputting some value at each time step along the way. The backward neuron follows the same general principle, but it sees the input vectors in reverse order.</p><p name="e2ef" id="e2ef" class="graf graf--p graf-after--p">We’ll look at an example to make sense of all this.</p><figure name="2749" id="2749" class="graf graf--figure graf--layoutOutsetLeft graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 404px; max-height: 504px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 124.8%;"></div><img class="graf-image" data-image-id="1*Vsvw39SW0xEwRLijLRb3qg.png" data-width="404" data-height="504" src="https://cdn-images-1.medium.com/max/600/1*Vsvw39SW0xEwRLijLRb3qg.png"></div><figcaption class="imageCaption">This is a typical recurrent neural network: at each timestep, the hidden state is updated based on the latest input.</figcaption></figure><figure name="89e5" id="89e5" class="graf graf--figure graf--layoutOutsetLeft graf-after--figure"><div class="aspectRatioPlaceholder is-locked" style="max-width: 525px; max-height: 437px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 83.2%;"></div><img class="graf-image" data-image-id="1*JZNjmHjYFVcHrPKTDmvoXQ.png" data-width="606" data-height="504" data-action="zoom" data-action-value="1*JZNjmHjYFVcHrPKTDmvoXQ.png" src="https://cdn-images-1.medium.com/max/600/1*JZNjmHjYFVcHrPKTDmvoXQ.png"></div><figcaption class="imageCaption">This is a bidirectional recurrent neural network. There are two neurons: one that takes inputs like normal, and one that takes them in reverse. Their output is combined to produce one output.</figcaption></figure><p name="ea03" id="ea03" class="graf graf--p graf-after--figure">Let’s walk through this timestep-by-timestep. At <strong class="markup--strong markup--p-strong">t=0</strong>, our vanilla RNN cell takes the input, updates its hidden state, and outputs a value. Now let’s look at the BiRNN: the “forward” half of our BiRNN neuron does exactly the same thing, but the “backward” half looks through all of our inputs — in reverse order, <strong class="markup--strong markup--p-strong">t=T..0</strong> — and updates its hidden state with each one. Then when we get to the <strong class="markup--strong markup--p-strong">t=0</strong> input vector, it updates its hidden state one last time and outputs a final value. We then take this final output value and combine it with the “forward” half (“combine” is pretty loosely-defined, usually just by concatenation or addition). Moving on to <strong class="markup--strong markup--p-strong">t=1</strong>, our “forward” part reads in the next input, updates state, and outputs another value. Combined with the second-to-last output of our “backward” counterpart, and we have the second output of our BiRNN neuron. Rinse and repeat.</p><p name="1bf8" id="1bf8" class="graf graf--p graf-after--p">And that’s the general idea. Neat, right? BiRNNs (and their more adept cousin, BiLSTMs) are used all over the place. Maybe we’ll see them popping up in some of the other case studies that we’ll be looking at.</p><h4 name="7549" id="7549" class="graf graf--h4 graf-after--p">Autoencoders</h4><p name="cdb3" id="cdb3" class="graf graf--p graf-after--h4">Remember when we talked about <a href="https://ayearofai.com/lenny-2-autoencoders-and-word-embeddings-oh-my-576403b0113a#.oevzdnnnp" data-href="https://ayearofai.com/lenny-2-autoencoders-and-word-embeddings-oh-my-576403b0113a#.oevzdnnnp" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">autoencoders</a>? Turns out we can use RNNs there too!</p><p name="107c" id="107c" class="graf graf--p graf-after--p">Let’s refresh: what is an autoencoder? Put simply, it’s a clever way of tricking a neural network to learn a useful representation of some data. Let’s say we have a dataset of images of faces, and we want to compress the thousands of numbers representing RGB values of pixels down into a 500-dimensional latent vector. We construct a network as such, where the middle layer has 500 neurons:</p><figure name="08e3" id="08e3" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 677px; max-height: 506px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 74.7%;"></div><img class="graf-image" data-image-id="0*M1bVZtZ6UPTyXoiy." data-width="677" data-height="506" src="https://cdn-images-1.medium.com/max/800/0*M1bVZtZ6UPTyXoiy."></div><figcaption class="imageCaption"><a href="https://en.wikipedia.org/wiki/Autoencoder#/media/File:Autoencoder_structure.png" data-href="https://en.wikipedia.org/wiki/Autoencoder#/media/File:Autoencoder_structure.png" class="markup--anchor markup--figure-anchor" rel="nofollow noopener noopener noopener" target="_blank">https://en.wikipedia.org/wiki/Autoencoder#/media/File:Autoencoder_structure.png</a></figcaption></figure><p name="fe44" id="fe44" class="graf graf--p graf-after--figure">…and train it to reproduce the input in the output.</p><p name="46d5" id="46d5" class="graf graf--p graf-after--p">Let’s explore this idea a little further. Imagine that data is flowing through our network, starting with the input layer and through each subsequent layer. We can view each layer as performing a transformation, converting our input to another vector, and then that vector into another, until we get our output. If we train our network to reproduce the input, that means that each intermediate vector must still represent the same information as the input, in some form or another. Essentially, the activations of each layer are a new representation of our input vector. If our network trains well, we can convert a 10,000-dimensional vector of pixel values into a 500-dimensional vector of image features which can be converted back into a 10,000-dimensional vector of pixel values that approximates what the input would have been.</p><figure name="f1d3" id="f1d3" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 201px; max-height: 34px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 16.900000000000002%;"></div><img class="graf-image" data-image-id="0*RP5VZyqDJ9JI5wBk." data-width="201" data-height="34" src="https://cdn-images-1.medium.com/max/800/0*RP5VZyqDJ9JI5wBk."></div></figure><figure name="1008" id="1008" class="graf graf--figure graf-after--figure"><div class="aspectRatioPlaceholder is-locked" style="max-width: 200px; max-height: 34px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 17%;"></div><img class="graf-image" data-image-id="0*Tc8mc_NWMmQZ15ND." data-width="200" data-height="34" src="https://cdn-images-1.medium.com/max/800/0*Tc8mc_NWMmQZ15ND."></div></figure><p name="c1de" id="c1de" class="graf graf--p graf-after--figure">Let’s make this a tad more concrete. We have two functions, <strong class="markup--strong markup--p-strong">f</strong> and <strong class="markup--strong markup--p-strong">g</strong>. <strong class="markup--strong markup--p-strong">f</strong> is our encoder, mapping from an <strong class="markup--strong markup--p-strong">n</strong>-long vector to an <strong class="markup--strong markup--p-strong">m</strong>-long vector. (<strong class="markup--strong markup--p-strong">n</strong> is the size of our input, <strong class="markup--strong markup--p-strong">m</strong> is the size of our latent representation.) <strong class="markup--strong markup--p-strong">g</strong> is our decoder, which maps back from an <strong class="markup--strong markup--p-strong">m</strong>-long vector to an <strong class="markup--strong markup--p-strong">n</strong>-long vector. In the normal autoencoder setting, both <strong class="markup--strong markup--p-strong">f</strong> and <strong class="markup--strong markup--p-strong">g</strong> are neural networks trained jointly (or different parts of a single network, same thing really) to reconstruct x.</p><figure name="f029" id="f029" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 182px; max-height: 37px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 20.3%;"></div><img class="graf-image" data-image-id="0*1R_heM-ujpUvlbGM." data-width="182" data-height="37" src="https://cdn-images-1.medium.com/max/800/0*1R_heM-ujpUvlbGM."></div></figure><p name="1680" id="1680" class="graf graf--p graf-after--figure">So, where do RNNs fit in? Let’s say our inputs are now sequences of vectors instead of a single vector. We can use a similar concept, with both the encoder and decoder represented using an RNN. Here’s how it works: we feed our input sequence into the <em class="markup--em markup--p-em">encoder RNN</em>. With each input vector of the sequence, this <em class="markup--em markup--p-em">encoder</em> updates its internal state. Eventually, once it has seen the entire input, we have some final network internal state which represents our entire input sequence. Neat! Now, we make the hidden state of our <em class="markup--em markup--p-em">decoder RNN</em> the initial hidden state of our <em class="markup--em markup--p-em">encoder</em>, and ask it to spit out a sequence. Ideally, it spits out something close to what the initial sequence was.</p><p name="b8c2" id="b8c2" class="graf graf--p graf-after--p">Going back to our math-y definitions, we see that it basically fits in to the same framework, except we have <strong class="markup--strong markup--p-strong">q</strong> n-long vectors going into <strong class="markup--strong markup--p-strong">f</strong> and coming out of <strong class="markup--strong markup--p-strong">g</strong>. So <strong class="markup--strong markup--p-strong">q</strong> <strong class="markup--strong markup--p-strong">n</strong>-long vectors go in to <strong class="markup--strong markup--p-strong">f</strong>, and a single <strong class="markup--strong markup--p-strong">m</strong>-long vector comes out. We then give this <strong class="markup--strong markup--p-strong">m</strong>-long vector back to <strong class="markup--strong markup--p-strong">g</strong>, which spits out <strong class="markup--strong markup--p-strong">q</strong> <strong class="markup--strong markup--p-strong">n</strong>-long vectors.</p><p name="5a64" id="5a64" class="graf graf--p graf-after--p">That was a lot of letters, but you get the idea (I hope).</p><p name="b829" id="b829" class="graf graf--p graf-after--p">Like much of deep learning, the concept itself is pretty simple, but the implications are pretty cool. We can take any sequence — a variable-length sequence, mind you — and convert it into a fixed-size vector. And then convert that back to a variable-length sequence.</p><p name="5546" id="5546" class="graf graf--p graf-after--p">It turns out this model is actually incredibly powerful, so let’s take a look at one particularly useful (and successful) application: machine translation.</p><h4 name="4be2" id="4be2" class="graf graf--h4 graf-after--p">Neural Machine Translation</h4><p name="9d22" id="9d22" class="graf graf--p graf-after--h4">Let’s take these ideas we just learned about sequence-to-sequence (or seq2seq, for short) RNNs and apply them to machine translation. We throw in a sequence of words in one language, and it outputs a sequence of words in another. Simple enough, right?</p><p name="cd0e" id="cd0e" class="graf graf--p graf-after--p">The model we’re going to look at specifically is Google’s implementation of NMT. You can read all the gory details <a href="https://arxiv.org/pdf/1609.08144.pdf" data-href="https://arxiv.org/pdf/1609.08144.pdf" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">in their paper</a>, but for now why don’t I give you the watered-down version.</p><p name="3f1b" id="3f1b" class="graf graf--p graf-after--p">At it’s core, the GNMT architecture is just another seq2seq model. We have an encoder, consisting of 8 LSTM layers with skip connections (the first layer is bidirectional). We also have a decoder, once again containing 8 LSTM layers with skip connections. (A skip connection in a neural network is a connection which skips a layer and connects to the next available layer.) The decoder network outputs a probability distribution of words (well, sort of — we’ll talk more about that later), which we sample from to get our [translated] sentence. 🎉</p><p name="afc6" id="afc6" class="graf graf--p graf-after--p">Here’s a scary diagram from the paper:</p><figure name="6a00" id="6a00" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 656px; max-height: 363px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 55.300000000000004%;"></div><img class="graf-image" data-image-id="0*mk1BeF8ANMbAVOzD." data-width="656" data-height="363" src="https://cdn-images-1.medium.com/max/800/0*mk1BeF8ANMbAVOzD."></div><figcaption class="imageCaption"><a href="https://arxiv.org/abs/1609.08144" data-href="https://arxiv.org/abs/1609.08144" class="markup--anchor markup--figure-anchor" rel="nofollow noopener noopener noopener" target="_blank">https://arxiv.org/abs/1609.08144</a></figcaption></figure><p name="a450" id="a450" class="graf graf--p graf-after--figure">But there are a few other aspects to the GNMT that are important to note (there’s actually lots of interesting stuff going on in this architecture, so I really recommend you do <a href="https://arxiv.org/pdf/1609.08144.pdf" data-href="https://arxiv.org/pdf/1609.08144.pdf" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">read the paper</a>).</p><p name="1fbc" id="1fbc" class="graf graf--p graf-after--p">Let’s turn our <em class="markup--em markup--p-em">attention</em> to the center of the above diagram. This is a critical part of the GNMT architecture (and GNMT is certainly not the first to use attention) which allows the decoder to focus on certain parts of the encoder’s output as it produces output. Specifically, the GNMT architecture differs from the traditional seq2seq model in that our encoder does <em class="markup--em markup--p-em">not</em><strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em"> </em></strong>produce a single fixed-width vector (the final hidden state) representing the entire output. Instead, we actually look at the output from each time step, and each time step gives us some latent representation. While decoding, we combine all of these hidden vectors into one <em class="markup--em markup--p-em">context</em> vector using something called <em class="markup--em markup--p-em">soft attention</em>.</p><figure name="369f" id="369f" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 243px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 34.699999999999996%;"></div><img class="graf-image" data-image-id="0*ua03RdgdNWPw1_Jd." data-width="704" data-height="244" data-action="zoom" data-action-value="0*ua03RdgdNWPw1_Jd." src="https://cdn-images-1.medium.com/max/800/0*ua03RdgdNWPw1_Jd."></div><figcaption class="imageCaption"><a href="https://arxiv.org/abs/1609.08144" data-href="https://arxiv.org/abs/1609.08144" class="markup--anchor markup--figure-anchor" rel="nofollow noopener noopener noopener" target="_blank">https://arxiv.org/abs/1609.08144</a></figcaption></figure><p name="ddb5" id="ddb5" class="graf graf--p graf-after--figure">More concretely, that works like this (at every decoder time step). We first look at the output of the first decoder layer from the <em class="markup--em markup--p-em">last</em> time step. Following the notation from the paper, we’ll call that <strong class="markup--strong markup--p-strong">yi-1</strong>. We also have a series of encoder outputs, <strong class="markup--strong markup--p-strong">x1…xM</strong>, one for each encoder timestep. For each <em class="markup--em markup--p-em">encoder</em> timestep, we give our special attention function <strong class="markup--strong markup--p-strong">yi-1 </strong>and <strong class="markup--strong markup--p-strong">xt</strong> and get back a <em class="markup--em markup--p-em">single fixed-size vector</em> <strong class="markup--strong markup--p-strong">st</strong>, which we then run through a softmax. So, we’ve converted our encoder information from that timestep (and some decoder information) into a single attention vector — this attention vector tells us which parts of the encoder output we should look at more closely. We multiply this attention vector by our encoder output <strong class="markup--strong markup--p-strong">xt</strong>, which has the effect of “focusing” more on certain values and less on others. Finally, we take the sum of those “focused” vectors over each encoder timestep to produce our attention context for this timestep <strong class="markup--strong markup--p-strong">ai</strong>, which is fed to every decoder layer.</p><p name="8681" id="8681" class="graf graf--p graf-after--p">Oh yeah, that attention function? That’s just yet <em class="markup--em markup--p-em">another</em> neural network.</p><p name="145a" id="145a" class="graf graf--p graf-after--p">Attention mechanisms like this one are pretty common in many deep learning architectures. This is an example of soft attention: we learn a distribution over our inputs and compute a weighted sum. This process is fully-differentiated, so we can use standard backpropogation to figure out how to train our attention model. Another possible mechanism is called <em class="markup--em markup--p-em">hard attention</em>, in which we select just one of the possible inputs and “focus” solely on that input. This process is not differentiable, so we need to use some other algorithm (usually some kind of reinforcement learning) to train a hard attention algorithm.</p><p name="10b2" id="10b2" class="graf graf--p graf-after--p">GNMT combines all kinds of other cool ideas to achieve state-of-the-art results, including a wordpiece model which segments words into smaller “wordpieces” to help translate rarer words and neat parallelization techniques that let them train this monstrosity of an architecture in reasonable time.</p><p name="818e" id="818e" class="graf graf--p graf-after--p">A few months ago, <a href="https://blog.google/products/translate/found-translation-more-accurate-fluent-sentences-google-translate/" data-href="https://blog.google/products/translate/found-translation-more-accurate-fluent-sentences-google-translate/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Google put their GNMT model into production</a>. Cutting-edge research is being implemented in the real world at an incredible rapid pace within the field of machine learning, and this is just one of countless examples.</p><h4 name="0c13" id="0c13" class="graf graf--h4 graf-after--p">Long-Term Recurrent Convolutional Networks</h4><p name="c3b8" id="c3b8" class="graf graf--p graf-after--h4">(Not to be confused with LCRNs.)</p><p name="e750" id="e750" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">The Problem</strong>: We have a sequence of images that we need to make predictions for. CNNs are good at processing images, RNNs are good at processing sequences…how do we put the two together?</p><p name="606b" id="606b" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">The Solution</strong>: The solution proposed in <a href="https://arxiv.org/pdf/1411.4389.pdf" data-href="https://arxiv.org/pdf/1411.4389.pdf" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">this paper</a> is as straightforward as you would expect: take your image, extract features using a CNN, and feed this feature vector to your LSTM.</p><figure name="fdf8" id="fdf8" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 355px; max-height: 319px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 89.9%;"></div><img class="graf-image" data-image-id="0*qiQ7DvCkHydXAFZ1." data-width="355" data-height="319" src="https://cdn-images-1.medium.com/max/800/0*qiQ7DvCkHydXAFZ1."></div><figcaption class="imageCaption"><a href="https://arxiv.org/abs/1411.4389" data-href="https://arxiv.org/abs/1411.4389" class="markup--anchor markup--figure-anchor" rel="nofollow noopener noopener noopener" target="_blank">https://arxiv.org/abs/1411.4389</a></figcaption></figure><p name="1c9f" id="1c9f" class="graf graf--p graf-after--figure">That’s really all there is to it, and the reason it works is because (<a href="https://ayearofai.com/rohan-lenny-2-convolutional-neural-networks-5f4cd480a60b?gi=25a4c415cc58" data-href="https://ayearofai.com/rohan-lenny-2-convolutional-neural-networks-5f4cd480a60b?gi=25a4c415cc58" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">as we’ve seen before</a>) CNNs are incredibly adept at converting raw pixel data to a more meaningful representation. This saves the LSTMs the problem of parsing through the pixels to figure out what’s going on in the image and allows the LSTM weights to focus on converting a vector of image features into some meaningful sequence (say, a caption). It’s the same reason that using a word embedding is often preferred to a one-hot vector when feeding in words to an NLP model: the more meaningful your representation is, the easier it is to make further predictions with it.</p><h4 name="b189" id="b189" class="graf graf--h4 graf-after--p">Image Captioning</h4><p name="3e15" id="3e15" class="graf graf--p graf-after--h4">(To be confused with LCRNs!)</p><p name="7c86" id="7c86" class="graf graf--p graf-after--p">So there has been a lot of really impressive work on image captioning lately, but I wanna give a special shout-out to <a href="http://cs.stanford.edu/people/karpathy/cvpr2015.pdf" data-href="http://cs.stanford.edu/people/karpathy/cvpr2015.pdf" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">this 2015 paper</a> from Karpathy <em class="markup--em markup--p-em">et al</em>. It was one of the first ML papers I had ever read, and really got me excited about the field. And, it uses RNNs, so that’s cool too.</p><p name="ca5d" id="ca5d" class="graf graf--p graf-after--p">The idea behind image captioning is kind of self-explanatory, but I’ll explain it anyway. You give the model an image, it gives you a caption. Which is kind of insane if you think about it — a computer can go from pixels to interpreting what it’s seeing, and from that generate real and grammatical sentences to explain what it sees. I still can’t really believe stuff like this actually works, but somehow it does.</p><p name="3370" id="3370" class="graf graf--p graf-after--p">The model described in this specific paper combines our old pal convolutional neural networks with our newly-discovered RNNs. Step 1 is to pass our image through a convolutional neural network and extract some features from the last fully-connected layer. This lets us convert our pixel representation of the image into something that’s hopefully a bit more meaningful. We take this image feature vector and use it to initialize the hidden state of our RNN.</p><p name="de23" id="de23" class="graf graf--p graf-after--p">This is where it gets cool. We feed our network a start token, and it gives us a word (more accurately, a distribution of words, which we sample to get the first word of our caption). We feed this word back as the next input, and sample another word from the output. And again, and again, and again, until we finally sample an end token and have a complete caption.</p><p name="0330" id="0330" class="graf graf--p graf-after--p">It’s not strictly necessary to feed the word that we sampled back to the network, but that’s pretty common practice to help the network condition its output on the previous word (the hidden state is critical for this too, of course). The results from this particular paper were pretty cool, you can see some of the results <a href="http://cs.stanford.edu/people/karpathy/deepimagesent/" data-href="http://cs.stanford.edu/people/karpathy/deepimagesent/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">here</a>.</p><h4 name="6ec4" id="6ec4" class="graf graf--h4 graf-after--p">Neural Machine Translation, Again</h4><p name="aba2" id="aba2" class="graf graf--p graf-after--h4">Yes, NMTs are just that cool that I need to talk about them again.</p><p name="7a0e" id="7a0e" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">The Problem</strong>: With our good ol’ GNMT architecture, we can train a massive model to convert from language A to language B. That’s great — except, if we support more than a hundred languages, we need to train more than <strong class="markup--strong markup--p-strong">10,000</strong> different language-pair models, each of which can take months to converge. That’s no good, and it’s the reason that when Google put GNMT in production, they only did so for eight language-pairs (still a monumental achievement). But…what if we didn’t need to train a separate model for each language pair? What if we could train one model for all the language pairs — impossible, right?</p><p name="1717" id="1717" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">The Solution</strong>: Apparently it’s not impossible, and to make things even crazier, <strong class="markup--strong markup--p-strong">we can use the original GNMT architecture without modification</strong>. The only real change is that we prepend a special language token to the beginning of each sequence telling it what language to translate to. (We also use one shared wordpiece model for all language, instead of one per language pair.)</p><p name="90b7" id="90b7" class="graf graf--p graf-after--p">So we’ve condensed tens of thousands of NMT models into a single model that is able to translate to and from any language it was trained on. <a href="https://arxiv.org/pdf/1611.04558.pdf" data-href="https://arxiv.org/pdf/1611.04558.pdf" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">The paper</a> elaborates on the implications and benefits of this more than I will, but to summarize:</p><ul class="postList"><li name="2db1" id="2db1" class="graf graf--li graf-after--p">One model instead of tens of thousands. Months of training time saved, simpler production deployment, fewer parameters — simplicity wins out over complexity.</li><li name="66a7" id="66a7" class="graf graf--li graf-after--li">We might have more training data for some language pairs than others. When we have separate models for each language pair, this means that the pairs with less data will have significantly poorer performance. If we put them all into one model, the language pairs with less data can still benefit from all of the data in the other language pairs, because all of the language pairs share weights (since they all use the same model).</li><li name="2a4f" id="2a4f" class="graf graf--li graf-after--li">This one is absolutely nuts. If we train our network to translate English → Spanish and Spanish → French, <strong class="markup--strong markup--li-strong">our network automatically knows how to translate English → French</strong> (reasonably well).</li></ul><p name="0314" id="0314" class="graf graf--p graf-after--li">Expanding on that last point some more: the authors of the paper even found evidence of an <em class="markup--em markup--p-em">interlingua</em>, or an intermediate representation that is shared by multiple languages. Being able to learn an interlingua is the ideal end goal to create a fully generalized multilingual NMT: we learn an encoder/decoder to convert to/from the interlingua for each language, and we immediately know how to translate to and from that language. We aren’t <em class="markup--em markup--p-em">quite</em> there yet, but this is a major step in that direction. Creating a larger multilingual NMT model and giving it even more data could be all it takes to achieve new state-of-the-art translation results.</p><h4 name="e74d" id="e74d" class="graf graf--h4 graf-after--p">So, yeah</h4><p name="1de9" id="1de9" class="graf graf--p graf-after--h4">RNNs are pretty awesome. There are new RNN papers published literally every day and it’s impossible to cover everything — if you think I missed something important, definitely <a href="https://twitter.com/LennyKhazan" data-href="https://twitter.com/LennyKhazan" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">let me know</a>. (From Rohan: Except Neural Turing Machines and Learning to Learn. Those are dope, we know it, and we’re going to be covering them soon!)</p><h3 name="449c" id="449c" class="graf graf--h3 graf-after--p">Building a Vanilla Recurrent Neural Network</h3><p name="55e8" id="55e8" class="graf graf--p graf-after--h3">Let’s get practical for a minute and see how we can build one of these things in practice. We’ll stick with char-rnn (the single layer character level language model we talked about much earlier) with raw numpy so we can see the nitty-gritty details, but if you’re using one of these in practice <em class="markup--em markup--p-em">there are much better solutions!</em> For out-of-the-box functional deep learning models <a href="https://keras.io/" data-href="https://keras.io/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Keras</a> is the de facto framework that people seem to use. For more creative models and all kinds of other fancy stuff I’m a fan of the newly-released <a href="http://pytorch.org/" data-href="http://pytorch.org/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">PyTorch</a>, or the “older” <a href="https://www.tensorflow.org/" data-href="https://www.tensorflow.org/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">TensorFlow</a>.</p><p name="5997" id="5997" class="graf graf--p graf-after--p">I’m going to walk us through <a href="https://gist.github.com/karpathy/d4dee566867f8291f086" data-href="https://gist.github.com/karpathy/d4dee566867f8291f086" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">this implementation</a> line by line so we can see exactly what’s going on. It’s really well-commented, so feel free to peruse it on your own too.</p><p name="8745" id="8745" class="graf graf--p graf-after--p">Afterwards, I challenge you to code an LSTM!</p><figure name="4989" id="4989" class="graf graf--figure graf--iframe graf-after--p"><div class="aspectRatioPlaceholder is-locked"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 35.699999999999996%;"></div><div class="iframeContainer"><IFRAME width="700" height="250" src="/media/515164c3f20643d9534d745371d34b9f?postId=10300100899b" data-media-id="515164c3f20643d9534d745371d34b9f" data-thumbnail="https://i.embed.ly/1/image?url=https%3A%2F%2Favatars0.githubusercontent.com%2Fu%2F241138%3Fv%3D3%26s%3D400&amp;key=4fce0568f2ce49e8b54624ef71a8a5bd" allowfullscreen frameborder="0"></IFRAME></div></div></figure><p name="8a37" id="8a37" class="graf graf--p graf-after--figure"><code class="markup--code markup--p-code">import numpy as np</code></p><p name="54ad" id="54ad" class="graf graf--p graf-after--p">Well, duh.</p><p name="a14a" id="a14a" class="graf graf--p graf-after--p"><code class="markup--code markup--p-code">data = open(‘input.txt’, ‘r’).read()<br>chars = list(set(data))<br>data_size, vocab_size = len(data), len(chars)<br>print ‘data has %d characters, %d unique.’ % (data_size, vocab_size)<br>char_to_ix = { ch:i for i,ch in enumerate(chars) }<br>ix_to_char = { i:ch for i,ch in enumerate(chars) }</code></p><p name="3f08" id="3f08" class="graf graf--p graf-after--p">We load in our data and get a list of all of the characters that appear in it. We set up two dictionaries: one mapping characters to an index, and one for the reverse. We’ll use this when converting characters to/from a one-hot encoding later on.</p><p name="f0a4" id="f0a4" class="graf graf--p graf-after--p"><code class="markup--code markup--p-code">hidden_size = 100<br>seq_length = 25<br>learning_rate = 1e-1</code></p><p name="b15a" id="b15a" class="graf graf--p graf-after--p">Typical hyperparam stuff. Our RNN layer will have a hidden size of 100, and we’ll train our network on batches of 25 characters at a time. Since we’ll be training our network with BPTT, we need to make sure the sequences are sufficiently short that we can unroll the network all the way and keep everything in memory. Finally, set the learning rate to .1.</p><p name="a836" id="a836" class="graf graf--p graf-after--p"><code class="markup--code markup--p-code">Wxh = np.random.randn(hidden_size, vocab_size)*0.01 # input to hidden</code><br><code class="markup--code markup--p-code">Whh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden<br>Why = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output<br>bh = np.zeros((hidden_size, 1)) # hidden bias<br>by = np.zeros((vocab_size, 1)) # output bias</code></p><p name="7dce" id="7dce" class="graf graf--p graf-after--p">We set up our parameters — note that this is just a typical RNN, no fancy LSTM cells. We have weight matrices for updating our hidden state with each input, updating our hidden state with each timestep, and producing an output (and biases for our hidden state + output). We could be doing some fancy weight initialization here, but some normally-distributed randomness is sufficient for breaking symmetry.</p><p name="c19b" id="c19b" class="graf graf--p graf-after--p">Now let’s talk loss function. We start by computing the forward pass, then computing the backward pass, just like with any neural network.</p><p name="08d8" id="08d8" class="graf graf--p graf-after--p"><code class="markup--code markup--p-code">xs, hs, ys, ps = {}, {}, {}, {}<br>hs[-1] = np.copy(hprev)<br>loss = 0</code></p><p name="7017" id="7017" class="graf graf--p graf-after--p">We start off by just setting up some variables to store our one-hot inputs, hidden states, outputs, and softmax probabilities.</p><p name="03ad" id="03ad" class="graf graf--p graf-after--p"><code class="markup--code markup--p-code">for t in xrange(len(inputs)):</code></p><p name="7bca" id="7bca" class="graf graf--p graf-after--p">Go through each timestep, and for each timestep…</p><p name="d87e" id="d87e" class="graf graf--p graf-after--p"><code class="markup--code markup--p-code">xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation<br>xs[t][inputs[t]] = 1</code></p><p name="0962" id="0962" class="graf graf--p graf-after--p">Convert our input character at this timestep to a one-hot vector.</p><p name="ea9f" id="ea9f" class="graf graf--p graf-after--p"><code class="markup--code markup--p-code">hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state</code></p><p name="de7c" id="de7c" class="graf graf--p graf-after--p">Update our hidden state. We saw this formula already — use our <strong class="markup--strong markup--p-strong">Wxh</strong> and <strong class="markup--strong markup--p-strong">Whh</strong> matrices to update our hidden state based on the last state and our input, and add a bias.</p><p name="15c1" id="15c1" class="graf graf--p graf-after--p"><code class="markup--code markup--p-code">ys[t] = np.dot(Why, hs[t]) + by</code></p><p name="26df" id="26df" class="graf graf--p graf-after--p">Compute our output…</p><p name="4308" id="4308" class="graf graf--p graf-after--p"><code class="markup--code markup--p-code">ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars</code></p><p name="46cd" id="46cd" class="graf graf--p graf-after--p">…and convert it to a probability distribution with a softmax.</p><p name="b3bb" id="b3bb" class="graf graf--p graf-after--p"><code class="markup--code markup--p-code">loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)</code></p><p name="011a" id="011a" class="graf graf--p graf-after--p">Accumulate the loss for this time step as the negative log of the predicted probability. Ideally, we would have a probability of 1 for the <em class="markup--em markup--p-em">actual</em> next character. If it is 1, the loss is 0, log(1) = 0. As the predicted probability approaches 0, the loss approaches inf, because log(0) = -inf.</p><p name="d366" id="d366" class="graf graf--p graf-after--p">That’s it for the forward pass (not bad, right? Boiled down, it’s like six lines of code. Piece of cake).</p><p name="b1db" id="b1db" class="graf graf--p graf-after--p"><code class="markup--code markup--p-code">dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)</code><br><code class="markup--code markup--p-code"> dbh, dby = np.zeros_like(bh), np.zeros_like(by)</code><br><code class="markup--code markup--p-code"> dhnext = np.zeros_like(hs[0])</code></p><p name="cdb0" id="cdb0" class="graf graf--p graf-after--p">Setting up some variables for our backward pass — the gradients of our weight matrices, the gradients for our biases, and the gradients from the next timestep (we’ll see how that works in a bit).</p><p name="2fd8" id="2fd8" class="graf graf--p graf-after--p"><code class="markup--code markup--p-code">for t in reversed(xrange(len(inputs))):</code></p><p name="14bf" id="14bf" class="graf graf--p graf-after--p">Go through our sequence in reverse as we back up the gradients.</p><p name="9148" id="9148" class="graf graf--p graf-after--p"><code class="markup--code markup--p-code u-paddingRight0 u-marginRight0">dy = np.copy(ps[t])<br> dy[targets[t]] -= 1 # backprop into y. see <a href="http://cs231n.github.io/neural-networks-case-study/#grad" data-href="http://cs231n.github.io/neural-networks-case-study/#grad" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">http://cs231n.github.io/neural-networks-case-study/#grad</a> if confused here</code></p><p name="3fb3" id="3fb3" class="graf graf--p graf-after--p">First, get the gradient of the output, dy. <a href="http://cs231n.github.io/neural-networks-case-study/#grad" data-href="http://cs231n.github.io/neural-networks-case-study/#grad" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">As it turns out</a>, the gradient of the cross-entropy loss is really as copying over the distribution and subtracting 1 from the correct class.</p><p name="b8b8" id="b8b8" class="graf graf--p graf-after--p"><a href="https://ayearofai.com/rohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d" data-href="https://ayearofai.com/rohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Remember backpropogation</a>? When we have a weighted sum, the gradient of the weights is just the corresponding value that it is being multiplied by, because the other terms drop out and that one weight is treated as a constant. So, computing the gradient of our Why matrix is super simple: just multiply the gradient of loss w.r.t. the output (dy) by the derivative of the output w.r.t. Why (which is just the hidden state at our given timestep), and we get the derivative of the loss w.r.t. Why.</p><figure name="9790" id="9790" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 426px; max-height: 86px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 20.200000000000003%;"></div><img class="graf-image" data-image-id="0*TVvKSJJqaM9CDjlk." data-width="426" data-height="86" src="https://cdn-images-1.medium.com/max/800/0*TVvKSJJqaM9CDjlk."></div></figure><p name="247c" id="247c" class="graf graf--p graf-after--figure"><code class="markup--code markup--p-code">dWhy += np.dot(dy, hs[t].T)</code></p><p name="2ff0" id="2ff0" class="graf graf--p graf-after--p">Like the other gradients (except dy, of course) we accumulate these gradients over all timesteps and apply them at the end.</p><figure name="c397" id="c397" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 321px; max-height: 86px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 26.8%;"></div><img class="graf-image" data-image-id="0*QKDwSXVEJ9fHQ4hT." data-width="321" data-height="86" src="https://cdn-images-1.medium.com/max/800/0*QKDwSXVEJ9fHQ4hT."></div></figure><p name="1c4f" id="1c4f" class="graf graf--p graf-after--figure"><code class="markup--code markup--p-code">dby += dy</code></p><p name="a781" id="a781" class="graf graf--p graf-after--p">The derivative of loss w.r.t. output (dy) multiplied by the derivative of our output w.r.t. the bias (which is 1) gives us the derivative of our output w.r.t. the bias. So far so good.</p><p name="6890" id="6890" class="graf graf--p graf-after--p"><code class="markup--code markup--p-code">dh = np.dot(Why.T, dy) + dhnext # backprop into h</code></p><figure name="9602" id="9602" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 265px; max-height: 74px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 27.900000000000002%;"></div><img class="graf-image" data-image-id="1*cVr1t2s7gsC4Sd6vowSkHA.png" data-width="265" data-height="74" src="https://cdn-images-1.medium.com/max/800/1*cVr1t2s7gsC4Sd6vowSkHA.png"></div></figure><p name="6dbe" id="6dbe" class="graf graf--p graf-after--figure">We compute dL/dh using the chain rule, and accumulate it over all timesteps (hence <code class="markup--code markup--p-code">+ dhnext</code>). We’ll need this for the next step.</p><p name="6d9e" id="6d9e" class="graf graf--p graf-after--p"><code class="markup--code markup--p-code">dhraw = (1 — hs[t] * hs[t]) * dh # backprop through tanh nonlinearity</code></p><p name="796c" id="796c" class="graf graf--p graf-after--p">This computes the derivative of the <code class="markup--code markup--p-code">np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh)</code> line from earlier.</p><p name="7fa1" id="7fa1" class="graf graf--p graf-after--p"><code class="markup--code markup--p-code">dbh += dhraw</code></p><p name="f74d" id="f74d" class="graf graf--p graf-after--p">Which is also our bh derivative, for the same reason that the by derivative was just dy.</p><p name="a0f2" id="a0f2" class="graf graf--p graf-after--p"><code class="markup--code markup--p-code">dWxh += np.dot(dhraw, xs[t].T)<br>dWhh += np.dot(dhraw, hs[t-1].T)</code></p><p name="cea8" id="cea8" class="graf graf--p graf-after--p">We accumulate our weight gradients.</p><figure name="1bb0" id="1bb0" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 286px; max-height: 77px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 26.900000000000002%;"></div><img class="graf-image" data-image-id="1*uf-YEbf0258UhbDc5QZLRw.png" data-width="286" data-height="77" src="https://cdn-images-1.medium.com/max/800/1*uf-YEbf0258UhbDc5QZLRw.png"></div></figure><figure name="d79b" id="d79b" class="graf graf--figure graf-after--figure"><div class="aspectRatioPlaceholder is-locked" style="max-width: 287px; max-height: 77px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 26.8%;"></div><img class="graf-image" data-image-id="1*Vl1LVzPJSZKDplA9J5cElg.png" data-width="287" data-height="77" src="https://cdn-images-1.medium.com/max/800/1*Vl1LVzPJSZKDplA9J5cElg.png"></div></figure><p name="012f" id="012f" class="graf graf--p graf-after--figure"><code class="markup--code markup--p-code">dhnext = np.dot(Whh.T, dhraw)</code></p><p name="bb22" id="bb22" class="graf graf--p graf-after--p">And finally, store dh for this timestep so we can use it for the previous one.</p><p name="1ea0" id="1ea0" class="graf graf--p graf-after--p"><code class="markup--code markup--p-code">for dparam in [dWxh, dWhh, dWhy, dbh, dby]:<br>np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients</code></p><p name="a901" id="a901" class="graf graf--p graf-after--p">Last but not least, a little gradient clipping so we don’t get no exploding gradients.</p><p name="8392" id="8392" class="graf graf--p graf-after--p"><code class="markup--code markup--p-code">return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]</code></p><p name="4dc5" id="4dc5" class="graf graf--p graf-after--p">And then return all the gradients so we can apply an optimizer step. And that’s it for the backprop code; not <em class="markup--em markup--p-em">too</em> bad, right?</p><p name="4eaf" id="4eaf" class="graf graf--p graf-after--p"><code class="markup--code markup--p-code">def sample(h, seed_ix, n):</code></p><p name="ed15" id="ed15" class="graf graf--p graf-after--p">This method is used for sampling a generated sequence from the network, starting with state <code class="markup--code markup--p-code">h</code>, first letter <code class="markup--code markup--p-code">seed_ix</code>, with length <code class="markup--code markup--p-code">n</code>.</p><p name="d445" id="d445" class="graf graf--p graf-after--p"><code class="markup--code markup--p-code">x = np.zeros((vocab_size, 1)) <br>x[seed_ix] = 1</code></p><p name="a8f6" id="a8f6" class="graf graf--p graf-after--p">Set up our one-hot encoded input vector based on the seed character.</p><p name="8a82" id="8a82" class="graf graf--p graf-after--p"><code class="markup--code markup--p-code">ixes = []</code></p><p name="7e2b" id="7e2b" class="graf graf--p graf-after--p">And an array to keep track of our sequence.</p><p name="3cae" id="3cae" class="graf graf--p graf-after--p"><code class="markup--code markup--p-code">for t in xrange(n):</code></p><p name="c8e6" id="c8e6" class="graf graf--p graf-after--p">To generate each character in our sequence…</p><p name="18cf" id="18cf" class="graf graf--p graf-after--p"><code class="markup--code markup--p-code">h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)</code></p><p name="6d86" id="6d86" class="graf graf--p graf-after--p">Update our hidden state! We saw this formula in the last function, too.</p><p name="ce6f" id="ce6f" class="graf graf--p graf-after--p"><code class="markup--code markup--p-code">y = np.dot(Why, h) + by<br>p = np.exp(y) / np.sum(np.exp(y))</code></p><p name="d108" id="d108" class="graf graf--p graf-after--p">Generate our output and run it through a softmax. Again, straight from the last function.</p><p name="62ae" id="62ae" class="graf graf--p graf-after--p"><code class="markup--code markup--p-code">ix = np.random.choice(range(vocab_size), p=p.ravel())</code></p><p name="fd39" id="fd39" class="graf graf--p graf-after--p">Sample from our output distribution using some numpy magic.</p><p name="cec1" id="cec1" class="graf graf--p graf-after--p"><code class="markup--code markup--p-code">x = np.zeros((vocab_size, 1))<br>x[ix] = 1<br>ixes.append(ix)</code></p><p name="e8b3" id="e8b3" class="graf graf--p graf-after--p">Convert the sampled value into a one-hot encoding and append it to the array.</p><p name="4928" id="4928" class="graf graf--p graf-after--p"><code class="markup--code markup--p-code">return ixes</code></p><p name="e074" id="e074" class="graf graf--p graf-after--p">…and of course, return the final sequence when we’re done.</p><p name="6968" id="6968" class="graf graf--p graf-after--p"><code class="markup--code markup--p-code">n, p = 0, 0</code></p><p name="2eb1" id="2eb1" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">n</strong> is the number of training iterations we’ve done. <strong class="markup--strong markup--p-strong">p</strong> is the index into our training data for where we are now.</p><p name="ee12" id="ee12" class="graf graf--p graf-after--p"><code class="markup--code markup--p-code">mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)</code></p><p name="9951" id="9951" class="graf graf--p graf-after--p">Set up memory variables for the Adagrad algorithm (out of scope of this post, maybe next time — it’s just a variant on gradient descent).</p><p name="ffc4" id="ffc4" class="graf graf--p graf-after--p"><code class="markup--code markup--p-code">while True:</code></p><p name="bcde" id="bcde" class="graf graf--p graf-after--p">Training loop.</p><p name="a04e" id="a04e" class="graf graf--p graf-after--p"><code class="markup--code markup--p-code">if p+seq_length+1 &gt;= len(data) or n == 0:</code></p><p name="3444" id="3444" class="graf graf--p graf-after--p">This is a little check to see if we need to reset our memory because we’re starting back at the beginning of our data.</p><p name="a0be" id="a0be" class="graf graf--p graf-after--p"><code class="markup--code markup--p-code">hprev = np.zeros((hidden_size,1)) # reset RNN memory</code></p><p name="038a" id="038a" class="graf graf--p graf-after--p">…and if we are, reset the memory.</p><p name="9c3d" id="9c3d" class="graf graf--p graf-after--p"><code class="markup--code markup--p-code">p = 0</code></p><p name="bd23" id="bd23" class="graf graf--p graf-after--p">And reset the data pointer.</p><p name="0161" id="0161" class="graf graf--p graf-after--p"><code class="markup--code markup--p-code">inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]<br>targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]</code></p><p name="70c2" id="70c2" class="graf graf--p graf-after--p">We grab a <code class="markup--code markup--p-code">seq_length</code>-long piece of the data as our input to the network. At each timestep, we want to predict the next character; this means that our “targets” will be the next character for each input. We get a snippet of the input the same length as the input, but offset by 1, for the target.</p><p name="3f42" id="3f42" class="graf graf--p graf-after--p"><code class="markup--code markup--p-code">if n % 100 == 0:<br>sample_ix = sample(hprev, inputs[0], 200)<br>txt = ‘’.join(ix_to_char[ix] for ix in sample_ix)<br>print ‘ — — \n %s \n — — ‘ % (txt, )</code></p><p name="a7eb" id="a7eb" class="graf graf--p graf-after--p">Here we just print to the terminal a sample every 100 training steps so we can see how its doing. Ideally, this will print out gibberish the first few times, before gradually printing out more and more reasonable language.</p><p name="e412" id="e412" class="graf graf--p graf-after--p"><code class="markup--code markup--p-code">loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)</code></p><p name="538d" id="538d" class="graf graf--p graf-after--p">Do a forward pass, backward pass, and get the gradients.</p><p name="9d99" id="9d99" class="graf graf--p graf-after--p"><code class="markup--code markup--p-code">smooth_loss = smooth_loss * 0.999 + loss * 0.001</code></p><p name="4ab1" id="4ab1" class="graf graf--p graf-after--p">Adagrad stuff.</p><p name="3823" id="3823" class="graf graf--p graf-after--p"><code class="markup--code markup--p-code">if n % 100 == 0: print ‘iter %d, loss: %f’ % (n, smooth_loss) # print progress</code></p><p name="b38c" id="b38c" class="graf graf--p graf-after--p">Keep up with progress.</p><p name="995f" id="995f" class="graf graf--p graf-after--p"><code class="markup--code markup--p-code">for param, dparam, mem in zip([Wxh, Whh, Why, bh, by], [dWxh, dWhh, dWhy, dbh, dby], [mWxh, mWhh, mWhy, mbh, mby]):<br>mem += dparam * dparam<br>param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update</code></p><p name="9509" id="9509" class="graf graf--p graf-after--p">More Adagrad. We should really do an article on optimization algorithms.</p><p name="a2fc" id="a2fc" class="graf graf--p graf-after--p"><code class="markup--code markup--p-code">p += seq_length # move data pointer<br>n += 1 # iteration counter</code></p><p name="a889" id="a889" class="graf graf--p graf-after--p">Annnddd finally, we update our data pointer and iteration counter.</p><p name="7850" id="7850" class="graf graf--p graf-after--p">And that’s it. We have an RNN. Neat-o. Reminder: your challenge is to code an LSTM… and TensorFlow doesn’t count!</p><h3 name="5a53" id="5a53" class="graf graf--h3 graf-after--p">Conclusion</h3><p name="8a00" id="8a00" class="graf graf--p graf-after--h3">Wow. That was a <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">lot</em></strong>. If you came in knowing nothing or very little about recurrent neural nets, you sure as hell know a lot now. And you don’t just know about something cool; you know about something <em class="markup--em markup--p-em">very important</em> — something that can equip you to read and understand some of the most prominent and hottest recent research papers in machine learning.</p><p name="eec5" id="eec5" class="graf graf--p graf-after--p">Something this article didn’t do so good at was making sure the calculus and derivatives were in the context of operating on vectors (because, remember, RNNs/LSTMs operate over vectors). In many cases the derivatives were in the 1-D context. It’s not something you need to worry about, but you might want to look into.</p><p name="f6a4" id="f6a4" class="graf graf--p graf-after--p">We’re finally at the point where we can focus our energies on this blog towards cooler stuff including hot research papers like Neural Turing Machines or Learning to Learn, case studies eg. AlphaGo, other parts of machine learning and artificial intelligence (I’m, Rohan, personally looking forward to optimization!), or different algorithms like GANs. There’s very little compulsory content or “groundwork” we need to cover anymore. So, now, we’re officially onto the cool stuff.</p><p name="4f33" id="4f33" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">That’s right.</strong> A Year Of AI is officially… <em class="markup--em markup--p-em">cool</em>.</p><figure name="f41b" id="f41b" class="graf graf--figure graf-after--p graf--trailing"><div class="aspectRatioPlaceholder is-locked" style="max-width: 80px; max-height: 80px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 100%;"></div><img class="graf-image" data-image-id="1*necEBipfgD-Z3_9R9usFgg.png" data-width="80" data-height="80" src="https://cdn-images-1.medium.com/max/800/1*necEBipfgD-Z3_9R9usFgg.png"></div></figure></div></div></section></div><footer class="u-paddingTop10"><div class="container u-maxWidth740"><div class="row"><div class="col u-size12of12"></div></div><div class="row"><div class="col u-size12of12 js-postTags"><div class="u-paddingBottom10"><ul class="tags tags--postTags tags--borderless"><li><a class="link u-baseColor--link"  href="https://ayearofai.com/tagged/machine-learning?source=post" data-action-source="post" data-collection-slug="a-year-of-artificial-intelligence">Machine Learning</a></li><li><a class="link u-baseColor--link"  href="https://ayearofai.com/tagged/artificial-intelligence?source=post" data-action-source="post" data-collection-slug="a-year-of-artificial-intelligence">Artificial Intelligence</a></li><li><a class="link u-baseColor--link"  href="https://ayearofai.com/tagged/data-science?source=post" data-action-source="post" data-collection-slug="a-year-of-artificial-intelligence">Data Science</a></li><li><a class="link u-baseColor--link"  href="https://ayearofai.com/tagged/deep-learning?source=post" data-action-source="post" data-collection-slug="a-year-of-artificial-intelligence">Deep Learning</a></li><li><a class="link u-baseColor--link"  href="https://ayearofai.com/tagged/algorithms?source=post" data-action-source="post" data-collection-slug="a-year-of-artificial-intelligence">Algorithms</a></li></ul></div></div></div><div class="row js-postActionsFooter"><div class="postActions col u-size12of12"><div class="u-floatLeft buttonSet buttonSet--withLabels"><div class="buttonSet-inner"><div class="js-actionRecommend" data-post-id="10300100899b" data-is-icon-29px="true" data-has-recommend-list="true" data-source="post_actions_footer"><button class="button button--primary button--large button--chromeless is-touchIconFadeInPulse u-accentColor--buttonNormal button--withIcon button--withSvgIcon u-accentColor--iconLight js-actionRecommendButton"  title="Recommend to share this article with your followers and let the author know you liked it" aria-label="Recommend to share this article with your followers and let the author know you liked it" data-action="sign-in-prompt" data-sign-in-action="upvote" data-requires-token="true" data-redirect="https://medium.com/_/vote/p/10300100899b" data-action-source="post_actions_footer"><span class="button-defaultState"><span class="svgIcon svgIcon--heart svgIcon--29px"><svg class="svgIcon-use" width="29" height="29" viewBox="0 0 29 29" ><path d="M16.215 23.716c-.348.288-.984.826-1.376 1.158a.526.526 0 0 1-.68 0c-.36-.307-.92-.78-1.22-1.03C9.22 20.734 3 15.527 3 10.734 3 7.02 5.916 4 9.5 4c1.948 0 3.77.898 5 2.434C15.73 4.898 17.552 4 19.5 4c3.584 0 6.5 3.02 6.5 6.734 0 4.9-6.125 9.96-9.785 12.982zM19.5 5.2c-1.774 0-3.423.923-4.41 2.468a.699.699 0 0 1-.59.323.706.706 0 0 1-.59-.32c-.988-1.54-2.637-2.47-4.41-2.47-2.922 0-5.3 2.49-5.3 5.54 0 4.23 6.19 9.41 9.517 12.19.217.18.566.48.783.66l.952-.79c3.496-2.88 9.348-7.72 9.348-12.05 0-3.05-2.378-5.53-5.3-5.53z"/></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--heartFilled svgIcon--29px"><svg class="svgIcon-use" width="29" height="29" viewBox="0 0 29 29" ><path d="M19.5 4c-1.948 0-3.77.898-5 2.434C13.27 4.898 11.448 4 9.5 4 5.916 4 3 7.02 3 10.734c0 4.793 6.227 10 9.95 13.11.296.25.853.723 1.212 1.03.196.166.48.166.677 0 .39-.332 1.02-.87 1.37-1.158 3.66-3.022 9.79-8.08 9.79-12.982C26 7.02 23.08 4 19.5 4z" fill-rule="evenodd"/></svg></span></span></button><button class="button button--chromeless u-baseColor--buttonNormal"  data-action="show-recommends" data-action-value="10300100899b">88</button></div></div><div class="buttonSet-inner"><button class="button button--large button--dark button--chromeless is-touchIconBlackPulse u-baseColor--buttonDark button--withIcon button--withSvgIcon"  data-action="scroll-to-responses" data-action-source="post_actions_footer"><span class="svgIcon svgIcon--response svgIcon--29px"><svg class="svgIcon-use" width="29" height="29" viewBox="0 0 29 29" ><path d="M21.27 20.058c1.89-1.826 2.754-4.17 2.754-6.674C24.024 8.21 19.67 4 14.1 4 8.53 4 4 8.21 4 13.384c0 5.175 4.53 9.385 10.1 9.385 1.007 0 2-.14 2.95-.41.285.25.592.49.918.7 1.306.87 2.716 1.31 4.19 1.31.276-.01.494-.14.6-.36a.625.625 0 0 0-.052-.65c-.61-.84-1.042-1.71-1.282-2.58a5.417 5.417 0 0 1-.154-.75zm-3.85 1.324l-.083-.28-.388.12a9.72 9.72 0 0 1-2.85.424c-4.96 0-8.99-3.706-8.99-8.262 0-4.556 4.03-8.263 8.99-8.263 4.95 0 8.77 3.71 8.77 8.27 0 2.25-.75 4.35-2.5 5.92l-.24.21v.32c0 .07 0 .19.02.37.03.29.1.6.19.92.19.7.49 1.4.89 2.08-.93-.14-1.83-.49-2.67-1.06-.34-.22-.88-.48-1.16-.74z"/></svg></span></button><button class="button button--chromeless u-baseColor--buttonNormal"  data-action="scroll-to-responses">4</button></div></div><div class="u-floatRight buttonSet buttonSet--narrow"><button class="button button--large button--dark button--chromeless is-touchIconBlackPulse u-baseColor--buttonDark button--withIcon button--withSvgIcon"  title="Share on Twitter" aria-label="Share on Twitter" data-action="share-on-twitter" data-action-source="post_actions_footer"><span class="svgIcon svgIcon--twitter svgIcon--29px"><svg class="svgIcon-use" width="29" height="29" viewBox="0 0 29 29" ><path d="M21.967 11.8c.018 5.93-4.607 11.18-11.177 11.18-2.172 0-4.25-.62-6.047-1.76l-.268.422-.038.5.186.013.168.012c.3.02.44.032.6.046 2.06-.026 3.95-.686 5.49-1.86l1.12-.85-1.4-.048c-1.57-.055-2.92-1.08-3.36-2.51l-.48.146-.05.5c.22.03.48.05.75.08.48-.02.87-.07 1.25-.15l2.33-.49-2.32-.49c-1.68-.35-2.91-1.83-2.91-3.55 0-.05 0-.01-.01.03l-.49-.1-.25.44c.63.36 1.35.57 2.07.58l1.7.04L7.4 13c-.978-.662-1.59-1.79-1.618-3.047a4.08 4.08 0 0 1 .524-1.8l-.825.07a12.188 12.188 0 0 0 8.81 4.515l.59.033-.06-.59v-.02c-.05-.43-.06-.63-.06-.87a3.617 3.617 0 0 1 6.27-2.45l.2.21.28-.06c1.01-.22 1.94-.59 2.73-1.09l-.75-.56c-.1.36-.04.89.12 1.36.23.68.58 1.13 1.17.85l-.21-.45-.42-.27c-.52.8-1.17 1.48-1.92 2L22 11l.016.28c.013.2.014.35 0 .52v.04zm.998.038c.018-.22.017-.417 0-.66l-.498.034.284.41a8.183 8.183 0 0 0 2.2-2.267l.97-1.48-1.6.755c.17-.08.3-.02.34.03a.914.914 0 0 1-.13-.292c-.1-.297-.13-.64-.1-.766l.36-1.254-1.1.695c-.69.438-1.51.764-2.41.963l.48.15a4.574 4.574 0 0 0-3.38-1.484 4.616 4.616 0 0 0-4.61 4.613c0 .29.02.51.08.984l.01.02.5-.06.03-.5c-3.17-.18-6.1-1.7-8.08-4.15l-.48-.56-.36.64c-.39.69-.62 1.48-.65 2.28.04 1.61.81 3.04 2.06 3.88l.3-.92c-.55-.02-1.11-.17-1.6-.45l-.59-.34-.14.67c-.02.08-.02.16 0 .24-.01 2.12 1.55 4.01 3.69 4.46l.1-.49-.1-.49c-.33.07-.67.12-1.03.14-.18-.02-.43-.05-.64-.07l-.76-.09.23.73c.57 1.84 2.29 3.14 4.28 3.21l-.28-.89a8.252 8.252 0 0 1-4.85 1.66c-.12-.01-.26-.02-.56-.05l-.17-.01-.18-.01L2.53 21l1.694 1.07a12.233 12.233 0 0 0 6.58 1.917c7.156 0 12.2-5.73 12.18-12.18l-.002.04z"/></svg></span></button><button class="button button--large button--dark button--chromeless is-touchIconBlackPulse u-baseColor--buttonDark button--withIcon button--withSvgIcon"  title="Share on Facebook" aria-label="Share on Facebook" data-action="share-on-facebook" data-action-source="post_actions_footer"><span class="svgIcon svgIcon--facebook svgIcon--29px"><svg class="svgIcon-use" width="29" height="29" viewBox="0 0 29 29" ><path d="M16.39 23.61v-5.808h1.846a.55.55 0 0 0 .546-.48l.36-2.797a.551.551 0 0 0-.547-.62H16.39V12.67c0-.67.12-.813.828-.813h1.474a.55.55 0 0 0 .55-.55V8.803a.55.55 0 0 0-.477-.545c-.436-.06-1.36-.116-2.22-.116-2.5 0-4.13 1.62-4.13 4.248v1.513H10.56a.551.551 0 0 0-.55.55v2.797c0 .304.248.55.55.55h1.855v5.76c-4.172-.96-7.215-4.7-7.215-9.1 0-5.17 4.17-9.36 9.31-9.36 5.14 0 9.31 4.19 9.31 9.36 0 4.48-3.155 8.27-7.43 9.15M14.51 4C8.76 4 4.1 8.684 4.1 14.46c0 5.162 3.75 9.523 8.778 10.32a.55.55 0 0 0 .637-.543v-6.985a.551.551 0 0 0-.55-.55H11.11v-1.697h1.855a.55.55 0 0 0 .55-.55v-2.063c0-2.02 1.136-3.148 3.03-3.148.567 0 1.156.027 1.597.06v1.453h-.924c-1.363 0-1.93.675-1.93 1.912v1.78c0 .3.247.55.55.55h2.132l-.218 1.69H15.84c-.305 0-.55.24-.55.55v7.02c0 .33.293.59.623.54 5.135-.7 9.007-5.11 9.007-10.36C24.92 8.68 20.26 4 14.51 4"/></svg></span></button></div></div></div></div><div class="js-postPromotionWrapper postPromotionWrapper" data-location-id="footer_above_post_attribution"></div><div class="container u-maxWidth740 js-postAttributionFooterContainer u-paddingTop20 u-paddingBottom20 u-marginTop10 u-borderTopLightest u-xs-paddingTop10 u-xs-paddingBottom10"><div class="row js-postFooterInfo"><div class="col u-size6of12 u-xs-size12of12"><li class="u-block u-paddingBottom18 js-cardUser"><div class="u-marginLeft20 u-floatRight"><span class="followState js-followState buttonSet-inner" data-user-id="cb55958ea3bb"><button class="button button--small u-noUserSelect button--withChrome u-baseColor--buttonNormal button--withHover button--unblock js-unblockButton"  data-action="sign-in-prompt" data-sign-in-action="toggle-block-user" data-requires-token="true" data-action-source="footer_card"><span class="button-label  button-defaultState">Blocked</span><span class="button-label button-hoverState">Unblock</span></button><button class="button button--primary button--small u-noUserSelect button--withChrome u-accentColor--buttonNormal button--follow js-followButton"  data-action="sign-in-prompt" data-sign-in-action="toggle-subscribe-user" data-requires-token="true" data-redirect="https://medium.com/_/subscribe/user/cb55958ea3bb" data-action-source="footer_card_follow"><span class="button-label  button-defaultState js-buttonLabel">Follow</span><span class="button-label button-activeState">Following</span></button></span></div><div class="u-tableCell "><a class="link avatar u-baseColor--link"  href="https://ayearofai.com/@mckapur?source=footer_card" title="Go to the profile of Rohan Kapur" aria-label="Go to the profile of Rohan Kapur" data-action-source="footer_card" data-user-id="cb55958ea3bb" data-collection-slug="a-year-of-artificial-intelligence" dir="auto"><img  src="https://cdn-images-1.medium.com/fit/c/60/60/1*pGDfwp8yLzgCeG1RkPUxgQ.jpeg" class="avatar-image avatar-image--small" alt="Go to the profile of Rohan Kapur"></a></div><div class="u-tableCell u-verticalAlignMiddle u-breakWord u-paddingLeft15"><h3 class="u-fontSize18 u-lineHeightTighter u-marginBottom4"><a class="link link--primary u-accentColor--hoverTextNormal"  href="https://ayearofai.com/@mckapur" property="cc:attributionName" title="Go to the profile of Rohan Kapur" aria-label="Go to the profile of Rohan Kapur" rel="author cc:attributionUrl" data-user-id="cb55958ea3bb" data-collection-slug="a-year-of-artificial-intelligence" dir="auto">Rohan Kapur</a></h3><p class="u-fontSize14 u-lineHeightBaseSans u-textColorDark u-marginBottom4">rohankapur.com</p></div></li></div><div class="col u-size6of12 u-xs-size12of12 u-xs-marginTop30"><li class="u-block u-paddingBottom18 js-cardCollection"><div class="u-marginLeft20 u-floatRight"><button class="button button--primary button--small u-noUserSelect button--withChrome u-accentColor--buttonNormal js-relationshipButton"  data-action="sign-in-prompt" data-sign-in-action="toggle-follow-collection" data-requires-token="true" data-redirect="https://medium.com/_/subscribe/collection/a-year-of-artificial-intelligence" data-collection-id="bb87da25612c"><span class="button-label  js-buttonLabel">Follow</span></button></div><div class="u-tableCell "><a class="link avatar avatar--roundedRectangle u-baseColor--link"  href="https://ayearofai.com?source=footer_card" title="Go to A Year of Artificial Intelligence" aria-label="Go to A Year of Artificial Intelligence" data-action-source="footer_card" data-collection-slug="a-year-of-artificial-intelligence"><img src="https://cdn-images-1.medium.com/fit/c/60/60/1*NZsNSuNxe_O2YW1ybboOvA.jpeg" class="avatar-image u-size60x60" alt="A Year of Artificial Intelligence"></a></div><div class="u-tableCell u-verticalAlignMiddle u-breakWord u-paddingLeft15"><h3 class="u-fontSize18 u-lineHeightTighter u-marginBottom4"><a class="link link--primary u-accentColor--hoverTextNormal"  href="https://ayearofai.com?source=footer_card" rel="collection" data-action-source="footer_card" data-collection-slug="a-year-of-artificial-intelligence">A Year of Artificial Intelligence</a></h3><p class="u-fontSize14 u-lineHeightBaseSans u-textColorDark u-marginBottom4">Our ongoing effort to make the mathematics, science, linguistics, and philosophy of artificial intelligence fun and simple.</p><div class="buttonSet"></div></div></li></div></div></div><div class="js-postFooterPlacements"></div><div class="u-padding0 u-clearfix u-backgroundGrayLightest u-print-hide supplementalPostContent js-responsesWrapper"></div><div class="supplementalPostContent js-readNext"></div><div class="supplementalPostContent js-heroPromo"></div></footer></article></main><div class="u-marginAuto u-maxWidth1000"><div class="js-postShareWidget u-foreground u-sm-hide u-transition--fadeOut300 u-fixed"><ul><li class="u-uiTextSemibold u-textAlignCenter u-textColorNormal u-fontSize12 u-textUppercase">Share</li><li class="u-textAlignCenter"><div class="js-actionRecommend" data-post-id="10300100899b" data-is-icon-29px="true" data-is-vertical="true" data-has-recommend-list="true" data-source="post_share_widget"><button class="button button--primary button--large button--chromeless is-touchIconFadeInPulse u-accentColor--buttonNormal button--withIcon button--withSvgIcon u-accentColor--iconLight js-actionRecommendButton"  title="Recommend to share this article with your followers and let the author know you liked it" aria-label="Recommend to share this article with your followers and let the author know you liked it" data-action="sign-in-prompt" data-sign-in-action="upvote" data-requires-token="true" data-redirect="https://medium.com/_/vote/p/10300100899b" data-action-source="post_share_widget"><span class="button-defaultState"><span class="svgIcon svgIcon--heart svgIcon--29px"><svg class="svgIcon-use" width="29" height="29" viewBox="0 0 29 29" ><path d="M16.215 23.716c-.348.288-.984.826-1.376 1.158a.526.526 0 0 1-.68 0c-.36-.307-.92-.78-1.22-1.03C9.22 20.734 3 15.527 3 10.734 3 7.02 5.916 4 9.5 4c1.948 0 3.77.898 5 2.434C15.73 4.898 17.552 4 19.5 4c3.584 0 6.5 3.02 6.5 6.734 0 4.9-6.125 9.96-9.785 12.982zM19.5 5.2c-1.774 0-3.423.923-4.41 2.468a.699.699 0 0 1-.59.323.706.706 0 0 1-.59-.32c-.988-1.54-2.637-2.47-4.41-2.47-2.922 0-5.3 2.49-5.3 5.54 0 4.23 6.19 9.41 9.517 12.19.217.18.566.48.783.66l.952-.79c3.496-2.88 9.348-7.72 9.348-12.05 0-3.05-2.378-5.53-5.3-5.53z"/></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--heartFilled svgIcon--29px"><svg class="svgIcon-use" width="29" height="29" viewBox="0 0 29 29" ><path d="M19.5 4c-1.948 0-3.77.898-5 2.434C13.27 4.898 11.448 4 9.5 4 5.916 4 3 7.02 3 10.734c0 4.793 6.227 10 9.95 13.11.296.25.853.723 1.212 1.03.196.166.48.166.677 0 .39-.332 1.02-.87 1.37-1.158 3.66-3.022 9.79-8.08 9.79-12.982C26 7.02 23.08 4 19.5 4z" fill-rule="evenodd"/></svg></span></span></button><button class="button button--chromeless u-baseColor--buttonNormal  u-block u-marginAuto u-marginTopNegative5"  data-action="show-recommends" data-action-value="10300100899b">88</button></div></li><li class="u-textAlignCenter"><button class="button button--large button--dark button--chromeless is-touchIconBlackPulse u-baseColor--buttonDark button--withIcon button--withSvgIcon"  title="Share on Twitter" aria-label="Share on Twitter" data-action="share-on-twitter" data-action-source="post_share_widget"><span class="svgIcon svgIcon--twitter svgIcon--29px"><svg class="svgIcon-use" width="29" height="29" viewBox="0 0 29 29" ><path d="M21.967 11.8c.018 5.93-4.607 11.18-11.177 11.18-2.172 0-4.25-.62-6.047-1.76l-.268.422-.038.5.186.013.168.012c.3.02.44.032.6.046 2.06-.026 3.95-.686 5.49-1.86l1.12-.85-1.4-.048c-1.57-.055-2.92-1.08-3.36-2.51l-.48.146-.05.5c.22.03.48.05.75.08.48-.02.87-.07 1.25-.15l2.33-.49-2.32-.49c-1.68-.35-2.91-1.83-2.91-3.55 0-.05 0-.01-.01.03l-.49-.1-.25.44c.63.36 1.35.57 2.07.58l1.7.04L7.4 13c-.978-.662-1.59-1.79-1.618-3.047a4.08 4.08 0 0 1 .524-1.8l-.825.07a12.188 12.188 0 0 0 8.81 4.515l.59.033-.06-.59v-.02c-.05-.43-.06-.63-.06-.87a3.617 3.617 0 0 1 6.27-2.45l.2.21.28-.06c1.01-.22 1.94-.59 2.73-1.09l-.75-.56c-.1.36-.04.89.12 1.36.23.68.58 1.13 1.17.85l-.21-.45-.42-.27c-.52.8-1.17 1.48-1.92 2L22 11l.016.28c.013.2.014.35 0 .52v.04zm.998.038c.018-.22.017-.417 0-.66l-.498.034.284.41a8.183 8.183 0 0 0 2.2-2.267l.97-1.48-1.6.755c.17-.08.3-.02.34.03a.914.914 0 0 1-.13-.292c-.1-.297-.13-.64-.1-.766l.36-1.254-1.1.695c-.69.438-1.51.764-2.41.963l.48.15a4.574 4.574 0 0 0-3.38-1.484 4.616 4.616 0 0 0-4.61 4.613c0 .29.02.51.08.984l.01.02.5-.06.03-.5c-3.17-.18-6.1-1.7-8.08-4.15l-.48-.56-.36.64c-.39.69-.62 1.48-.65 2.28.04 1.61.81 3.04 2.06 3.88l.3-.92c-.55-.02-1.11-.17-1.6-.45l-.59-.34-.14.67c-.02.08-.02.16 0 .24-.01 2.12 1.55 4.01 3.69 4.46l.1-.49-.1-.49c-.33.07-.67.12-1.03.14-.18-.02-.43-.05-.64-.07l-.76-.09.23.73c.57 1.84 2.29 3.14 4.28 3.21l-.28-.89a8.252 8.252 0 0 1-4.85 1.66c-.12-.01-.26-.02-.56-.05l-.17-.01-.18-.01L2.53 21l1.694 1.07a12.233 12.233 0 0 0 6.58 1.917c7.156 0 12.2-5.73 12.18-12.18l-.002.04z"/></svg></span></button></li><li class="u-textAlignCenter"><button class="button button--large button--dark button--chromeless is-touchIconBlackPulse u-baseColor--buttonDark button--withIcon button--withSvgIcon"  title="Share on Facebook" aria-label="Share on Facebook" data-action="share-on-facebook" data-action-source="post_share_widget"><span class="svgIcon svgIcon--facebook svgIcon--29px"><svg class="svgIcon-use" width="29" height="29" viewBox="0 0 29 29" ><path d="M16.39 23.61v-5.808h1.846a.55.55 0 0 0 .546-.48l.36-2.797a.551.551 0 0 0-.547-.62H16.39V12.67c0-.67.12-.813.828-.813h1.474a.55.55 0 0 0 .55-.55V8.803a.55.55 0 0 0-.477-.545c-.436-.06-1.36-.116-2.22-.116-2.5 0-4.13 1.62-4.13 4.248v1.513H10.56a.551.551 0 0 0-.55.55v2.797c0 .304.248.55.55.55h1.855v5.76c-4.172-.96-7.215-4.7-7.215-9.1 0-5.17 4.17-9.36 9.31-9.36 5.14 0 9.31 4.19 9.31 9.36 0 4.48-3.155 8.27-7.43 9.15M14.51 4C8.76 4 4.1 8.684 4.1 14.46c0 5.162 3.75 9.523 8.778 10.32a.55.55 0 0 0 .637-.543v-6.985a.551.551 0 0 0-.55-.55H11.11v-1.697h1.855a.55.55 0 0 0 .55-.55v-2.063c0-2.02 1.136-3.148 3.03-3.148.567 0 1.156.027 1.597.06v1.453h-.924c-1.363 0-1.93.675-1.93 1.912v1.78c0 .3.247.55.55.55h2.132l-.218 1.69H15.84c-.305 0-.55.24-.55.55v7.02c0 .33.293.59.623.54 5.135-.7 9.007-5.11 9.007-10.36C24.92 8.68 20.26 4 14.51 4"/></svg></span></button></li><li class="u-textAlignCenter"><button class="button button--large button--dark button--chromeless is-touchIconFadeInPulse u-baseColor--buttonDark button--withIcon button--withSvgIcon button--bookmark js-bookmarkButton"  title="Bookmark this story to read later" aria-label="Bookmark this story to read later" data-action="sign-in-prompt" data-sign-in-action="add-to-bookmarks" data-requires-token="true" data-redirect="https://medium.com/_/bookmark/p/10300100899b"><span class="button-defaultState"><span class="svgIcon svgIcon--bookmark svgIcon--29px"><svg class="svgIcon-use" width="29" height="29" viewBox="0 0 29 29" ><path d="M19.385 4h-9.77A2.623 2.623 0 0 0 7 6.615V23.01a1.022 1.022 0 0 0 1.595.847l5.905-4.004 5.905 4.004A1.022 1.022 0 0 0 22 23.011V6.62A2.625 2.625 0 0 0 19.385 4zM21 23l-5.91-3.955-.148-.107a.751.751 0 0 0-.884 0l-.147.107L8 23V6.615C8 5.725 8.725 5 9.615 5h9.77C20.275 5 21 5.725 21 6.615V23z" fill-rule="evenodd"/></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--bookmarkFilled svgIcon--29px"><svg class="svgIcon-use" width="29" height="29" viewBox="0 0 29 29" ><path d="M19.385 4h-9.77A2.623 2.623 0 0 0 7 6.615V23.01a1.022 1.022 0 0 0 1.595.847l5.905-4.004 5.905 4.004A1.022 1.022 0 0 0 22 23.011V6.62A2.625 2.625 0 0 0 19.385 4z" fill-rule="evenodd"/></svg></span></span></button></li></ul></div></div><div class="u-fixed u-bottom0 u-sizeFullWidth u-backgroundWhite u-boxShadowTop u-borderBox u-paddingTop10 u-paddingBottom10 u-zIndexMetabar u-xs-paddingLeft10 u-xs-paddingRight10 js-stickyFooter"><div class="u-maxWidth700 u-marginAuto u-flexCenter"><div class="u-fontSize16 u-flex1 u-flexCenter"><div class="u-flex0 u-inlineBlock u-paddingRight20 u-xs-paddingRight10"><a class="link avatar avatar--roundedRectangle u-baseColor--link"  href="https://ayearofai.com" title="Go to A Year of Artificial Intelligence" aria-label="Go to A Year of Artificial Intelligence" data-collection-slug="a-year-of-artificial-intelligence"><img src="https://cdn-images-1.medium.com/fit/c/40/40/1*NZsNSuNxe_O2YW1ybboOvA.jpeg" class="avatar-image avatar-image--smaller" alt="A Year of Artificial Intelligence"></a></div><div class="u-flex1 u-inlineBlock"><div class="u-xs-hide">Never miss a story from<strong> A Year of Artificial Intelligence</strong>, when you sign up for Medium. <a class="link link--accent u-accentColor--textNormal u-accentColor--textDarken u-baseColor--link"  href="https://medium.com/@Medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg" data-action-source="sticky_footer">Learn more</a></div><div class="u-xs-show">Never miss a story from<strong> A Year of Artificial Intelligence</strong></div></div></div><div class="u-marginLeft50 u-xs-marginAuto"><button class="button button--primary button--dark is-active u-noUserSelect button--withChrome u-accentColor--buttonDark u-uiTextSemibold u-textUppercase u-fontSize12 button--followCollection js-followCollectionButton"  data-action="sign-in-prompt" data-sign-in-action="toggle-subscribe-collection" data-requires-token="true" data-redirect="https://medium.com/_/subscribe/collection/a-year-of-artificial-intelligence" data-action-source="sticky_footer_collection_follow"><span class="button-label  button-defaultState js-buttonLabel">Get updates</span><span class="button-label button-activeState">Get updates</span></button></div></div></div><style class="js-collectionStyle">
.u-accentColor--borderLight {border-color: #868484 !important;}
.u-accentColor--borderNormal {border-color: #868484 !important;}
.u-accentColor--borderDark {border-color: #737171 !important;}
.u-accentColor--iconLight .svgIcon,.u-accentColor--iconLight.svgIcon {fill: #868484 !important;}
.u-accentColor--iconNormal .svgIcon,.u-accentColor--iconNormal.svgIcon {fill: #868484 !important;}
.u-accentColor--iconDark .svgIcon,.u-accentColor--iconDark.svgIcon {fill: #737171 !important;}
.u-accentColor--textNormal {color: #737171 !important;}
.u-accentColor--hoverTextNormal:hover {color: #737171 !important;}
.u-accentColor--textNormal.u-accentColor--textDarken:hover {color: #696867 !important;}
.u-accentColor--textDark {color: #696867 !important;}
.u-accentColor--backgroundLight {background-color: #868484 !important;}
.u-accentColor--backgroundNormal {background-color: #868484 !important;}
.u-accentColor--backgroundDark {background-color: #737171 !important;}
.u-accentColor--buttonDark {border-color: #737171 !important; color: #696867 !important;}
.u-accentColor--buttonDark:hover {border-color: #696867 !important;}
.u-accentColor--buttonDark .icon:before,.u-accentColor--buttonDark .svgIcon{color: #737171 !important; fill: #737171 !important;}
.u-accentColor--buttonNormal {border-color: #868484 !important; color: #737171 !important;}
.u-accentColor--buttonNormal:hover {border-color: #737171 !important;}
.u-accentColor--buttonNormal .icon:before,.u-accentColor--buttonNormal .svgIcon{color: #868484 !important; fill: #868484 !important;}
.u-accentColor--buttonNormal.button--filled .icon:before,.u-accentColor--buttonNormal.button--filled .svgIcon{color: rgba(255, 255, 255, 1) !important; fill: rgba(255, 255, 255, 1) !important;}
.u-accentColor--buttonDark.button--filled,.u-accentColor--buttonDark.button--withChrome.is-active,.u-accentColor--fillWhenActive.is-active {background-color: #737171 !important; border-color: #737171 !important; color: rgba(255, 255, 255, 1) !important; fill: rgba(255, 255, 255, 1) !important;}
.u-accentColor--buttonNormal.button--filled,.u-accentColor--buttonNormal.button--withChrome.is-active {background-color: #868484 !important; border-color: #868484 !important; color: rgba(255, 255, 255, 1) !important; fill: rgba(255, 255, 255, 1) !important;}
.postArticle.is-withAccentColors .markup--user,.postArticle.is-withAccentColors .markup--query {color: #737171 !important;}.u-tintBgColor {background-color: rgba(0, 0, 0, 1) !important;}.u-tintBgColor .u-fadeLeft:before {background-image: linear-gradient(to right, rgba(0, 0, 0, 1) 0%, rgba(0, 0, 0, 0) 100%) !important;}.u-tintBgColor .u-fadeRight:after {background-image: linear-gradient(to right, rgba(0, 0, 0, 0) 0%, rgba(0, 0, 0, 1) 100%) !important;}
.u-tintSpectrum .u-baseColor--borderLight {border-color: #868484 !important;}
.u-tintSpectrum .u-baseColor--borderNormal {border-color: #B1AEAE !important;}
.u-tintSpectrum .u-baseColor--borderDark {border-color: #D9D6D6 !important;}
.u-tintSpectrum .u-baseColor--iconLight .svgIcon,.u-tintSpectrum .u-baseColor--iconLight.svgIcon {fill: #868484 !important;}
.u-tintSpectrum .u-baseColor--iconNormal .svgIcon,.u-tintSpectrum .u-baseColor--iconNormal.svgIcon {fill: #B1AEAE !important;}
.u-tintSpectrum .u-baseColor--iconDark .svgIcon,.u-tintSpectrum .u-baseColor--iconDark.svgIcon {fill: #D9D6D6 !important;}
.u-tintSpectrum .u-baseColor--textNormal {color: #B1AEAE !important;}
.u-tintSpectrum .u-baseColor--textNormal.u-baseColor--textDarken:hover {color: #ECE9E9 !important;}
.u-tintSpectrum .u-baseColor--textDark {color: #ECE9E9 !important;}
.u-tintSpectrum .u-baseColor--backgroundLight {background-color: #868484 !important;}
.u-tintSpectrum .u-baseColor--backgroundNormal {background-color: #B1AEAE !important;}
.u-tintSpectrum .u-baseColor--backgroundDark {background-color: #D9D6D6 !important;}
.u-tintSpectrum .u-baseColor--buttonLight {border-color: #868484 !important; color: #868484 !important;}
.u-tintSpectrum .u-baseColor--buttonLight:hover {border-color: #868484 !important;}
.u-tintSpectrum .u-baseColor--buttonLight .icon:before,.u-tintSpectrum .u-baseColor--buttonLight .svgIcon {color: #868484 !important; fill: #868484 !important;}
.u-tintSpectrum .u-baseColor--buttonDark {border-color: #D9D6D6 !important; color: #ECE9E9 !important;}
.u-tintSpectrum .u-baseColor--buttonDark:hover {border-color: #ECE9E9 !important;}
.u-tintSpectrum .u-baseColor--buttonDark .icon:before,.u-tintSpectrum .u-baseColor--buttonDark .svgIcon {color: #D9D6D6 !important; fill: #D9D6D6 !important;}
.u-tintSpectrum .u-baseColor--buttonNormal {border-color: #B1AEAE !important; color: #B1AEAE !important;}
.u-tintSpectrum .u-baseColor--buttonNormal:hover {border-color: #D9D6D6 !important;}
.u-tintSpectrum .u-baseColor--buttonNormal .icon:before,.u-tintSpectrum .u-baseColor--buttonNormal .svgIcon {color: #B1AEAE !important; fill: #B1AEAE !important;}
.u-tintSpectrum .u-baseColor--buttonDark.button--filled,.u-tintSpectrum .u-baseColor--buttonDark.button--withChrome.is-active {background-color: #D9D6D6 !important; border-color: #D9D6D6 !important; color: rgba(0, 0, 0, 1) !important; fill: rgba(0, 0, 0, 1) !important;}
.u-tintSpectrum .u-baseColor--buttonNormal.button--filled,.u-tintSpectrum .u-baseColor--buttonNormal.button--withChrome.is-active {background-color: #B1AEAE !important; border-color: #B1AEAE !important; color: rgba(0, 0, 0, 1) !important; fill: rgba(0, 0, 0, 1) !important;}
.u-tintSpectrum .u-baseColor--link {color: #B1AEAE !important;}
.u-tintSpectrum .u-baseColor--link.link--darkenOnHover:hover {color: #ECE9E9 !important;}
.u-tintSpectrum .u-baseColor--link.link--darken:hover,.u-tintSpectrum .u-baseColor--link.link--darken:focus,.u-tintSpectrum .u-baseColor--link.link--darken:active {color: #ECE9E9 !important;}
.u-tintSpectrum .u-baseColor--link.link--dark {color: #ECE9E9 !important;}
.u-tintSpectrum .u-baseColor--link.link--dark.link--darken:hover,.u-tintSpectrum .u-baseColor--link.link--dark.link--darken:focus,.u-tintSpectrum .u-baseColor--link.link--dark.link--darken:active {color: #ECE9E9 !important;}
.u-tintSpectrum .u-baseColor--link.link--darker {color: #ECE9E9 !important;}
.u-tintSpectrum .u-baseColor--placeholderNormal ::-webkit-input-placeholder {color: #868484;}
.u-tintSpectrum .u-baseColor--placeholderNormal ::-moz-placeholder {color: #868484;}
.u-tintSpectrum .u-baseColor--placeholderNormal :-ms-input-placeholder {color: #868484;}
.u-tintSpectrum .svgIcon--logoNew path:nth-child(1) {stroke: none !important; fill: #3C3B3B !important;}
.u-tintSpectrum .svgIcon--logoNew path:nth-child(2) {stroke: none !important; fill: #565555 !important;}
.u-tintSpectrum .svgIcon--logoNew path:nth-child(3) {stroke: none !important; fill: #868484 !important;}
.u-tintSpectrum .svgIcon--logoNew path:nth-child(4) {stroke: none !important; fill: #B1AEAE !important;}
.u-tintSpectrum .u-accentColor--borderLight {border-color: #868484 !important;}
.u-tintSpectrum .u-accentColor--borderNormal {border-color: #B1AEAE !important;}
.u-tintSpectrum .u-accentColor--borderDark {border-color: #D9D6D6 !important;}
.u-tintSpectrum .u-accentColor--iconLight .svgIcon,.u-tintSpectrum .u-accentColor--iconLight.svgIcon {fill: #868484 !important;}
.u-tintSpectrum .u-accentColor--iconNormal .svgIcon,.u-tintSpectrum .u-accentColor--iconNormal.svgIcon {fill: #B1AEAE !important;}
.u-tintSpectrum .u-accentColor--iconDark .svgIcon,.u-tintSpectrum .u-accentColor--iconDark.svgIcon {fill: #D9D6D6 !important;}
.u-tintSpectrum .u-accentColor--textNormal {color: #B1AEAE !important;}
.u-tintSpectrum .u-accentColor--hoverTextNormal:hover {color: #B1AEAE !important;}
.u-tintSpectrum .u-accentColor--textNormal.u-accentColor--textDarken:hover {color: #ECE9E9 !important;}
.u-tintSpectrum .u-accentColor--textDark {color: #ECE9E9 !important;}
.u-tintSpectrum .u-accentColor--backgroundLight {background-color: #868484 !important;}
.u-tintSpectrum .u-accentColor--backgroundNormal {background-color: #B1AEAE !important;}
.u-tintSpectrum .u-accentColor--backgroundDark {background-color: #D9D6D6 !important;}
.u-tintSpectrum .u-accentColor--buttonDark {border-color: #D9D6D6 !important; color: #ECE9E9 !important;}
.u-tintSpectrum .u-accentColor--buttonDark:hover {border-color: #ECE9E9 !important;}
.u-tintSpectrum .u-accentColor--buttonDark .icon:before,.u-tintSpectrum .u-accentColor--buttonDark .svgIcon{color: #D9D6D6 !important; fill: #D9D6D6 !important;}
.u-tintSpectrum .u-accentColor--buttonNormal {border-color: #B1AEAE !important; color: #B1AEAE !important;}
.u-tintSpectrum .u-accentColor--buttonNormal:hover {border-color: #D9D6D6 !important;}
.u-tintSpectrum .u-accentColor--buttonNormal .icon:before,.u-tintSpectrum .u-accentColor--buttonNormal .svgIcon{color: #B1AEAE !important; fill: #B1AEAE !important;}
.u-tintSpectrum .u-accentColor--buttonNormal.button--filled .icon:before,.u-tintSpectrum .u-accentColor--buttonNormal.button--filled .svgIcon{color: rgba(0, 0, 0, 1) !important; fill: rgba(0, 0, 0, 1) !important;}
.u-tintSpectrum .u-accentColor--buttonDark.button--filled,.u-tintSpectrum .u-accentColor--buttonDark.button--withChrome.is-active,.u-tintSpectrum .u-accentColor--fillWhenActive.is-active {background-color: #D9D6D6 !important; border-color: #D9D6D6 !important; color: rgba(0, 0, 0, 1) !important; fill: rgba(0, 0, 0, 1) !important;}
.u-tintSpectrum .u-accentColor--buttonNormal.button--filled,.u-tintSpectrum .u-accentColor--buttonNormal.button--withChrome.is-active {background-color: #B1AEAE !important; border-color: #B1AEAE !important; color: rgba(0, 0, 0, 1) !important; fill: rgba(0, 0, 0, 1) !important;}
.u-tintSpectrum .postArticle.is-withAccentColors .markup--user,.u-tintSpectrum .postArticle.is-withAccentColors .markup--query {color: #B1AEAE !important;}
.u-accentColor--highlightFaint {background-color: rgba(243, 240, 239, 1) !important;}
.u-accentColor--highlightStrong.is-active .svgIcon {fill: rgba(226, 223, 222, 1) !important;}
.postArticle.is-withAccentColors .markup--quote.is-other {background-color: rgba(243, 240, 239, 1) !important;}
body.is-withMagicUnderlines .postArticle.is-withAccentColors .markup--quote.is-other {background-color: transparent !important; background-image: linear-gradient(to bottom, rgba(243, 240, 239, 1), rgba(243, 240, 239, 1));}
.postArticle.is-withAccentColors .markup--quote.is-me {background-color: rgba(233, 230, 230, 1) !important;}
body.is-withMagicUnderlines .postArticle.is-withAccentColors .markup--quote.is-me {background-color: transparent !important; background-image: linear-gradient(to bottom, rgba(233, 230, 230, 1), rgba(233, 230, 230, 1));}
.postArticle.is-withAccentColors .markup--quote.is-targeted {background-color: rgba(226, 223, 222, 1) !important;}
body.is-withMagicUnderlines .postArticle.is-withAccentColors .markup--quote.is-targeted {background-color: transparent !important; background-image: linear-gradient(to bottom, rgba(226, 223, 222, 1), rgba(226, 223, 222, 1));}
.postArticle.is-withAccentColors .markup--quote.is-selected {background-color: rgba(226, 223, 222, 1) !important;}
body.is-withMagicUnderlines .postArticle.is-withAccentColors .markup--quote.is-selected {background-color: transparent !important; background-image: linear-gradient(to bottom, rgba(226, 223, 222, 1), rgba(226, 223, 222, 1));}
.postArticle.is-withAccentColors .markup--highlight {background-color: rgba(226, 223, 222, 1) !important;}
body.is-withMagicUnderlines .postArticle.is-withAccentColors .markup--highlight {background-color: transparent !important; background-image: linear-gradient(to bottom, rgba(226, 223, 222, 1), rgba(226, 223, 222, 1));}</style><style class="js-collectionStyleConstant">.u-imageBgColor {background-color: rgba(0, 0, 0, 0.24705882352941178);}
.u-imageSpectrum .u-baseColor--borderLight {border-color: rgba(255, 255, 255, 0.6980392156862745) !important;}
.u-imageSpectrum .u-baseColor--borderNormal {border-color: rgba(255, 255, 255, 0.8980392156862745) !important;}
.u-imageSpectrum .u-baseColor--borderDark {border-color: rgba(255, 255, 255, 0.9490196078431372) !important;}
.u-imageSpectrum .u-baseColor--iconLight .svgIcon,.u-imageSpectrum .u-baseColor--iconLight.svgIcon {fill: rgba(255, 255, 255, 0.8) !important;}
.u-imageSpectrum .u-baseColor--iconNormal .svgIcon,.u-imageSpectrum .u-baseColor--iconNormal.svgIcon {fill: rgba(255, 255, 255, 0.9490196078431372) !important;}
.u-imageSpectrum .u-baseColor--iconDark .svgIcon,.u-imageSpectrum .u-baseColor--iconDark.svgIcon {fill: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-baseColor--textNormal {color: rgba(255, 255, 255, 0.9490196078431372) !important;}
.u-imageSpectrum .u-baseColor--textNormal.u-baseColor--textDarken:hover {color: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-baseColor--textDark {color: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-baseColor--backgroundLight {background-color: rgba(255, 255, 255, 0.8980392156862745) !important;}
.u-imageSpectrum .u-baseColor--backgroundNormal {background-color: rgba(255, 255, 255, 0.9490196078431372) !important;}
.u-imageSpectrum .u-baseColor--backgroundDark {background-color: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-baseColor--buttonLight {border-color: rgba(255, 255, 255, 0.6980392156862745) !important; color: rgba(255, 255, 255, 0.8) !important;}
.u-imageSpectrum .u-baseColor--buttonLight:hover {border-color: rgba(255, 255, 255, 0.6980392156862745) !important;}
.u-imageSpectrum .u-baseColor--buttonLight .icon:before,.u-imageSpectrum .u-baseColor--buttonLight .svgIcon {color: rgba(255, 255, 255, 0.8) !important; fill: rgba(255, 255, 255, 0.8) !important;}
.u-imageSpectrum .u-baseColor--buttonDark {border-color: rgba(255, 255, 255, 0.9490196078431372) !important; color: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-baseColor--buttonDark:hover {border-color: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-baseColor--buttonDark .icon:before,.u-imageSpectrum .u-baseColor--buttonDark .svgIcon {color: rgba(255, 255, 255, 1) !important; fill: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-baseColor--buttonNormal {border-color: rgba(255, 255, 255, 0.8980392156862745) !important; color: rgba(255, 255, 255, 0.9490196078431372) !important;}
.u-imageSpectrum .u-baseColor--buttonNormal:hover {border-color: rgba(255, 255, 255, 0.9490196078431372) !important;}
.u-imageSpectrum .u-baseColor--buttonNormal .icon:before,.u-imageSpectrum .u-baseColor--buttonNormal .svgIcon {color: rgba(255, 255, 255, 0.9490196078431372) !important; fill: rgba(255, 255, 255, 0.9490196078431372) !important;}
.u-imageSpectrum .u-baseColor--buttonDark.button--filled,.u-imageSpectrum .u-baseColor--buttonDark.button--withChrome.is-active {background-color: rgba(255, 255, 255, 1) !important; border-color: rgba(255, 255, 255, 1) !important; color: rgba(0, 0, 0, 0.24705882352941178) !important; fill: rgba(0, 0, 0, 0.24705882352941178) !important;}
.u-imageSpectrum .u-baseColor--buttonNormal.button--filled,.u-imageSpectrum .u-baseColor--buttonNormal.button--withChrome.is-active {background-color: rgba(255, 255, 255, 0.9490196078431372) !important; border-color: rgba(255, 255, 255, 0.9490196078431372) !important; color: rgba(0, 0, 0, 0.24705882352941178) !important; fill: rgba(0, 0, 0, 0.24705882352941178) !important;}
.u-imageSpectrum .u-baseColor--link {color: rgba(255, 255, 255, 0.9490196078431372) !important;}
.u-imageSpectrum .u-baseColor--link.link--darkenOnHover:hover {color: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-baseColor--link.link--darken:hover,.u-imageSpectrum .u-baseColor--link.link--darken:focus,.u-imageSpectrum .u-baseColor--link.link--darken:active {color: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-baseColor--link.link--dark {color: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-baseColor--link.link--dark.link--darken:hover,.u-imageSpectrum .u-baseColor--link.link--dark.link--darken:focus,.u-imageSpectrum .u-baseColor--link.link--dark.link--darken:active {color: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-baseColor--link.link--darker {color: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-baseColor--placeholderNormal ::-webkit-input-placeholder {color: rgba(255, 255, 255, 0.8);}
.u-imageSpectrum .u-baseColor--placeholderNormal ::-moz-placeholder {color: rgba(255, 255, 255, 0.8);}
.u-imageSpectrum .u-baseColor--placeholderNormal :-ms-input-placeholder {color: rgba(255, 255, 255, 0.8);}
.u-imageSpectrum .svgIcon--logoNew path:nth-child(1) {stroke: none !important; fill: rgba(255, 255, 255, 0.4) !important;}
.u-imageSpectrum .svgIcon--logoNew path:nth-child(2) {stroke: none !important; fill: rgba(255, 255, 255, 0.4980392156862745) !important;}
.u-imageSpectrum .svgIcon--logoNew path:nth-child(3) {stroke: none !important; fill: rgba(255, 255, 255, 0.6980392156862745) !important;}
.u-imageSpectrum .svgIcon--logoNew path:nth-child(4) {stroke: none !important; fill: rgba(255, 255, 255, 0.8980392156862745) !important;}
.u-imageSpectrum .u-accentColor--borderLight {border-color: rgba(255, 255, 255, 0.6980392156862745) !important;}
.u-imageSpectrum .u-accentColor--borderNormal {border-color: rgba(255, 255, 255, 0.8980392156862745) !important;}
.u-imageSpectrum .u-accentColor--borderDark {border-color: rgba(255, 255, 255, 0.9490196078431372) !important;}
.u-imageSpectrum .u-accentColor--iconLight .svgIcon,.u-imageSpectrum .u-accentColor--iconLight.svgIcon {fill: rgba(255, 255, 255, 0.8) !important;}
.u-imageSpectrum .u-accentColor--iconNormal .svgIcon,.u-imageSpectrum .u-accentColor--iconNormal.svgIcon {fill: rgba(255, 255, 255, 0.9490196078431372) !important;}
.u-imageSpectrum .u-accentColor--iconDark .svgIcon,.u-imageSpectrum .u-accentColor--iconDark.svgIcon {fill: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-accentColor--textNormal {color: rgba(255, 255, 255, 0.9490196078431372) !important;}
.u-imageSpectrum .u-accentColor--hoverTextNormal:hover {color: rgba(255, 255, 255, 0.9490196078431372) !important;}
.u-imageSpectrum .u-accentColor--textNormal.u-accentColor--textDarken:hover {color: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-accentColor--textDark {color: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-accentColor--backgroundLight {background-color: rgba(255, 255, 255, 0.8980392156862745) !important;}
.u-imageSpectrum .u-accentColor--backgroundNormal {background-color: rgba(255, 255, 255, 0.9490196078431372) !important;}
.u-imageSpectrum .u-accentColor--backgroundDark {background-color: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-accentColor--buttonDark {border-color: rgba(255, 255, 255, 0.9490196078431372) !important; color: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-accentColor--buttonDark:hover {border-color: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-accentColor--buttonDark .icon:before,.u-imageSpectrum .u-accentColor--buttonDark .svgIcon{color: rgba(255, 255, 255, 1) !important; fill: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-accentColor--buttonNormal {border-color: rgba(255, 255, 255, 0.8980392156862745) !important; color: rgba(255, 255, 255, 0.9490196078431372) !important;}
.u-imageSpectrum .u-accentColor--buttonNormal:hover {border-color: rgba(255, 255, 255, 0.9490196078431372) !important;}
.u-imageSpectrum .u-accentColor--buttonNormal .icon:before,.u-imageSpectrum .u-accentColor--buttonNormal .svgIcon{color: rgba(255, 255, 255, 0.9490196078431372) !important; fill: rgba(255, 255, 255, 0.9490196078431372) !important;}
.u-imageSpectrum .u-accentColor--buttonNormal.button--filled .icon:before,.u-imageSpectrum .u-accentColor--buttonNormal.button--filled .svgIcon{color: rgba(0, 0, 0, 0.24705882352941178) !important; fill: rgba(0, 0, 0, 0.24705882352941178) !important;}
.u-imageSpectrum .u-accentColor--buttonDark.button--filled,.u-imageSpectrum .u-accentColor--buttonDark.button--withChrome.is-active,.u-imageSpectrum .u-accentColor--fillWhenActive.is-active {background-color: rgba(255, 255, 255, 1) !important; border-color: rgba(255, 255, 255, 1) !important; color: rgba(0, 0, 0, 0.24705882352941178) !important; fill: rgba(0, 0, 0, 0.24705882352941178) !important;}
.u-imageSpectrum .u-accentColor--buttonNormal.button--filled,.u-imageSpectrum .u-accentColor--buttonNormal.button--withChrome.is-active {background-color: rgba(255, 255, 255, 0.9490196078431372) !important; border-color: rgba(255, 255, 255, 0.9490196078431372) !important; color: rgba(0, 0, 0, 0.24705882352941178) !important; fill: rgba(0, 0, 0, 0.24705882352941178) !important;}
.u-imageSpectrum .postArticle.is-withAccentColors .markup--user,.u-imageSpectrum .postArticle.is-withAccentColors .markup--query {color: rgba(255, 255, 255, 0.9490196078431372) !important;}
.u-imageSpectrum .u-accentColor--highlightFaint {background-color: rgba(255, 255, 255, 0.2) !important;}
.u-imageSpectrum .u-accentColor--highlightStrong.is-active .svgIcon {fill: rgba(255, 255, 255, 0.6) !important;}
.postArticle.is-withAccentColors .u-imageSpectrum .markup--quote.is-other {background-color: rgba(255, 255, 255, 0.2) !important;}
body.is-withMagicUnderlines .postArticle.is-withAccentColors .u-imageSpectrum .markup--quote.is-other {background-color: transparent !important; background-image: linear-gradient(to bottom, rgba(255, 255, 255, 0.2), rgba(255, 255, 255, 0.2));}
.postArticle.is-withAccentColors .u-imageSpectrum .markup--quote.is-me {background-color: rgba(255, 255, 255, 0.4) !important;}
body.is-withMagicUnderlines .postArticle.is-withAccentColors .u-imageSpectrum .markup--quote.is-me {background-color: transparent !important; background-image: linear-gradient(to bottom, rgba(255, 255, 255, 0.4), rgba(255, 255, 255, 0.4));}
.postArticle.is-withAccentColors .u-imageSpectrum .markup--quote.is-targeted {background-color: rgba(255, 255, 255, 0.6) !important;}
body.is-withMagicUnderlines .postArticle.is-withAccentColors .u-imageSpectrum .markup--quote.is-targeted {background-color: transparent !important; background-image: linear-gradient(to bottom, rgba(255, 255, 255, 0.6), rgba(255, 255, 255, 0.6));}
.postArticle.is-withAccentColors .u-imageSpectrum .markup--quote.is-selected {background-color: rgba(255, 255, 255, 0.6) !important;}
body.is-withMagicUnderlines .postArticle.is-withAccentColors .u-imageSpectrum .markup--quote.is-selected {background-color: transparent !important; background-image: linear-gradient(to bottom, rgba(255, 255, 255, 0.6), rgba(255, 255, 255, 0.6));}
.postArticle.is-withAccentColors .u-imageSpectrum .markup--highlight {background-color: rgba(255, 255, 255, 0.6) !important;}
body.is-withMagicUnderlines .postArticle.is-withAccentColors .u-imageSpectrum .markup--highlight {background-color: transparent !important; background-image: linear-gradient(to bottom, rgba(255, 255, 255, 0.6), rgba(255, 255, 255, 0.6));}.u-resetSpectrum .u-tintBgColor {background-color: rgba(255, 255, 255, 1) !important;}.u-resetSpectrum .u-tintBgColor .u-fadeLeft:before {background-image: linear-gradient(to right, rgba(255, 255, 255, 1) 0%, rgba(255, 255, 255, 0) 100%) !important;}.u-resetSpectrum .u-tintBgColor .u-fadeRight:after {background-image: linear-gradient(to right, rgba(255, 255, 255, 0) 0%, rgba(255, 255, 255, 1) 100%) !important;}
.u-resetSpectrum .u-baseColor--borderLight {border-color: rgba(0, 0, 0, 0.2980392156862745) !important;}
.u-resetSpectrum .u-baseColor--borderNormal {border-color: rgba(0, 0, 0, 0.4980392156862745) !important;}
.u-resetSpectrum .u-baseColor--borderDark {border-color: rgba(0, 0, 0, 0.6) !important;}
.u-resetSpectrum .u-baseColor--iconLight .svgIcon,.u-resetSpectrum .u-baseColor--iconLight.svgIcon {fill: rgba(0, 0, 0, 0.2980392156862745) !important;}
.u-resetSpectrum .u-baseColor--iconNormal .svgIcon,.u-resetSpectrum .u-baseColor--iconNormal.svgIcon {fill: rgba(0, 0, 0, 0.4980392156862745) !important;}
.u-resetSpectrum .u-baseColor--iconDark .svgIcon,.u-resetSpectrum .u-baseColor--iconDark.svgIcon {fill: rgba(0, 0, 0, 0.6) !important;}
.u-resetSpectrum .u-baseColor--textNormal {color: rgba(0, 0, 0, 0.4980392156862745) !important;}
.u-resetSpectrum .u-baseColor--textNormal.u-baseColor--textDarken:hover {color: rgba(0, 0, 0, 0.6) !important;}
.u-resetSpectrum .u-baseColor--textDark {color: rgba(0, 0, 0, 0.6) !important;}
.u-resetSpectrum .u-baseColor--backgroundLight {background-color: rgba(0, 0, 0, 0.09803921568627451) !important;}
.u-resetSpectrum .u-baseColor--backgroundNormal {background-color: rgba(0, 0, 0, 0.2) !important;}
.u-resetSpectrum .u-baseColor--backgroundDark {background-color: rgba(0, 0, 0, 0.2980392156862745) !important;}
.u-resetSpectrum .u-baseColor--buttonLight {border-color: rgba(0, 0, 0, 0.2980392156862745) !important; color: rgba(0, 0, 0, 0.2980392156862745) !important;}
.u-resetSpectrum .u-baseColor--buttonLight:hover {border-color: rgba(0, 0, 0, 0.2980392156862745) !important;}
.u-resetSpectrum .u-baseColor--buttonLight .icon:before,.u-resetSpectrum .u-baseColor--buttonLight .svgIcon {color: rgba(0, 0, 0, 0.2980392156862745) !important; fill: rgba(0, 0, 0, 0.2980392156862745) !important;}
.u-resetSpectrum .u-baseColor--buttonDark {border-color: rgba(0, 0, 0, 0.6) !important; color: rgba(0, 0, 0, 0.6) !important;}
.u-resetSpectrum .u-baseColor--buttonDark:hover {border-color: rgba(0, 0, 0, 0.8) !important;}
.u-resetSpectrum .u-baseColor--buttonDark .icon:before,.u-resetSpectrum .u-baseColor--buttonDark .svgIcon {color: rgba(0, 0, 0, 0.6) !important; fill: rgba(0, 0, 0, 0.6) !important;}
.u-resetSpectrum .u-baseColor--buttonNormal {border-color: rgba(0, 0, 0, 0.4980392156862745) !important; color: rgba(0, 0, 0, 0.4980392156862745) !important;}
.u-resetSpectrum .u-baseColor--buttonNormal:hover {border-color: rgba(0, 0, 0, 0.6) !important;}
.u-resetSpectrum .u-baseColor--buttonNormal .icon:before,.u-resetSpectrum .u-baseColor--buttonNormal .svgIcon {color: rgba(0, 0, 0, 0.4980392156862745) !important; fill: rgba(0, 0, 0, 0.4980392156862745) !important;}
.u-resetSpectrum .u-baseColor--buttonDark.button--filled,.u-resetSpectrum .u-baseColor--buttonDark.button--withChrome.is-active {background-color: rgba(0, 0, 0, 0.2980392156862745) !important; border-color: rgba(0, 0, 0, 0.2980392156862745) !important; color: rgba(255, 255, 255, 1) !important; fill: rgba(255, 255, 255, 1) !important;}
.u-resetSpectrum .u-baseColor--buttonNormal.button--filled,.u-resetSpectrum .u-baseColor--buttonNormal.button--withChrome.is-active {background-color: rgba(0, 0, 0, 0.2) !important; border-color: rgba(0, 0, 0, 0.2) !important; color: rgba(255, 255, 255, 1) !important; fill: rgba(255, 255, 255, 1) !important;}
.u-resetSpectrum .u-baseColor--link {color: rgba(0, 0, 0, 0.4980392156862745) !important;}
.u-resetSpectrum .u-baseColor--link.link--darkenOnHover:hover {color: rgba(0, 0, 0, 0.6) !important;}
.u-resetSpectrum .u-baseColor--link.link--darken:hover,.u-resetSpectrum .u-baseColor--link.link--darken:focus,.u-resetSpectrum .u-baseColor--link.link--darken:active {color: rgba(0, 0, 0, 0.6) !important;}
.u-resetSpectrum .u-baseColor--link.link--dark {color: rgba(0, 0, 0, 0.6) !important;}
.u-resetSpectrum .u-baseColor--link.link--dark.link--darken:hover,.u-resetSpectrum .u-baseColor--link.link--dark.link--darken:focus,.u-resetSpectrum .u-baseColor--link.link--dark.link--darken:active {color: rgba(0, 0, 0, 0.8) !important;}
.u-resetSpectrum .u-baseColor--link.link--darker {color: rgba(0, 0, 0, 0.8) !important;}
.u-resetSpectrum .u-baseColor--placeholderNormal ::-webkit-input-placeholder {color: rgba(0, 0, 0, 0.2980392156862745);}
.u-resetSpectrum .u-baseColor--placeholderNormal ::-moz-placeholder {color: rgba(0, 0, 0, 0.2980392156862745);}
.u-resetSpectrum .u-baseColor--placeholderNormal :-ms-input-placeholder {color: rgba(0, 0, 0, 0.2980392156862745);}
.u-resetSpectrum .svgIcon--logoNew path:nth-child(1) {stroke: none !important; fill: rgba(0, 0, 0, 0.2) !important;}
.u-resetSpectrum .svgIcon--logoNew path:nth-child(2) {stroke: none !important; fill: rgba(0, 0, 0, 0.2980392156862745) !important;}
.u-resetSpectrum .svgIcon--logoNew path:nth-child(3) {stroke: none !important; fill: rgba(0, 0, 0, 0.4) !important;}
.u-resetSpectrum .svgIcon--logoNew path:nth-child(4) {stroke: none !important; fill: rgba(0, 0, 0, 0.4980392156862745) !important;}
.u-resetSpectrum .u-accentColor--borderLight {border-color: rgba(2, 184, 117, 1) !important;}
.u-resetSpectrum .u-accentColor--borderNormal {border-color: rgba(2, 184, 117, 1) !important;}
.u-resetSpectrum .u-accentColor--borderDark {border-color: rgba(0, 171, 107, 1) !important;}
.u-resetSpectrum .u-accentColor--iconLight .svgIcon,.u-resetSpectrum .u-accentColor--iconLight.svgIcon {fill: rgba(2, 184, 117, 1) !important;}
.u-resetSpectrum .u-accentColor--iconNormal .svgIcon,.u-resetSpectrum .u-accentColor--iconNormal.svgIcon {fill: rgba(0, 171, 107, 1) !important;}
.u-resetSpectrum .u-accentColor--iconDark .svgIcon,.u-resetSpectrum .u-accentColor--iconDark.svgIcon {fill: rgba(28, 153, 99, 1) !important;}
.u-resetSpectrum .u-accentColor--textNormal {color: rgba(0, 171, 107, 1) !important;}
.u-resetSpectrum .u-accentColor--hoverTextNormal:hover {color: rgba(0, 171, 107, 1) !important;}
.u-resetSpectrum .u-accentColor--textNormal.u-accentColor--textDarken:hover {color: rgba(28, 153, 99, 1) !important;}
.u-resetSpectrum .u-accentColor--textDark {color: rgba(28, 153, 99, 1) !important;}
.u-resetSpectrum .u-accentColor--backgroundLight {background-color: rgba(2, 184, 117, 1) !important;}
.u-resetSpectrum .u-accentColor--backgroundNormal {background-color: rgba(0, 171, 107, 1) !important;}
.u-resetSpectrum .u-accentColor--backgroundDark {background-color: rgba(28, 153, 99, 1) !important;}
.u-resetSpectrum .u-accentColor--buttonDark {border-color: rgba(0, 171, 107, 1) !important; color: rgba(28, 153, 99, 1) !important;}
.u-resetSpectrum .u-accentColor--buttonDark:hover {border-color: rgba(28, 153, 99, 1) !important;}
.u-resetSpectrum .u-accentColor--buttonDark .icon:before,.u-resetSpectrum .u-accentColor--buttonDark .svgIcon{color: rgba(28, 153, 99, 1) !important; fill: rgba(28, 153, 99, 1) !important;}
.u-resetSpectrum .u-accentColor--buttonNormal {border-color: rgba(2, 184, 117, 1) !important; color: rgba(0, 171, 107, 1) !important;}
.u-resetSpectrum .u-accentColor--buttonNormal:hover {border-color: rgba(0, 171, 107, 1) !important;}
.u-resetSpectrum .u-accentColor--buttonNormal .icon:before,.u-resetSpectrum .u-accentColor--buttonNormal .svgIcon{color: rgba(0, 171, 107, 1) !important; fill: rgba(0, 171, 107, 1) !important;}
.u-resetSpectrum .u-accentColor--buttonNormal.button--filled .icon:before,.u-resetSpectrum .u-accentColor--buttonNormal.button--filled .svgIcon{color: rgba(255, 255, 255, 1) !important; fill: rgba(255, 255, 255, 1) !important;}
.u-resetSpectrum .u-accentColor--buttonDark.button--filled,.u-resetSpectrum .u-accentColor--buttonDark.button--withChrome.is-active,.u-resetSpectrum .u-accentColor--fillWhenActive.is-active {background-color: rgba(28, 153, 99, 1) !important; border-color: rgba(28, 153, 99, 1) !important; color: rgba(255, 255, 255, 1) !important; fill: rgba(255, 255, 255, 1) !important;}
.u-resetSpectrum .u-accentColor--buttonNormal.button--filled,.u-resetSpectrum .u-accentColor--buttonNormal.button--withChrome.is-active {background-color: rgba(0, 171, 107, 1) !important; border-color: rgba(0, 171, 107, 1) !important; color: rgba(255, 255, 255, 1) !important; fill: rgba(255, 255, 255, 1) !important;}
.u-resetSpectrum .postArticle.is-withAccentColors .markup--user,.u-resetSpectrum .postArticle.is-withAccentColors .markup--query {color: rgba(0, 171, 107, 1) !important;}</style></div></div></div><div class="loadingBar"></div><script>// <![CDATA[
window["obvInit"] = function (opt_embedded) {window["obvInit"]["embedded"] = opt_embedded; window["obvInit"]["ready"] = true;}
// ]]></script><script>// <![CDATA[
var GLOBALS = {"audioUrl":"https://d1fcbxp97j4nb2.cloudfront.net","baseUrl":"https://ayearofai.com","buildLabel":"28586-6143d83","currentUser":{"userId":"lo_ced97fb2944f","isVerified":false,"subscriberEmail":""},"currentUserHasUnverifiedEmail":false,"isAuthenticated":false,"isCurrentUserVerified":false,"mediumTwitterScreenName":"medium","miroUrl":"https://cdn-images-1.medium.com","moduleUrls":{"base":"https://cdn-static-1.medium.com/_/fp/gen-js/main-base.bundle.2nXtJ8XknWJqOjh2ZNHRVw.js","notes":"https://cdn-static-1.medium.com/_/fp/gen-js/main-notes.bundle.AouowiblLn4SBKZF6cWDtA.js","posters":"https://cdn-static-1.medium.com/_/fp/gen-js/main-posters.bundle.4rYp3nph7xNXmlmak5XWxQ.js","common-async":"https://cdn-static-1.medium.com/_/fp/gen-js/main-common-async.bundle.R35emyz9xZL7YdHbIl-SvQ.js","stats":"https://cdn-static-1.medium.com/_/fp/gen-js/main-stats.bundle.XUyD-cfC9DG2nc9iq5x7GA.js","home-screens":"https://cdn-static-1.medium.com/_/fp/gen-js/main-home-screens.bundle.0oM8iNH9LgWeJY00O_ao2Q.js","misc-screens":"https://cdn-static-1.medium.com/_/fp/gen-js/main-misc-screens.bundle.caQfKUCZ18Wg2IbFBoaXiA.js"},"previewConfig":{"weightThreshold":1,"weightImageParagraph":0.51,"weightIframeParagraph":0.8,"weightTextParagraph":0.08,"weightEmptyParagraph":0,"weightP":0.003,"weightH":0.005,"weightBq":0.003,"minPTextLength":60,"truncateBoundaryChars":20,"detectTitle":true,"detectTitleLevThreshold":0.15},"productName":"Medium","supportsEdit":false,"termsUrl":"//medium.com/policy/9db0094a1e0f","textshotHost":"textshot.medium.com","transactionId":"1492393969082:201c27e9570c","useragent":{"browser":"python requests","family":"","os":"","version":2.2,"supportsDesktopEdit":false,"supportsInteract":false,"supportsView":true,"isMobile":false,"isTablet":false,"isNative":false,"supportsFileAPI":false,"isTier1":false,"clientVersion":"","unknownParagraphsBad":false,"clientChannel":"","supportsRealScrollEvents":false,"supportsVhUnits":false,"ruinsViewportSections":false,"supportsHtml5Video":false,"supportsMagicUnderlines":false,"isWebView":false,"isFacebookWebView":false,"supportsProgressiveMedia":false,"supportsPromotedPosts":true,"isBot":false,"isNativeIphone":false,"supportsCssVariables":false,"supportsScrollableMetabar":false},"variants":{"allow_access":true,"allow_signup":true,"allow_test_auth":"disallow","signin_services":"twitter,facebook,google,email,google-fastidv","signup_services":"twitter,facebook,google,email,google-fastidv","android_rating_prompt_recommend_threshold":5,"google_sign_in_android":true,"enable_onboarding":true,"ios_custom_miro_url":"https://cdn-images-1.medium.com","reengagement_notification_duration":3,"enable_adsnative_integration":true,"browsable_stream_config_bucket":"curated-topics","ios_small_post_preview_truncation_length":5.5,"ios_large_post_preview_truncation_length":5.5,"disable_ios_catalog_badging":true,"enable_series_creation":true,"enable_your_series_pages":true,"enable_productionized_series":true,"enable_dedicated_series_tab_api_ios":true,"enable_clap_milestone_notifications":true,"enable_series_stats_page":true,"enable_prepublish_share_settings":true,"enable_direct_auth_connect":true,"enable_post_import":true,"enable_sponsored_post_labelling":true,"enable_logged_in_follow_on_collection_post":true,"promoted_story_placement_locations":"POST_PAGE_FOOTER","show_topics":true,"enable_search_collection_by_tag_recency_filter":true,"search_collection_by_tag_filter_min_votes":10,"enable_sms_app_promo":true,"enable_export_members":true,"enable_series_card_background_creation":true,"can_see_subscription_branding":true,"enable_subscriptions_landing_page":true,"enable_partner_program_landing_page":true,"enable_hide_broken_links":true,"enable_pay_for_custom_domain":true,"enable_promos_from_dynamo":true,"enable_promos_in_placement":true,"enable_series_promo_in_email":true,"enable_sms":true,"enable_series_in_user_profiles":true,"enable_new_logged_out_bento_operation":true,"enable_topic_brief_relations":true},"xsrfToken":"","iosAppId":"828256236","supportEmail":"yourfriends@medium.com","teamName":"Team Medium","fp":{"/icons/favicon.svg":"https://cdn-static-1.medium.com/_/fp/icons/favicon.KjTfUJo7yJH_fCoUzzH3cg.svg","/icons/favicon-dev-editor.ico":"https://cdn-static-1.medium.com/_/fp/icons/favicon-dev-editor.YKKRxBO8EMvIqhyCwIiJeQ.ico","/icons/favicon-hatch-editor.ico":"https://cdn-static-1.medium.com/_/fp/icons/favicon-hatch-editor.BuEyHIqlyh2s_XEk4Rl32Q.ico","/icons/favicon-medium-editor.ico":"https://cdn-static-1.medium.com/_/fp/icons/favicon-medium-editor.PiakrZWB7Yb80quUVQWM6g.ico"},"authBaseUrl":"https://medium.com","imageUploadSizeMb":25,"isAuthDomainRequest":false,"domainCollectionSlug":"a-year-of-artificial-intelligence","algoliaApiEndpoint":"https://MQ57UUUQZ2-dsn.algolia.net","algoliaAppId":"MQ57UUUQZ2","algoliaSearchOnlyApiKey":"394474ced050e3911ae2249ecc774921","iosAppStoreUrl":"https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8","iosAppLinkBaseUrl":"medium:","algoliaIndexPrefix":"medium_","androidPlayStoreUrl":"https://play.google.com/store/apps/details?id=com.medium.reader","googleClientId":"216296035834-k1k6qe060s2tp2a2jam4ljdcms00sttg.apps.googleusercontent.com","androidPackage":"com.medium.reader","androidPlayStoreMarketScheme":"market://details?id=com.medium.reader","googleAuthUri":"https://accounts.google.com/o/oauth2/auth","androidScheme":"medium","layoutData":{"useDynamicScripts":false,"googleAnalyticsTrackingCode":"UA-24232453-2","jsShivUrl":"https://cdn-static-1.medium.com/_/fp/js/shiv.RI2ePTZ5gFmMgLzG5bEVAA.js","useDynamicCss":false,"faviconUrl":"https://cdn-static-1.medium.com/_/fp/icons/favicon-medium.TAS6uQ-Y7kcKgi0xjcYHXw.ico","faviconImageId":"1*W0nmth_X8nFKjn6BZ388UQ.png","fontSets":[{"id":1,"url":"https://cdn-static-1.medium.com/_/fp/css/fonts-base.by5Oi_VbnwEIvhnWIsuUjA.css"},{"id":4,"url":"https://cdn-static-1.medium.com/_/fp/css/fonts-lazy-base.g08Jj5TZPAiuPWj5YNUsSg.css"},{"id":6,"url":"https://cdn-static-1.medium.com/_/fp/css/fonts-latin-base.141WxxXgxGxNcfeza73H7Q.css"},{"id":7,"url":"https://cdn-static-1.medium.com/_/fp/css/fonts-lazy-latin-base.jMU532QDmysQMOINr-cr2A.css"}],"editorFaviconUrl":"https://cdn-static-1.medium.com/_/fp/icons/favicon-medium-editor.PiakrZWB7Yb80quUVQWM6g.ico"},"authBaseUrlRev":"moc.muidem//:sptth","isDnt":false,"stripePublishableKey":"pk_live_7FReX44VnNIInZwrIIx6ghjl","archiveUploadSizeMb":100,"paymentData":{"currencies":{"1":{"label":"US Dollar","external":"usd"}},"countries":{"1":{"label":"United States of America","external":"US"}},"accountTypes":{"1":{"label":"Individual","external":"individual"},"2":{"label":"Company","external":"company"}}},"previewConfig2":{"weightThreshold":1,"weightImageParagraph":0.05,"raiseImage":true,"enforceHeaderHierarchy":true,"isImageInsetRight":true},"isAmp":false,"iosScheme":"medium","isSwBoot":false,"lightstep":{"accessToken":"ce5be895bef60919541332990ac9fef2","carrier":"{\"ot-tracer-spanid\":\"00478ee86baae7c5\",\"ot-tracer-traceid\":\"368051c4509483c0\",\"ot-tracer-sampled\":\"true\"}","host":"collector-medium.lightstep.com"},"facebook":{"key":"542599432471018","secret":"c14df7146e9052a1131f3c900c1f0644","token":"542599432471018|1JqjIwxSfY9jOt_KwjWEl1R7T6I","namespace":"medium-com","scope":{"default":["public_profile","email","user_friends"],"connect":["public_profile","email","user_friends"],"login":["public_profile","email","user_friends"],"share":["public_profile","email","user_friends","publish_actions"]},"smartPublishWhitelistedPublications":["bcc38c8f6edf","f3726e2a5878","828a270689e","81c7d351c056","f30e42fd7ff8","8bf1d7d3081b","d16afa0ae7c","d8f3f6ad9c31","e74de0cedea9","15f753907972","c8c6a6b01ebd","3412b9729488","2ce4bbcf83bb","544c7006046e","7bfcdbc6b30a","a268fd916824","458a773bccd2"],"instantArticles":{"published":true,"developmentMode":false}},"mailingListArchiveUploadSizeMb":2,"availableMembershipPlans":[],"editorsPicksTopicId":"3985d2a191c5","popularOnMediumTopicId":"9d34e48ecf94","memberContentTopicId":"13d7efd82fb2","isDoNotAuth":false,"goldfinchUrl":"https://goldfinch.medium.com"}
// ]]></script><script charset="UTF-8" src="https://cdn-static-1.medium.com/_/fp/gen-js/main-base.bundle.2nXtJ8XknWJqOjh2ZNHRVw.js" async></script><script>// <![CDATA[
window["obvInit"]({"value":{"id":"10300100899b","versionId":"4e632fdafd3b","creatorId":"cb55958ea3bb","creator":{"userId":"cb55958ea3bb","name":"Rohan Kapur","username":"mckapur","createdAt":1383813905817,"lastPostCreatedAt":1492253036060,"imageId":"1*pGDfwp8yLzgCeG1RkPUxgQ.jpeg","backgroundImageId":"","bio":"rohankapur.com","twitterScreenName":"mckapur","socialStats":{"userId":"cb55958ea3bb","usersFollowedCount":204,"usersFollowedByCount":592,"type":"SocialStats"},"social":{"userId":"lo_6d640aa2d2ff","targetUserId":"cb55958ea3bb","type":"Social"},"facebookAccountId":"1004843339565980","allowNotes":1,"type":"User"},"homeCollection":{"id":"bb87da25612c","name":"A Year of Artificial Intelligence","slug":"a-year-of-artificial-intelligence","tags":["DATA SCIENCE","ARTIFICIAL INTELLIGENCE","MACHINE LEARNING","TECHNOLOGY","COMPUTER SCIENCE"],"creatorId":"cb55958ea3bb","description":"Our ongoing effort to make the mathematics, science, linguistics, and philosophy of artificial intelligence fun and simple.","shortDescription":"Our ongoing effort to make the mathematics, science…","image":{"imageId":"1*NZsNSuNxe_O2YW1ybboOvA.jpeg","filter":"","backgroundSize":"","originalWidth":0,"originalHeight":0,"strategy":"resample","height":0,"width":0},"metadata":{"followerCount":521,"postCount":6,"activeAt":1492072051871},"virtuals":{"permissions":{"canPublish":false,"canPublishAll":false,"canRepublish":false,"canRemove":false,"canManageAll":false,"canSubmit":false,"canEditPosts":false,"canAddWriters":false,"canViewStats":false,"canSendNewsletter":false,"canViewLockedPosts":false,"canViewCloaked":false,"canEditOwnPosts":false,"canBeAssignedAuthor":false},"isSubscribed":false,"isNewsletterSubscribed":false,"memberOfMembershipPlanId":""},"logo":{"imageId":"1*NZsNSuNxe_O2YW1ybboOvA.jpeg","filter":"","backgroundSize":"","originalWidth":1500,"originalHeight":1000,"strategy":"resample","height":0,"width":0},"twitterUsername":"mckapur","facebookPageName":"mckapur","publicEmail":"me@rohankapur.com","domain":"ayearofai.com","sections":[{"type":2,"collectionHeaderMetadata":{"title":"A Year of Artificial Intelligence","description":"Where we make the mathematics, science, linguistics, and philosophy of artificial intelligence fun and simple.","backgroundImage":{"id":"1*UlHnUWTtTHWOrOuJdHjVww.png","originalWidth":2000,"originalHeight":1333,"focusPercentX":51.785714285714285,"focusPercentY":43.31550802139037},"logoImage":{"id":"1*NZsNSuNxe_O2YW1ybboOvA.jpeg","originalWidth":1500,"originalHeight":1000,"alt":"A Year of Artificial Intelligence"},"alignment":1,"layout":6}},{"type":1,"postListMetadata":{"source":1,"layout":2,"number":1,"postIds":[],"sectionHeader":"New"}},{"type":1,"postListMetadata":{"source":1,"layout":4,"number":14,"postIds":[]}}],"tintColor":"#FF000000","lightText":true,"favicon":{"imageId":"","filter":"","backgroundSize":"","originalWidth":0,"originalHeight":0,"strategy":"resample","height":0,"width":0},"colorPalette":{"defaultBackgroundSpectrum":{"colorPoints":[{"color":"#FF868484","point":0},{"color":"#FF7C7B7A","point":0.1},{"color":"#FF737171","point":0.2},{"color":"#FF696867","point":0.3},{"color":"#FF5F5E5E","point":0.4},{"color":"#FF555454","point":0.5},{"color":"#FF4A4949","point":0.6},{"color":"#FF3F3E3E","point":0.7},{"color":"#FF343333","point":0.8},{"color":"#FF272727","point":0.9},{"color":"#FF1A1A1A","point":1}],"backgroundColor":"#FFFFFFFF"},"tintBackgroundSpectrum":{"colorPoints":[{"color":"#FF000000","point":0},{"color":"#FF1E1D1D","point":0.1},{"color":"#FF3C3B3B","point":0.2},{"color":"#FF565555","point":0.3},{"color":"#FF6F6D6D","point":0.4},{"color":"#FF868484","point":0.5},{"color":"#FF9C9A99","point":0.6},{"color":"#FFB1AEAE","point":0.7},{"color":"#FFC5C3C2","point":0.8},{"color":"#FFD9D6D6","point":0.9},{"color":"#FFECE9E9","point":1}],"backgroundColor":"#FF000000"},"highlightSpectrum":{"colorPoints":[{"color":"#FFF5F2F1","point":0},{"color":"#FFF3F0EF","point":0.1},{"color":"#FFF1EEED","point":0.2},{"color":"#FFEFECEC","point":0.3},{"color":"#FFEDEAEA","point":0.4},{"color":"#FFEBE8E8","point":0.5},{"color":"#FFE9E6E6","point":0.6},{"color":"#FFE7E5E4","point":0.7},{"color":"#FFE5E3E2","point":0.8},{"color":"#FFE4E1E0","point":0.9},{"color":"#FFE2DFDE","point":1}],"backgroundColor":"#FFFFFFFF"}},"navItems":[{"type":1,"title":"Algorithms","tagSlug":"algorithms","url":"https://ayearofai.com/tagged/algorithms","source":"tagSlug"},{"type":1,"title":"Today I Learned","tagSlug":"today-i-learned","url":"https://ayearofai.com/tagged/today-i-learned","source":"tagSlug"},{"type":1,"title":"Case Studies","tagSlug":"case-studies","url":"https://ayearofai.com/tagged/case-studies","source":"tagSlug"},{"type":1,"title":"Philosophical","tagSlug":"philosophical","url":"https://ayearofai.com/tagged/philosophical","source":"tagSlug"},{"type":1,"title":"Meta","tagSlug":"meta","url":"https://ayearofai.com/tagged/meta","source":"tagSlug"}],"colorBehavior":2,"instantArticlesState":0,"acceleratedMobilePagesState":0,"ampLogo":{"imageId":"","filter":"","backgroundSize":"","originalWidth":0,"originalHeight":0,"strategy":"resample","height":0,"width":0},"header":{"title":"A Year of Artificial Intelligence","description":"Where we make the mathematics, science, linguistics, and philosophy of artificial intelligence fun and simple.","backgroundImage":{"id":"1*UlHnUWTtTHWOrOuJdHjVww.png","originalWidth":2000,"originalHeight":1333,"focusPercentX":51.785714285714285,"focusPercentY":43.31550802139037},"logoImage":{"id":"1*NZsNSuNxe_O2YW1ybboOvA.jpeg","originalWidth":1500,"originalHeight":1000,"alt":"A Year of Artificial Intelligence"},"alignment":1,"layout":6},"type":"Collection"},"homeCollectionId":"bb87da25612c","title":"Rohan & Lenny #3: Recurrent Neural Networks & LSTMs","detectedLanguage":"en","latestVersion":"4e632fdafd3b","latestPublishedVersion":"4e632fdafd3b","hasUnpublishedEdits":false,"latestRev":4674,"createdAt":1492065760544,"updatedAt":1492389083311,"acceptedAt":0,"firstPublishedAt":1492072051153,"latestPublishedAt":1492389083311,"vote":false,"experimentalCss":"","displayAuthor":"","content":{"subtitle":"The ultimate guide to machine learning’s favorite child.","bodyModel":{"paragraphs":[{"name":"be8e","type":4,"text":"Sequences upon sequences upon sequences. Sequen-ception.","markups":[],"layout":5,"metadata":{"id":"1*khIKl9t4XmZGSsKhW_Yg2w.png","originalWidth":2000,"originalHeight":907}},{"name":"84b9","type":3,"text":"Rohan & Lenny #3: Recurrent Neural Networks & LSTMs","markups":[]},{"name":"9485","type":13,"text":"The ultimate guide to machine learning’s favorite child.","markups":[]},{"name":"4569","type":7,"text":"This is the third group (Lenny and Rohan) entry in our journey to extend our knowledge of artificial intelligence and convey that knowledge in a simple, fun, and accessible manner. Learn more about our motives in this introduction post.","markups":[{"type":3,"start":25,"end":30,"anchorType":2,"userId":"de8e2540b759"},{"type":3,"start":35,"end":40,"anchorType":2,"userId":"cb55958ea3bb"},{"type":3,"start":55,"end":62,"href":"https://medium.com/a-year-of-artificial-intelligence","title":"","rel":"","anchorType":0},{"type":3,"start":218,"end":230,"href":"https://medium.com/a-year-of-artificial-intelligence/0-2016-is-the-year-i-venture-into-artificial-intelligence-d702d65eb919#.bfjoaqxu5","title":"","rel":"","anchorType":0}]},{"name":"a16a","type":1,"text":"It seems like most of our posts on this blog start with “We’re back!”, so… you know the drill. It’s been a while since our last post — just over 5 months — but it certainly doesn’t feel that way. Whether our articles are more spaced out than we’d like them to be, well, we haven’t actually discussed that yet. But I, Rohan, would definitely like to get into a more frequent routine. Since November, we’ve been grinding on school (basically, getting it over and done with), banging out Contra v2, and lazing around more than we should. End of senior year is a fun time.","markups":[{"type":3,"start":485,"end":491,"href":"http://getcontra.com","title":"","rel":"noopener","anchorType":0}],"hasDropCap":true},{"name":"ac01","type":4,"text":"","markups":[],"layout":4,"metadata":{"id":"1*N5l-Uj5yUVPIDwIuGUvnfg.jpeg","originalWidth":570,"originalHeight":389}},{"name":"93ae","type":1,"text":"It’s 2017. We started A Year Of AI in 2016. Last year. Don’t panic, though. If you’ve read our letter, you’ll know that, despite our name and inception date, we’re not going anywhere anytime soon. There’s a good chance we’ll move off Medium, but we’re still both obsessed with AI and writing these posts to hopefully make other people obsessed, as well.","markups":[{"type":3,"start":95,"end":101,"href":"https://ayearofai.com/the-goal-of-our-blog-c104d7b6377a#.75o5qyayi","title":"","rel":"noopener","anchorType":0},{"type":1,"start":24,"end":28},{"type":2,"start":24,"end":29}]},{"name":"f6d1","type":1,"text":"I wrote the first article on this blog just over a year ago, and mentioned that my goal for the year was to be accepted into Stanford University as an undergrad student. A few months ago, I achieved this goal. At Stanford, I’ll probably be studying Symbolic Systems, which is a program that explores both the humanities and STEM to inform an understanding of artificial intelligence and the nature of minds. Needless to say, A Year of AI will continue to document the new things I learn 😀.","markups":[{"type":3,"start":249,"end":265,"href":"https://symsys.stanford.edu/","title":"","rel":"noopener nofollow","anchorType":0}]},{"name":"b842","type":1,"text":"Anyways, you can find plenty of articles on recurrent neural networks (RNNs) online. My favorite one, personally, is from Andrej Karpathy’s blog. I read it about 1.5 years ago when I was learning about RNNs. We definitely think there’s space to simplify the topic even more, though. As usual, that’s our aim for the article — to teach you RNNs in a fun, simple manner. We’re also importantly doing this for completion purposes; we want people to hop onto A Year of AI and be able to work their way up all the way from logistic regression to neural machine translation (don’t worry, you’ll find out what means soon enough), and thus recurrent neural networks is a vital addition. After this, we want to look at and summarize/simplify a bunch of new super interesting research papers, and for most of them RNNs are a key ingredient. Finally, we think this article contains so much meat and ties together content unlike any other RNN tutorial on the interwebs.","markups":[{"type":3,"start":88,"end":100,"href":"http://karpathy.github.io/2015/05/21/rnn-effectiveness/","title":"","rel":"noopener","anchorType":0}]},{"name":"60e2","type":1,"text":"Before we get started, you should try to familiarize yourself with “vanilla” neural networks. If you need a refresher, check out our neural networks and backpropogation mega-post from earlier this year. This is so you know the basics of machine learning, linear algebra, neural network architecture, cost functions, optimization methods, training/test sets, activation functions/what they do, softmax, etc. Reading our article on convolutional neural networks may also make you more comfortable entering this post, especially because we often reference CNNs. Checking out this article I wrote on vanishing gradients will help later on, as well.","markups":[{"type":3,"start":133,"end":178,"href":"https://ayearofai.com/rohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d#.quwnoqtot","title":"","rel":"noopener","anchorType":0},{"type":3,"start":430,"end":459,"href":"https://ayearofai.com/rohan-lenny-2-convolutional-neural-networks-5f4cd480a60b#.ebwc0kf4z","title":"","rel":"noopener","anchorType":0},{"type":3,"start":572,"end":576,"href":"https://ayearofai.com/rohan-4-the-vanishing-gradient-problem-ec68f76ffb9b#.58nevwsoa","title":"","rel":"noopener","anchorType":0}]},{"name":"81ec","type":1,"text":"Rule of thumb: the more you know, the better!","markups":[]},{"name":"3288","type":3,"text":"Table of Contents","markups":[]},{"name":"003f","type":1,"text":"I can’t link to each section, but here’s what we cover in this article (save the intro and conclusion):","markups":[]},{"name":"8cfa","type":10,"text":"What can RNNs do? Where we look at… what RNNs can do!","markups":[{"type":1,"start":0,"end":18}]},{"name":"7f98","type":10,"text":"Why? Where we talk about the gap that RNNs fill in machine learning’s suite of algorithms.","markups":[{"type":1,"start":0,"end":5}]},{"name":"de84","type":10,"text":"Show me. Where we visualize RNNs for the first time.","markups":[{"type":1,"start":0,"end":9}]},{"name":"f2f4","type":10,"text":"Formalism. Where we walk through how an RNN mathematically works with proper notation.","markups":[{"type":1,"start":0,"end":11}]},{"name":"c163","type":10,"text":"An example? Okay! Where we walk through, qualitatively, a simple application of RNNs and how the RNN operates in this application, including techniques we can use.","markups":[{"type":1,"start":0,"end":18}]},{"name":"394e","type":10,"text":"Training (or, why vanilla RNNs suck.) Where we talk about how to train RNNs, and why vanilla RNNs are bad at learning.","markups":[{"type":1,"start":0,"end":38}]},{"name":"5756","type":10,"text":"Fixing the problem with LSTMs (Part I). Where we introduce the solution to vanilla RNNs’ inability to learn: LSTMs.","markups":[{"type":1,"start":0,"end":40}]},{"name":"654d","type":10,"text":"Fixing the problem with LSTMs (Part II). Where we analyze on a close, technical level, the reasons LSTMs don’t suffer from vanishing gradients as much (and why they still do, to an extent). Then we conclude LSTMs with final thoughts on and facts about them.","markups":[{"type":1,"start":0,"end":41}]},{"name":"2dfc","type":10,"text":"Yay RNNs! Where you get to see neat little things RNNs have done!","markups":[{"type":1,"start":0,"end":10},{"type":2,"start":16,"end":20}]},{"name":"e56c","type":10,"text":"In Practice. Where we look at more technical and important applications and case studies of RNNs, including other variations of RNNs, especially as relevant in hot/recent research papers.","markups":[{"type":1,"start":0,"end":13}]},{"name":"7bc0","type":10,"text":"Building a Vanilla Recurrent Neural Network. Where you get to code your very first RNN! Woohoo!","markups":[{"type":1,"start":0,"end":45}]},{"name":"02a8","type":3,"text":"What can RNNs do?","markups":[]},{"name":"1c1d","type":1,"text":"There are a number of very important tasks that ANNs and CNNs cannot solve, that RNNs are used for instead. Tasks like: image captioning, language translation, sentiment classification, predictive typing, video classification, natural language processing, speech recognition, and a lot more interesting things that have been presented in recent research papers (for example… learning to learn by gradient descent by gradient descent!).","markups":[{"type":3,"start":375,"end":432,"href":"https://arxiv.org/pdf/1606.04474.pdf","title":"","rel":"noopener","anchorType":0},{"type":2,"start":282,"end":286}]},{"name":"1d0f","type":4,"text":"Image captioning, taken from CS231n slides: http://cs231n.stanford.edu/slides/2016/winter1516_lecture10.pdf","markups":[{"type":3,"start":44,"end":107,"href":"http://cs231n.stanford.edu/slides/2016/winter1516_lecture10.pdf","title":"","rel":"nofollow noopener noopener","anchorType":0}],"layout":1,"metadata":{"id":"1*X5dk-xGw2yNYsEB3QvHWIA.png","originalWidth":713,"originalHeight":400}},{"name":"02ef","type":1,"text":"RNNs are very powerful. Y’know how regular neural networks have been proved to be “universal function approximators” ? If you didn’t:","markups":[]},{"name":"8206","type":6,"text":"In the mathematical theory of artificial neural networks, the universal approximation theorem states that a feed-forward network with a single hidden layer containing a finite number of neurons can approximate continuous functions on compact subsets of R^n, under mild assumptions on the activation function.","markups":[]},{"name":"314e","type":1,"text":"That’s pretty confusing. Basically, what this states is that an artificial neural network can compute any function. Even if someone gives you an extremely wiggly, complex looking function, it’s guaranteed that there exists a neural network that can produce (or at least extremely closely approximate) it. The proof itself is very complex, but this is a brilliant article offering a visual approach as to why it’s true.","markups":[{"type":3,"start":343,"end":347,"href":"http://neuralnetworksanddeeplearning.com/chap4.html","title":"","rel":"noopener","anchorType":0},{"type":2,"start":407,"end":408}]},{"name":"a351","type":1,"text":"So, that’s great. ANNs are universal function approximators. RNNs take it a step further, though; they can compute/describe programs. In fact, some RNNs with proper weights and architecture qualify as Turing Complete:","markups":[{"type":3,"start":98,"end":132,"href":"http://stats.stackexchange.com/a/221142/98975","title":"","rel":"noopener","anchorType":0},{"type":2,"start":124,"end":132}]},{"name":"8f10","type":6,"text":"A Turing Complete system means a system in which a program can be written that will find an answer (although with no guarantees regarding runtime or memory).","markups":[]},{"name":"c6a2","type":6,"text":"So, if somebody says “my new thing is Turing Complete” that means in principle (although often not in practice) it could be used to solve any computation problem.","markups":[{"type":1,"start":89,"end":110}]},{"name":"1429","type":6,"text":"— http://stackoverflow.com/a/7320/1260708","markups":[{"type":3,"start":2,"end":41,"href":"http://stackoverflow.com/a/7320/1260708","title":"","rel":"nofollow noopener","anchorType":0}]},{"name":"2ba5","type":1,"text":"That’s cool, isn’t it? Now, this is all theoretical, and in practice means less than you think, so don’t get too hyped. Hopefully, though, this gives some more insight into why RNNs are super important for future developments in machine learning — and why you should read on.","markups":[]},{"name":"662c","type":1,"text":"At this point, if you weren’t previously hooked on learning what the heck these things are, you should be now. (If you still aren’t, just bare with me. Things will get spicy soon.) So, let’s dive in.","markups":[]},{"name":"72f6","type":3,"text":"Why?","markups":[]},{"name":"54dc","type":1,"text":"We took a bit of a detour to talk about how great RNNs are, but haven’t focused on why ANNs can’t perform well in the tasks that RNNs can.","markups":[{"type":2,"start":83,"end":86}]},{"name":"b88b","type":6,"text":"Why do we need another neural network model? Why do we need recurrent neural networks when we already have the beloved ANNs (and CNNs) in all their glory?","markups":[]},{"name":"95c9","type":1,"text":"It boils down to a few things:","markups":[]},{"name":"3d40","type":9,"text":"ANNs can’t deal with sequential or “temporal” data","markups":[]},{"name":"2a22","type":9,"text":"ANNs lack memory","markups":[]},{"name":"0d57","type":9,"text":"ANNs have a fixed architecture","markups":[]},{"name":"fbb9","type":9,"text":"RNNs are more “biologically realistic” because of the recurrent connectivity found in the visual cortex of the brain","markups":[]},{"name":"341a","type":1,"text":"Let’s address the first three points individually. The first issue refers to the fact that ANNs have a fixed input size and a fixed output size. ANNs have an elaborate list of hyperparameters, and this notably includes the number of neurons in the input layer and output layer. But what if we wanted input data and/or output data of variable size, instead of something that needs to have its size as a preset constant? RNNs allow us to do that. In this aspect, they offer more flexibility than ANNs.","markups":[{"type":2,"start":103,"end":120},{"type":2,"start":126,"end":143},{"type":2,"start":333,"end":341}]},{"name":"605a","type":4,"text":"We might choose this architecture for our ANN, with 4 inputs and 1 output. But that’s it — we can’t input a vector with 5 values, for example. https://qph.ec.quoracdn.net/main-qimg-050d12c5da82f4f97fdd942d7777b8e4.","markups":[{"type":3,"start":143,"end":213,"href":"https://qph.ec.quoracdn.net/main-qimg-050d12c5da82f4f97fdd942d7777b8e4","title":"","rel":"nofollow noopener","anchorType":0}],"layout":1,"metadata":{"id":"1*BQ0SxdqC9Pl_3ZQtd3e45A.png","originalWidth":500,"originalHeight":309}},{"name":"1c8e","type":1,"text":"I’ll give you a couple examples of why this matters.","markups":[]},{"name":"e228","type":1,"text":"It’s unclear how we could use an ANN by itself to perform a task like image captioning, because the network would need to output a sentence — a list of words in a specific order — which is a sequence. It would be a sequence of vectors, because each word would need to be represented numerically. In machine learning and data science, we represent words numerically as vectors; these are called word embeddings. An ANN can only output a single word/label, like in image classification where we treat the output as the label with the highest value in the final vector that is a softmax probability distribution over all classes. The only way to make sentences work with ANNs would be to have billions of output neurons that each map to a single possible sentence in the permutation of all [sensible] sentences that can be formed by the vocabulary we have. And that doesn’t sound like a good idea.","markups":[{"type":2,"start":190,"end":199},{"type":2,"start":200,"end":201},{"type":2,"start":436,"end":443}]},{"name":"8f28","type":4,"text":"A reminder of what the output of an ANN looks like — a probability distribution over classes — and how we convert that into a single final result (one-hot encoding): by taking the label with the greatest probability and making it 1, with the rest 0.","markups":[],"layout":1,"metadata":{"id":"1*GFVoFpD6cdCY_PGqnjhOlQ.png","originalWidth":305,"originalHeight":182}},{"name":"f8ec","type":1,"text":"Wow, that was a lot of words. Nevertheless, I hope it’s clear that, with ANNs, there’s no feasible way to output a sequence.","markups":[]},{"name":"f308","type":1,"text":"Now, what about inputting a sequence into an ANN? In other words, “temporal” data: data that varies over time, and is thus a sequence. Take the example of sentiment classification where we input a sentence (sequence of words = sequence of vectors = sequence of set of values where each value goes into an individual neuron) and want to output its sentiment: positive or negative. The output part seems easy, because it’s just one neuron that’s either rounded to 1 (positive) or 0 (negative). And, for the input, you might be thinking: couldn’t we input each “set of values” separately? Input the first word, wait for the neural net to fully feed forward and produce an output, then input the next word, etc. etc.","markups":[{"type":2,"start":16,"end":26}]},{"name":"8029","type":1,"text":"Let’s take the case of this utterly false, and most certainly negative sentence, to evaluate:","markups":[]},{"name":"6e25","type":4,"text":"This is just an alternative fact, believe me! Lenny is actually a great coder. The best I know of. The best.","markups":[{"type":1,"start":71,"end":72},{"type":2,"start":66,"end":108}],"layout":1,"metadata":{"id":"1*yq_zmka1ssikrmD9GkWmnw.png","originalWidth":625,"originalHeight":139}},{"name":"322d","type":1,"text":"We’d input “Lenny” first, then “Khazan”, then “is”, etc. But, at each feedforward iteration, the output would be completely useless. Why? Because the output would be dependent on only that word. We’d be finding the sentiment of a single word, which is useless, because we want the sentiment of the entire sentence. Sentiment analysis only makes sense when all the words come together, dependent on each other, to form a sentence.","markups":[{"type":2,"start":179,"end":184}]},{"name":"13ff","type":1,"text":"Think of it this way — this means you’re essentially running a neural network a bunch of times, just with new data at each separate iteration. Those run-throughs aren’t linked in any way; they’re independent. Once you feedforward and fully run the neural network, it forgets everything it just did. This sentence only makes sense and can only be interpretable because it’s a collection of words put together in a specific order to form meaning. The relevance of each word is dependent on the words that precede it: the context. This is why RNNs are being used heavily in NLP; they retain context by having memory. ANNs have no memory.","markups":[{"type":2,"start":436,"end":443},{"type":2,"start":606,"end":612}]},{"name":"e71f","type":1,"text":"I like this quote from another article on RNNs:","markups":[]},{"name":"9cd3","type":6,"text":"Humans don’t start their thinking from scratch every second. As you read this essay, you understand each word based on your understanding of previous words. You don’t throw everything away and start thinking from scratch again. Your thoughts have persistence.","markups":[]},{"name":"b760","type":6,"text":"— http://colah.github.io/posts/2015–08-Understanding-LSTMs/","markups":[{"type":3,"start":2,"end":59,"href":"http://colah.github.io/posts/2015-08-Understanding-LSTMs/","title":"","rel":"nofollow noopener","anchorType":0}]},{"name":"e0e4","type":1,"text":"(Furthermore, take the case where we had sequential data in both the input and the output. Translating one language to another is a good example of this. Clearly, ANNs aren’t the answer.)","markups":[{"type":2,"start":60,"end":65}]},{"name":"fa2f","type":1,"text":"RNNs don’t just need memory; they need long term memory. Let’s take the example of predictive typing. Let’s say we typed the following sentence in an SMS message to 911, and the operating system needs to fill in the blank:","markups":[{"type":2,"start":39,"end":48}]},{"name":"2c67","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*TugitPvwm_IZqAdAPR-7UA.png","originalWidth":543,"originalHeight":90}},{"name":"3077","type":4,"text":"The face of a criminal?","markups":[],"layout":1,"metadata":{"id":"1*7l0aNgpXDXZnY-P9C8K4IA.jpeg","originalWidth":250,"originalHeight":250}},{"name":"325c","type":1,"text":"Here, if the RNN wasn’t able to look back much (ie. before “should”), then many different options could arise:","markups":[]},{"name":"da28","type":4,"text":"Lenny in the military? Make it into a TV show! I’d watch it.","markups":[],"layout":1,"metadata":{"id":"1*aMJu60wscb9m4A-Zn_xyqQ.png","originalWidth":552,"originalHeight":266}},{"name":"930e","type":1,"text":"The word “sent” would indicate to the RNN that a location needs to be outputted. However, if the RNN was able to retain information from all the way back, such as the word “criminal”, then it would be much more confident that:","markups":[]},{"name":"0990","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*4CZskdiGqIx29BQylqnYuA.png","originalWidth":557,"originalHeight":102}},{"name":"c9f1","type":1,"text":"The probability of outputting “jail” drastically increases when it sees the word “criminal” is present. That’s why context matters, be it predictive typing, image captioning, machine translation, etc. The output or outputs of a recurrent neural network will always be functionally dependent on (meaning, a function of) information from the very beginning, but how much it chooses to “forget” or “retain” (that is, varying degrees of influence from earlier information) depends on the weights that it learns from the training data.","markups":[]},{"name":"eb76","type":1,"text":"As it turns out, RNNs — especially deep ones — are rarely good at retaining much information, due to an issue called the vanishing gradient problem. That’s where we turn to other variants of RNNs such as LSTMs and GRUs. But, more on that later.","markups":[]},{"name":"c00c","type":1,"text":"To address the third point, one more constraint with ANNs is that they have a fixed number of computation/processing steps (because, once again, the number of hidden layers is a hyperparameter). With RNNs, we can have much more dynamic processing since we operate over vectors. Each neuron in an RNN is almost like an entire layer in an ANN; this will make more sense as we bring up an illustration for you. Exciting stuff.","markups":[{"type":1,"start":408,"end":423}]},{"name":"4abf","type":3,"text":"Show me.","markups":[]},{"name":"0160","type":1,"text":"OK, that’s enough teasing. Three sections into the article, and you’re yet to see what an RNN looks like, or appreciate how it really works. Everything comes in due time, though!","markups":[]},{"name":"d391","type":1,"text":"The first thing I’m going to do is show you what a normal ANN diagram looks like:","markups":[]},{"name":"ce30","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*GapzcZDrwnVbflhlRoWZ9g.png","originalWidth":310,"originalHeight":220}},{"name":"deda","type":1,"text":"Each neuron stores a single scalar value. Thus, each layer can be considered a vector.","markups":[]},{"name":"f077","type":1,"text":"Now I’m going to show you what this ANN looks like in our RNN visual notation:","markups":[]},{"name":"c5e9","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*ntKLnv52DCUnkseNcm91iQ.png","originalWidth":300,"originalHeight":65}},{"name":"f1fc","type":1,"text":"The two diagrams above represent the same thing. The latter, obviously, looks more succinct than the former. That’s because, with our RNN visual notation, each neuron (inputs, hidden(s), and outputs) contains a vector of information. The term “cell” is also used, and is interchangeable with neuron. (I’ll use the latter instead of the former.) Red is the input neuron, blue is the hidden neuron, and green is the output neuron. Therefore, an entire ANN layer is encapsulated into one neuron with our RNN illustration. All operations in RNNs, like the mapping from one neuron’s state to another, are over entire vectors, compared to individual scalars that are summed up with ANNs.","markups":[{"type":1,"start":211,"end":218}]},{"name":"abee","type":1,"text":"Let’s flip it the other way:","markups":[]},{"name":"535f","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*HewvhrMCcdy-oQcpeeNJ9w.png","originalWidth":50,"originalHeight":231}},{"name":"8a15","type":1,"text":"This is in fact a type of recurrent neural network — a one to one recurrent net, because it maps one input to one output. A one to one recurrent net is equivalent to an artificial neural net.","markups":[{"type":1,"start":55,"end":65}]},{"name":"b960","type":1,"text":"We can have a one to many recurrent net, where one input is mapped to multiple outputs. An example of this would be image captioning — the input would be the image in some processed form (usually the result of a CNN analyzing the image), and the output would be a sequence of words. Such an RNN may look like this:","markups":[{"type":2,"start":21,"end":25}]},{"name":"7fff","type":4,"text":"Changed the shades of the green nodes… hope that’s OK!","markups":[],"layout":1,"metadata":{"id":"1*-Jv3TxauJBwBgWwjoe_UkA.png","originalWidth":190,"originalHeight":230}},{"name":"ea3c","type":1,"text":"This may be confusing at first, so I’m going to make sure I walk slowly through it. On the x-axis we have time, and on the y-axis we have depth/layers:","markups":[]},{"name":"c4eb","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*OEyIsiEi5SJ9l3GrB5DpuA.png","originalWidth":323,"originalHeight":287}},{"name":"22b2","type":1,"text":"When I refer to “time” on the x-axis, I’m referring to the order at which these operations occur. Time could also be literal for temporal data, where the input is a sequence. When I say “depth” on the y-axis, I’m referring to the mapping from the input layer, to the hidden layer(s), to the output layer, where layer number and thus depth increases.","markups":[]},{"name":"f94b","type":1,"text":"It may look like we have seven neurons now, but we still have three: one input neuron, one hidden neuron, and one output neuron. The difference is that these neurons now experience multiple “timesteps” where they take on different values, which are, again, vectors. The input neuron in our example above doesn’t, because it’s not representing sequential data (one to many), but for other architectures it could.","markups":[]},{"name":"e200","type":1,"text":"The hidden neuron will take on the vector value h_1 first, then h_2, and finally h_3. At each timestep, the hidden neuron’s vector h_t is a function of the vector at the previous timestep h_t-1, except for h_1 which is dependent only on the input x_1. In the diagram above, each hidden vector then gives rise to an output y_t, and this is how we map one input to multiple outputs. You can visualize these functional dependencies with the arrows, which illustrates flow of information in the network.","markups":[{"type":1,"start":48,"end":51},{"type":1,"start":64,"end":67},{"type":1,"start":81,"end":84},{"type":1,"start":131,"end":135},{"type":1,"start":188,"end":193},{"type":1,"start":206,"end":210},{"type":1,"start":247,"end":250},{"type":1,"start":322,"end":325},{"type":2,"start":229,"end":233}]},{"name":"64eb","type":1,"text":"As we progress on the x-axis, the current timestep increases. As we progress on the y-axis, the neuron in question changes. Each point on this graph thus represents one neuron — be it input, hidden, or output — at some timestep, being fed information from a neuron (be it itself or another) at the previous timestep.","markups":[]},{"name":"5402","type":1,"text":"The RNN would execute like so:","markups":[]},{"name":"3623","type":10,"text":"Input x_1","markups":[{"type":1,"start":6,"end":9}]},{"name":"1923","type":10,"text":"Compute h_1 based on x_1 (the arrow implies functional dependency)","markups":[{"type":1,"start":8,"end":11},{"type":1,"start":21,"end":24}]},{"name":"00ff","type":10,"text":"Compute h_2 based on h_1","markups":[{"type":1,"start":8,"end":11},{"type":1,"start":21,"end":24}]},{"name":"ca4c","type":10,"text":"Compute h_3 based on h_2","markups":[{"type":1,"start":8,"end":11},{"type":1,"start":21,"end":24}]},{"name":"9fac","type":10,"text":"Compute y_1 based on h_1","markups":[{"type":1,"start":8,"end":11},{"type":1,"start":21,"end":24}]},{"name":"649b","type":10,"text":"Compute y_2 based on h_2","markups":[{"type":1,"start":8,"end":11},{"type":1,"start":21,"end":24}]},{"name":"ba63","type":10,"text":"Compute y_3 based on h_3","markups":[{"type":1,"start":8,"end":11},{"type":1,"start":21,"end":24}]},{"name":"bba5","type":1,"text":"You could compute y_t either immediately after h_t has been computed, or, like above, compute all outputs once all hidden states have been computed. I’m not entirely sure which is more common in practice.","markups":[{"type":1,"start":18,"end":22},{"type":1,"start":47,"end":51}]},{"name":"6c68","type":1,"text":"This allows for more complex and interesting networks than ANNs because we can have as many timesteps as we want.","markups":[]},{"name":"e78f","type":1,"text":"The value of the output neuron at each timestep represents a word in the sentence, in the order the sentence will be constructed. The caption this RNN produces is hence 3 words long. (It’s actually 2, because the RNN would need to output a period or \x3cEND\x3e marker at the final timestep, but we’ll get into that later.)","markups":[]},{"name":"a009","type":1,"text":"In case you don’t understand yet exactly why RNNs work, I’ll walk through how these functional dependencies come to fruition when you apply it to a one to many scenario such as image captioning.","markups":[{"type":2,"start":41,"end":45}]},{"name":"2856","type":4,"text":"Lenny and I on student scholarship at WWDC 2013. Good times!","markups":[],"layout":1,"metadata":{"id":"1*iBtLegQFwfsqWZpTVAjrEw.jpeg","originalWidth":1298,"originalHeight":782}},{"name":"6b74","type":1,"text":"When you combine an RNN and CNN, you — in practice — get an “LCRN”. The architecture for LCRNs are more complex than what I’m going to present in the next paragraph; rather, I’m going to simplify it to convey my point. We’ll actually get fully into how they work later.","markups":[]},{"name":"e029","type":1,"text":"Imagine an RNN tries to caption this image. An accurate result might be:","markups":[]},{"name":"f14e","type":7,"text":"Two people happily posing for a photo inside a building.","markups":[]},{"name":"482a","type":1,"text":"The input to the RNN would be the output of a CNN that processes this image. (However, to be pedantic, it would be the output of the CNN without a classification/softmax layer — that is, pulled from the final fully connected layer.) The CNN might pick up on the fact that there are two primary human face-like objects present in the image, which, paired with what the RNN has learned via training, may induce the first hidden state¹ of the recurrent neural network to be one where the most likely candidate word is “two”.","markups":[{"type":1,"start":431,"end":432}]},{"name":"f5ec","type":1,"text":"Pro-tip¹: The term “hidden state” refers to the vector of a hidden neuron at a given timestep. “First hidden state” refers to the hidden state at timestep 1.","markups":[{"type":1,"start":7,"end":8}]},{"name":"10db","type":1,"text":"The first output, which represents the word “two”, was functionally dependent on the first hidden state, which in itself was a function of the input to the RNN. Thus, “two” was ultimately determined from the information that the CNN gave us and the experience/weights of the RNN. Now, the second word, “people”, is functionally dependent on the second hidden state. However, note that the second hidden state is just a function of the first hidden state. This means that the word “people” was the most likely candidate given the hidden state where “two” was likely. In other words, the RNN recognized that, given the word “two”, the word “people” should be next, based on the RNN’s experience from training and the initial image [analysis] we inputted.","markups":[{"type":2,"start":435,"end":441}]},{"name":"b004","type":1,"text":"The same will occur for every following word; the nth word will be based on the nth hidden state, which, ultimately, is a function of every hidden state before it, and thus could be interpreted purely as an extremely complex and layered function of the input. The weights do the heavy lifting by making sense of all this information and deducing an output from it.","markups":[]},{"name":"d51d","type":1,"text":"To put it bluntly, you can boil down what the RNN is “thinking” to this:","markups":[]},{"name":"4a94","type":6,"text":"Based on what I’ve seen from the input, based on the current timestep I’m at, and based on what I know from all my training, I need to output: “x”.","markups":[{"type":1,"start":143,"end":147}]},{"name":"a918","type":1,"text":"Thus, each outputted word is dependent on the words before it, all the way back to the input image data. However, this relationship is indirect. It’s indirect because the outputs are only dependent on the hidden states, not on each other (ie. the RNN doesn’t deduce “people” from “two”, it deduces “people”, partly, from the information — the hidden state — that gave rise to “two”). In LCRNs, though, this is explicit instead of implicit; we “sample” the output of one timestep by taking it and literally feeding it back as input into the next timestep. In a sense, LCRNs can hence be interpreted as having many to many architecture.","markups":[{"type":2,"start":220,"end":224},{"type":2,"start":363,"end":372}]},{"name":"cdc3","type":1,"text":"The exact quantitative relationships depend on the RNN’s weights. But, generally, this is the concept of memory in play. Creating a coherent sentence as we go along is only really possible if we can recall what we said before. And RNNs are able to do exactly that; they remember what they said before and figure out, based on their image captioning expertise, what from this is useful to continue accurately speaking.","markups":[]},{"name":"0d89","type":6,"text":"Yep, I went to France for a holiday. And I actually learned to speak some \x3cwait, shit, what was the language again? oh yea, “France”…\x3e French!","markups":[]},{"name":"5857","type":1,"text":"Obviously, an RNN needs to be trained and have proper weights for this to all function properly. RNNs aren’t magic; they only work because trained networks identified and learned patterns in data during training time that they now look for during prediction.","markups":[]},{"name":"34be","type":1,"text":"Perhaps this was a bit over-explaining on my part, but hopefully I nailed down some important and core ideas about how RNNs function.","markups":[]},{"name":"05f6","type":1,"text":"So far we’ve looked at one to one and one to many recurrent networks. We can also have many to one:","markups":[{"type":2,"start":87,"end":98}]},{"name":"6c8d","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*KjYQyc-JD_zs5ERQypM9EA.png","originalWidth":190,"originalHeight":228}},{"name":"da0a","type":1,"text":"With many to one (and many to many), the input is in the form of a sequence, and so the hidden states are functionally dependent on both the input at that timestep and the previous hidden state. This is different to one to many, where the hidden state after h_1 is only dependent on the previous hidden state. That’s why, in the image above, the second hidden state has two arrows directed at it.","markups":[{"type":1,"start":258,"end":262},{"type":2,"start":132,"end":137}]},{"name":"4715","type":1,"text":"Only one output exists in many to one architecture. An example application is sentiment classification, where the input is a sentence (sequence of words) and the output is a probability indicating that the inputted sentence was positive.","markups":[]},{"name":"a55f","type":1,"text":"The final type of recurrent net is many to many, where both the input and output are sequential:","markups":[]},{"name":"86b5","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*MPpLCBI1J6r6VmsDm7G4_g.png","originalWidth":310,"originalHeight":214}},{"name":"7038","type":1,"text":"A use case would be machine translation where a sequence of words in one language needs to be translated to a sequence of words in another.","markups":[]},{"name":"2538","type":1,"text":"We can also go deeper and have multiple hidden layers, and/or a greater number of timesteps:","markups":[]},{"name":"7896","type":4,"text":"We’re getting deeper and deeper!","markups":[],"layout":1,"metadata":{"id":"1*vfUWsAgW-c5hUntaPhb1aQ.png","originalWidth":313,"originalHeight":305}},{"name":"2703","type":1,"text":"Really, this could be considered as multiple RNNs. Technically, you can consider each “hidden layer” as an RNN itself, given each neuron operates on vectors and updates through time; in ANN context, that volume of operations would be considered an entire network. So this is like stacking RNNs on top of each other. However, in this article I’ll refer to it as multiple hidden layers; different papers and lecturers may take different approaches.","markups":[{"type":2,"start":36,"end":49}]},{"name":"a0dc","type":1,"text":"When we have many timesteps (usually hundreds) and multiple hidden layers, the architecture of the network becomes much more complex and interesting. One feature of this RNN, in particular, is that all the outputs, including the first, depend on not just the input up to that timestep, but all of the inputs. (You can see this because the green neuron is only introduced after the final input timestep.) If this RNN was to translate English to Chinese, the first word of translated Chinese isn’t just dependent on the first word of the inputted English; it’s dependent on the entire sentence.","markups":[{"type":2,"start":371,"end":376},{"type":2,"start":576,"end":583}]},{"name":"98d9","type":1,"text":"One way to demonstrate why this matters is to use Google Translate:","markups":[]},{"name":"af3c","type":4,"text":"One of my favorite Green Day lyrics, from the song “Fashion Victim” on WARNING:. Side-note: Based on my experience with Google Translate in Chinese class over the last 8 years, this translation is probably off.","markups":[{"type":1,"start":19,"end":28},{"type":2,"start":197,"end":206}],"layout":1,"metadata":{"id":"1*mptNrzbgaDT3YuQL-tpEOw.png","originalWidth":680,"originalHeight":145}},{"name":"3229","type":1,"text":"Now I’ll input “He’s a victim” and “of his own time” separately. You’ll notice that when you join the two translated outputs, this won’t be equal to the corresponding phrase in the first translation:","markups":[]},{"name":"aef0","type":4,"text":"What happens if we break up the English into different parts, translate, and join together the translated Chinese parts?","markups":[],"layout":1,"metadata":{"id":"1*lO5oCsSXTy4Loic3SASMlw.png","originalWidth":276,"originalHeight":245}},{"name":"44aa","type":4,"text":"They’re not equal.","markups":[{"type":1,"start":0,"end":18}],"layout":1,"metadata":{"id":"1*oJRFlstyAI-MkBguW6otfA.png","originalWidth":551,"originalHeight":110}},{"name":"a661","type":1,"text":"What gives? Well, the way sentences are constructed in languages can differ in varying scenarios. Some words in English may also map to multiple different words in Chinese, depending on how it’s used. It all depends on the context and the entire sentence as a whole — the meaning you’re trying to convey. This is the exact approach a human translator would take.","markups":[{"type":1,"start":223,"end":231}]},{"name":"fad3","type":1,"text":"Another type of many to many architecture exists where each neuron has a state at every timestep, in a “synchronized” fashion. Here, each output is only dependent on the inputs that were fed in during or before it. Because of this, synchronized many to many probably wouldn’t be suitable for translation.","markups":[]},{"name":"5a39","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*84IkP_dqLUfImZ5SyZLwjA.png","originalWidth":190,"originalHeight":231}},{"name":"cef1","type":1,"text":"An application for this could be video classification where each frame needs to be mapped to some sort of class or label. Interesting note — an RNN is better at this task than CNNs are because what’s going on in a scene is much easier to understand if you’ve watched the video up to that point and thus can contextualize it. That’s what humans do!","markups":[{"type":1,"start":325,"end":347}]},{"name":"f0a6","type":1,"text":"Quick note: we can “wrap” the RNN into a much more succinct form, where we collapse the depth and time properties, like so:","markups":[]},{"name":"e5dd","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*7POP9GXAsRlbRRrsrhr-jA.png","originalWidth":190,"originalHeight":250}},{"name":"fc78","type":1,"text":"This notation demonstrates that RNNs take input, process that input through multiple timesteps and hidden layers, and produce output. The arrow both leaving and entering the RNN conveys that an RNN hidden state is functionally dependent on the hidden state at the preceding timestep; it’s sort of like a loop that feeds itself.","markups":[]},{"name":"3ba7","type":1,"text":"When you ever read about “unrolling” an RNN into a feedforward network that looks like it’s in the same collapsed format as the diagram above, this means we expand it to show all timesteps and hidden layers like we did before.","markups":[]},{"name":"0d76","type":1,"text":"Another quick note: when somebody or a research paper mentions that they are using “512 RNN units”, this translates to: “1 RNN neuron that outputs a 512-wide vector”; that is, a vector with 512 values. At first, I thought this meant that maybe at each timestep there were 512 separate neurons somehow working in conjunction, but nope, it’s luckily much simpler than that… albeit strangely worded.","markups":[]},{"name":"2ae1","type":1,"text":"Furthermore, one “RNN unit” usually refers to an RNN with one hidden layer; thus, instead of defining RNN as something that is multilayer inherently, we often see people use the phrase like: “stacking RNNs on top of each other”. Each RNN will have its on weights, but connecting them gives rise to an overarching multilayer RNN. In this article, we treat recurrent neural networks as a model that can have variable timesteps t and fixed layers ℓ, just make sure you understand that this is not always the case. Our formalism, especially for weights, will slightly differ.","markups":[{"type":1,"start":425,"end":427},{"type":1,"start":444,"end":445}]},{"name":"e5b5","type":3,"text":"Formalism","markups":[]},{"name":"a93d","type":1,"text":"So, now, let’s walk through the formal mathematical notation involved in RNNs.","markups":[]},{"name":"ad6a","type":1,"text":"If an input or output neuron has a value at timestep t, we denote the vector as:","markups":[{"type":1,"start":53,"end":54}]},{"name":"a4f8","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*_D9bOLepOSSbC2zgK7wreQ.png","originalWidth":141,"originalHeight":77}},{"name":"a6fd","type":1,"text":"For the hidden neurons it’s a bit different; since we can have multiple hidden layers, we denote the hidden state vector at timestep t and hidden layer ℓ as:","markups":[{"type":1,"start":133,"end":135},{"type":1,"start":152,"end":154}]},{"name":"7987","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*QJFWCdOVxGAge0ZT17hw1g.png","originalWidth":157,"originalHeight":41}},{"name":"7ba4","type":1,"text":"The input is obviously some preset values that we know. The outputs and hidden states are not; they are calculated.","markups":[{"type":2,"start":89,"end":93}]},{"name":"982f","type":1,"text":"Let’s start with hidden states. First, we’ll revisit the most complex recurrent net we came across earlier — the many to many architecture:","markups":[]},{"name":"44bd","type":4,"text":"Many to many, non-synchronized.","markups":[],"layout":1,"metadata":{"id":"1*TJxcM6GI8dMHEq6sK3Ky8Q.png","originalWidth":250,"originalHeight":206}},{"name":"5f3c","type":1,"text":"This RNN has: sequential input, sequential output, multiple timesteps, and multiple hidden layers. The formula we derive for this RNN should generalize for all others.","markups":[]},{"name":"af44","type":1,"text":"First, let’s list out the possible functional dependencies for a given hidden state, based on the arrows and flow of information in the diagram:","markups":[]},{"name":"5021","type":9,"text":"An input","markups":[]},{"name":"8970","type":9,"text":"Hidden state at the previous timestep, same layer","markups":[]},{"name":"429d","type":9,"text":"Hidden state at the current timestep, previous layer","markups":[]},{"name":"dc21","type":1,"text":"A hidden state can have two functional dependencies at max. Just by looking at the diagram, the only impossible combination is to be dependent on both the input and a hidden state at the current timestep but previous layer. This is because the only hidden states that are dependent on input exist in the first hidden layer, where no such previous layer exists.","markups":[]},{"name":"b721","type":1,"text":"If this is all difficult to follow, make sure once again to look at and trace back the arrows in the RNN that illustrate flow of information throughout the network.","markups":[]},{"name":"5a0a","type":1,"text":"Because of the impossible combination, we define two separate equations: an equation for the hidden state at hidden layer 1, and for layers after 1.","markups":[]},{"name":"8095","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*n5QR9Q9ZGnWFRf7pROtliA.png","originalWidth":400,"originalHeight":107}},{"name":"7d12","type":1,"text":"This probably looks a bit confusing; let me break it down for you. The function ƒw computes the numeric hidden state vector for timestep t and layer ℓ; it contains the “activation function” you’re used to hearing about with ANNs. W are the weights of the recurrent net, and thus ƒ is conditioned on W. We haven’t exactly defined ƒ just yet, but what’s important to note is the two parameters it takes. Once you do, this notation simply states what we have stated before in plain English:","markups":[{"type":1,"start":80,"end":83},{"type":1,"start":137,"end":139},{"type":1,"start":149,"end":150},{"type":1,"start":230,"end":232},{"type":1,"start":279,"end":281},{"type":1,"start":299,"end":300},{"type":1,"start":329,"end":330}]},{"name":"a42e","type":6,"text":"Where ℓ = 1, the hidden state at time t and layer ℓ is a function of the hidden state vector at time t-1 and layer ℓ as well as the input vector at time t. Where ℓ \x3e 1, this hidden state is a function of the hidden state vector at time t-1 and layer ℓ as well as the hidden state vector at time t, layer ℓ-1.","markups":[{"type":1,"start":6,"end":8},{"type":1,"start":38,"end":40},{"type":1,"start":50,"end":52},{"type":1,"start":101,"end":105},{"type":1,"start":115,"end":117},{"type":1,"start":153,"end":154},{"type":1,"start":162,"end":167},{"type":1,"start":236,"end":240},{"type":1,"start":250,"end":252},{"type":1,"start":295,"end":296},{"type":1,"start":304,"end":307}]},{"name":"7713","type":1,"text":"You might notice that we have a couple issues:","markups":[]},{"name":"69f7","type":9,"text":"When t = 1 — that is, when each neuron is at the initial timestep — then no previous timestep exists. However, we still attempt to pass h_0 as a parameter to ƒw.","markups":[{"type":1,"start":5,"end":10},{"type":1,"start":136,"end":140},{"type":1,"start":158,"end":160}]},{"name":"7b09","type":9,"text":"If no input exists at time t — thus, x_t does not exist — then we still attempt to pass x_t as a parameter.","markups":[{"type":1,"start":27,"end":28},{"type":1,"start":37,"end":41},{"type":1,"start":88,"end":92}]},{"name":"2062","type":1,"text":"Our respective solutions follow:","markups":[]},{"name":"c5c1","type":9,"text":"Define h_0 for any layer as 0","markups":[{"type":1,"start":7,"end":11}]},{"name":"68f6","type":9,"text":"Consider x_t where no input exists at timestep t as 0","markups":[{"type":1,"start":9,"end":13},{"type":1,"start":47,"end":48}]},{"name":"0ed3","type":1,"text":"If these are 0, then the invalid functional dependency stops existing, and our formal notation still holds up.","markups":[]},{"name":"cf77","type":1,"text":"We actually have five different types of weight matrices:","markups":[]},{"name":"61fa","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*r09EQtFlEA1kIiOJD2aZ6g.png","originalWidth":390,"originalHeight":233}},{"name":"b66b","type":1,"text":"Pro-tip: The indices for each weight matrix tell you what they are used for in the recurrent net. W_xh maps an input vector x to a hidden state vector h. W_hht maps a hidden state vector h to another hidden state vector h along the time axis, ie. from h_t-1 to h_t. On the other hand, W_hhd maps a hidden state vector h to another hidden state vector h along the depth axis, ie. from h^(ℓ-1)_t to h^ℓ_t. W_hy maps a hidden state vector h to an output vector y.","markups":[{"type":1,"start":98,"end":103},{"type":1,"start":124,"end":126},{"type":1,"start":151,"end":152},{"type":1,"start":154,"end":160},{"type":1,"start":187,"end":188},{"type":1,"start":220,"end":222},{"type":1,"start":252,"end":258},{"type":1,"start":261,"end":264},{"type":1,"start":285,"end":291},{"type":1,"start":318,"end":320},{"type":1,"start":351,"end":353},{"type":1,"start":384,"end":394},{"type":1,"start":397,"end":402},{"type":1,"start":404,"end":409},{"type":1,"start":436,"end":438},{"type":1,"start":458,"end":459}]},{"name":"283a","type":1,"text":"Like with ANNs, we also learn and add a constant bias vector, denoted b_h, that can vertically shift what we pass to the activation function. We can also shift our outputs with b_y. More about bias units here.","markups":[{"type":3,"start":204,"end":208,"href":"https://ayearofai.com/rohan-5-what-are-bias-units-828d942b4f52","title":"","rel":"noopener","anchorType":0},{"type":1,"start":70,"end":73},{"type":1,"start":177,"end":180}]},{"name":"246f","type":1,"text":"For both b_h and W_hht/W_hhd, we actually have multiple weight matrices depending on the value of ℓ, as indicated by the superscript. This is because each hidden layer can have a different set of weights (the network would be extremely uninteresting if this wasn’t the case), including the bias vector. However, inside a single hidden layer, all timesteps share the same weight matrix. This is important because the number of timesteps is a variable; we may train on sequences with up to 20 values, but in practice output sequences with up to 30 values — 10 extra timesteps. If each timestep had an independent weight to learn, those last 10 timesteps wouldn’t have anything to use.","markups":[{"type":1,"start":9,"end":13},{"type":1,"start":17,"end":28},{"type":1,"start":98,"end":99}]},{"name":"6da9","type":1,"text":"W_hy is just one matrix because only the final layer gives rise to the outputs denoted y. At the final hidden layer ℓ, we could suggest that W_hhd will not exist because W_hy will be in its place.","markups":[{"type":1,"start":0,"end":5},{"type":1,"start":87,"end":88},{"type":1,"start":116,"end":117},{"type":1,"start":141,"end":147},{"type":1,"start":170,"end":175}]},{"name":"e3ed","type":1,"text":"Now we’ll define the function ƒw:","markups":[{"type":1,"start":30,"end":32}]},{"name":"d692","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*9YGuqXdNiknmZR2HScMDKw.png","originalWidth":404,"originalHeight":226}},{"name":"e06a","type":1,"text":"The function is very similar to the ANN hidden function you’ve seen before; it applies the correct weights to the corresponding parameters, adds the bias, and passes this weighted sum through an activation or “squashing” function to introduce non-linearities. The key difference, though, is that this is not a weighted sum but rather a weighted sum vector; any W ⋅ h, along with the bias, will have the dimensions of a vector. The tanh function will thus simply output a vector where each value is the tanh of what it was in the inputted vector (sort of like an element-wise tanh). Remember, this contrasts ANNs because RNNs operate over vectors versus scalars.","markups":[{"type":1,"start":361,"end":366},{"type":1,"start":388,"end":389},{"type":1,"start":431,"end":436},{"type":1,"start":506,"end":507},{"type":2,"start":322,"end":323},{"type":2,"start":349,"end":355}]},{"name":"d904","type":1,"text":"If you’ve followed our blog so far, you most likely know about two activation functions: sigmoid and ReLU. tanh is another such function. We mostly use the tanh function with RNNs. This is, I think, mostly because of their role in LSTMs (a variant of RNNs that are used more than RNNs — more on that later), the fact that they produce gradients with a greater range, and that their second derivative don’t die off as quickly.","markups":[]},{"name":"25ba","type":1,"text":"Similar to sigmoid, the tanh function has two horizontal asymptotes and a smooth S-shape. The main difference is that the tanh function asymptotes at y = -1 instead of y = 0, intercepting the y-axis at y = 0 instead of y = 0.5. Thus, the tanh function has a greater range than the sigmoid.","markups":[]},{"name":"0f69","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*NPI9iLLVlYLQ2gu9A9xp0A.png","originalWidth":324,"originalHeight":166}},{"name":"9d21","type":1,"text":"If interested, the tanh equation follows (though I won’t walk you through it):","markups":[]},{"name":"a172","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*w7LV9vY1hCAXcLk2K_peEg.png","originalWidth":241,"originalHeight":84}},{"name":"85eb","type":1,"text":"The final equation is mapping a hidden state to an output.","markups":[]},{"name":"f531","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*n7simJp73WxCRx_Bz4dXwg.png","originalWidth":177,"originalHeight":44}},{"name":"93b1","type":1,"text":"This is one such possible equation. Depending on the context, we might also remove the bias vector, apply a non-linearity like sigmoid/softmax (for example if the output needs to be a probability distribution), etc.","markups":[{"type":2,"start":8,"end":11}]},{"name":"3584","type":1,"text":"And that’s how we express recurrent nets, mathematically!","markups":[]},{"name":"2650","type":1,"text":"Quick note: Notation may and will differ between various lectures, research paper, articles, etc. For example — some research papers may start indexing at 0 instead of 1. More drastically, most RNN notation is much more general than mine to promote simplicity, ie. doesn’t cover edge cases like I did or obfuscates certain indices like ℓ with hidden to hidden weight matrices. So, just keep note that specifics don’t always transfer over and avoid being confused by this. The reason I was meticulous about notation in this article is that I wanted to ensure you understood exactly how RNNs work, fueled by my frustration with the very same problem ~1.5 years ago.","markups":[{"type":1,"start":336,"end":338},{"type":2,"start":210,"end":215}]},{"name":"ed09","type":3,"text":"An example? Okay!","markups":[]},{"name":"e53b","type":1,"text":"Let’s take a look at a quick example of an RNN in action. I’m going to adapt a super dumbed down one from Andrej Karpathy’s Stanford CS231n RNN lecture, where a one to many “character level language model” single layer recurrent neural network needs to output “hello”. We’ll kick it of by giving the RNN the letter “h” , such that it needs to complete the word by outputting the other four letters.","markups":[{"type":3,"start":140,"end":151,"href":"https://www.youtube.com/watch?v=cO0a0QYmFm8&index=10&list=PLlJy-eBtNFt6EuMxFYRiNRS07MCWN5UIA","title":"","rel":"noopener","anchorType":0}]},{"name":"1901","type":1,"text":"Sidenote: this model nicknamed “char-rnn” — remember it for later, where we get to code our own!","markups":[]},{"name":"fc9c","type":1,"text":"The neural network has the vocabulary: h, e, l , o. That is, it only knows these four characters; exactly enough to produce the word “hello”. We will input the first character, “h”, and from there expect the output at the following timesteps to be: “e”, “l”, “l”, and “o” respectively, to form:","markups":[]},{"name":"56af","type":7,"text":"hello","markups":[]},{"name":"9f5f","type":1,"text":"We can represent input and output via one hot encoding, where each character is a vector with a 1 at the corresponding character position and otherwise all 0s. For example, since our vocabulary is [h, e, l, o], we can represent characters using a vector with four values, where a 1 in the first, second, third, and fourth position would represent “h”, “e”, “l”, and “o” respectively.","markups":[]},{"name":"28cd","type":4,"text":"This is called “one-hot encoding”, because only one of the values in the vector is equal to 1 and thus on (or “hot”).","markups":[],"layout":1,"metadata":{"id":"1*pgWSPyximAFHqZtUkiLeKg.png","originalWidth":440,"originalHeight":192}},{"name":"c682","type":1,"text":"This is what we’d expect with a trained RNN:","markups":[]},{"name":"0f80","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*mmuQb8msqqQLtz580_lpvw.png","originalWidth":250,"originalHeight":227}},{"name":"17a3","type":1,"text":"As you can see, we input the first letter and the word is completed. We don’t know exactly what the hidden states will be — that’s why they’re hidden!","markups":[]},{"name":"04ad","type":1,"text":"One interesting technique would be to sample the output at each timestep and feed it into the next as input:","markups":[]},{"name":"9eb9","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*KyVSttLGcexQWDLvSWD0Lg.png","originalWidth":250,"originalHeight":226}},{"name":"08da","type":1,"text":"When we “sample” from a distribution, we select a random character probabilistically following the distribution. For example, in the diagram above, the character with the highest likeliness is “e” at the first timestep’s output. Let’s say this likeliness is, concretely, 0.9. Now, when we sample into the next timestep’s input, there’s a 90% chance we select “e”; most of the time we will pick the most likely character, but not every time. This adds a level of randomness so you don’t end up in a loop where you keep sampling the same letter or sequence of letters over and over again.","markups":[{"type":2,"start":364,"end":368}]},{"name":"0880","type":1,"text":"As mentioned earlier, this is used pretty heavily with LCRNs. It’s even more effective than only relying on the memory of the RNN to output the correct letter at the future timesteps. In a sense, this makes the recurrent net many to many. (Though, not really, because we still only have one preset input.)","markups":[]},{"name":"9cac","type":1,"text":"However, to be clear, this does not mean that the RNN can only rely on these sampled inputs. For example, at timestep 3 the input is “l” and the expected output is also “l”. However, at timestep 4, the input is again “l” but the output is now “o”, to complete the word. Memory is still needed to make a distinction like this.","markups":[{"type":2,"start":58,"end":63}]},{"name":"db16","type":1,"text":"In numerical form, it would look something like this:","markups":[]},{"name":"5acf","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*AguGRuRZg6e6RZ7Ctvn2Ww.png","originalWidth":280,"originalHeight":371}},{"name":"c815","type":1,"text":"Of course, we won’t get a one-hot vector output during prediction mode; rather, we will get a probability distribution over each letter (so we’d apply softmax to the output), and will sample from this distribution to get a single character output.","markups":[]},{"name":"afba","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*6067M6Oqz2zNoyyyQC1Suw.png","originalWidth":300,"originalHeight":574}},{"name":"9419","type":1,"text":"Each hidden state would contain a similar sort of vector, though not necessarily something we could interpret like we can for the output.","markups":[]},{"name":"58bf","type":1,"text":"The RNN is saying: given “h”, “e” is most likely to be the next character. Given “he”, “l” is the next likely character. With “hel”, “l” should be next, and with “hell”, the final character should be “o”.","markups":[]},{"name":"b7ec","type":1,"text":"But, if the neural network wasn’t trained on the word “hello”, and thus didn’t have optimal weights (ie. just randomly initialized weights), then we’d have garble like “hleol” coming out.","markups":[]},{"name":"399a","type":1,"text":"One more important thing to note: start and end tokens. They signify when input begins and when output ends. For example, when the final character is outputted (“o”), we can sample this back as input and expect that the “\x3cEND\x3e” token (however we choose to represent it — could also use a period) will be outputted at the next timestep; this is the RNN telling us that it has completed the word and its processing as a whole. The use case isn’t as obvious in this fabricated example, because we know when “hello” has been completed, but consider a real-life scenario where we don’t: image captioning. In image captioning, the caption could be 1, 2, 3, or n words long, given a reasonable upper limit of n. The end token tells us when the caption has been completed, so we can halt the RNN and complete the prediction loop (which would keep going forever if we were using while or stop after the upper limit/max possible preset constant value of n is reached).","markups":[{"type":10,"start":870,"end":875},{"type":1,"start":34,"end":54}]},{"name":"a127","type":1,"text":"Start tokens are more used for generating content from complete scratch. For example, imagine an RNN read and learned from a bunch of Shakespeare. (This is an actual funny application of character level language models that Karpathy implemented, and we’ll see it in action on a later section.) Now, based on what the RNN learned, we want it to create a brand new Shakespearean sonnet! Feeding in a “\x3cSTART\x3e” token enables it to kick this process off and begin writing without us giving the network some arbitrary pre-determined initial word or character.","markups":[]},{"name":"85be","type":1,"text":"I’ve also noticed that another potential use case of start tokens is when we have some other sort of initial input, like CNN produced image data with image captioning, that doesn’t “fit” what we’ll normally use for input at timesteps after t=1 (the word outputted at the previous timestep via sampling). As a result, we feed this data directly to the first hidden state and set the input as “\x3cSTART\x3e” instead.","markups":[{"type":1,"start":240,"end":244},{"type":2,"start":101,"end":109}]},{"name":"2aca","type":1,"text":"Now, just to be clear, the RNN doesn’t magically output these end tokens and recognize the start tokens. We have to add them, along with start tokens, to the training data and vocabulary such that they can be outputted by the recurrent net during prediction time.","markups":[]},{"name":"1c00","type":1,"text":"This is how we can get RNNs to “write”! More on some examples of text RNNs have actually generated, Shakespeare most certainly included, in a later section.","markups":[]},{"name":"6b00","type":3,"text":"Training (or, why vanilla RNNs suck.)","markups":[]},{"name":"58b5","type":1,"text":"For a recurrent net to be useful, it needs to learn proper weights via training. That’s no surprise.","markups":[]},{"name":"8a33","type":1,"text":"Recall this snippet from earlier:","markups":[]},{"name":"f72f","type":6,"text":"But, if the neural network wasn’t trained on the word “hello”, and thus didn’t have optimal weights (ie. just randomly initialized weights), then we’d have garble like “hleol” coming out.","markups":[]},{"name":"f9d1","type":1,"text":"This is, of course, because we initialize the W weights randomly at first, so random stuff will come out.","markups":[{"type":1,"start":46,"end":48}]},{"name":"c7a6","type":1,"text":"But, through multiple iterations of training with a first-order optimization algorithm like gradient descent, we perturb the weights such that the probability of each correct character being outputted at their respective timestep increases. The actual output would be “hello” in one-hot encoding form, and we’d compute the discrepancy between this output and what the recurrent net predicts (we’d get the error at each timestep and then add this up) as the total error to then calculate the gradient/update value.","markups":[]},{"name":"2ded","type":1,"text":"So, each output contributes to the error somehow. If the error is an addition of the outputs, then, if we had something like Y outputs, we’d need to backpropagate them individually and add these up. This is because derivatives are distributed evenly when we’re differentiating a sum:","markups":[{"type":1,"start":125,"end":126}]},{"name":"6145","type":4,"text":"For any arbitrary weight W.","markups":[{"type":1,"start":25,"end":26}],"layout":1,"metadata":{"id":"1*d5mzuu-EmcZz0IFukW6XsQ.png","originalWidth":651,"originalHeight":79}},{"name":"bb6e","type":1,"text":"But, you should know that, with artificial neural networks, calculating these gradients isn’t that easy. We have so many weights contributing to the output, and thus need to figure out exactly how much these weights contribute, and by how much we modify them to decrease overall error. To do this, we use the backpropagation algorithm; this algorithm propagates the error between the predicted output of a recurrent net and the actual output in the dataset all the way back to the beginning of the network. Using the chain rule from differential calculus, backprop helps us calculate the gradients of the output error w.r.t. each individual weight (sort of like the error of each individual weight).","markups":[]},{"name":"c4f4","type":1,"text":"Once we have those gradients, we have to use an optimization algorithm to calculate the update values and make the updates. We can use the vanilla gradient descent algorithm to do this, but there are many other possible, better variants as well; learn about them by reading this article, if you want. (I think we’re long overdue for our own mega-post on optimization!)","markups":[{"type":3,"start":274,"end":286,"href":"http://sebastianruder.com/optimizing-gradient-descent/","title":"","rel":"noopener","anchorType":0}]},{"name":"0be4","type":1,"text":"Backpropagation with RNNs is called “Backpropagation Through Time” (short for BPTT), since it operates on sequences in time. But don’t be fooled — there’s not much difference between normal backprop and BPTT; when it comes down to it, BPTT is just backprop, but on RNNs! Remember that when you “unroll” an RNN, it essentially becomes a feedforward network; not an ANN, but a feedforward network in the sense that we can visualize where all the information is flowing and observe the activations at each neuron and timestep, all the way from the input to the final output. Like ANNs, RNNs have functional dependencies that link the entire network together; it’s just that RNNs operate over vectors instead (yay for matrix calculus?) and extend in depth as well as time. There’s more work to do to compute the gradients, but it’s no surprise that backprop works pretty much the same way for recurrent nets that it would for normal ones. Because of this, I’m not going to walk through all the math and show the derivatives etc. Read our backprop mega-post for all that jazz.","markups":[{"type":2,"start":752,"end":763}]},{"name":"224b","type":1,"text":"One thing to note is that, since we have multiple timesteps in our RNN, each timestep in a single layer will want to change the weight in a different way and have different gradients. However, remember that each hidden layer uses only one weight matrix because the number of timesteps is a variable. Thus, we just average or sum the weight updates between these timesteps and apply this as an update to the W_hh for that entire layer. Also, a general practice is to train on shorter sequences first and then gradually increase sequence size as we train on more and more data.","markups":[{"type":1,"start":407,"end":412}]},{"name":"1fee","type":1,"text":"Now, if you haven’t already, make sure to read this article that I wrote on vanishing and exploding gradients before proceeding:","markups":[]},{"name":"c672","type":14,"text":"Rohan #4: The vanishing gradient problem\nOh no — an obstacle to deep learning!ayearofai.com","markups":[{"type":3,"start":0,"end":91,"href":"https://ayearofai.com/rohan-4-the-vanishing-gradient-problem-ec68f76ffb9b","title":"https://ayearofai.com/rohan-4-the-vanishing-gradient-problem-ec68f76ffb9b","rel":"","anchorType":0},{"type":1,"start":0,"end":40},{"type":2,"start":41,"end":78}],"mixtapeMetadata":{"mediaResourceId":"bb894bd0a8e1cfee65aea593ec3751b3","thumbnailImageId":"1*m2gDBT_nc-iE7R4AM3sHBQ.jpeg","href":"https://ayearofai.com/rohan-4-the-vanishing-gradient-problem-ec68f76ffb9b"}},{"name":"be11","type":1,"text":"You may be thinking: how does this issue apply to RNNs? Well, RNNs are very deep models; on top of often having multiple hidden layers, each hidden layer in practice can have hundreds of timesteps. That’s like an ANN with hundreds of entire hidden layers! That’s deep. (Well, it’s more long because we’re dealing with the time axis here, but you know what I mean.) tanh derivatives are very similar to sigmoid derivatives in range, so the problem of vanishing gradients is thus even more drastic with RNNs than with ANNs, and training them becomes almost impossible.","markups":[{"type":2,"start":175,"end":183},{"type":2,"start":263,"end":267},{"type":2,"start":268,"end":269},{"type":2,"start":286,"end":291}]},{"name":"4878","type":1,"text":"Imagine trying to propagate the error to the 1st timestep in an RNN with k timesteps. The derivative would look something like this:","markups":[{"type":1,"start":73,"end":74}]},{"name":"44b3","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*gKbRtQfPwGK2d7jnKZdv5w.png","originalWidth":346,"originalHeight":79}},{"name":"0e72","type":1,"text":"With a tanh activation function, that’s freaking crazy. Then, for getting the derivative of the error with respect to a weight matrix W_hh, we’d add — or, as mentioned before, we could average as well — each of these hidden state error gradients, then multiplied by the derivative of the hidden state with respect to the weight, such that we can backprop from the error to the weight:","markups":[{"type":1,"start":134,"end":138}]},{"name":"0639","type":4,"text":"Assuming our sequence is of length k.","markups":[{"type":1,"start":35,"end":36}],"layout":1,"metadata":{"id":"1*jf52uXcsAX6Nn8ghLYoJWQ.png","originalWidth":773,"originalHeight":80}},{"name":"9baa","type":1,"text":"So we’d be effectively adding together a bunch of terms that have vanished — the exception being very late gradients with a small number of terms — and so dJ/dWhh would only capture gradient signals from the last few timesteps. (Or, for exploding gradients, it would become infinity).","markups":[{"type":1,"start":155,"end":163}]},{"name":"fd7e","type":1,"text":"But, you might be asking, instead of tanh — which is bounded between -1 and 1, and has a similar problem to sigmoid where the peak of the derivative is smaller than 1 — why don’t we just use ReLUs? Don’t ReLUs, or perhaps leaky ReLUs, solve the vanishing gradient problem?","markups":[]},{"name":"6a76","type":1,"text":"Well, not entirely; it’s not enough to solve the problem. With RNNs, the problem really lies in the architecture. Even though we could use ReLU to ensure many of the values in the gradient computation are not between -1, 0, and 1 such that they vanish — or vice-versa, explode — we do still indeed have a lot of other variables other than the activation function derivative in the gradient computation such as the weights; you can revisit the mega-post on backprop we wrote to confirm this. Since weights are also normally randomly initialized in the range -1 to 1, and RNNs are like super deep ANNs, these weights keep multiplying on top of each other and potentially cause the gradients to vanish.","markups":[]},{"name":"4faf","type":1,"text":"This is more my suspicion though — I’m yet to confirm this is the case by testing. I was curious so I asked this exact question on Quora:","markups":[]},{"name":"6eaa","type":11,"text":"","markups":[],"layout":1,"iframe":{"mediaResourceId":"aa3fb6f891aba6d55742cf7dabf3f7f7","iframeWidth":560,"iframeHeight":560,"thumbnailUrl":"https://i.embed.ly/1/image?url=https%3A%2F%2Fwww.quora.com%2Fstatic%2Fimages%2Flogo%2Fwordmark_default.png&key=4fce0568f2ce49e8b54624ef71a8a5bd"}},{"name":"e537","type":1,"text":"From this, something interesting I learned is that: since ReLUs are unbounded (it’s not restricted to be between -1 and 1 or 0 and 1) unlike sigmoid/tanh, and RNNs are very deep, the activations, especially later ones, can become too big. This is because hidden states have a multiplicative relationship; one hidden state is a multiple of the previous ones, where that multiple specifically is a weight. If we use ReLU, then the hidden state isn’t limited by any range, and we could have a bunch of numbers bigger than 1 multiplying by each other.","markups":[{"type":2,"start":68,"end":70}]},{"name":"b4fc","type":1,"text":"It ends up being sort of like the exploding gradient problem, but with the values inside the neurons, not gradients. This is also what then causes the gradients to explode: large activations → large gradients → large change in weights → even bigger activations, because updating the weights in the wrong direction ever so slightly can cause the entire network to explode. This makes learning unstable:","markups":[{"type":2,"start":135,"end":139}]},{"name":"2c04","type":6,"text":"This means that the computation within the RNN can potentially blow up to infinity without sensible weights. This makes learning VERY unstable because a slight shift in the weights in the wrong direction during backprop can blow up the activations during the forward pass. So that’s why you see most people using sigmoid/tanh units, despite the vanishing gradient descent problem.","markups":[]},{"name":"4f2a","type":1,"text":"Also well said:","markups":[]},{"name":"e86a","type":6,"text":"With RNN’s, the problem is that you are repeatedly applying your RNN to itself, which tends to [mostly] cause exponential blowup or [rarely, but sometimes] shrinkage.","markups":[]},{"name":"4cb6","type":1,"text":"Other issues with ReLU functions are discussed in the article I wrote, and they similarly apply to RNNs. Generally speaking, though, they just don’t work that well, especially compared to other options we have. Making RNNs perform well with ReLUs is actually a pretty hot topic of research right now, but until someone figures out something genius, RNNs are a lost cause.","markups":[]},{"name":"4a29","type":1,"text":"And that’s why vanilla RNNs suck. Seriously. In practice, nobody uses them. Even if you didn’t fully grasp this section on how the vanishing and exploding gradient/activation problem is applicable to them, it doesn’t matter anyways. Because, everything you’ve read up to this point so far… throw it all away. Forget about it.","markups":[{"type":1,"start":15,"end":32}]},{"name":"044a","type":1,"text":"Just kidding. Don’t do that.","markups":[]},{"name":"1d2b","type":3,"text":"Fixing the problem with LSTMs (Part I)","markups":[]},{"name":"daad","type":1,"text":"You shouldn’t do that because RNNs actually aren’t a lost cause. They’re far from it. We just need to make a few… modifications.","markups":[{"type":2,"start":44,"end":51}]},{"name":"9491","type":1,"text":"Enter the LSTM.","markups":[]},{"name":"32fa","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*JuC5afKk7QIntsvyEn-IFA.png","originalWidth":298,"originalHeight":170}},{"name":"731a","type":1,"text":"Makes sense, no?","markups":[]},{"name":"ea5b","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*tB6QdzunJV08wyep0ZhVMA.png","originalWidth":350,"originalHeight":357}},{"name":"03ff","type":1,"text":"How about this?","markups":[]},{"name":"0c36","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*Oin8uuuQzp_wqtHAX1yyjQ.png","originalWidth":400,"originalHeight":314}},{"name":"0d8c","type":1,"text":"OK. Clearly something’s not registering here. But that’s fine; LSTM diagrams are frikin’ difficult for beginners to grasp. I too remember when I first searched up “LSTM” on Google to encounter something similar to the works of art above. I reacted like this:","markups":[]},{"name":"16e7","type":4,"text":"MRW first Google Image-ing LSTMs.","markups":[],"layout":1,"metadata":{"id":"1*S7ABwK33X7no_MP3epry6A.gif","originalWidth":195,"originalHeight":229}},{"name":"7cd4","type":1,"text":"In this section, I’m going to embark on a mission to design the first simple, comprehensible, and beautiful LSTM diagram. Wish me luck, because I’ll probably fail.","markups":[]},{"name":"b288","type":1,"text":"With that being said, let’s dive into Long Short-Term Memory networks. (Yes, that’s what LSTM stands for.)","markups":[{"type":1,"start":38,"end":69}]},{"name":"bb27","type":1,"text":"With RNNs, the real “substance” of the model were the hidden neurons; these were the units that did processing on the input, through time, to produce the outputs. Specifically, at each timestep, a hidden neuron embodies a hidden state that is computed by feeding the weighted sum vector of the input and/or previous hidden states with an added bias vector through a tanh squashing function. We can have multiple hidden neurons, each of which when unrolled forms an entire hidden “layer”.","markups":[]},{"name":"66c2","type":1,"text":"If you need a refresher on this, look through the “Formalism” section once again.","markups":[]},{"name":"cd18","type":1,"text":"With LSTMs, we still have hidden states, but they’re computed through a much more complex mechanism: LSTM cells. Instead of computing each hidden state as a direct function of inputs and other hidden states, we compute it as a function of the LSTM cell’s value (the “cell state”) at that timestep. Each cell state is in turn functionally dependent on the previous cell state and any available input or previous hidden states. That’s right — hidden states are computed from cell states, and cell states are (in part) computed from older and/or shallower hidden states.","markups":[{"type":2,"start":363,"end":364}]},{"name":"095f","type":1,"text":"The cell state at a specific timestep t is denoted c_t. Like a hidden state, a cell state is just a vector.","markups":[{"type":1,"start":38,"end":40},{"type":1,"start":51,"end":54}]},{"name":"e92a","type":4,"text":"For simplicity’s sake, I’ve obfuscated layer index ℓ.","markups":[{"type":1,"start":51,"end":52}],"layout":1,"metadata":{"id":"1*sr8XQzvr-WTNSbgWwI9qbQ.png","originalWidth":500,"originalHeight":245}},{"name":"68f5","type":1,"text":"If the diagram above seems a bit trippy, let me break it down for you.","markups":[]},{"name":"8bd2","type":1,"text":"c_t, as highlighted by the three arrows pointing towards it, has multiple potential functional dependencies. Four to be exact, though only a maximum of three can exist at once. These are:","markups":[{"type":1,"start":0,"end":3}]},{"name":"4f8f","type":9,"text":"The previous hidden state in time: h_t-1. Again, if t = 1, then this won’t exist. If it does, this would be the first arrow pointing into the left side of c_t.","markups":[{"type":1,"start":35,"end":40},{"type":1,"start":52,"end":57},{"type":1,"start":155,"end":158}]},{"name":"f283","type":9,"text":"The previous cell state: c_t-1. If t = 1, the dependency obviously won’t exist. This refers to the second arrow pointing into the left side of c_t.","markups":[{"type":1,"start":25,"end":30},{"type":1,"start":35,"end":40},{"type":1,"start":143,"end":146}]},{"name":"7237","type":9,"text":"Input at the current timestep: x_t. There may very well be no input available, for example if we are at a hidden layer ℓ \x3e 1. So this dependency doesn’t always exist. When it does, it’s the arrow pointing into the bottom of c_t.","markups":[{"type":1,"start":31,"end":34},{"type":1,"start":119,"end":124},{"type":1,"start":224,"end":227}]},{"name":"83f3","type":9,"text":"The previous hidden state in depth: h^(ℓ-1)_t. This applies for any hidden layer ℓ \x3e 1. In such case, it would — like the input x_t — be the arrow pointing into the bottom.","markups":[{"type":1,"start":36,"end":45},{"type":1,"start":81,"end":86},{"type":1,"start":128,"end":131}]},{"name":"2a71","type":1,"text":"Only three can exist at once because the last two are mutually exclusive.","markups":[]},{"name":"5160","type":1,"text":"From there, we pass information to the next cell state c_t+1 and compute h_t. As you can hopefully see, h_t then goes on to also influence c_t+1 (as indicated by the horizontal arrow), along with higher level cell states or final outputs (the vertical arrow).","markups":[{"type":1,"start":55,"end":60},{"type":1,"start":73,"end":76},{"type":1,"start":104,"end":107},{"type":1,"start":139,"end":145}]},{"name":"0f0f","type":1,"text":"Right now the cells are a black box… literally; we know what is inputted to them and what they output, but we don’t know their internal process. So… what’s inside these cells? What do they do? What are the exact computations involved? How have the equations changed?","markups":[]},{"name":"ee47","type":1,"text":"To help answer the question, I want you to imagine something: a factory. Inside this factory we have workers who perform their own tasks. Those tasks are, specifically, operating on some sort of product that runs down a conveyer belt. Think of, hell, I don’t know — chicken nuggets! The first worker cuts an appropriately sized piece, the second worker applies egg wash, the third worker adds breadcrumbs, the fourth worker chucks it in the fryer, etc.","markups":[{"type":1,"start":220,"end":233}]},{"name":"5a97","type":4,"text":"I’m not sure what product this conveyer belt carries, but it certainly doesn’t look appetizing (or like chicken nuggets).","markups":[],"layout":1,"metadata":{"id":"1*4avrG18SFOMJGI4CpDIsoA.png","originalWidth":490,"originalHeight":600}},{"name":"02ea","type":1,"text":"You’re thinking: “OK Rohan, but how does this relate to LSTMs?”. Good question.","markups":[]},{"name":"9004","type":1,"text":"Basically, think of the conveyer belt as the cell state, the chicken-nugget-in-progress as information flowing through the cell, and the workers as operations we apply to this information. The final product is the finished chicken nugget — or, the cell state value.","markups":[]},{"name":"b54e","type":4,"text":"Chicken. Nugget.","markups":[],"layout":1,"metadata":{"id":"1*qNUGFMhlnl0-mNLIVvyGAg.png","originalWidth":459,"originalHeight":95}},{"name":"bca7","type":1,"text":"The reason we use the analogy of a conveyer belt is because information can flow through a cell super super easily. It’s theoretically possible for information to speed past a single cell state without being modified at all. In fact, I think the term ‘modified’ is a really strong one here. With vanilla RNNs, each hidden state takes all the information it has from before and fully transforms it by applying a function over it. LSTM cells instead take information and make minor modifications (like additions or multiplications) to it while it flows through.","markups":[{"type":1,"start":383,"end":393},{"type":1,"start":480,"end":493},{"type":2,"start":383,"end":393},{"type":2,"start":480,"end":493}]},{"name":"eca6","type":4,"text":"Ew. Vanilla RNNs.","markups":[],"layout":1,"metadata":{"id":"1*I_nQdhxdoDa7KrZBTFeHSQ.png","originalWidth":330,"originalHeight":241}},{"name":"72ea","type":1,"text":"Vanilla RNNs look something like this. And it’s why the vanishing gradient problem exists; during backprop, gradients cannot flow back past these transformations easily, because the tanh derivatives and weights chain up and multiply together and tend to zero. We then add up or average all these gradients that are basically zero, and we get zero.","markups":[]},{"name":"9b8e","type":4,"text":"LSTMs 💦 💦 💦","markups":[],"layout":1,"metadata":{"id":"1*360GYNV8kyF5ATWefrSasA.png","originalWidth":450,"originalHeight":276}},{"name":"0a23","type":1,"text":"This is an extreme a simplification — and I’ll go on to fill in the blanks later — but it’s sort of what an LSTM looks like. The previous timestep’s cell state value flows through and instead of transforming the information, we tweak it by adding (another vector) to it. The added term is some function ƒw of previous information, but this is not the same function as with vanilla RNNs — it’s heavily changed to make sense in this context (more on that soon), do more interesting things, and also reduce the vanishing gradient problem.","markups":[{"type":1,"start":303,"end":306},{"type":1,"start":343,"end":346},{"type":2,"start":240,"end":247},{"type":2,"start":343,"end":346}]},{"name":"ba34","type":1,"text":"Another neat way to think of it is like a live circuit: there are two paths where information, like current, can flow through. After the information passes through ƒw, it’s added to the information flowing towards c_t. Thus, in equation form it could look something like this:","markups":[{"type":1,"start":163,"end":166},{"type":1,"start":214,"end":217}]},{"name":"ea17","type":4,"text":"Again… sort of. We’ll get into the actual equations soon. This is a good proxy to convey my point.","markups":[{"type":1,"start":35,"end":41},{"type":2,"start":41,"end":98}],"layout":1,"metadata":{"id":"1*qGqSrpJmO5h6ZGeIT7RK3w.png","originalWidth":208,"originalHeight":39}},{"name":"c99b","type":1,"text":"With a bit of substitution, we can expand this to:","markups":[]},{"name":"2268","type":4,"text":"Technically, this could expand even more, if you did some sort of recursive substitution to fully simplify the unrolled recurrence formula. You could express c_t for some large value of t as a really really really really long function of, ultimately, c_1.","markups":[{"type":1,"start":158,"end":162},{"type":1,"start":186,"end":188},{"type":1,"start":251,"end":254}],"layout":1,"metadata":{"id":"1*OZs7rDSty0VhDhzTLH4Dgg.png","originalWidth":444,"originalHeight":39}},{"name":"48be","type":1,"text":"Why is this better? Well, if you have basic differentiation knowledge, you’ll know that addition distributes gradients equally. When we take the derivative of this whole expression, it’ll become a long addition of the derivatives of individual terms. As Andrej Karpathy puts it, this additive interaction creates “gradient super-highways”, where gradients can flow back super easily.","markups":[]},{"name":"740d","type":4,"text":"Look — it’s a long conveyer belt! (In a sense, we can use this conveyor belt analogy for the whole unrolled LSTM as well. Each cell state is a subsection of the conveyer belt.)","markups":[{"type":1,"start":93,"end":112},{"type":2,"start":93,"end":112}],"layout":1,"metadata":{"id":"1*n26drGfEkc-Xnmqc2Lw7cw.png","originalWidth":2161,"originalHeight":690}},{"name":"2d87","type":4,"text":"Look — it’s an outdated machine learning algorithm!","markups":[],"layout":1,"metadata":{"id":"1*szBIWNdr0O0doBI8rfGjzw.png","originalWidth":610,"originalHeight":193}},{"name":"68da","type":1,"text":"In the former, gradients are always added together, never multiplied. In the latter, gradients are always multiplied. Thus, in the former, when we inject a gradient at the end, it’ll easily flow back all the way to the beginning. Contributions by the ƒw function will be made to this gradient flowing on the bottom conveyer belt as well.","markups":[{"type":1,"start":251,"end":254}]},{"name":"779d","type":1,"text":"This is what gradient flow would look like:","markups":[]},{"name":"8074","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*dqOCXyepO590ORWV3VgBKw.png","originalWidth":740,"originalHeight":262}},{"name":"7ea0","type":1,"text":"Before, we discussed that when multiplicative interaction exists between gradients, the gradients either vanish (if they are mostly \x3c 1, as is usually the case for us) or explode (if they are mostly \x3e 1). Here’s some real calculus to demonstrate this:","markups":[]},{"name":"6f86","type":4,"text":"Former is akin to RNNs. Latter is akin to LSTMs.","markups":[],"layout":1,"metadata":{"id":"1*09oGK1btsVezIoMyBAwqbw.png","originalWidth":287,"originalHeight":288}},{"name":"0175","type":1,"text":"Imagine f being any sort of function, like our ƒw. When we apply a function to itself repeatedly, the chain rule shows that the overall derivative is the multiplication of multiple different derivative terms. But, when we add functions together, the derivative is simply the addition of the individual derivatives. This won’t vanish or explode quickly, so our LSTMs won’t vanish or explode quickly. Yay!","markups":[{"type":1,"start":8,"end":10},{"type":1,"start":47,"end":49}]},{"name":"4e50","type":1,"text":"Furthermore, if some of our gradients vanish — for whatever reason — then it should still be OK. It won’t be optimal, but since our gradient terms add together, if some of them vanish it doesn’t mean the whole thing will vanish (versus if they were multiplied together). Look: 2 + 0 = 2 but 2 × 0 = 0.","markups":[]},{"name":"bbb5","type":4,"text":"A gradient super highway? Sounds good to me! http://www.dyoung.com/assets/images/Articles%20images/article4_PPH.jpg","markups":[{"type":3,"start":45,"end":115,"href":"http://www.dyoung.com/assets/images/Articles%20images/article4_PPH.jpg","title":"","rel":"nofollow noopener","anchorType":0}],"layout":1,"metadata":{"id":"1*K7rYONPTfcpCb0xTvO3ydw.jpeg","originalWidth":466,"originalHeight":240}},{"name":"a58a","type":1,"text":"So far, we haven’t really explored LSTMs. We’ve more setup a foundation for them. And there’s one glaring issue with our foundation: if we just keep adding information to cell state, it could just grow and grow and grow, and essentially act as a counter that only increments. This is not very useful, and could regularly lead to explosion. We want more fine and rich control over memory. Well, worry not, because this is exactly what LSTMs are capable of doing.","markups":[{"type":2,"start":19,"end":26}]},{"name":"7ba6","type":1,"text":"LSTM cells handle memory in a very intelligent way, enabling them to learn long-term dependencies and perform well. How, exactly? Well, the cell is sort of like an internal memory state that allows for context; it “forgets”, a.k.a. resets, information it doesn’t find useful from the previous cell state, “writes” in new information it does find useful from the current input and/or previous hidden state, and similarly only “reads” out part of its information — the good stuff — in the computation of h_t. This respectively corresponds to the concepts of: resetting memory, writing to memory, and reading from memory. Very similar to how a modern computer system works, and we often describe an LSTM cell as a “memory cell”.","markups":[{"type":1,"start":502,"end":505},{"type":2,"start":336,"end":340}]},{"name":"6706","type":1,"text":"The “writing to memory” part is additive — it’s what I showed you in the initial diagrams. Information flows through and we add stuff we think is relevant to it. The “resetting memory” part is multiplicative, and occurs before writing to memory; when information from the previous cell state initially flows in, we multiply it by a vector with values between 0 and 1 to reset or retain parts of it we find useless and useful respectively. The “reading from memory” part is also multiplicative with a similar 0–1 range vector, but it doesn’t modify the information flowing through the cell states. Rather, it modifies the information flowing into the hidden states and thus decides what the hidden state is influenced by.","markups":[]},{"name":"6813","type":1,"text":"Both of these multiplications are element wise, like so:","markups":[{"type":2,"start":34,"end":46}]},{"name":"2255","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*YuIuYxt0oYEvGMoTz_J59g.png","originalWidth":167,"originalHeight":82}},{"name":"c869","type":1,"text":"In this equation, when a = 0 the information of c is lost. This is what resetting does, and retaining is the vice versa. I also imagine that values such as 0.5 could be used to diminish the importance of certain information, but not completely wipe it out.","markups":[{"type":1,"start":23,"end":29},{"type":1,"start":48,"end":50}]},{"name":"c920","type":1,"text":"Our (unfinished) cell state computational graph now looks like this:","markups":[]},{"name":"f4a0","type":4,"text":"This is closer to what an LSTM looks like, though we’re not exactly there yet.","markups":[],"layout":1,"metadata":{"id":"1*_mbUA8vdaTbYreXpdPJccA.png","originalWidth":419,"originalHeight":158}},{"name":"ba90","type":1,"text":"Sidenote: don’t be scared whenever you see the word “multiplicative” and don’t immediately think of “vanishing” or “exploding”. It depends on the context. Here, as I’ll show mathematically in a bit, it’s fine.","markups":[]},{"name":"9705","type":1,"text":"This concept in general is known as gating, because we “gate” what can flow in and out of the LSTM cell. What we actually multiply and add by to reset, write, and read are known as the “gates”. There are four such gates:","markups":[{"type":1,"start":36,"end":42}]},{"name":"dafe","type":9,"text":"f: forget gate. This is the “reset” tool that wipes out, diminishes, or retains information from the previous cell state. It’s the first interaction we make, and it’s multiplicative. That is, we multiply it with the cell state. The sigmoid function is used to compute the forget gate such that its values can be in the range 0 to 1. When a value is 1, we “remember” something, and when it is 0 we “forget”. We might choose to forget, for example, when see a period or some sort of end of sentence marker. This is counterintuitive… I guess it should really be called the “remember gate”!","markups":[{"type":1,"start":0,"end":1},{"type":2,"start":3,"end":16},{"type":2,"start":571,"end":584}]},{"name":"aeb2","type":9,"text":"g: ?. This gate doesn’t really have a name, but it’s partly responsible for the “write” process. It stores a value between -1 and 1 that represents how much we want to add to the cell state by, and represents the input to the cell state. It’s computed with the tanh function. We apply a bounded function to it such that the cell state acts as a stable counter, and it also introduces more complexity. (And it works well.)","markups":[{"type":1,"start":0,"end":1},{"type":1,"start":2,"end":3},{"type":1,"start":5,"end":6},{"type":2,"start":2,"end":6}]},{"name":"0612","type":9,"text":"i: input gate. This is the other gate responsible for the “write” process. It controls how much of g we “let in”, and is thus between 0 and 1, computed with sigmoid. It’s similar to the forget gate in this sense, in that it blocks input like the forget gate blocks the incoming cell state. We multiply i by g and add this to the cell state. Since i is in the range 0 to 1, and g is in the range -1 to 1, we add a value between -1 and 1 to the cell state. Intuitively, this sort of acts as decrementing or incrementing the counter.","markups":[{"type":1,"start":0,"end":1},{"type":1,"start":2,"end":3},{"type":1,"start":99,"end":100},{"type":1,"start":302,"end":304},{"type":1,"start":307,"end":309},{"type":1,"start":340,"end":341},{"type":1,"start":347,"end":349},{"type":1,"start":377,"end":378},{"type":2,"start":2,"end":14},{"type":2,"start":99,"end":100},{"type":2,"start":302,"end":303},{"type":2,"start":307,"end":309},{"type":2,"start":347,"end":349},{"type":2,"start":377,"end":378}]},{"name":"57eb","type":9,"text":"o: output gate. This is also passed through sigmoid, and is a number between 0 and 1 that modulates which aspects the hidden state can draw from the cell state. It enables the “read from memory” operation. It multiplies with the tanh of the cell state to compute the hidden state. So, I didn’t bring this up before, but the cell state leaks into a tanh before h_t is computed.","markups":[{"type":1,"start":0,"end":1},{"type":1,"start":15,"end":16},{"type":1,"start":233,"end":234},{"type":1,"start":360,"end":363},{"type":2,"start":2,"end":14},{"type":2,"start":15,"end":16},{"type":2,"start":233,"end":234}]},{"name":"5cb0","type":1,"text":"Here’s our updated computational graph for the cell state:","markups":[]},{"name":"d768","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*3xq3p-nVgxQXXPSXueVWdw.png","originalWidth":533,"originalHeight":251}},{"name":"01f6","type":1,"text":"Looks like I’m starting to create a complex diagram of my own. Damn. 😞 I guess LSTMs and immediately interpretable diagrams just weren’t meant to be!","markups":[]},{"name":"7eb5","type":1,"text":"Basically, f interacts with the cell state through a multiplication. i interacts with g through a multiplication as well, the result of which interacts with the cell state through an addition. Finally, the cell state leaks into a tanh (that’s the shape of the tanh function in the circle), the result of which then interacts with o through multiplication to compute h_t. This does not disrupt the cell state, which flows to the next timestep. h_t then flows forward (and it could flow upward as well).","markups":[{"type":1,"start":11,"end":13},{"type":1,"start":69,"end":71},{"type":1,"start":86,"end":88},{"type":1,"start":330,"end":332},{"type":1,"start":366,"end":369},{"type":1,"start":443,"end":447}]},{"name":"e614","type":1,"text":"Here’s the equation form:","markups":[]},{"name":"94fe","type":4,"text":"Each gate should actually be indexed by timestep t — we’ll see why soon.","markups":[{"type":1,"start":49,"end":51}],"layout":1,"metadata":{"id":"1*B9Qd1pW1kYM_zcg0IhPfUA.png","originalWidth":222,"originalHeight":84}},{"name":"2518","type":1,"text":"As you can see, our cell state has no activation function; the activation function is simply the identity function! Yet, the cell state usually doesn’t explode — it stays stable by “forgetting” and “writing”, and does interesting things with this gating to promote context, fine control over memory, and long-term dependency learning.","markups":[]},{"name":"b30a","type":1,"text":"So, how are the gates calculated? Well, all of these gates have their own learnable weights and are functions of the last timestep’s hidden state flowing in and any current timestep inputs, not the cell state (contrary to what I may have implied earlier with the gradient flow diagrams). This should make sense when you think about it; I mean, firstly, the g and i gates literally represent input, so they better be functionally dependent on hidden states and input data! On an intuitive level, the gates help us modify the cell state, and we modify the cell state based on our current context. External stimulus that provide context should be used to compute these gates, and since context = input + hidden states our gates are functionally dependent on input and hidden states.","markups":[{"type":1,"start":357,"end":359},{"type":1,"start":363,"end":365},{"type":2,"start":133,"end":146}]},{"name":"27fb","type":1,"text":"Since every gate has a different value at each timestep, we index by timestep t just like for hidden states, cell states, or something similar.","markups":[{"type":1,"start":78,"end":80}]},{"name":"6fb5","type":1,"text":"We could generalize for multiple hidden layers as well:","markups":[]},{"name":"53d0","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*BWz6E_IFi6UTLkSNSoq1Yg.png","originalWidth":290,"originalHeight":41}},{"name":"1ae0","type":1,"text":"But, for simplicity’s sake, let’s assume we are at the first hidden layer, or that there is only one hidden layer in the LSTM. This way, we can obfuscate the ℓ term and ignore influence from hidden states in the previous depth. We’ll also forget about edge cases and assume input exists at the current timestep. In practice, we obviously can’t make said assumptions, but for the sake of demonstrating the equations it becomes too tedious otherwise.","markups":[{"type":1,"start":158,"end":160}]},{"name":"c042","type":1,"text":"Sidenote: we make this assumption for the rest of the discussion on LSTMs in this article.","markups":[{"type":1,"start":0,"end":90}]},{"name":"5a86","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*hP6I692iv7oc6AkcWINDaw.png","originalWidth":384,"originalHeight":180}},{"name":"033c","type":1,"text":"Like with the RNN hidden state, the index of each weight matrix is descriptive; for example, W_xf are the weights that map input x to the forget gate f. Each gate has weight matrices that map input and hidden states to itself, including biases.","markups":[{"type":1,"start":93,"end":98},{"type":1,"start":129,"end":131},{"type":1,"start":150,"end":151}]},{"name":"6989","type":1,"text":"And this is the beauty of LSTMs; the whole thing is end-to-end differentiable. These gates can learn when to allow data to flow and what data should flow depending on the context it sees (the input and the hidden states). It learns this based on patterns it sees while training. In this sense, it’s sort of like how a CNN learns feature detectors for images, but the patterns are way more complex and less human interpretable with LSTMs. This is why they perform so well.","markups":[{"type":2,"start":95,"end":101}]},{"name":"1be4","type":4,"text":"😨 😱 😰 : perhaps your immediate reaction.","markups":[{"type":1,"start":0,"end":3}],"layout":1,"metadata":{"id":"1*VJL6ONtLK77GpO2XmFCH7g.png","originalWidth":551,"originalHeight":321}},{"name":"b263","type":1,"text":"Okay, this looks scarier, but it’s actually not much different to what we had before, especially once you look past the intimidating web of arrows. One notable change is that we’re showing the previous hidden state in time and the current input flowing in. This diagram makes the assumption that we’re in the first layer and at some timestep \x3e 1 where input exists. We then show how the f, i, g, and o gates are computed from this information — the hidden state and inputs are fed into an activation function like sigmoid (or, for g, a tanh; you can tell because it’s double the height of the others) — and it’s expressed through the web of arrows. It’s implied that we weight the two terms entering our activation functions, adding them up with a bias vector, but it’s not necessarily explicit in the diagram.","markups":[{"type":1,"start":387,"end":388},{"type":1,"start":390,"end":391},{"type":1,"start":393,"end":396},{"type":1,"start":400,"end":401},{"type":1,"start":531,"end":532}]},{"name":"b56c","type":1,"text":"Let’s embed this into our overall LSTM diagram for a single timestep:","markups":[]},{"name":"689a","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*0h88NXeFxkb-xD1rBq4lgA.png","originalWidth":770,"originalHeight":349}},{"name":"147c","type":1,"text":"Now let’s zoom out and view our entire unrolled single layer, three timestep LSTM:","markups":[]},{"name":"4127","type":4,"text":"","markups":[],"layout":5,"metadata":{"id":"1*-lhIk-yAsXk88gcPvEeIRQ.png","originalWidth":4225,"originalHeight":1204}},{"name":"c9d0","type":1,"text":"It’s beautiful, isn’t it? The full screen width size just adds to the effect! Here’s a link to the full res version.","markups":[{"type":3,"start":78,"end":84,"href":"https://drive.google.com/file/d/0BwbWRPtraa2zQUsydXRKSkd3YUU/view?usp=sharing","title":"","rel":"noopener","anchorType":0}]},{"name":"c678","type":1,"text":"The only thing that would look more beautiful would be multiple LSTM cells that stack on top of each other (multiple hidden layers)! 😍","markups":[]},{"name":"e117","type":3,"text":"Fixing the problem with LSTMs (Part II)","markups":[]},{"name":"f89a","type":1,"text":"You’ve come a long way, young padawan. But there’s still a bit left to go. Part I focused on the motivation for LSTMs, how they work, and a bit on why they reduce the vanishing gradient problem. Now, having a full understanding of LSTMs, Part II will hone in on the latter part—analyzing on a more close, technical level why our gradients stop vanishing as quickly. You won’t find a lot of this information online easily; I had to search and ask left and right to find an explanation better and more comprehensive than what you’ll find in other current tutorials.","markups":[]},{"name":"a182","type":1,"text":"Firstly, truncated BPTT is often used with LSTMs; it’s a method to speed up training. In particular, note that if we input a sequence of length 1000 into an LSTM, and want to train it, it’s equivalent to training a 1000 layer neural network. Doing forward and backwards passes into this is very memory and time consuming, especially while backpropagating the error when we need to compute a derivative like this:","markups":[]},{"name":"7285","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*UqC4IRIfcDfoiwD8zvqW2A.png","originalWidth":87,"originalHeight":79}},{"name":"06fe","type":1,"text":"…which would include a lot of terms.","markups":[{"type":2,"start":23,"end":26}]},{"name":"792f","type":1,"text":"When we backprop the error, and add all the gradients up, this is what we get:","markups":[]},{"name":"b325","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*ucOvP6wOs9MHzH8WKiykbQ.png","originalWidth":675,"originalHeight":79}},{"name":"9c5f","type":1,"text":"Truncated BPTT does two things:","markups":[]},{"name":"5bfb","type":9,"text":"Instead of doing a forward pass on the whole sequence and then doing a backwards pass, we process the sequence timestep by timestep and do a backwards pass “every so often”. That is — we compute h_1 and c_1, then h_2 and c_2, then h_3 and c_3, and then at some point in time, quantified by k1, we do a backwards pass. Every k1 timesteps, we perform BPTT; if k1 = 10, for example, then once we compute h_10 and c_10 we perform BPTT. Same for h_20 and c_20, and so on so forth. When we perform the backwards pass, our error J won’t be the same as if we did a full forwards pass and full backwards pass, since we haven’t observed all the outputs yet—we wouldn’t have even computed all the potential outputs yet! Instead, the error describes what we’ve observed and computed so far, because we process the sequence timestep by timestep. Intuitively, it’s like we train on a small subset of the training sequence, and this subset increases in length each time, which enables us to continue learning long-term dependencies. We could denote the error at timestep t —where t is a multiple of k1 — with truncated backprop as J^t. So:","markups":[{"type":1,"start":195,"end":199},{"type":1,"start":203,"end":206},{"type":1,"start":213,"end":217},{"type":1,"start":221,"end":224},{"type":1,"start":231,"end":234},{"type":1,"start":239,"end":242},{"type":1,"start":290,"end":294},{"type":1,"start":324,"end":327},{"type":1,"start":358,"end":365},{"type":1,"start":401,"end":406},{"type":1,"start":410,"end":415},{"type":1,"start":441,"end":446},{"type":1,"start":450,"end":454},{"type":1,"start":522,"end":524},{"type":1,"start":1056,"end":1057},{"type":1,"start":1065,"end":1067},{"type":1,"start":1084,"end":1086},{"type":1,"start":1115,"end":1119}]},{"name":"dd7e","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*Br6EoWvUmGTNoX3NqkZVpA.png","originalWidth":445,"originalHeight":80}},{"name":"eb5a","type":1,"text":"For example, if t = 20 and k1 = 10, our second (because 20 ÷ 10 = 2) round of BPTT would be:","markups":[{"type":1,"start":16,"end":22},{"type":1,"start":27,"end":34},{"type":2,"start":40,"end":46}]},{"name":"6f7d","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*pg91-TmNosH9B7Py0wjCdQ.png","originalWidth":667,"originalHeight":80}},{"name":"f441","type":9,"text":"On top of this, instead of backpropagating from J^t all the way to the first timestep c_1, we set a cut-off point. This cut-off point, quantified by k2, is the timestep at which our cell states stop contributing to the overall gradient. For example, if k2 = 10, and we’re backpropagating at t = 20, then c_10 is the final cell state to contribute to the overall gradient. Everything before c_10 will have no say. This is designed such that we avoid computing derivatives between cell states far apart in time, which would include a huge number of terms (as mentioned earlier). The equation is now:","markups":[{"type":1,"start":48,"end":52},{"type":1,"start":86,"end":89},{"type":1,"start":149,"end":151},{"type":1,"start":253,"end":260},{"type":1,"start":291,"end":297},{"type":1,"start":304,"end":309},{"type":1,"start":390,"end":395}]},{"name":"73ca","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*GmP4nvsdBTyyRwo7ffRW2g.png","originalWidth":515,"originalHeight":80}},{"name":"956c","type":1,"text":"So, with t = 20, k2 = 10, and k1 = 10, our second round of BPTT would follow:","markups":[{"type":1,"start":9,"end":15},{"type":1,"start":17,"end":24},{"type":1,"start":30,"end":37}]},{"name":"12a1","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*WyHlRZljjmHEaFKsGg0JQg.png","originalWidth":674,"originalHeight":80}},{"name":"a297","type":1,"text":"Both k1 and k2 are hyperparameters. k1 does not have to equal k2.","markups":[{"type":1,"start":5,"end":7},{"type":1,"start":12,"end":14},{"type":1,"start":36,"end":39},{"type":1,"start":62,"end":64}]},{"name":"93ed","type":1,"text":"These two techniques combined enables truncated BPTT to not lose the ability to learn long term dependencies. Here’s a formal definition:","markups":[]},{"name":"72ee","type":6,"text":"[Truncated BPTT] processes the sequence one timestep at a time, and every k1 timesteps, it runs BPTT for k2 timesteps, so a parameter update can be cheap if k2 is small. Consequently, its hidden states have been exposed to many timesteps and so may contain useful information about the far past, which would be opportunistically exploited.","markups":[{"type":1,"start":74,"end":76},{"type":1,"start":105,"end":107},{"type":1,"start":157,"end":159}]},{"name":"ae91","type":6,"text":"— “Training Recurrent Neural Networks”, 2.8.6, Page 23","markups":[{"type":3,"start":2,"end":38,"href":"http://www.cs.utoronto.ca/~ilya/pubs/ilya_sutskever_phd_thesis.pdf","title":"","rel":"","anchorType":0}]},{"name":"28a4","type":1,"text":"The same paper gives nice pseudocode for truncated BPTT:","markups":[]},{"name":"f5d9","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*0SnUb2iYt1RNa7JsGG-7gQ.png","originalWidth":413,"originalHeight":113}},{"name":"399f","type":1,"text":"The rest of the math in this section will not be in the context of using truncated backprop, because it’s a technique vs. something rooted in the mathematical foundation of LSTMs.","markups":[]},{"name":"d546","type":1,"text":"Moving on — before, we saw this diagram:","markups":[]},{"name":"dc69","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*n26drGfEkc-Xnmqc2Lw7cw.png","originalWidth":2161,"originalHeight":690}},{"name":"48f4","type":1,"text":"In this context, ƒw = i ⊙ g, because it’s the value we’re adding to the cell state.","markups":[{"type":1,"start":17,"end":27}]},{"name":"db0e","type":1,"text":"But this diagram is a bit of a lie. Why? It ignores forget gates. So, does the presence of forget gates affect the vanishing gradient problem? Quite significantly, actually. How? Let’s bring up our cell state equation to see:","markups":[]},{"name":"4484","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*i6rbrX0k9mKLXewD4korCw.png","originalWidth":236,"originalHeight":39}},{"name":"09ae","type":1,"text":"With the forget gate, we now include a multiplicative interaction. Our new diagram will look like this:","markups":[]},{"name":"931c","type":4,"text":"Do not confuse forget gate ƒ with function ƒw in this diagram. I know, it’s confusing… 😢","markups":[{"type":1,"start":27,"end":28},{"type":1,"start":43,"end":45}],"layout":1,"metadata":{"id":"1*UVx1vL6ADQGTBeKSaWX7bw.png","originalWidth":2436,"originalHeight":986}},{"name":"bf2c","type":1,"text":"When our gradients flow back, they will be affected by this multiplicative interaction. So, let’s compute the new derivative:","markups":[]},{"name":"0504","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*60XFfJvc0t9a0ekdMTAp_Q.png","originalWidth":113,"originalHeight":79}},{"name":"f174","type":1,"text":"This seems super neat, actually. Obviously the gradient will be f, because f acts as a blocker and controls how much c_t-1 influences c_t; it’s the gate that you can fully or partially open and close that lets information from c_t-1 flow through! It’s just intuitive that it would propagate back perfectly.","markups":[{"type":1,"start":64,"end":65},{"type":1,"start":75,"end":77},{"type":1,"start":117,"end":123},{"type":1,"start":134,"end":137},{"type":1,"start":227,"end":233},{"type":2,"start":33,"end":42}]},{"name":"d859","type":1,"text":"But, if you’ve payed close attention so far, you might be asking: “wait, what happened to ƒw’s contribution to the gradient?” If you’re a hardcore mathematician, you might also be worried that we’re content with leaving the gradient as just f. This is because the gates f, i, and g are all functions of c_t-1; they are functions of h_t-1, which is, in turn, a function of c_t-1! The diagram shows this visually, as well. It seems we’re failing to apply calculus properly. We’d need to backprop through f and through i ⊙ g to complete the derivative.","markups":[{"type":1,"start":90,"end":92},{"type":1,"start":125,"end":126},{"type":1,"start":241,"end":242},{"type":1,"start":270,"end":271},{"type":1,"start":272,"end":274},{"type":1,"start":280,"end":282},{"type":1,"start":303,"end":308},{"type":1,"start":332,"end":337},{"type":1,"start":372,"end":377},{"type":1,"start":502,"end":503},{"type":1,"start":516,"end":522},{"type":2,"start":67,"end":90},{"type":2,"start":92,"end":126}]},{"name":"3f0c","type":1,"text":"Let’s walk through the differentiation to show why you’re actually not wrong, but neither am I:","markups":[{"type":1,"start":76,"end":78},{"type":2,"start":77,"end":78}]},{"name":"f0c6","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*x1mvDnbZOmZ1CnHJo23tZg.png","originalWidth":380,"originalHeight":162}},{"name":"80e0","type":1,"text":"Now, with the first derivative, we need to apply product rule. Why? Because we’re differentiating the product of two functions of c_t-1. The former being the forget gate, and the latter being just c_t-1. Let’s do it:","markups":[{"type":1,"start":130,"end":135},{"type":1,"start":197,"end":202}]},{"name":"b39b","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*TwjnkG6vtuIke1pkAzTzzA.png","originalWidth":271,"originalHeight":124}},{"name":"00fd","type":1,"text":"Then, from product rule:","markups":[]},{"name":"416d","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*cgq4UnWxun6rQ6H00gDzWg.png","originalWidth":381,"originalHeight":164}},{"name":"7dd1","type":1,"text":"That’s the first derivative done. We purposely choose not to compute the derivative of the forget gate with respect to the previous cell state on previous. You’ll see why in a bit.","markups":[]},{"name":"e8f2","type":1,"text":"Now let’s tackle the second one:","markups":[]},{"name":"be20","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*1T0iaNg6vY4pEOz-ybkjuw.png","originalWidth":150,"originalHeight":77}},{"name":"10b0","type":1,"text":"You’ll notice that it’s also two functions of c_t-1 multiplied together, so we use the product rule again:","markups":[{"type":1,"start":46,"end":52}]},{"name":"f85e","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*tVfrKCc1T7eRgypliDe19A.png","originalWidth":320,"originalHeight":124}},{"name":"618f","type":1,"text":"So:","markups":[]},{"name":"c8bf","type":4,"text":"Once again, we purposely do not simplify the gate derivative terms.","markups":[],"layout":1,"metadata":{"id":"1*s2nnva6Yhb2AuZDYYALbEA.png","originalWidth":361,"originalHeight":164}},{"name":"41e0","type":1,"text":"Thus, our overall derivative becomes:","markups":[]},{"name":"7b20","type":4,"text":"Notice that the first term in this derivative is our forget gate.","markups":[],"layout":1,"metadata":{"id":"1*o0dzU_s9WxoTYfOkQo1a0A.png","originalWidth":461,"originalHeight":79}},{"name":"9307","type":1,"text":"Pay attention to the caption of the diagram.","markups":[]},{"name":"4712","type":1,"text":"This is actually our real derivative. Modern LSTM implementations just use an auto differentiation library to compute derivatives, so they’ll probably come up with this. However, effectively, our gradient is just the forget gate, because the other three terms tend towards zero. Yup — they vanish. Why?","markups":[{"type":2,"start":21,"end":26},{"type":2,"start":179,"end":190}]},{"name":"b5a4","type":1,"text":"When we backprop error in LSTMs, we backprop through cell states to propagate the error from the outputs to the cell state we want. For example, if we want to backprop the error from the output at time t down k timesteps, then we need to compute the derivative of the cell state at time t to the cell state at time t-k. Look what happens when we do that:","markups":[{"type":1,"start":202,"end":203},{"type":1,"start":209,"end":211},{"type":1,"start":287,"end":289},{"type":1,"start":315,"end":318}]},{"name":"db33","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*dBFbl6NCqp94Lnyb0taFWg.png","originalWidth":642,"originalHeight":79}},{"name":"57e3","type":1,"text":"We didn’t simplify the gate w.r.t. cell state derivatives for a reason; as we backpropagate through time, they begin to vanish. Thus, whatever they multiplied with is killed off from making contributions to the gradient, too. So, effectively:","markups":[]},{"name":"08ea","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*Q9BJ7JxQ08YvfBW_OsJabw.png","originalWidth":458,"originalHeight":71}},{"name":"e607","type":1,"text":"The rationale behind this is pretty simple, and we don’t need math for it; these gates are the outputs of non-linearities eg. sigmoid and tanh. If we were to get the derivative of them in getting our cell state derivative, then this derivative would contain the derivatives of sigmoid/tanh in them. But, just because we don’t need to use math to show this, doesn’t mean we don’t want to 😏 :","markups":[{"type":2,"start":326,"end":330},{"type":2,"start":379,"end":384}]},{"name":"0dbb","type":4,"text":"I obfuscated the input to the sigmoid function for the input gate, just for simplicity.","markups":[],"layout":1,"metadata":{"id":"1*-mUDovQ8ovejmWPNoFSI1g.png","originalWidth":541,"originalHeight":79}},{"name":"f6be","type":1,"text":"Recall from our vanishing gradient article that the max output of sigmoid’s first order derivative is 0.25, and it’s something similar for tanh. This becomes textbook vanishing gradient problem. As we backprop through more and more cell states, the gradient terms become longer and longer, and this will definitely vanish. When they don’t vanish, they’ll be super minor contributions, so we can just leave them out for brevity.","markups":[]},{"name":"95ec","type":6,"text":"Sidenote: one person reached out to me unsure of why gradients with long terms — aka, that are equal to the product of a lot of terms — usually vanishes/explodes. Here’s what I said in response:","markups":[]},{"name":"eabd","type":6,"text":"“If you have long gradient terms, you probably have the vanishing gradient problem, unless you can guarantee those terms are around 1 each. If they’re not, it’ll explode or vanish. And, given the nature of the problems where this is an issue, it’s very unlikely they’ll be around 1 each. Especially if they are the output some non-linear function like sigmoid/tanh or their derivatives.","markups":[]},{"name":"944e","type":6,"text":"For example, let’s say the gradient term = k_1 × k_2 × k_3 × … × k_100. 100 terms in this product.","markups":[]},{"name":"1583","type":6,"text":"If each of these terms is, let’s say, around 0.5, then you have 0.5¹⁰⁰ = some absurdly low number. If you have each term be arond 1.5, then you have 1.5¹⁰⁰ which is some absurdly high number.\n\nWhen we introduce tanh/sigmoid and/or their derivatives in these huge products, you can guarantee that they’ll saturate and die off. As mentioned, the max for sigmoid’s first order derivative is 0.25, so just imagine something like 0.25¹⁰⁰.","markups":[]},{"name":"59c9","type":1,"text":"Ultimately, the reason I obfuscate these terms that vanish in the derivative is because I would like to show the effect of the forget gate on gradient flow now. If I included the other terms, the same implications would be present, but the math would just take longer to type out and render.","markups":[]},{"name":"e361","type":1,"text":"Because ƒw = i ⊙ g, we can redraw our diagram showing that ƒw won’t make any contributions to the gradient flow back. Again — ƒw does, but it’s effectively negligible, so we can just exclude it from our updated gradient flow diagram, which follows:","markups":[{"type":1,"start":8,"end":18},{"type":1,"start":59,"end":62},{"type":1,"start":126,"end":128}]},{"name":"3217","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*RsHULZCgY6p-5bKkE99Q-Q.png","originalWidth":1095,"originalHeight":453}},{"name":"8811","type":1,"text":"But wait! This doesn’t look good; the gradients have to multiply by this f_t gate at each timestep. Before, they didn’t have to multiply by anything (or, in other words, they multiplied by 1) and flowed past super easily.","markups":[{"type":1,"start":73,"end":76}]},{"name":"9feb","type":1,"text":"Machine learning researchers coined a name for the type of function we had before we introduced the forget gate where the derivative of one cell state w.r.t. the previous is 1.0: “Constant Error Carousel” (CEC). With our new function, the derivative is equal to f. You’ll see this referred to as a “linear carousel” in papers.","markups":[{"type":1,"start":174,"end":177},{"type":1,"start":262,"end":263}]},{"name":"fdbb","type":1,"text":"Before we introduced a forget gate — where all we had was the additive interaction from ƒw — our cell state function was a CEC:","markups":[{"type":1,"start":88,"end":91}]},{"name":"2bf9","type":4,"text":"A CEC — same as before, but no forget gate.","markups":[],"layout":1,"metadata":{"id":"1*9O__qOVOK1wxJDFy6m4YAQ.png","originalWidth":208,"originalHeight":84}},{"name":"61b2","type":1,"text":"The derivative of this cell state w.r.t. the previous one, again as long as we don’t backprop through the i and g gates, is just 1. That’s why gradients flow back super comfortably, without vanishing at all. Basically, for a CEC to exist in this context, the coefficient of c_t-1 needs to be 1.","markups":[{"type":1,"start":106,"end":108},{"type":1,"start":112,"end":113},{"type":1,"start":274,"end":280}]},{"name":"7ab2","type":1,"text":"Once we introduced this multiplicative interaction (for good reason), we got a linear carousel; the coefficient of c_t-1 is f. So, in our case, when f = 1 (when we’re not going to forget) our function becomes a CEC, and our gradients will pretty much never vanish. If it’s close to 0, though, the gradient term will immediately die. Gradients will stay on the carousel for a while until the forget gate is triggered; the effect on the gradient is like a step function, in that it’s constant with a value of 1 and then drops off to zero/dies when we have f ≈ 0.","markups":[{"type":1,"start":115,"end":121},{"type":1,"start":124,"end":125},{"type":1,"start":149,"end":154},{"type":1,"start":554,"end":559}]},{"name":"3337","type":1,"text":"Intuitively, this seems problematic. Let’s do some math to investigate:","markups":[]},{"name":"0955","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*UbqEhAyW7bMv_tDf-cvuWg.png","originalWidth":372,"originalHeight":164}},{"name":"2abc","type":1,"text":"The derivative of a cell state to the previous is f_t. The derivative of a cell state to two prior cell states is f_t ⊙ f_t-1. Thus:","markups":[{"type":1,"start":50,"end":53},{"type":1,"start":114,"end":125}]},{"name":"1596","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*p9OndETS7tR-zUU-1TuaTw.png","originalWidth":603,"originalHeight":83}},{"name":"8238","type":1,"text":"As we backpropagate through time, these forget gates keep chaining up and multiplying together to form the overall gradient term.","markups":[]},{"name":"6f12","type":1,"text":"Now, imagine an LSTM with 100 timesteps. If we wanted to get the derivative of the error w.r.t. a weight like W_xi, to optimize it, remember that with BPTT we add up or average all the gradients from the different timesteps:","markups":[{"type":1,"start":110,"end":114}]},{"name":"d867","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*rhb_2DO5MulJvzV9QxasMg.png","originalWidth":692,"originalHeight":101}},{"name":"a73a","type":1,"text":"OK. Now let’s look at an early (in time) term, like the gradient propagated from the error to the third cell:","markups":[]},{"name":"9f34","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*fAaYlJPgsPjRGJGoyc8XCw.png","originalWidth":106,"originalHeight":79}},{"name":"9972","type":1,"text":"Remember that J is an addition of errors from Y individual outputs, so we backpropagate through each of the outputs first:","markups":[{"type":1,"start":14,"end":16},{"type":1,"start":45,"end":48}]},{"name":"7c21","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*vsybuvtlGl-cQqUI1Pqbag.png","originalWidth":609,"originalHeight":164}},{"name":"4355","type":1,"text":"The first few terms, where we backprop y_k to c_3 where k \x3c 3, would just be equal to zero because c_3 only exists after these outputs have been computed.","markups":[{"type":1,"start":39,"end":43},{"type":1,"start":46,"end":50},{"type":1,"start":56,"end":61},{"type":1,"start":99,"end":103}]},{"name":"e686","type":1,"text":"Let’s assume that Y = 100 and continue with our assumption that t = 100 (so each timestep gives rise to an output), for simplicity. With this, let’s now look at the last term in this sum.","markups":[{"type":1,"start":18,"end":26},{"type":1,"start":64,"end":72}]},{"name":"53d1","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*JJIQxpb1mHjDn5KaoOKQkA.png","originalWidth":501,"originalHeight":79}},{"name":"6208","type":1,"text":"That’s a lot of forget gates chained together. If one of these forget gates is [approximately] zero, the whole gradient dies. If these also tend to be a small number between 0 and 1, the whole thing will vanish, and c_3 won’t make any contributions to the gradient here.","markups":[{"type":1,"start":47,"end":124},{"type":1,"start":216,"end":220}]},{"name":"fb13","type":1,"text":"This isn’t intrinsically an issue though! Because, when a forget gate is zero, it means that cell is no longer making any contributions past that point. If f_4 is zero, then any y outputs at/past timestep 4 won’t be influenced by c_3 (as well as c_2 and c_1) because we “erased” it from memory. Therefore that particular gradient should be zero. If y_80 is zero, then any outputs at/past timestep 80 won’t be influenced by c_1, c_2, … , c_79. Same story here. If these forget gates are between 0 and 1, then the influence of our cell decays over time anyways, and our gradients will be very small, so they’ll reflect that. Gers 1999 calls this “releasing resources”.","markups":[{"type":3,"start":623,"end":632,"href":"https://pdfs.semanticscholar.org/1154/0131eae85b2e11d53df7f1360eeb6476e7f4.pdf","title":"","rel":"nofollow","anchorType":0},{"type":1,"start":156,"end":160},{"type":1,"start":178,"end":179},{"type":1,"start":230,"end":234},{"type":1,"start":246,"end":250},{"type":1,"start":254,"end":257},{"type":1,"start":349,"end":353},{"type":1,"start":423,"end":441},{"type":2,"start":11,"end":25}]},{"name":"b6f8","type":1,"text":"Cell c_3 will still contribute to the overall gradient, though. For example, take this term:","markups":[{"type":1,"start":5,"end":9}]},{"name":"0851","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*KZjK3wcZpYG_qnjkMB1HkA.png","originalWidth":474,"originalHeight":79}},{"name":"8736","type":1,"text":"Here, we’re looking at y_12 instead of y_100. Chances are that, if you have a sequence of length 100, your 100th cell state isn’t drawing from your 3rd; the forget gate would have been triggered at some point by then. However, the 12th cell state probably will still be drawing from the ones before it.","markups":[{"type":1,"start":23,"end":28},{"type":1,"start":39,"end":44}]},{"name":"5cd8","type":1,"text":"If we decide not to forget in the first 12 timesteps, ie. f_1 … f_12 are each not far from 1, then c_3 would have more influence over y_12 and the error that stems from y_12. Thus, the gradient would not vanish and c_3 still contributes to update W_xi, it just doesn’t contribute a gradient where it’s not warranted to (that is, where it doesn’t actually contribute to any activation, because it’s been forgotten). To summarize: one activated forget gate will indeed kill off gradient flow to cell(s), but that is a good thing because the network is learning that that gradient from the future has no benefit and is completely irrelevant to those particular cell(s), since those cells have been forgotten by then. In practice, different cells learn different ranges of context, some short, some long. This is a benefit for LSTMs.","markups":[{"type":1,"start":58,"end":69},{"type":1,"start":99,"end":103},{"type":1,"start":134,"end":139},{"type":1,"start":169,"end":173},{"type":1,"start":215,"end":218},{"type":1,"start":247,"end":251},{"type":2,"start":811,"end":819}]},{"name":"7b24","type":1,"text":"So, given a gradient between two cell states in time, when all of these forget gates are [approximately] equal to 1, the gradient signal will remain stable, because we’re multiplying by 1 at each timestep — effectively, not multiplying by anything at all. In such a case, our gradient flow diagram would look like this:","markups":[]},{"name":"9b4b","type":4,"text":"It’s… it’s beautiful!","markups":[],"layout":1,"metadata":{"id":"1*rBJm9F6zz8drQnDlWd7wvQ.png","originalWidth":2190,"originalHeight":694}},{"name":"f12e","type":1,"text":"The gradient will have literally zero interactions or disturbances, and will just flow through like it’s driving 150 mph on an empty countryside America highway. The beauty of CECs is that they’re always like this.","markups":[{"type":2,"start":197,"end":204}]},{"name":"87cf","type":1,"text":"But, let’s get back to reality. LSTMs aren’t CECs. One disadvantage of these forget gates chaining together is that it could block learning. That is, when we set out to train our LSTM, the forget gates have not been learned; we have to learn them while we learn everything else. So, if they all start around 0, no gradients will flow through our cell states when we perform BPTT, and learning won’t happen at all.","markups":[]},{"name":"3db3","type":1,"text":"The obvious solution is to set the forget gate bias to a very large value when training, so it starts at 1 instead of 0 (because y = 1 is to the far right of the sigmoid function, so adding to the input will ensure ~1 will be the output). In early stages of training, the forget gates equalling/approximating 1 will result in learning not being blocked. So many papers do this and mention it explicitly such that this forget gate bias could even be considered a hyperparameter.","markups":[{"type":1,"start":129,"end":134}]},{"name":"d763","type":1,"text":"By introducing forget gates, we stray from CECs and thus the guarantee that our gradients will never ever vanish. But, again, we do it for good reason. And when gradients vanish it’s because we chose to forget that cell — so it’s not necessarily a bad thing. We just need to make sure the forget gates don’t block learning in initial stages of training.","markups":[]},{"name":"e1c7","type":1,"text":"We can try computing some more derivatives, just for fun! Let’s sub in real values for the timesteps, backprop across more than one timestep, and do it for a gate this time.","markups":[]},{"name":"62f7","type":1,"text":"We’ll expand c_4 and express it in terms of our gates only. In the process, each c_t, except c_1, will collapse into a few interactions between the f, i, and g gate:","markups":[{"type":1,"start":13,"end":17},{"type":1,"start":81,"end":86},{"type":1,"start":93,"end":96},{"type":1,"start":148,"end":149},{"type":1,"start":151,"end":152},{"type":1,"start":153,"end":154},{"type":1,"start":157,"end":160}]},{"name":"26eb","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*2LZxa4YAMGCJqOrirI1-_w.png","originalWidth":669,"originalHeight":166}},{"name":"5092","type":1,"text":"Now, let’s get the derivative of c_4 with respect to one of the earliest possible gates, like g_2. In the expression above, this turns out to just be the coefficient of g_2:","markups":[{"type":1,"start":33,"end":36},{"type":1,"start":94,"end":97},{"type":1,"start":169,"end":172}]},{"name":"c936","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*rHDurdaN9SKnChWfyVk38w.png","originalWidth":205,"originalHeight":79}},{"name":"708a","type":1,"text":"We experience the same neatness here as with the cell state backprop! It makes complete sense that the gradient would be i_2 ⊙ f_3 ⊙ f_4, since i_2 controls what influence g_2 has over c_2, f_3 controls what influence c_2 has on c_3, and f_4 controls what influence c_3 has over c_4. Notice the chaining up of the forget gates 👻; everything about the carousels I just talked about — and what they imply about vanishing gradients — applies here.","markups":[{"type":1,"start":121,"end":126},{"type":1,"start":127,"end":136},{"type":1,"start":144,"end":148},{"type":1,"start":172,"end":176},{"type":1,"start":185,"end":188},{"type":1,"start":190,"end":194},{"type":1,"start":218,"end":222},{"type":1,"start":229,"end":232},{"type":1,"start":238,"end":242},{"type":1,"start":266,"end":270},{"type":1,"start":279,"end":282}]},{"name":"6229","type":1,"text":"I’ll leave it up to you to derive something similar for the other gates.","markups":[]},{"name":"f15d","type":1,"text":"And that’s it! That’s why LSTMs rock their socks off when it comes to keeping their gradients in check,.","markups":[]},{"name":"627a","type":1,"text":"Here’s a neat GIF showing a visual representation of the gradients that exist at each timestep, starting from timestep 128 and going all the way to the first, during backprop. More noise represents greater values:","markups":[]},{"name":"2414","type":11,"text":"Super highway indeed. imgur.com/gallery/vaNahKE.","markups":[{"type":3,"start":22,"end":47,"href":"http://imgur.com/gallery/vaNahKE","title":"","rel":"noopener","anchorType":0}],"layout":3,"iframe":{"mediaResourceId":"c239248e2e0b9a4aadc7b43d8c08ca12","iframeWidth":854,"iframeHeight":480,"thumbnailUrl":"https://i.embed.ly/1/image?url=http%3A%2F%2Fi.imgur.com%2FvaNahKEh.jpg&key=4fce0568f2ce49e8b54624ef71a8a5bd"}},{"name":"868a","type":1,"text":"As you can see, the vanilla RNN’s gradients die off way quicker than the LSTM’s. The RNN is almost immediate in comparison. LSTMs seem like a super highway indeed, although it does seem that they do vanish. In this diagram, it can be suggested that the gradients perhaps die for the LSTM eventually because we chose to forget early cell states; again, this depends on the application at hand, and is learnable. (I’m not sure if this GIF uses truncated backprop, so that could be another thing. In general, I don’t know the context of this GIF.) Also, part of the gradient signal definitely vanishes—it’s the signals that pass through the f/i/g gates that we looked at earlier and obfuscated from the cell state→cell state derivative. We showed they would vanish because of tanh/sigmoid derivatives; initially, these signals will make a fairly significant contribution, but over time they’ll get smaller and smaller. That’s the explanation for this GIF.","markups":[{"type":1,"start":638,"end":644},{"type":2,"start":400,"end":409}]},{"name":"2b66","type":1,"text":"Exploding gradients is still an issue, though. Recall that when we have a bunch of gradient terms added together, if some of them vanish it doesn’t mean the whole thing will vanish (versus if they were multiplied together). However, if some of the gradients explode, the whole thing explodes; x + 0 = x, but x + ∞ = ∞. If cell states become unstable and grow too much in some rare scenario, then our gradients could explode. In such a case we’d need to implement gradient clipping, which is where we choose some arbitrary threshold that gradients cannot be larger than; so, grad = min(grad, clip_threshold). This would enable the LSTM to deal with such cases without essentially collapsing. Many successful LSTM applications use gradient clipping.","markups":[{"type":10,"start":574,"end":606},{"type":1,"start":312,"end":313},{"type":1,"start":316,"end":317}]},{"name":"69b6","type":1,"text":"Usually, though, exploding gradients are avoided because sooner or later the forget gate in the carousel is triggered and we reset the memory.","markups":[]},{"name":"3679","type":1,"text":"There are variants of LSTMs. People have tried modifying the model, like computing the hidden state without using tanh activation (so h_t = o ⊙ c_t) or ditching the i input gate and only using g, since that would still satisfy the -1 to 1 range. The results didn’t change by much.","markups":[{"type":1,"start":134,"end":147},{"type":1,"start":165,"end":167},{"type":1,"start":193,"end":194}]},{"name":"dc3e","type":1,"text":"In fact, some researchers even applied evolutionary algorithms to spawn and test a bunch of variants on the standard LSTM equations. Most of the good ones just worked roughly the same.","markups":[]},{"name":"c7f8","type":1,"text":"This highlights an issue with LSTMs — they are definitely fairly handwavy. We use them because their architecture allows us to prevent gradients from vanishing such that we can learn long-term dependencies, but there’s not much theoretical or empirical backing for them. ANNs and RNNs make sense in that they’re biologically inspired and that they’re essentially just deep composite functions that have parameters we can optimize. LSTMs stray so far from statistical methods and introduce complex concepts/architectures that work but aren’t necessarily justified from the get-go. Fully understanding why LSTMs work so well and coming up with better/simpler architectures is a hot topic of research right now.","markups":[]},{"name":"4bca","type":1,"text":"There are also other variants of RNNs, similar to LSTMs, like GRUs (Gated Recurrent Units). LSTM is still the king, but GRUs in particular have grown in popularity and are seen in many recent, well-respected research papers. It’s a must learn next to LSTMs, but this article would get too bloated with it. TL;DR: GRUs have a less complex architecture than LSTMs but achieves similar results, and they can control the flow of information without requiring a memory cell. And, they’re fairly new. (See, told you “coming up with better/simpler architectures is a hot topic of research right now” is true!)","markups":[{"type":3,"start":62,"end":66,"href":"https://en.wikipedia.org/wiki/Gated_recurrent_unit","title":"","rel":"noopener","anchorType":0}]},{"name":"c57a","type":3,"text":"Yay RNNs!","markups":[]},{"name":"8a09","type":1,"text":"Wanna see a couple cool things? Practical applications were talked about in the first section, and the next section will walk through more technical applications of RNNs ie. in recent research papers. This section, instead, contains some fun things RNNs have done or produced that’ll ease yourself from the hell that was actually understanding them. As simply as we try to convey things on this blog, LSTMs be LSTMs.","markups":[]},{"name":"7337","type":1,"text":"Sidenote: now, don’t be frightened by “RNNs”. Do be frightened by “vanilla RNNs”, in most contexts, but we use RNN as an umbrella term, and it most often refers to an LSTM or a similar variant like GRU.","markups":[{"type":1,"start":46,"end":49},{"type":2,"start":46,"end":49}]},{"name":"f30f","type":1,"text":"Many if not all of these are taken from Andrej Karpathy’s CS231n lecture, or his blog post on the same subject:","markups":[{"type":3,"start":58,"end":72,"href":"https://www.youtube.com/watch?v=cO0a0QYmFm8&index=10&list=PLlJy-eBtNFt6EuMxFYRiNRS07MCWN5UIA","title":"","rel":"noopener","anchorType":0}]},{"name":"d19b","type":14,"text":"The Unreasonable Effectiveness of Recurrent Neural Networks\nMusings of a Computer Scientist.karpathy.github.io","markups":[{"type":3,"start":0,"end":110,"href":"http://karpathy.github.io/2015/05/21/rnn-effectiveness/","title":"http://karpathy.github.io/2015/05/21/rnn-effectiveness/","rel":"","anchorType":0},{"type":1,"start":0,"end":59},{"type":2,"start":60,"end":92}],"mixtapeMetadata":{"mediaResourceId":"3019eda2afc61d3398ef9b0a1762edf9","thumbnailImageId":"0*7sIxt7RqO7deGldw.","href":"http://karpathy.github.io/2015/05/21/rnn-effectiveness/"}},{"name":"83d4","type":1,"text":"You should most certainly visit either his blog post or lecture for info on exactly how these experiments were conducted, and for more interpretation on their results. Taking a look at the ‘Visualizing the predictions and the “neuron” firings in the RNN’ section would also be helpful to gain more insight and intuition on how RNNs work and learn over time.","markups":[{"type":1,"start":11,"end":25}]},{"name":"c051","type":1,"text":"A recurrent neural network generated this body of text, after it “read” a bunch of Shakespeare:","markups":[]},{"name":"802f","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*BkvFHx8nYL1-NHmzZ_BbCQ.png","originalWidth":574,"originalHeight":619}},{"name":"62d6","type":1,"text":"Similarly, Karpathy gave an LSTM a lot of Paul Graham’s startup advice and life wisdom to read, and it produced this:","markups":[]},{"name":"3654","type":7,"text":"“The surprised in investors weren’t going to raise money. I’m not the company with the time there are all interesting quickly, don’t have to get off the same programmers. There’s a super-angel round fundraising, why do you can do. If you have a different physical investment are become in people who reduced in a startup with the way to argument the acquirer could see them just that you’re also the founders will part of users’ affords that and an alternation to the idea. [2] Don’t work at first member to see the way kids will seem in advance of a bad successful startup. And if you have to act the big company too.”","markups":[]},{"name":"ee9f","type":1,"text":"A lot of relevant terminology, but it doesn’t really… come together 😖.","markups":[]},{"name":"1fc3","type":1,"text":"An LSTM can even generate valid XML, after reading Wikipedia!:","markups":[]},{"name":"89df","type":8,"text":"\x3cpage\x3e\n  \x3ctitle\x3eAntichrist\x3c/title\x3e\n  \x3cid\x3e865\x3c/id\x3e\n  \x3crevision\x3e\n    \x3cid\x3e15900676\x3c/id\x3e\n    \x3ctimestamp\x3e2002-08-03T18:14:12Z\x3c/timestamp\x3e\n    \x3ccontributor\x3e\n      \x3cusername\x3eParis\x3c/username\x3e\n      \x3cid\x3e23\x3c/id\x3e\n    \x3c/contributor\x3e\n    \x3cminor /\x3e\n    \x3ccomment\x3eAutomated conversion\x3c/comment\x3e\n    \x3ctext xml:space=\"preserve\"\x3e#REDIRECT [[Christianity]]\x3c/text\x3e\n  \x3c/revision\x3e\n\x3c/page\x3e","markups":[{"type":10,"start":0,"end":365}]},{"name":"7056","type":1,"text":"After a similar LSTM read the raw Latex version of an advanced algebra textbook, it came up with this — put frankly — fancy looking bogus. Let’s be real, you could definitely believe this was actual math 😜:","markups":[]},{"name":"221d","type":11,"text":"","markups":[],"layout":1,"iframe":{"mediaResourceId":"32e75fb2f7f388775689a155c5c27d86","iframeWidth":600,"iframeHeight":780}},{"name":"e129","type":1,"text":"An LSTM also read the Linux source code, and tried to write some code of its own:","markups":[]},{"name":"ce46","type":8,"text":"/*\n * Increment the size file of the new incorrect UI_FILTER group information\n * of the size generatively.\n */\nstatic int indicate_policy(void)\n{\n  int error;\n  if (fd == MARN_EPT) {\n    /*\n     * The kernel blank will coeld it to userspace.\n     */\n    if (ss-\x3esegment \x3c mem_total)\n      unblock_graph_and_set_blocked();\n    else\n      ret = 1;\n    goto bail;\n  }\n  segaddr = in_SB(in.addr);\n  selector = seg / 16;\n  setup_works = true;\n  for (i = 0; i \x3c blocks; i++) {\n    seq = buf[i++];\n    bpf = bd-\x3ebd.next + i * search;\n    if (fd) {\n      current = blocked;\n    }\n  }\n  rw-\x3ename = \"Getjbbregs\";\n  bprm_self_clearl(&iv-\x3eversion);\n  regs-\x3enew = blocks[(BPF_STATS \x3c\x3c info-\x3ehistoridac)] | PFMR_CLOBATHINC_SECONDS \x3c\x3c 12;\n  return segtable;\n}","markups":[{"type":10,"start":0,"end":745},{"type":1,"start":112,"end":118},{"type":1,"start":119,"end":122},{"type":1,"start":123,"end":138},{"type":1,"start":139,"end":143},{"type":1,"start":149,"end":152},{"type":1,"start":162,"end":164},{"type":1,"start":169,"end":171},{"type":1,"start":255,"end":257},{"type":1,"start":261,"end":263},{"type":1,"start":271,"end":272},{"type":1,"start":327,"end":331},{"type":1,"start":342,"end":343},{"type":1,"start":351,"end":355},{"type":1,"start":376,"end":377},{"type":1,"start":405,"end":406},{"type":1,"start":411,"end":412},{"type":1,"start":431,"end":432},{"type":1,"start":441,"end":444},{"type":1,"start":448,"end":449},{"type":1,"start":455,"end":456},{"type":1,"start":466,"end":468},{"type":1,"start":480,"end":481},{"type":1,"start":487,"end":489},{"type":1,"start":500,"end":501},{"type":1,"start":504,"end":506},{"type":1,"start":514,"end":515},{"type":1,"start":518,"end":519},{"type":1,"start":532,"end":534},{"type":1,"start":556,"end":557},{"type":1,"start":581,"end":583},{"type":1,"start":588,"end":589},{"type":1,"start":623,"end":624},{"type":1,"start":626,"end":628},{"type":1,"start":644,"end":646},{"type":1,"start":650,"end":651},{"type":1,"start":670,"end":672},{"type":1,"start":677,"end":679},{"type":1,"start":692,"end":693},{"type":1,"start":718,"end":720},{"type":1,"start":727,"end":733},{"type":2,"start":0,"end":111},{"type":2,"start":188,"end":250}]},{"name":"da41","type":1,"text":"SUPERINTELLIGENCE MUCH‽ SELF-RECURSIVE IMPROVEMENT MUCH‽ THE END OF THE UNIVERSE MUCH‽","markups":[{"type":1,"start":0,"end":86},{"type":2,"start":0,"end":86}]},{"name":"7094","type":1,"text":"Nope. Just some code doesn’t compile or make any sense. It even has its own bogus comments!","markups":[]},{"name":"b3fc","type":1,"text":"Generating music? Easy! A fun watch:","markups":[]},{"name":"eae8","type":11,"text":"","markups":[],"layout":1,"iframe":{"mediaResourceId":"8de70b7fa3e5cb979099278112052953","iframeWidth":854,"iframeHeight":480,"thumbnailUrl":"https://i.embed.ly/1/image?url=https%3A%2F%2Fi.ytimg.com%2Fvi%2FA2gyidoFsoI%2Fhqdefault.jpg&key=4fce0568f2ce49e8b54624ef71a8a5bd"}},{"name":"c80a","type":1,"text":"A more informative watch:","markups":[]},{"name":"565f","type":11,"text":"","markups":[],"layout":1,"iframe":{"mediaResourceId":"3f992ef4ac506dafa8d2d8badfc31dc2","iframeWidth":854,"iframeHeight":480,"thumbnailUrl":"https://i.embed.ly/1/image?url=https%3A%2F%2Fi.ytimg.com%2Fvi%2FaSr8_QQYpYM%2Fhqdefault.jpg&key=4fce0568f2ce49e8b54624ef71a8a5bd"}},{"name":"6518","type":1,"text":"Something even cooler and… creepier (seriously, the results after the first couple iterations of training are so unsettling):","markups":[]},{"name":"7c5c","type":11,"text":"","markups":[],"layout":1,"iframe":{"mediaResourceId":"b5f70a5d514e61ad4646217c70974843","iframeWidth":854,"iframeHeight":480,"thumbnailUrl":"https://i.embed.ly/1/image?url=https%3A%2F%2Fi.ytimg.com%2Fvi%2FNG-LATBZNBs%2Fhqdefault.jpg&key=4fce0568f2ce49e8b54624ef71a8a5bd"}},{"name":"7eb1","type":3,"text":"In Practice","markups":[]},{"name":"a21c","type":1,"text":"So we’ve seen how RNNs work in theory; now where do they fit in in practice?","markups":[]},{"name":"e928","type":1,"text":"As it turns out, recurrent neural networks can do a whole lot. I’ll try to cover a few of the important, significant, and interesting uses that have cropped up over the last few years.","markups":[]},{"name":"e30a","type":13,"text":"Bidirectional Recurrent Neural Networks","markups":[]},{"name":"5985","type":1,"text":"The Problem: Giving the network access to a sequence of vectors is fine and dandy, but what if we want our output at time t to be conditioned on an input vector that comes at a later timestep? Take the example of speech recognition, where our input vectors are some kind of audio features at time t and the output is the predicted phoneme at that time. In our traditional RNN architecture, the output at time t is conditioned only on input vectors 1..t, but as it turns out future information might be useful too. The sounds at time step t+1 (and maybe t+2, t+3, …) are likely part of the same phoneme, and therefore could help us make more accurate predictions. But our network won’t have access to them until we already output a prediction at time t. That’s bad.","markups":[{"type":3,"start":331,"end":338,"href":"https://en.wikipedia.org/wiki/Phoneme","title":"","rel":"noopener","anchorType":0},{"type":1,"start":0,"end":11},{"type":1,"start":122,"end":123},{"type":1,"start":297,"end":298},{"type":1,"start":409,"end":410},{"type":1,"start":448,"end":452},{"type":1,"start":538,"end":542},{"type":1,"start":553,"end":556},{"type":1,"start":558,"end":561},{"type":1,"start":750,"end":751},{"type":2,"start":338,"end":339},{"type":2,"start":426,"end":430}]},{"name":"60ef","type":1,"text":"The Solution: We essentially “double up” each RNN neuron into two independent neurons — a “forward” neuron and a “backward” neuron. The forward neuron is the same as a regular RNN neuron, which gets inputs 0..T sequentially, updating its internal state and outputting some value at each time step along the way. The backward neuron follows the same general principle, but it sees the input vectors in reverse order.","markups":[{"type":1,"start":0,"end":12},{"type":1,"start":206,"end":210}]},{"name":"e2ef","type":1,"text":"We’ll look at an example to make sense of all this.","markups":[]},{"name":"2749","type":4,"text":"This is a typical recurrent neural network: at each timestep, the hidden state is updated based on the latest input.","markups":[],"layout":4,"metadata":{"id":"1*Vsvw39SW0xEwRLijLRb3qg.png","originalWidth":404,"originalHeight":504}},{"name":"89e5","type":4,"text":"This is a bidirectional recurrent neural network. There are two neurons: one that takes inputs like normal, and one that takes them in reverse. Their output is combined to produce one output.","markups":[],"layout":4,"metadata":{"id":"1*JZNjmHjYFVcHrPKTDmvoXQ.png","originalWidth":606,"originalHeight":504}},{"name":"ea03","type":1,"text":"Let’s walk through this timestep-by-timestep. At t=0, our vanilla RNN cell takes the input, updates its hidden state, and outputs a value. Now let’s look at the BiRNN: the “forward” half of our BiRNN neuron does exactly the same thing, but the “backward” half looks through all of our inputs — in reverse order, t=T..0 — and updates its hidden state with each one. Then when we get to the t=0 input vector, it updates its hidden state one last time and outputs a final value. We then take this final output value and combine it with the “forward” half (“combine” is pretty loosely-defined, usually just by concatenation or addition). Moving on to t=1, our “forward” part reads in the next input, updates state, and outputs another value. Combined with the second-to-last output of our “backward” counterpart, and we have the second output of our BiRNN neuron. Rinse and repeat.","markups":[{"type":1,"start":49,"end":52},{"type":1,"start":312,"end":318},{"type":1,"start":389,"end":392},{"type":1,"start":647,"end":650}]},{"name":"1bf8","type":1,"text":"And that’s the general idea. Neat, right? BiRNNs (and their more adept cousin, BiLSTMs) are used all over the place. Maybe we’ll see them popping up in some of the other case studies that we’ll be looking at.","markups":[]},{"name":"7549","type":13,"text":"Autoencoders","markups":[]},{"name":"cdb3","type":1,"text":"Remember when we talked about autoencoders? Turns out we can use RNNs there too!","markups":[{"type":3,"start":30,"end":42,"href":"https://ayearofai.com/lenny-2-autoencoders-and-word-embeddings-oh-my-576403b0113a#.oevzdnnnp","title":"","rel":"noopener","anchorType":0}]},{"name":"107c","type":1,"text":"Let’s refresh: what is an autoencoder? Put simply, it’s a clever way of tricking a neural network to learn a useful representation of some data. Let’s say we have a dataset of images of faces, and we want to compress the thousands of numbers representing RGB values of pixels down into a 500-dimensional latent vector. We construct a network as such, where the middle layer has 500 neurons:","markups":[]},{"name":"08e3","type":4,"text":"https://en.wikipedia.org/wiki/Autoencoder#/media/File:Autoencoder_structure.png","markups":[{"type":3,"start":0,"end":79,"href":"https://en.wikipedia.org/wiki/Autoencoder#/media/File:Autoencoder_structure.png","title":"","rel":"nofollow noopener noopener","anchorType":0}],"layout":1,"metadata":{"id":"0*M1bVZtZ6UPTyXoiy.","originalWidth":677,"originalHeight":506}},{"name":"fe44","type":1,"text":"…and train it to reproduce the input in the output.","markups":[]},{"name":"46d5","type":1,"text":"Let’s explore this idea a little further. Imagine that data is flowing through our network, starting with the input layer and through each subsequent layer. We can view each layer as performing a transformation, converting our input to another vector, and then that vector into another, until we get our output. If we train our network to reproduce the input, that means that each intermediate vector must still represent the same information as the input, in some form or another. Essentially, the activations of each layer are a new representation of our input vector. If our network trains well, we can convert a 10,000-dimensional vector of pixel values into a 500-dimensional vector of image features which can be converted back into a 10,000-dimensional vector of pixel values that approximates what the input would have been.","markups":[]},{"name":"f1d3","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"0*RP5VZyqDJ9JI5wBk.","originalWidth":201,"originalHeight":34}},{"name":"1008","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"0*Tc8mc_NWMmQZ15ND.","originalWidth":200,"originalHeight":34}},{"name":"c1de","type":1,"text":"Let’s make this a tad more concrete. We have two functions, f and g. f is our encoder, mapping from an n-long vector to an m-long vector. (n is the size of our input, m is the size of our latent representation.) g is our decoder, which maps back from an m-long vector to an n-long vector. In the normal autoencoder setting, both f and g are neural networks trained jointly (or different parts of a single network, same thing really) to reconstruct x.","markups":[{"type":1,"start":60,"end":61},{"type":1,"start":66,"end":67},{"type":1,"start":69,"end":70},{"type":1,"start":103,"end":104},{"type":1,"start":123,"end":124},{"type":1,"start":139,"end":140},{"type":1,"start":167,"end":168},{"type":1,"start":212,"end":213},{"type":1,"start":254,"end":255},{"type":1,"start":274,"end":275},{"type":1,"start":329,"end":330},{"type":1,"start":335,"end":336}]},{"name":"f029","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"0*1R_heM-ujpUvlbGM.","originalWidth":182,"originalHeight":37}},{"name":"1680","type":1,"text":"So, where do RNNs fit in? Let’s say our inputs are now sequences of vectors instead of a single vector. We can use a similar concept, with both the encoder and decoder represented using an RNN. Here’s how it works: we feed our input sequence into the encoder RNN. With each input vector of the sequence, this encoder updates its internal state. Eventually, once it has seen the entire input, we have some final network internal state which represents our entire input sequence. Neat! Now, we make the hidden state of our decoder RNN the initial hidden state of our encoder, and ask it to spit out a sequence. Ideally, it spits out something close to what the initial sequence was.","markups":[{"type":2,"start":251,"end":262},{"type":2,"start":309,"end":316},{"type":2,"start":521,"end":532},{"type":2,"start":565,"end":572}]},{"name":"b8c2","type":1,"text":"Going back to our math-y definitions, we see that it basically fits in to the same framework, except we have q n-long vectors going into f and coming out of g. So q n-long vectors go in to f, and a single m-long vector comes out. We then give this m-long vector back to g, which spits out q n-long vectors.","markups":[{"type":1,"start":109,"end":110},{"type":1,"start":137,"end":138},{"type":1,"start":157,"end":158},{"type":1,"start":163,"end":164},{"type":1,"start":165,"end":166},{"type":1,"start":189,"end":190},{"type":1,"start":205,"end":206},{"type":1,"start":248,"end":249},{"type":1,"start":270,"end":271},{"type":1,"start":289,"end":290},{"type":1,"start":291,"end":292}]},{"name":"5a64","type":1,"text":"That was a lot of letters, but you get the idea (I hope).","markups":[]},{"name":"b829","type":1,"text":"Like much of deep learning, the concept itself is pretty simple, but the implications are pretty cool. We can take any sequence — a variable-length sequence, mind you — and convert it into a fixed-size vector. And then convert that back to a variable-length sequence.","markups":[]},{"name":"5546","type":1,"text":"It turns out this model is actually incredibly powerful, so let’s take a look at one particularly useful (and successful) application: machine translation.","markups":[]},{"name":"4be2","type":13,"text":"Neural Machine Translation","markups":[]},{"name":"9d22","type":1,"text":"Let’s take these ideas we just learned about sequence-to-sequence (or seq2seq, for short) RNNs and apply them to machine translation. We throw in a sequence of words in one language, and it outputs a sequence of words in another. Simple enough, right?","markups":[]},{"name":"cd0e","type":1,"text":"The model we’re going to look at specifically is Google’s implementation of NMT. You can read all the gory details in their paper, but for now why don’t I give you the watered-down version.","markups":[{"type":3,"start":115,"end":129,"href":"https://arxiv.org/pdf/1609.08144.pdf","title":"","rel":"noopener","anchorType":0}]},{"name":"3f1b","type":1,"text":"At it’s core, the GNMT architecture is just another seq2seq model. We have an encoder, consisting of 8 LSTM layers with skip connections (the first layer is bidirectional). We also have a decoder, once again containing 8 LSTM layers with skip connections. (A skip connection in a neural network is a connection which skips a layer and connects to the next available layer.) The decoder network outputs a probability distribution of words (well, sort of — we’ll talk more about that later), which we sample from to get our [translated] sentence. 🎉","markups":[]},{"name":"afc6","type":1,"text":"Here’s a scary diagram from the paper:","markups":[]},{"name":"6a00","type":4,"text":"https://arxiv.org/abs/1609.08144","markups":[{"type":3,"start":0,"end":32,"href":"https://arxiv.org/abs/1609.08144","title":"","rel":"nofollow noopener noopener","anchorType":0}],"layout":1,"metadata":{"id":"0*mk1BeF8ANMbAVOzD.","originalWidth":656,"originalHeight":363}},{"name":"a450","type":1,"text":"But there are a few other aspects to the GNMT that are important to note (there’s actually lots of interesting stuff going on in this architecture, so I really recommend you do read the paper).","markups":[{"type":3,"start":177,"end":191,"href":"https://arxiv.org/pdf/1609.08144.pdf","title":"","rel":"noopener","anchorType":0}]},{"name":"1fbc","type":1,"text":"Let’s turn our attention to the center of the above diagram. This is a critical part of the GNMT architecture (and GNMT is certainly not the first to use attention) which allows the decoder to focus on certain parts of the encoder’s output as it produces output. Specifically, the GNMT architecture differs from the traditional seq2seq model in that our encoder does not produce a single fixed-width vector (the final hidden state) representing the entire output. Instead, we actually look at the output from each time step, and each time step gives us some latent representation. While decoding, we combine all of these hidden vectors into one context vector using something called soft attention.","markups":[{"type":1,"start":370,"end":371},{"type":2,"start":15,"end":24},{"type":2,"start":367,"end":371},{"type":2,"start":645,"end":652},{"type":2,"start":683,"end":697}]},{"name":"369f","type":4,"text":"https://arxiv.org/abs/1609.08144","markups":[{"type":3,"start":0,"end":32,"href":"https://arxiv.org/abs/1609.08144","title":"","rel":"nofollow noopener noopener","anchorType":0}],"layout":1,"metadata":{"id":"0*ua03RdgdNWPw1_Jd.","originalWidth":704,"originalHeight":244}},{"name":"ddb5","type":1,"text":"More concretely, that works like this (at every decoder time step). We first look at the output of the first decoder layer from the last time step. Following the notation from the paper, we’ll call that yi-1. We also have a series of encoder outputs, x1…xM, one for each encoder timestep. For each encoder timestep, we give our special attention function yi-1 and xt and get back a single fixed-size vector st, which we then run through a softmax. So, we’ve converted our encoder information from that timestep (and some decoder information) into a single attention vector — this attention vector tells us which parts of the encoder output we should look at more closely. We multiply this attention vector by our encoder output xt, which has the effect of “focusing” more on certain values and less on others. Finally, we take the sum of those “focused” vectors over each encoder timestep to produce our attention context for this timestep ai, which is fed to every decoder layer.","markups":[{"type":1,"start":203,"end":207},{"type":1,"start":251,"end":256},{"type":1,"start":355,"end":360},{"type":1,"start":364,"end":366},{"type":1,"start":407,"end":409},{"type":1,"start":728,"end":730},{"type":1,"start":940,"end":942},{"type":2,"start":132,"end":136},{"type":2,"start":298,"end":305},{"type":2,"start":382,"end":406}]},{"name":"8681","type":1,"text":"Oh yeah, that attention function? That’s just yet another neural network.","markups":[{"type":2,"start":50,"end":57}]},{"name":"145a","type":1,"text":"Attention mechanisms like this one are pretty common in many deep learning architectures. This is an example of soft attention: we learn a distribution over our inputs and compute a weighted sum. This process is fully-differentiated, so we can use standard backpropogation to figure out how to train our attention model. Another possible mechanism is called hard attention, in which we select just one of the possible inputs and “focus” solely on that input. This process is not differentiable, so we need to use some other algorithm (usually some kind of reinforcement learning) to train a hard attention algorithm.","markups":[{"type":2,"start":358,"end":372}]},{"name":"10b2","type":1,"text":"GNMT combines all kinds of other cool ideas to achieve state-of-the-art results, including a wordpiece model which segments words into smaller “wordpieces” to help translate rarer words and neat parallelization techniques that let them train this monstrosity of an architecture in reasonable time.","markups":[]},{"name":"818e","type":1,"text":"A few months ago, Google put their GNMT model into production. Cutting-edge research is being implemented in the real world at an incredible rapid pace within the field of machine learning, and this is just one of countless examples.","markups":[{"type":3,"start":18,"end":61,"href":"https://blog.google/products/translate/found-translation-more-accurate-fluent-sentences-google-translate/","title":"","rel":"noopener","anchorType":0}]},{"name":"0c13","type":13,"text":"Long-Term Recurrent Convolutional Networks","markups":[]},{"name":"c3b8","type":1,"text":"(Not to be confused with LCRNs.)","markups":[]},{"name":"e750","type":1,"text":"The Problem: We have a sequence of images that we need to make predictions for. CNNs are good at processing images, RNNs are good at processing sequences…how do we put the two together?","markups":[{"type":1,"start":0,"end":11}]},{"name":"606b","type":1,"text":"The Solution: The solution proposed in this paper is as straightforward as you would expect: take your image, extract features using a CNN, and feed this feature vector to your LSTM.","markups":[{"type":3,"start":39,"end":49,"href":"https://arxiv.org/pdf/1411.4389.pdf","title":"","rel":"noopener","anchorType":0},{"type":1,"start":0,"end":12}]},{"name":"fdf8","type":4,"text":"https://arxiv.org/abs/1411.4389","markups":[{"type":3,"start":0,"end":31,"href":"https://arxiv.org/abs/1411.4389","title":"","rel":"nofollow noopener noopener","anchorType":0}],"layout":1,"metadata":{"id":"0*qiQ7DvCkHydXAFZ1.","originalWidth":355,"originalHeight":319}},{"name":"1c9f","type":1,"text":"That’s really all there is to it, and the reason it works is because (as we’ve seen before) CNNs are incredibly adept at converting raw pixel data to a more meaningful representation. This saves the LSTMs the problem of parsing through the pixels to figure out what’s going on in the image and allows the LSTM weights to focus on converting a vector of image features into some meaningful sequence (say, a caption). It’s the same reason that using a word embedding is often preferred to a one-hot vector when feeding in words to an NLP model: the more meaningful your representation is, the easier it is to make further predictions with it.","markups":[{"type":3,"start":70,"end":90,"href":"https://ayearofai.com/rohan-lenny-2-convolutional-neural-networks-5f4cd480a60b?gi=25a4c415cc58","title":"","rel":"noopener","anchorType":0}]},{"name":"b189","type":13,"text":"Image Captioning","markups":[]},{"name":"3e15","type":1,"text":"(To be confused with LCRNs!)","markups":[]},{"name":"7c86","type":1,"text":"So there has been a lot of really impressive work on image captioning lately, but I wanna give a special shout-out to this 2015 paper from Karpathy et al. It was one of the first ML papers I had ever read, and really got me excited about the field. And, it uses RNNs, so that’s cool too.","markups":[{"type":3,"start":118,"end":133,"href":"http://cs.stanford.edu/people/karpathy/cvpr2015.pdf","title":"","rel":"noopener","anchorType":0},{"type":2,"start":148,"end":153}]},{"name":"ca5d","type":1,"text":"The idea behind image captioning is kind of self-explanatory, but I’ll explain it anyway. You give the model an image, it gives you a caption. Which is kind of insane if you think about it — a computer can go from pixels to interpreting what it’s seeing, and from that generate real and grammatical sentences to explain what it sees. I still can’t really believe stuff like this actually works, but somehow it does.","markups":[]},{"name":"3370","type":1,"text":"The model described in this specific paper combines our old pal convolutional neural networks with our newly-discovered RNNs. Step 1 is to pass our image through a convolutional neural network and extract some features from the last fully-connected layer. This lets us convert our pixel representation of the image into something that’s hopefully a bit more meaningful. We take this image feature vector and use it to initialize the hidden state of our RNN.","markups":[]},{"name":"de23","type":1,"text":"This is where it gets cool. We feed our network a start token, and it gives us a word (more accurately, a distribution of words, which we sample to get the first word of our caption). We feed this word back as the next input, and sample another word from the output. And again, and again, and again, until we finally sample an end token and have a complete caption.","markups":[]},{"name":"0330","type":1,"text":"It’s not strictly necessary to feed the word that we sampled back to the network, but that’s pretty common practice to help the network condition its output on the previous word (the hidden state is critical for this too, of course). The results from this particular paper were pretty cool, you can see some of the results here.","markups":[{"type":3,"start":323,"end":327,"href":"http://cs.stanford.edu/people/karpathy/deepimagesent/","title":"","rel":"noopener","anchorType":0}]},{"name":"6ec4","type":13,"text":"Neural Machine Translation, Again","markups":[]},{"name":"aba2","type":1,"text":"Yes, NMTs are just that cool that I need to talk about them again.","markups":[]},{"name":"7a0e","type":1,"text":"The Problem: With our good ol’ GNMT architecture, we can train a massive model to convert from language A to language B. That’s great — except, if we support more than a hundred languages, we need to train more than 10,000 different language-pair models, each of which can take months to converge. That’s no good, and it’s the reason that when Google put GNMT in production, they only did so for eight language-pairs (still a monumental achievement). But…what if we didn’t need to train a separate model for each language pair? What if we could train one model for all the language pairs — impossible, right?","markups":[{"type":1,"start":0,"end":11},{"type":1,"start":216,"end":222}]},{"name":"1717","type":1,"text":"The Solution: Apparently it’s not impossible, and to make things even crazier, we can use the original GNMT architecture without modification. The only real change is that we prepend a special language token to the beginning of each sequence telling it what language to translate to. (We also use one shared wordpiece model for all language, instead of one per language pair.)","markups":[{"type":1,"start":0,"end":12},{"type":1,"start":79,"end":141}]},{"name":"90b7","type":1,"text":"So we’ve condensed tens of thousands of NMT models into a single model that is able to translate to and from any language it was trained on. The paper elaborates on the implications and benefits of this more than I will, but to summarize:","markups":[{"type":3,"start":141,"end":150,"href":"https://arxiv.org/pdf/1611.04558.pdf","title":"","rel":"noopener","anchorType":0}]},{"name":"2db1","type":9,"text":"One model instead of tens of thousands. Months of training time saved, simpler production deployment, fewer parameters — simplicity wins out over complexity.","markups":[]},{"name":"66a7","type":9,"text":"We might have more training data for some language pairs than others. When we have separate models for each language pair, this means that the pairs with less data will have significantly poorer performance. If we put them all into one model, the language pairs with less data can still benefit from all of the data in the other language pairs, because all of the language pairs share weights (since they all use the same model).","markups":[]},{"name":"2a4f","type":9,"text":"This one is absolutely nuts. If we train our network to translate English → Spanish and Spanish → French, our network automatically knows how to translate English → French (reasonably well).","markups":[{"type":1,"start":106,"end":171}]},{"name":"0314","type":1,"text":"Expanding on that last point some more: the authors of the paper even found evidence of an interlingua, or an intermediate representation that is shared by multiple languages. Being able to learn an interlingua is the ideal end goal to create a fully generalized multilingual NMT: we learn an encoder/decoder to convert to/from the interlingua for each language, and we immediately know how to translate to and from that language. We aren’t quite there yet, but this is a major step in that direction. Creating a larger multilingual NMT model and giving it even more data could be all it takes to achieve new state-of-the-art translation results.","markups":[{"type":2,"start":91,"end":102},{"type":2,"start":441,"end":446}]},{"name":"e74d","type":13,"text":"So, yeah","markups":[]},{"name":"1de9","type":1,"text":"RNNs are pretty awesome. There are new RNN papers published literally every day and it’s impossible to cover everything — if you think I missed something important, definitely let me know. (From Rohan: Except Neural Turing Machines and Learning to Learn. Those are dope, we know it, and we’re going to be covering them soon!)","markups":[{"type":3,"start":176,"end":187,"href":"https://twitter.com/LennyKhazan","title":"","rel":"noopener","anchorType":0}]},{"name":"449c","type":3,"text":"Building a Vanilla Recurrent Neural Network","markups":[]},{"name":"55e8","type":1,"text":"Let’s get practical for a minute and see how we can build one of these things in practice. We’ll stick with char-rnn (the single layer character level language model we talked about much earlier) with raw numpy so we can see the nitty-gritty details, but if you’re using one of these in practice there are much better solutions! For out-of-the-box functional deep learning models Keras is the de facto framework that people seem to use. For more creative models and all kinds of other fancy stuff I’m a fan of the newly-released PyTorch, or the “older” TensorFlow.","markups":[{"type":3,"start":380,"end":385,"href":"https://keras.io/","title":"","rel":"noopener","anchorType":0},{"type":3,"start":529,"end":536,"href":"http://pytorch.org/","title":"","rel":"noopener","anchorType":0},{"type":3,"start":553,"end":563,"href":"https://www.tensorflow.org/","title":"","rel":"noopener","anchorType":0},{"type":2,"start":296,"end":328}]},{"name":"5997","type":1,"text":"I’m going to walk us through this implementation line by line so we can see exactly what’s going on. It’s really well-commented, so feel free to peruse it on your own too.","markups":[{"type":3,"start":29,"end":48,"href":"https://gist.github.com/karpathy/d4dee566867f8291f086","title":"","rel":"noopener","anchorType":0}]},{"name":"8745","type":1,"text":"Afterwards, I challenge you to code an LSTM!","markups":[]},{"name":"4989","type":11,"text":"","markups":[],"layout":1,"iframe":{"mediaResourceId":"515164c3f20643d9534d745371d34b9f","thumbnailUrl":"https://i.embed.ly/1/image?url=https%3A%2F%2Favatars0.githubusercontent.com%2Fu%2F241138%3Fv%3D3%26s%3D400&key=4fce0568f2ce49e8b54624ef71a8a5bd"}},{"name":"8a37","type":1,"text":"import numpy as np","markups":[{"type":10,"start":0,"end":18}]},{"name":"54ad","type":1,"text":"Well, duh.","markups":[]},{"name":"a14a","type":1,"text":"data = open(‘input.txt’, ‘r’).read()\nchars = list(set(data))\ndata_size, vocab_size = len(data), len(chars)\nprint ‘data has %d characters, %d unique.’ % (data_size, vocab_size)\nchar_to_ix = { ch:i for i,ch in enumerate(chars) }\nix_to_char = { i:ch for i,ch in enumerate(chars) }","markups":[{"type":10,"start":0,"end":277}]},{"name":"3f08","type":1,"text":"We load in our data and get a list of all of the characters that appear in it. We set up two dictionaries: one mapping characters to an index, and one for the reverse. We’ll use this when converting characters to/from a one-hot encoding later on.","markups":[]},{"name":"f0a4","type":1,"text":"hidden_size = 100\nseq_length = 25\nlearning_rate = 1e-1","markups":[{"type":10,"start":0,"end":54}]},{"name":"b15a","type":1,"text":"Typical hyperparam stuff. Our RNN layer will have a hidden size of 100, and we’ll train our network on batches of 25 characters at a time. Since we’ll be training our network with BPTT, we need to make sure the sequences are sufficiently short that we can unroll the network all the way and keep everything in memory. Finally, set the learning rate to .1.","markups":[]},{"name":"a836","type":1,"text":"Wxh = np.random.randn(hidden_size, vocab_size)*0.01 # input to hidden\nWhh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden\nWhy = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output\nbh = np.zeros((hidden_size, 1)) # hidden bias\nby = np.zeros((vocab_size, 1)) # output bias","markups":[{"type":10,"start":0,"end":69},{"type":10,"start":70,"end":303}]},{"name":"7dce","type":1,"text":"We set up our parameters — note that this is just a typical RNN, no fancy LSTM cells. We have weight matrices for updating our hidden state with each input, updating our hidden state with each timestep, and producing an output (and biases for our hidden state + output). We could be doing some fancy weight initialization here, but some normally-distributed randomness is sufficient for breaking symmetry.","markups":[]},{"name":"c19b","type":1,"text":"Now let’s talk loss function. We start by computing the forward pass, then computing the backward pass, just like with any neural network.","markups":[]},{"name":"08d8","type":1,"text":"xs, hs, ys, ps = {}, {}, {}, {}\nhs[-1] = np.copy(hprev)\nloss = 0","markups":[{"type":10,"start":0,"end":64}]},{"name":"7017","type":1,"text":"We start off by just setting up some variables to store our one-hot inputs, hidden states, outputs, and softmax probabilities.","markups":[]},{"name":"03ad","type":1,"text":"for t in xrange(len(inputs)):","markups":[{"type":10,"start":0,"end":29}]},{"name":"7bca","type":1,"text":"Go through each timestep, and for each timestep…","markups":[]},{"name":"d87e","type":1,"text":"xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation\nxs[t][inputs[t]] = 1","markups":[{"type":10,"start":0,"end":87}]},{"name":"0962","type":1,"text":"Convert our input character at this timestep to a one-hot vector.","markups":[]},{"name":"ea9f","type":1,"text":"hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state","markups":[{"type":10,"start":0,"end":78}]},{"name":"de7c","type":1,"text":"Update our hidden state. We saw this formula already — use our Wxh and Whh matrices to update our hidden state based on the last state and our input, and add a bias.","markups":[{"type":1,"start":63,"end":66},{"type":1,"start":71,"end":74}]},{"name":"15c1","type":1,"text":"ys[t] = np.dot(Why, hs[t]) + by","markups":[{"type":10,"start":0,"end":31}]},{"name":"26df","type":1,"text":"Compute our output…","markups":[]},{"name":"4308","type":1,"text":"ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars","markups":[{"type":10,"start":0,"end":76}]},{"name":"46cd","type":1,"text":"…and convert it to a probability distribution with a softmax.","markups":[]},{"name":"b3bb","type":1,"text":"loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)","markups":[{"type":10,"start":0,"end":67}]},{"name":"011a","type":1,"text":"Accumulate the loss for this time step as the negative log of the predicted probability. Ideally, we would have a probability of 1 for the actual next character. If it is 1, the loss is 0, log(1) = 0. As the predicted probability approaches 0, the loss approaches inf, because log(0) = -inf.","markups":[{"type":2,"start":139,"end":145}]},{"name":"d366","type":1,"text":"That’s it for the forward pass (not bad, right? Boiled down, it’s like six lines of code. Piece of cake).","markups":[]},{"name":"b1db","type":1,"text":"dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n dhnext = np.zeros_like(hs[0])","markups":[{"type":10,"start":0,"end":77},{"type":10,"start":78,"end":126},{"type":10,"start":127,"end":157}]},{"name":"cdb0","type":1,"text":"Setting up some variables for our backward pass — the gradients of our weight matrices, the gradients for our biases, and the gradients from the next timestep (we’ll see how that works in a bit).","markups":[]},{"name":"2fd8","type":1,"text":"for t in reversed(xrange(len(inputs))):","markups":[{"type":10,"start":0,"end":39}]},{"name":"14bf","type":1,"text":"Go through our sequence in reverse as we back up the gradients.","markups":[]},{"name":"9148","type":1,"text":"dy = np.copy(ps[t])\n dy[targets[t]] -= 1 # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here","markups":[{"type":10,"start":0,"end":137},{"type":3,"start":64,"end":120,"href":"http://cs231n.github.io/neural-networks-case-study/#grad","title":"","rel":"noopener","anchorType":0}]},{"name":"3fb3","type":1,"text":"First, get the gradient of the output, dy. As it turns out, the gradient of the cross-entropy loss is really as copying over the distribution and subtracting 1 from the correct class.","markups":[{"type":3,"start":43,"end":58,"href":"http://cs231n.github.io/neural-networks-case-study/#grad","title":"","rel":"noopener","anchorType":0}]},{"name":"b8b8","type":1,"text":"Remember backpropogation? When we have a weighted sum, the gradient of the weights is just the corresponding value that it is being multiplied by, because the other terms drop out and that one weight is treated as a constant. So, computing the gradient of our Why matrix is super simple: just multiply the gradient of loss w.r.t. the output (dy) by the derivative of the output w.r.t. Why (which is just the hidden state at our given timestep), and we get the derivative of the loss w.r.t. Why.","markups":[{"type":3,"start":0,"end":24,"href":"https://ayearofai.com/rohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d","title":"","rel":"noopener","anchorType":0}]},{"name":"9790","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"0*TVvKSJJqaM9CDjlk.","originalWidth":426,"originalHeight":86}},{"name":"247c","type":1,"text":"dWhy += np.dot(dy, hs[t].T)","markups":[{"type":10,"start":0,"end":27}]},{"name":"2ff0","type":1,"text":"Like the other gradients (except dy, of course) we accumulate these gradients over all timesteps and apply them at the end.","markups":[]},{"name":"c397","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"0*QKDwSXVEJ9fHQ4hT.","originalWidth":321,"originalHeight":86}},{"name":"1c4f","type":1,"text":"dby += dy","markups":[{"type":10,"start":0,"end":9}]},{"name":"a781","type":1,"text":"The derivative of loss w.r.t. output (dy) multiplied by the derivative of our output w.r.t. the bias (which is 1) gives us the derivative of our output w.r.t. the bias. So far so good.","markups":[]},{"name":"6890","type":1,"text":"dh = np.dot(Why.T, dy) + dhnext # backprop into h","markups":[{"type":10,"start":0,"end":49}]},{"name":"9602","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*cVr1t2s7gsC4Sd6vowSkHA.png","originalWidth":265,"originalHeight":74}},{"name":"6dbe","type":1,"text":"We compute dL/dh using the chain rule, and accumulate it over all timesteps (hence + dhnext). We’ll need this for the next step.","markups":[{"type":10,"start":83,"end":91}]},{"name":"6d9e","type":1,"text":"dhraw = (1 — hs[t] * hs[t]) * dh # backprop through tanh nonlinearity","markups":[{"type":10,"start":0,"end":69}]},{"name":"796c","type":1,"text":"This computes the derivative of the np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) line from earlier.","markups":[{"type":10,"start":36,"end":91}]},{"name":"7fa1","type":1,"text":"dbh += dhraw","markups":[{"type":10,"start":0,"end":12}]},{"name":"f74d","type":1,"text":"Which is also our bh derivative, for the same reason that the by derivative was just dy.","markups":[]},{"name":"a0f2","type":1,"text":"dWxh += np.dot(dhraw, xs[t].T)\ndWhh += np.dot(dhraw, hs[t-1].T)","markups":[{"type":10,"start":0,"end":63}]},{"name":"cea8","type":1,"text":"We accumulate our weight gradients.","markups":[]},{"name":"1bb0","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*uf-YEbf0258UhbDc5QZLRw.png","originalWidth":286,"originalHeight":77}},{"name":"d79b","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*Vl1LVzPJSZKDplA9J5cElg.png","originalWidth":287,"originalHeight":77}},{"name":"012f","type":1,"text":"dhnext = np.dot(Whh.T, dhraw)","markups":[{"type":10,"start":0,"end":29}]},{"name":"bb22","type":1,"text":"And finally, store dh for this timestep so we can use it for the previous one.","markups":[]},{"name":"1ea0","type":1,"text":"for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\nnp.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients","markups":[{"type":10,"start":0,"end":117}]},{"name":"a901","type":1,"text":"Last but not least, a little gradient clipping so we don’t get no exploding gradients.","markups":[]},{"name":"8392","type":1,"text":"return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]","markups":[{"type":10,"start":0,"end":58}]},{"name":"4dc5","type":1,"text":"And then return all the gradients so we can apply an optimizer step. And that’s it for the backprop code; not too bad, right?","markups":[{"type":2,"start":110,"end":113}]},{"name":"4eaf","type":1,"text":"def sample(h, seed_ix, n):","markups":[{"type":10,"start":0,"end":26}]},{"name":"ed15","type":1,"text":"This method is used for sampling a generated sequence from the network, starting with state h, first letter seed_ix, with length n.","markups":[{"type":10,"start":92,"end":93},{"type":10,"start":108,"end":115},{"type":10,"start":129,"end":130}]},{"name":"d445","type":1,"text":"x = np.zeros((vocab_size, 1)) \nx[seed_ix] = 1","markups":[{"type":10,"start":0,"end":45}]},{"name":"a8f6","type":1,"text":"Set up our one-hot encoded input vector based on the seed character.","markups":[]},{"name":"8a82","type":1,"text":"ixes = []","markups":[{"type":10,"start":0,"end":9}]},{"name":"7e2b","type":1,"text":"And an array to keep track of our sequence.","markups":[]},{"name":"3cae","type":1,"text":"for t in xrange(n):","markups":[{"type":10,"start":0,"end":19}]},{"name":"c8e6","type":1,"text":"To generate each character in our sequence…","markups":[]},{"name":"18cf","type":1,"text":"h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)","markups":[{"type":10,"start":0,"end":49}]},{"name":"6d86","type":1,"text":"Update our hidden state! We saw this formula in the last function, too.","markups":[]},{"name":"ce6f","type":1,"text":"y = np.dot(Why, h) + by\np = np.exp(y) / np.sum(np.exp(y))","markups":[{"type":10,"start":0,"end":57}]},{"name":"d108","type":1,"text":"Generate our output and run it through a softmax. Again, straight from the last function.","markups":[]},{"name":"62ae","type":1,"text":"ix = np.random.choice(range(vocab_size), p=p.ravel())","markups":[{"type":10,"start":0,"end":53}]},{"name":"fd39","type":1,"text":"Sample from our output distribution using some numpy magic.","markups":[]},{"name":"cec1","type":1,"text":"x = np.zeros((vocab_size, 1))\nx[ix] = 1\nixes.append(ix)","markups":[{"type":10,"start":0,"end":55}]},{"name":"e8b3","type":1,"text":"Convert the sampled value into a one-hot encoding and append it to the array.","markups":[]},{"name":"4928","type":1,"text":"return ixes","markups":[{"type":10,"start":0,"end":11}]},{"name":"e074","type":1,"text":"…and of course, return the final sequence when we’re done.","markups":[]},{"name":"6968","type":1,"text":"n, p = 0, 0","markups":[{"type":10,"start":0,"end":11}]},{"name":"2eb1","type":1,"text":"n is the number of training iterations we’ve done. p is the index into our training data for where we are now.","markups":[{"type":1,"start":0,"end":1},{"type":1,"start":51,"end":52}]},{"name":"ee12","type":1,"text":"mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)","markups":[{"type":10,"start":0,"end":77}]},{"name":"9951","type":1,"text":"Set up memory variables for the Adagrad algorithm (out of scope of this post, maybe next time — it’s just a variant on gradient descent).","markups":[]},{"name":"ffc4","type":1,"text":"while True:","markups":[{"type":10,"start":0,"end":11}]},{"name":"bcde","type":1,"text":"Training loop.","markups":[]},{"name":"a04e","type":1,"text":"if p+seq_length+1 \x3e= len(data) or n == 0:","markups":[{"type":10,"start":0,"end":41}]},{"name":"3444","type":1,"text":"This is a little check to see if we need to reset our memory because we’re starting back at the beginning of our data.","markups":[]},{"name":"a0be","type":1,"text":"hprev = np.zeros((hidden_size,1)) # reset RNN memory","markups":[{"type":10,"start":0,"end":52}]},{"name":"038a","type":1,"text":"…and if we are, reset the memory.","markups":[]},{"name":"9c3d","type":1,"text":"p = 0","markups":[{"type":10,"start":0,"end":5}]},{"name":"bd23","type":1,"text":"And reset the data pointer.","markups":[]},{"name":"0161","type":1,"text":"inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\ntargets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]","markups":[{"type":10,"start":0,"end":118}]},{"name":"70c2","type":1,"text":"We grab a seq_length-long piece of the data as our input to the network. At each timestep, we want to predict the next character; this means that our “targets” will be the next character for each input. We get a snippet of the input the same length as the input, but offset by 1, for the target.","markups":[{"type":10,"start":10,"end":20}]},{"name":"3f42","type":1,"text":"if n % 100 == 0:\nsample_ix = sample(hprev, inputs[0], 200)\ntxt = ‘’.join(ix_to_char[ix] for ix in sample_ix)\nprint ‘ — — \\n %s \\n — — ‘ % (txt, )","markups":[{"type":10,"start":0,"end":145}]},{"name":"a7eb","type":1,"text":"Here we just print to the terminal a sample every 100 training steps so we can see how its doing. Ideally, this will print out gibberish the first few times, before gradually printing out more and more reasonable language.","markups":[]},{"name":"e412","type":1,"text":"loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)","markups":[{"type":10,"start":0,"end":73}]},{"name":"538d","type":1,"text":"Do a forward pass, backward pass, and get the gradients.","markups":[]},{"name":"9d99","type":1,"text":"smooth_loss = smooth_loss * 0.999 + loss * 0.001","markups":[{"type":10,"start":0,"end":48}]},{"name":"4ab1","type":1,"text":"Adagrad stuff.","markups":[]},{"name":"3823","type":1,"text":"if n % 100 == 0: print ‘iter %d, loss: %f’ % (n, smooth_loss) # print progress","markups":[{"type":10,"start":0,"end":78}]},{"name":"b38c","type":1,"text":"Keep up with progress.","markups":[]},{"name":"995f","type":1,"text":"for param, dparam, mem in zip([Wxh, Whh, Why, bh, by], [dWxh, dWhh, dWhy, dbh, dby], [mWxh, mWhh, mWhy, mbh, mby]):\nmem += dparam * dparam\nparam += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update","markups":[{"type":10,"start":0,"end":210}]},{"name":"9509","type":1,"text":"More Adagrad. We should really do an article on optimization algorithms.","markups":[]},{"name":"a2fc","type":1,"text":"p += seq_length # move data pointer\nn += 1 # iteration counter","markups":[{"type":10,"start":0,"end":62}]},{"name":"a889","type":1,"text":"Annnddd finally, we update our data pointer and iteration counter.","markups":[]},{"name":"7850","type":1,"text":"And that’s it. We have an RNN. Neat-o. Reminder: your challenge is to code an LSTM… and TensorFlow doesn’t count!","markups":[]},{"name":"5a53","type":3,"text":"Conclusion","markups":[]},{"name":"8a00","type":1,"text":"Wow. That was a lot. If you came in knowing nothing or very little about recurrent neural nets, you sure as hell know a lot now. And you don’t just know about something cool; you know about something very important — something that can equip you to read and understand some of the most prominent and hottest recent research papers in machine learning.","markups":[{"type":1,"start":16,"end":19},{"type":2,"start":16,"end":19},{"type":2,"start":200,"end":214}]},{"name":"eec5","type":1,"text":"Something this article didn’t do so good at was making sure the calculus and derivatives were in the context of operating on vectors (because, remember, RNNs/LSTMs operate over vectors). In many cases the derivatives were in the 1-D context. It’s not something you need to worry about, but you might want to look into.","markups":[]},{"name":"f6a4","type":1,"text":"We’re finally at the point where we can focus our energies on this blog towards cooler stuff including hot research papers like Neural Turing Machines or Learning to Learn, case studies eg. AlphaGo, other parts of machine learning and artificial intelligence (I’m, Rohan, personally looking forward to optimization!), or different algorithms like GANs. There’s very little compulsory content or “groundwork” we need to cover anymore. So, now, we’re officially onto the cool stuff.","markups":[]},{"name":"4f33","type":1,"text":"That’s right. A Year Of AI is officially… cool.","markups":[{"type":1,"start":0,"end":13},{"type":2,"start":42,"end":46}]},{"name":"f41b","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*necEBipfgD-Z3_9R9usFgg.png","originalWidth":80,"originalHeight":80}}],"sections":[{"name":"20d1","startIndex":0},{"name":"4110","startIndex":3},{"name":"6590","startIndex":4},{"name":"da40","startIndex":258},{"name":"5df4","startIndex":297},{"name":"ccfc","startIndex":432},{"name":"70a1","startIndex":445}]},"postDisplay":{"coverless":true}},"virtuals":{"statusForCollection":"APPROVED","allowNotes":true,"previewImage":{"imageId":"1*khIKl9t4XmZGSsKhW_Yg2w.png","filter":"","backgroundSize":"","originalWidth":2000,"originalHeight":907,"strategy":"resample","height":0,"width":0},"wordCount":20536,"imageCount":115,"readingTime":83.9943396226415,"subtitle":"The ultimate guide to machine learning’s favorite child.","publishedInCount":1,"usersBySocialRecommends":[],"recommends":88,"socialRecommends":[],"isBookmarked":false,"tags":[{"slug":"machine-learning","name":"Machine Learning","postCount":12356,"virtuals":{"isFollowing":false},"metadata":{"followerCount":16196,"postCount":12356,"coverImage":{"id":"1*7MK-WgDf8HA8isSGDjPhyw.png","originalWidth":1800,"originalHeight":764}},"type":"Tag"},{"slug":"artificial-intelligence","name":"Artificial Intelligence","postCount":19403,"virtuals":{"isFollowing":false},"metadata":{"followerCount":584449,"postCount":19403,"coverImage":{"id":"1*gAn_BSffVBcwCIR6bDgK1g.jpeg"}},"type":"Tag"},{"slug":"data-science","name":"Data Science","postCount":9039,"virtuals":{"isFollowing":false},"metadata":{"followerCount":9762,"postCount":9039,"coverImage":{"id":"1*W2vzGrXR1ua5KN-X0m9oMw.jpeg","originalWidth":1920,"originalHeight":1431,"isFeatured":true}},"type":"Tag"},{"slug":"deep-learning","name":"Deep Learning","postCount":2237,"virtuals":{"isFollowing":false},"metadata":{"followerCount":5889,"postCount":2237,"coverImage":{"id":"1*khIKl9t4XmZGSsKhW_Yg2w.png","originalWidth":2000,"originalHeight":907}},"type":"Tag"},{"slug":"algorithms","name":"Algorithms","postCount":2129,"virtuals":{"isFollowing":false},"metadata":{"followerCount":1643,"postCount":2129,"coverImage":{"id":"1*khIKl9t4XmZGSsKhW_Yg2w.png","originalWidth":2000,"originalHeight":907}},"type":"Tag"}],"socialRecommendsCount":0,"responsesCreatedCount":4,"links":{"entries":[{"url":"http://imgur.com/gallery/vaNahKE","alts":[{"type":3,"url":"imgur://imgur.com/gallery/vaNahKE?from=fbreferral"},{"type":2,"url":"imgur://imgur.com/gallery/vaNahKE?from=fbreferral"}]},{"url":"https://blog.google/products/translate/found-translation-more-accurate-fluent-sentences-google-translate/","alts":[{"type":1,"url":"https://cdn.ampproject.org/c/blog.google:443/products/translate/found-translation-more-accurate-fluent-sentences-google-translate/amp/"}]},{"url":"https://www.youtube.com/watch?v=cO0a0QYmFm8&index=10&list=PLlJy-eBtNFt6EuMxFYRiNRS07MCWN5UIA","alts":[{"type":2,"url":"vnd.youtube://www.youtube.com/watch?v=cO0a0QYmFm8&list=PLlJy-eBtNFt6EuMxFYRiNRS07MCWN5UIA&index=10&feature=applinks"},{"type":3,"url":"vnd.youtube://www.youtube.com/watch?v=cO0a0QYmFm8&list=PLlJy-eBtNFt6EuMxFYRiNRS07MCWN5UIA&index=10&feature=applinks"}]},{"url":"https://ayearofai.com/the-goal-of-our-blog-c104d7b6377a#.75o5qyayi","alts":[{"type":2,"url":"medium://p/c104d7b6377a"},{"type":3,"url":"medium://p/c104d7b6377a"}]},{"url":"https://ayearofai.com/lenny-2-autoencoders-and-word-embeddings-oh-my-576403b0113a#.oevzdnnnp","alts":[{"type":2,"url":"medium://p/576403b0113a"},{"type":3,"url":"medium://p/576403b0113a"}]},{"url":"https://ayearofai.com/rohan-5-what-are-bias-units-828d942b4f52","alts":[{"type":2,"url":"medium://p/828d942b4f52"},{"type":3,"url":"medium://p/828d942b4f52"}]},{"url":"https://ayearofai.com/rohan-4-the-vanishing-gradient-problem-ec68f76ffb9b","alts":[{"type":3,"url":"medium://p/ec68f76ffb9b"},{"type":2,"url":"medium://p/ec68f76ffb9b"}]},{"url":"https://medium.com/a-year-of-artificial-intelligence/0-2016-is-the-year-i-venture-into-artificial-intelligence-d702d65eb919#.bfjoaqxu5","alts":[{"type":2,"url":"medium://p/d702d65eb919"},{"type":3,"url":"medium://p/d702d65eb919"}]},{"url":"https://ayearofai.com/rohan-4-the-vanishing-gradient-problem-ec68f76ffb9b#.58nevwsoa","alts":[{"type":2,"url":"medium://p/ec68f76ffb9b"},{"type":3,"url":"medium://p/ec68f76ffb9b"}]},{"url":"https://ayearofai.com/rohan-lenny-2-convolutional-neural-networks-5f4cd480a60b#.ebwc0kf4z","alts":[{"type":2,"url":"medium://p/5f4cd480a60b"},{"type":3,"url":"medium://p/5f4cd480a60b"}]},{"url":"https://ayearofai.com/rohan-lenny-2-convolutional-neural-networks-5f4cd480a60b?gi=25a4c415cc58","alts":[{"type":2,"url":"medium://p/5f4cd480a60b"},{"type":3,"url":"medium://p/5f4cd480a60b"}]},{"url":"https://ayearofai.com/rohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d#.quwnoqtot","alts":[{"type":2,"url":"medium://p/abf4609d4f9d"},{"type":3,"url":"medium://p/abf4609d4f9d"}]},{"url":"https://medium.com/a-year-of-artificial-intelligence","alts":[{"type":2,"url":"medium://a-year-of-artificial-intelligence"},{"type":3,"url":"medium://a-year-of-artificial-intelligence"}]},{"url":"https://ayearofai.com/rohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d","alts":[{"type":2,"url":"medium://p/abf4609d4f9d"},{"type":3,"url":"medium://p/abf4609d4f9d"}]}],"version":"0.3","generatedAt":1492389086840},"isLockedPreviewOnly":false,"takeoverId":"","metaDescription":"","totalClapCount":0},"coverless":true,"slug":"rohan-lenny-3-recurrent-neural-networks","translationSourcePostId":"","translationSourceCreatorId":"","isApprovedTranslation":false,"inResponseToPostId":"","inResponseToRemovedAt":0,"isTitleSynthesized":true,"allowResponses":true,"importedUrl":"","importedPublishedAt":0,"visibility":0,"uniqueSlug":"rohan-lenny-3-recurrent-neural-networks-10300100899b","previewContent":{"bodyModel":{"paragraphs":[{"name":"be8e","type":4,"text":"","markups":[],"layout":10,"metadata":{"id":"1*khIKl9t4XmZGSsKhW_Yg2w.png","originalWidth":2000,"originalHeight":907}},{"name":"84b9","type":3,"text":"Rohan & Lenny #3: Recurrent Neural Networks & LSTMs","markups":[],"alignment":1},{"name":"9485","type":13,"text":"The ultimate guide to machine learning’s favorite…","markups":[],"alignment":1}],"sections":[{"startIndex":0}]},"isFullContent":false},"license":0,"inResponseToMediaResourceId":"","canonicalUrl":"https://ayearofai.com/rohan-lenny-3-recurrent-neural-networks-10300100899b","approvedHomeCollectionId":"bb87da25612c","approvedHomeCollection":{"id":"bb87da25612c","name":"A Year of Artificial Intelligence","slug":"a-year-of-artificial-intelligence","tags":["DATA SCIENCE","ARTIFICIAL INTELLIGENCE","MACHINE LEARNING","TECHNOLOGY","COMPUTER SCIENCE"],"creatorId":"cb55958ea3bb","description":"Our ongoing effort to make the mathematics, science, linguistics, and philosophy of artificial intelligence fun and simple.","shortDescription":"Our ongoing effort to make the mathematics, science…","image":{"imageId":"1*NZsNSuNxe_O2YW1ybboOvA.jpeg","filter":"","backgroundSize":"","originalWidth":0,"originalHeight":0,"strategy":"resample","height":0,"width":0},"metadata":{"followerCount":521,"postCount":6,"activeAt":1492072051871},"virtuals":{"permissions":{"canPublish":false,"canPublishAll":false,"canRepublish":false,"canRemove":false,"canManageAll":false,"canSubmit":false,"canEditPosts":false,"canAddWriters":false,"canViewStats":false,"canSendNewsletter":false,"canViewLockedPosts":false,"canViewCloaked":false,"canEditOwnPosts":false,"canBeAssignedAuthor":false},"isSubscribed":false,"isNewsletterSubscribed":false,"memberOfMembershipPlanId":""},"logo":{"imageId":"1*NZsNSuNxe_O2YW1ybboOvA.jpeg","filter":"","backgroundSize":"","originalWidth":1500,"originalHeight":1000,"strategy":"resample","height":0,"width":0},"twitterUsername":"mckapur","facebookPageName":"mckapur","publicEmail":"me@rohankapur.com","domain":"ayearofai.com","sections":[{"type":2,"collectionHeaderMetadata":{"title":"A Year of Artificial Intelligence","description":"Where we make the mathematics, science, linguistics, and philosophy of artificial intelligence fun and simple.","backgroundImage":{"id":"1*UlHnUWTtTHWOrOuJdHjVww.png","originalWidth":2000,"originalHeight":1333,"focusPercentX":51.785714285714285,"focusPercentY":43.31550802139037},"logoImage":{"id":"1*NZsNSuNxe_O2YW1ybboOvA.jpeg","originalWidth":1500,"originalHeight":1000,"alt":"A Year of Artificial Intelligence"},"alignment":1,"layout":6}},{"type":1,"postListMetadata":{"source":1,"layout":2,"number":1,"postIds":[],"sectionHeader":"New"}},{"type":1,"postListMetadata":{"source":1,"layout":4,"number":14,"postIds":[]}}],"tintColor":"#FF000000","lightText":true,"favicon":{"imageId":"","filter":"","backgroundSize":"","originalWidth":0,"originalHeight":0,"strategy":"resample","height":0,"width":0},"colorPalette":{"defaultBackgroundSpectrum":{"colorPoints":[{"color":"#FF868484","point":0},{"color":"#FF7C7B7A","point":0.1},{"color":"#FF737171","point":0.2},{"color":"#FF696867","point":0.3},{"color":"#FF5F5E5E","point":0.4},{"color":"#FF555454","point":0.5},{"color":"#FF4A4949","point":0.6},{"color":"#FF3F3E3E","point":0.7},{"color":"#FF343333","point":0.8},{"color":"#FF272727","point":0.9},{"color":"#FF1A1A1A","point":1}],"backgroundColor":"#FFFFFFFF"},"tintBackgroundSpectrum":{"colorPoints":[{"color":"#FF000000","point":0},{"color":"#FF1E1D1D","point":0.1},{"color":"#FF3C3B3B","point":0.2},{"color":"#FF565555","point":0.3},{"color":"#FF6F6D6D","point":0.4},{"color":"#FF868484","point":0.5},{"color":"#FF9C9A99","point":0.6},{"color":"#FFB1AEAE","point":0.7},{"color":"#FFC5C3C2","point":0.8},{"color":"#FFD9D6D6","point":0.9},{"color":"#FFECE9E9","point":1}],"backgroundColor":"#FF000000"},"highlightSpectrum":{"colorPoints":[{"color":"#FFF5F2F1","point":0},{"color":"#FFF3F0EF","point":0.1},{"color":"#FFF1EEED","point":0.2},{"color":"#FFEFECEC","point":0.3},{"color":"#FFEDEAEA","point":0.4},{"color":"#FFEBE8E8","point":0.5},{"color":"#FFE9E6E6","point":0.6},{"color":"#FFE7E5E4","point":0.7},{"color":"#FFE5E3E2","point":0.8},{"color":"#FFE4E1E0","point":0.9},{"color":"#FFE2DFDE","point":1}],"backgroundColor":"#FFFFFFFF"}},"navItems":[{"type":1,"title":"Algorithms","tagSlug":"algorithms","url":"https://ayearofai.com/tagged/algorithms","source":"tagSlug"},{"type":1,"title":"Today I Learned","tagSlug":"today-i-learned","url":"https://ayearofai.com/tagged/today-i-learned","source":"tagSlug"},{"type":1,"title":"Case Studies","tagSlug":"case-studies","url":"https://ayearofai.com/tagged/case-studies","source":"tagSlug"},{"type":1,"title":"Philosophical","tagSlug":"philosophical","url":"https://ayearofai.com/tagged/philosophical","source":"tagSlug"},{"type":1,"title":"Meta","tagSlug":"meta","url":"https://ayearofai.com/tagged/meta","source":"tagSlug"}],"colorBehavior":2,"instantArticlesState":0,"acceleratedMobilePagesState":0,"ampLogo":{"imageId":"","filter":"","backgroundSize":"","originalWidth":0,"originalHeight":0,"strategy":"resample","height":0,"width":0},"header":{"title":"A Year of Artificial Intelligence","description":"Where we make the mathematics, science, linguistics, and philosophy of artificial intelligence fun and simple.","backgroundImage":{"id":"1*UlHnUWTtTHWOrOuJdHjVww.png","originalWidth":2000,"originalHeight":1333,"focusPercentX":51.785714285714285,"focusPercentY":43.31550802139037},"logoImage":{"id":"1*NZsNSuNxe_O2YW1ybboOvA.jpeg","originalWidth":1500,"originalHeight":1000,"alt":"A Year of Artificial Intelligence"},"alignment":1,"layout":6},"type":"Collection"},"newsletterId":"","webCanonicalUrl":"https://ayearofai.com/rohan-lenny-3-recurrent-neural-networks-10300100899b","mediumUrl":"https://ayearofai.com/rohan-lenny-3-recurrent-neural-networks-10300100899b","migrationId":"","notifyFollowers":true,"notifyTwitter":true,"isSponsored":false,"isRequestToPubDisabled":false,"notifyFacebook":false,"responseHiddenOnParentPostAt":0,"isSeries":false,"isSubscriptionLocked":false,"seriesLastAppendedAt":0,"type":"Post"},"mentionedUsers":[{"userId":"de8e2540b759","name":"Lenny Khazan","username":"lennykhazan","createdAt":1368901081367,"lastPostCreatedAt":1491975881207,"imageId":"1*vltCFMOhqgoMSe7Wa5xJaA.png","backgroundImageId":"","bio":"Tinkering with machine learning. http://getcontra.com/","twitterScreenName":"LennyKhazan","facebookAccountId":"","allowNotes":1,"mediumMemberAt":0,"mediumMemberWaitlistedAt":0,"type":"User"},{"userId":"cb55958ea3bb","name":"Rohan Kapur","username":"mckapur","createdAt":1383813905817,"lastPostCreatedAt":1492253036060,"imageId":"1*pGDfwp8yLzgCeG1RkPUxgQ.jpeg","backgroundImageId":"","bio":"rohankapur.com","twitterScreenName":"mckapur","socialStats":{"userId":"cb55958ea3bb","usersFollowedCount":204,"usersFollowedByCount":592,"type":"SocialStats"},"social":{"userId":"lo_6d640aa2d2ff","targetUserId":"cb55958ea3bb","type":"Social"},"facebookAccountId":"1004843339565980","allowNotes":1,"mediumMemberAt":0,"mediumMemberWaitlistedAt":0,"type":"User"}],"collaborators":[],"membershipPlans":[],"collectionUserRelations":[],"mode":null,"references":{"User":{"cb55958ea3bb":{"userId":"cb55958ea3bb","name":"Rohan Kapur","username":"mckapur","createdAt":1383813905817,"lastPostCreatedAt":1492253036060,"imageId":"1*pGDfwp8yLzgCeG1RkPUxgQ.jpeg","backgroundImageId":"","bio":"rohankapur.com","twitterScreenName":"mckapur","socialStats":{"userId":"cb55958ea3bb","usersFollowedCount":204,"usersFollowedByCount":592,"type":"SocialStats"},"social":{"userId":"lo_6d640aa2d2ff","targetUserId":"cb55958ea3bb","type":"Social"},"facebookAccountId":"1004843339565980","allowNotes":1,"type":"User"}},"Collection":{"bb87da25612c":{"id":"bb87da25612c","name":"A Year of Artificial Intelligence","slug":"a-year-of-artificial-intelligence","tags":["DATA SCIENCE","ARTIFICIAL INTELLIGENCE","MACHINE LEARNING","TECHNOLOGY","COMPUTER SCIENCE"],"creatorId":"cb55958ea3bb","description":"Our ongoing effort to make the mathematics, science, linguistics, and philosophy of artificial intelligence fun and simple.","shortDescription":"Our ongoing effort to make the mathematics, science…","image":{"imageId":"1*NZsNSuNxe_O2YW1ybboOvA.jpeg","filter":"","backgroundSize":"","originalWidth":0,"originalHeight":0,"strategy":"resample","height":0,"width":0},"metadata":{"followerCount":521,"postCount":6,"activeAt":1492072051871},"virtuals":{"permissions":{"canPublish":false,"canPublishAll":false,"canRepublish":false,"canRemove":false,"canManageAll":false,"canSubmit":false,"canEditPosts":false,"canAddWriters":false,"canViewStats":false,"canSendNewsletter":false,"canViewLockedPosts":false,"canViewCloaked":false,"canEditOwnPosts":false,"canBeAssignedAuthor":false},"isSubscribed":false,"isNewsletterSubscribed":false,"memberOfMembershipPlanId":""},"logo":{"imageId":"1*NZsNSuNxe_O2YW1ybboOvA.jpeg","filter":"","backgroundSize":"","originalWidth":1500,"originalHeight":1000,"strategy":"resample","height":0,"width":0},"twitterUsername":"mckapur","facebookPageName":"mckapur","publicEmail":"me@rohankapur.com","domain":"ayearofai.com","sections":[{"type":2,"collectionHeaderMetadata":{"title":"A Year of Artificial Intelligence","description":"Where we make the mathematics, science, linguistics, and philosophy of artificial intelligence fun and simple.","backgroundImage":{"id":"1*UlHnUWTtTHWOrOuJdHjVww.png","originalWidth":2000,"originalHeight":1333,"focusPercentX":51.785714285714285,"focusPercentY":43.31550802139037},"logoImage":{"id":"1*NZsNSuNxe_O2YW1ybboOvA.jpeg","originalWidth":1500,"originalHeight":1000,"alt":"A Year of Artificial Intelligence"},"alignment":1,"layout":6}},{"type":1,"postListMetadata":{"source":1,"layout":2,"number":1,"postIds":[],"sectionHeader":"New"}},{"type":1,"postListMetadata":{"source":1,"layout":4,"number":14,"postIds":[]}}],"tintColor":"#FF000000","lightText":true,"favicon":{"imageId":"","filter":"","backgroundSize":"","originalWidth":0,"originalHeight":0,"strategy":"resample","height":0,"width":0},"colorPalette":{"defaultBackgroundSpectrum":{"colorPoints":[{"color":"#FF868484","point":0},{"color":"#FF7C7B7A","point":0.1},{"color":"#FF737171","point":0.2},{"color":"#FF696867","point":0.3},{"color":"#FF5F5E5E","point":0.4},{"color":"#FF555454","point":0.5},{"color":"#FF4A4949","point":0.6},{"color":"#FF3F3E3E","point":0.7},{"color":"#FF343333","point":0.8},{"color":"#FF272727","point":0.9},{"color":"#FF1A1A1A","point":1}],"backgroundColor":"#FFFFFFFF"},"tintBackgroundSpectrum":{"colorPoints":[{"color":"#FF000000","point":0},{"color":"#FF1E1D1D","point":0.1},{"color":"#FF3C3B3B","point":0.2},{"color":"#FF565555","point":0.3},{"color":"#FF6F6D6D","point":0.4},{"color":"#FF868484","point":0.5},{"color":"#FF9C9A99","point":0.6},{"color":"#FFB1AEAE","point":0.7},{"color":"#FFC5C3C2","point":0.8},{"color":"#FFD9D6D6","point":0.9},{"color":"#FFECE9E9","point":1}],"backgroundColor":"#FF000000"},"highlightSpectrum":{"colorPoints":[{"color":"#FFF5F2F1","point":0},{"color":"#FFF3F0EF","point":0.1},{"color":"#FFF1EEED","point":0.2},{"color":"#FFEFECEC","point":0.3},{"color":"#FFEDEAEA","point":0.4},{"color":"#FFEBE8E8","point":0.5},{"color":"#FFE9E6E6","point":0.6},{"color":"#FFE7E5E4","point":0.7},{"color":"#FFE5E3E2","point":0.8},{"color":"#FFE4E1E0","point":0.9},{"color":"#FFE2DFDE","point":1}],"backgroundColor":"#FFFFFFFF"}},"navItems":[{"type":1,"title":"Algorithms","tagSlug":"algorithms","url":"https://ayearofai.com/tagged/algorithms","source":"tagSlug"},{"type":1,"title":"Today I Learned","tagSlug":"today-i-learned","url":"https://ayearofai.com/tagged/today-i-learned","source":"tagSlug"},{"type":1,"title":"Case Studies","tagSlug":"case-studies","url":"https://ayearofai.com/tagged/case-studies","source":"tagSlug"},{"type":1,"title":"Philosophical","tagSlug":"philosophical","url":"https://ayearofai.com/tagged/philosophical","source":"tagSlug"},{"type":1,"title":"Meta","tagSlug":"meta","url":"https://ayearofai.com/tagged/meta","source":"tagSlug"}],"colorBehavior":2,"instantArticlesState":0,"acceleratedMobilePagesState":0,"ampLogo":{"imageId":"","filter":"","backgroundSize":"","originalWidth":0,"originalHeight":0,"strategy":"resample","height":0,"width":0},"header":{"title":"A Year of Artificial Intelligence","description":"Where we make the mathematics, science, linguistics, and philosophy of artificial intelligence fun and simple.","backgroundImage":{"id":"1*UlHnUWTtTHWOrOuJdHjVww.png","originalWidth":2000,"originalHeight":1333,"focusPercentX":51.785714285714285,"focusPercentY":43.31550802139037},"logoImage":{"id":"1*NZsNSuNxe_O2YW1ybboOvA.jpeg","originalWidth":1500,"originalHeight":1000,"alt":"A Year of Artificial Intelligence"},"alignment":1,"layout":6},"type":"Collection"}},"Social":{"cb55958ea3bb":{"userId":"lo_6d640aa2d2ff","targetUserId":"cb55958ea3bb","type":"Social"}},"SocialStats":{"cb55958ea3bb":{"userId":"cb55958ea3bb","usersFollowedCount":204,"usersFollowedByCount":592,"type":"SocialStats"}}}})
// ]]></script></body></html>