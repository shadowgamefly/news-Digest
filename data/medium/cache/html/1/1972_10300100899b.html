<!DOCTYPE html><html xmlns:cc="http://creativecommons.org/ns#"><head prefix="og: http://ogp.me/ns# fb: http://ogp.me/ns/fb# medium-com: http://ogp.me/ns/fb/medium-com#"><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><meta name="viewport" content="width=device-width, initial-scale=1"><title>Rohan &amp; Lenny #3: Recurrent Neural Networks &amp; LSTMs</title><link rel="canonical" href="https://ayearofai.com/rohan-lenny-3-recurrent-neural-networks-10300100899b"><meta name="title" content="Rohan &amp; Lenny #3: Recurrent Neural Networks &amp; LSTMs"><meta name="referrer" content="unsafe-url"><meta name="description" content="It seems like most of our posts on this blog start with ‚ÄúWe‚Äôre back!‚Äù, so‚Ä¶ you know the drill. It‚Äôs been a while since our last post ‚Äî just over 5 months ‚Äî but it certainly doesn‚Äôt feel that way‚Ä¶"><meta property="og:title" content="Rohan &amp; Lenny #3: Recurrent Neural Networks &amp; LSTMs"><meta property="og:url" content="https://ayearofai.com/rohan-lenny-3-recurrent-neural-networks-10300100899b"><meta property="og:image" content="https://cdn-images-1.medium.com/max/1200/1*khIKl9t4XmZGSsKhW_Yg2w.png"><meta property="fb:app_id" content="542599432471018"><meta property="og:description" content="The ultimate guide to machine learning‚Äôs favorite child."><meta name="twitter:description" content="The ultimate guide to machine learning‚Äôs favorite child."><meta name="twitter:image:src" content="https://cdn-images-1.medium.com/max/1200/1*khIKl9t4XmZGSsKhW_Yg2w.png"><link rel="publisher" href="https://plus.google.com/103654360130207659246"><link rel="author" href="https://ayearofai.com/@mckapur"><meta property="author" content="Rohan Kapur"><meta property="og:type" content="article"><meta name="twitter:card" content="summary_large_image"><meta property="article:publisher" content="https://www.facebook.com/mckapur"><meta property="article:author" content="1004843339565980"><meta property="fb:smart_publish:robots" content="noauto"><meta name="robots" content="index, follow"><meta property="article:published_time" content="2017-04-13T08:27:31.153Z"><meta name="twitter:creator" content="@mckapur"><meta name="twitter:site" content="@mckapur"><meta property="og:site_name" content="A Year of Artificial Intelligence"><meta name="twitter:label1" value="Reading time"><meta name="twitter:data1" value="84 min read"><script type="application/ld+json">{"@context":"http://schema.org","@type":"NewsArticle","image":{"@type":"ImageObject","width":1920,"height":870,"url":"https://cdn-images-1.medium.com/max/1920/1*khIKl9t4XmZGSsKhW_Yg2w.png"},"datePublished":"2017-04-13T08:27:31.153Z","dateModified":"2017-04-17T00:31:23.311Z","headline":"Rohan & Lenny #3: Recurrent Neural Networks & LSTMs","name":"Rohan & Lenny #3: Recurrent Neural Networks & LSTMs","keywords":["Machine Learning","Artificial Intelligence","Data Science","Deep Learning","Algorithms"],"author":{"@type":"Person","name":"Rohan Kapur","url":"https://ayearofai.com/@mckapur"},"creator":["Rohan Kapur"],"publisher":{"@type":"Organization","name":"A Year of Artificial Intelligence","url":"https://ayearofai.com","logo":{"@type":"ImageObject","width":90,"height":60,"url":"https://cdn-images-1.medium.com/max/90/1*NZsNSuNxe_O2YW1ybboOvA.jpeg"}},"mainEntityOfPage":"https://ayearofai.com/rohan-lenny-3-recurrent-neural-networks-10300100899b"}</script><meta name="twitter:app:name:iphone" content="Medium"><meta name="twitter:app:id:iphone" content="828256236"><meta name="twitter:app:url:iphone" content="medium://p/10300100899b"><meta property="al:ios:app_name" content="Medium"><meta property="al:ios:app_store_id" content="828256236"><meta property="al:android:package" content="com.medium.reader"><meta property="al:android:app_name" content="Medium"><meta property="al:ios:url" content="medium://p/10300100899b"><meta property="al:android:url" content="medium://p/10300100899b"><meta property="al:web:url" content="https://ayearofai.com/rohan-lenny-3-recurrent-neural-networks-10300100899b"><link rel="search" type="application/opensearchdescription+xml" title="Medium" href="/osd.xml" /><link rel="alternate" href="android-app://com.medium.reader/https/medium.com/p/10300100899b" /><meta name="theme-color" content="#000000"><link rel="stylesheet" href="https://cdn-static-1.medium.com/_/fp/css/main-base.g2e09PlZK_j3Lw2AgNmbfQ.css"><script>if (window.top !== window.self) window.top.location = window.self.location.href;var OB_startTime = new Date().getTime(); var OB_loadErrors = []; function _onerror(e) { OB_loadErrors.push(e) }; if (document.addEventListener) document.addEventListener("error", _onerror, true); else if (document.attachEvent) document.attachEvent("onerror", _onerror); function _asyncScript(u) {var d = document, f = d.getElementsByTagName("script")[0], s = d.createElement("script"); s.type = "text/javascript"; s.async = true; s.src = u; f.parentNode.insertBefore(s, f);}function _asyncStyles(u) {var d = document, f = d.getElementsByTagName("script")[0], s = d.createElement("link"); s.rel = "stylesheet"; s.href = u; f.parentNode.insertBefore(s, f); return s}var _gaq = _gaq || [];_gaq.push(["_setAccount", "UA-24232453-2"]); _gaq.push(["_setDomainName", window.location.hostname]); _gaq.push(["_setAllowLinker", true]); _gaq.push(["_trackPageview"]);_asyncScript(("https:" == document.location.protocol ? "https://ssl" : "http://www") + ".google-analytics.com/ga.js");(new Image()).src = "/_/stat?event=pixel.load&origin=" + encodeURIComponent(location.origin);</script><script>(function () {var height = window.innerHeight || document.documentElement.clientHeight || document.body.clientHeight; var width = window.innerWidth || document.documentElement.clientWidth || document.body.clientWidth; document.write("<style>section.section-image--fullBleed.is-backgrounded {padding-top: " + Math.round(1.1 * height) + "px;}section.section-image--fullScreen.is-backgrounded, section.section-image--coverFade.is-backgrounded {min-height: " + height + "px; padding-top: " + Math.round(0.5 * height) + "px;}.u-sizeViewHeight100 {height: " + height + "px !important;}.u-sizeViewHeight110 {height: " + Math.round(1.1 * height) + "px !important;}.u-sizeViewHeightMin100 {min-height: " + height + "px !important;}.u-sizeViewHeightMax100 {max-height: " + height + "px !important;}section.section-image--coverFade {height: " + height + "px;}.section-aspectRatioViewportPlaceholder, .section-aspectRatioViewportCropPlaceholder {max-height: " + height + "px;}.section-aspectRatioViewportBottomSpacer, .section-aspectRatioViewportBottomPlaceholder {max-height: " + Math.round(0.5 * height) + "px;}.zoomable:before {top: " + (-1 * height) + "px; left: " + (-1 * width) + "px; padding: " + height + "px " + width + "px;}</style>");})()</script><!--[if lt IE 9]><script charset="UTF-8" src="https://cdn-static-1.medium.com/_/fp/js/shiv.RI2ePTZ5gFmMgLzG5bEVAA.js"></script><![endif]--><link rel="icon" href="https://cdn-static-1.medium.com/_/fp/icons/favicon-medium.TAS6uQ-Y7kcKgi0xjcYHXw.ico" class="js-favicon"><link rel="apple-touch-icon" sizes="152x152" href="https://cdn-images-1.medium.com/fit/c/152/152/1*NZsNSuNxe_O2YW1ybboOvA.jpeg"><link rel="apple-touch-icon" sizes="120x120" href="https://cdn-images-1.medium.com/fit/c/120/120/1*NZsNSuNxe_O2YW1ybboOvA.jpeg"><link rel="apple-touch-icon" sizes="76x76" href="https://cdn-images-1.medium.com/fit/c/76/76/1*NZsNSuNxe_O2YW1ybboOvA.jpeg"><link rel="apple-touch-icon" sizes="60x60" href="https://cdn-images-1.medium.com/fit/c/60/60/1*NZsNSuNxe_O2YW1ybboOvA.jpeg"><link rel="mask-icon" href="https://cdn-static-1.medium.com/_/fp/icons/favicon.KjTfUJo7yJH_fCoUzzH3cg.svg" color="#171717"></head><body itemscope class=" postShowScreen is-noJs"><script>document.body.className = document.body.className.replace(/(^|\s)is-noJs(\s|$)/, "$1is-js$2")</script><div class="site-main" id="container"><div class="butterBar butterBar--error"></div><div class="surface"><div id="prerendered" class="screenContent"><canvas class="canvas-renderer"></canvas><div class="container u-maxWidth740 u-xs-margin0 notesPositionContainer js-notesPositionContainer"></div><div class="metabar u-clearfix js-metabar u-textColorTransparentWhiteDarker u-tintBgColor u-tintSpectrum"><div class="metabar-inner u-marginAuto u-maxWidth1000 u-paddingLeft20 u-paddingRight20 js-metabarMiddle"><div class="metabar-block metabar-block--left u-floatLeft u-height65 u-xs-height56"><div class="u-alignMiddle u-inlineBlock u-verticalAlignTop u-height65 u-xs-height56 u-marginRight18"><div class="u-alignBlock"><a class="js-logCollection" href="https://ayearofai.com?source=logo-lo_ced97fb2944f---bb87da25612c"><img height="36" width="54" class="u-paddingTop5" src="https://cdn-images-1.medium.com/letterbox/54/36/50/50/1*NZsNSuNxe_O2YW1ybboOvA.jpeg?source=logoAvatar-lo_ced97fb2944f---bb87da25612c" alt="A Year of Artificial Intelligence" /></a></div></div><div class="u-alignMiddle u-inlineBlock u-verticalAlignTop u-height65 u-xs-height56 u-xs-hide"><div class="u-alignBlock"><div class="buttonSet u-lineHeightInherit u-marginLeft0"><div class="buttonSet-inner"><button class="button button--primary u-paddingLeft10 u-paddingRight10 u-height19 u-lineHeight13 u-verticalAlignMiddle u-fontSize12 u-uiTextMedium button--small u-noUserSelect button--withChrome u-accentColor--buttonNormal js-relationshipButton is-smallPill"  data-action="sign-in-prompt" data-sign-in-action="toggle-follow-collection" data-requires-token="true" data-redirect="https://medium.com/_/subscribe/collection/a-year-of-artificial-intelligence" data-collection-id="bb87da25612c"><span class="button-label  js-buttonLabel">Follow</span></button></div><a class="button button--light button--chromeless is-touchIconBlackPulse u-baseColor--buttonLight button--withIcon button--withSvgIcon"  href="https://twitter.com/mckapur" title="Visit ‚ÄúA Year of Artificial Intelligence‚Äù on Twitter" aria-label="Visit ‚ÄúA Year of Artificial Intelligence‚Äù on Twitter" rel="me" target="_blank"><span class="button-defaultState"><span class="svgIcon svgIcon--twitterFilled svgIcon--25px"><svg class="svgIcon-use" width="25" height="25" viewBox="0 0 25 25" ><path d="M21.725 5.338c-.744.47-1.605.804-2.513 1.006a3.978 3.978 0 0 0-2.942-1.293c-2.22 0-4.02 1.81-4.02 4.02 0 .32.034.63.07.94-3.31-.18-6.27-1.78-8.255-4.23a4.544 4.544 0 0 0-.574 2.01c.04 1.43.74 2.66 1.8 3.38-.63-.01-1.25-.19-1.79-.5v.08c0 1.93 1.38 3.56 3.23 3.95-.34.07-.7.12-1.07.14-.25-.02-.5-.04-.72-.07.49 1.58 1.97 2.74 3.74 2.8a8.49 8.49 0 0 1-5.02 1.72c-.3-.03-.62-.04-.93-.07A11.447 11.447 0 0 0 8.88 21c7.386 0 11.43-6.13 11.414-11.414.015-.21.01-.38 0-.578a7.604 7.604 0 0 0 2.01-2.08 7.27 7.27 0 0 1-2.297.645 3.856 3.856 0 0 0 1.72-2.23"/></svg></span></span></a><a class="button button--light button--chromeless is-touchIconBlackPulse u-baseColor--buttonLight button--withIcon button--withSvgIcon u-paddingLeft0"  href="//facebook.com/mckapur" title="Visit ‚ÄúA Year of Artificial Intelligence‚Äù on Facebook" aria-label="Visit ‚ÄúA Year of Artificial Intelligence‚Äù on Facebook" rel="me" target="_blank"><span class="button-defaultState"><span class="svgIcon svgIcon--facebookFilled svgIcon--25px"><svg class="svgIcon-use" width="25" height="25" viewBox="0 0 25 25" ><path d="M21 12.646C21 7.65 16.97 3.6 12 3.6s-9 4.05-9 9.046a9.026 9.026 0 0 0 7.59 8.924v-6.376H8.395V12.64h2.193v-1.88c0-2.186 1.328-3.375 3.267-3.375.93 0 1.728.07 1.96.1V9.77H14.47c-1.055 0-1.26.503-1.26 1.242v1.63h2.517l-.33 2.554H13.21V21.6c4.398-.597 7.79-4.373 7.79-8.954"/></svg></span></span></a></div></div></div></div><div class="metabar-block u-floatRight u-xs-absolute u-xs-textAlignRight u-xs-right0 u-xs-marginRight20 u-height65 u-xs-height56"><div class="u-alignMiddle u-inlineBlock u-verticalAlignTop u-height65 u-xs-height56"><div class="u-alignBlock"><div class="buttonSet u-lineHeightInherit"><a class="button button--primary button--light button--chromeless u-accentColor--buttonNormal is-inSiteNavBar u-lineHeight30 u-height32"  href="https://medium.com/m/signin?redirect=https%3A%2F%2Fayearofai.com%2Frohan-lenny-3-recurrent-neural-networks-10300100899b" data-action="sign-in-prompt" data-redirect="https://ayearofai.com/rohan-lenny-3-recurrent-neural-networks-10300100899b" data-action-source="nav_signup">Sign in / Sign up</a></div></div></div></div></div><div class="metabar-inner u-marginAuto u-maxWidth1000 js-metabarBottom"><nav role="navigation" class="metabar-block metabar-block--below u-overflowHiddenY u-height40"><ul class="u-textAlignLeft u-noWrap u-overflowX js-collectionNavItems u-sm-paddingLeft20 u-sm-paddingRight20 u-paddingBottom100"><li class="metabar-navItem js-collectionNavItem u-uiTextMedium u-fontSize14 u-inlineBlock u-textUppercase u-letterSpacing003 u-textColorNormal u-xs-paddingRight12 u-xs-marginRight0 u-paddingTop5 u-xs-paddingTop10"><a class="link link--darken u-accentColor--textDarken js-homeNav u-baseColor--link"  href="https://ayearofai.com">Home</a></li><li class="metabar-navItem js-collectionNavItem  u-uiTextMedium u-fontSize14 u-inlineBlock u-textUppercase u-letterSpacing003 u-textColorNormal u-xs-paddingRight12 u-xs-marginRight0 u-paddingTop5 u-xs-paddingTop10"><a class="link link--darken u-accentColor--textDarken link--noUnderline js-navItemLink u-baseColor--link"  href="https://ayearofai.com/tagged/algorithms">Algorithms</a></li><li class="metabar-navItem js-collectionNavItem  u-uiTextMedium u-fontSize14 u-inlineBlock u-textUppercase u-letterSpacing003 u-textColorNormal u-xs-paddingRight12 u-xs-marginRight0 u-paddingTop5 u-xs-paddingTop10"><a class="link link--darken u-accentColor--textDarken link--noUnderline js-navItemLink u-baseColor--link"  href="https://ayearofai.com/tagged/today-i-learned">Today I Learned</a></li><li class="metabar-navItem js-collectionNavItem  u-uiTextMedium u-fontSize14 u-inlineBlock u-textUppercase u-letterSpacing003 u-textColorNormal u-xs-paddingRight12 u-xs-marginRight0 u-paddingTop5 u-xs-paddingTop10"><a class="link link--darken u-accentColor--textDarken link--noUnderline js-navItemLink u-baseColor--link"  href="https://ayearofai.com/tagged/case-studies">Case Studies</a></li><li class="metabar-navItem js-collectionNavItem  u-uiTextMedium u-fontSize14 u-inlineBlock u-textUppercase u-letterSpacing003 u-textColorNormal u-xs-paddingRight12 u-xs-marginRight0 u-paddingTop5 u-xs-paddingTop10"><a class="link link--darken u-accentColor--textDarken link--noUnderline js-navItemLink u-baseColor--link"  href="https://ayearofai.com/tagged/philosophical">Philosophical</a></li><li class="metabar-navItem js-collectionNavItem  u-uiTextMedium u-fontSize14 u-inlineBlock u-textUppercase u-letterSpacing003 u-textColorNormal u-xs-paddingRight12 u-xs-marginRight0 u-paddingTop5 u-xs-paddingTop10"><a class="link link--darken u-accentColor--textDarken link--noUnderline js-navItemLink u-baseColor--link"  href="https://ayearofai.com/tagged/meta">Meta</a></li><li class="metabar-navItem js-collectionNavItem u-uiTextMedium u-fontSize14 u-inlineBlock u-textUppercase u-letterSpacing003 u-textColorNormal u-xs-paddingRight12 u-xs-marginRight0 u-paddingTop5 u-xs-paddingTop10"><a class="button button--chromeless u-baseColor--buttonNormal button--withIcon button--withSvgIcon u-top1"  href="https://ayearofai.com/search" title="Search" aria-label="Search"><span class="button-defaultState"><span class="svgIcon svgIcon--search svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"  viewBox="0 0 25 25"><path d="M20.067 18.933l-4.157-4.157a6 6 0 1 0-.884.884l4.157 4.157a.624.624 0 1 0 .884-.884zM6.5 11c0-2.62 2.13-4.75 4.75-4.75S16 8.38 16 11s-2.13 4.75-4.75 4.75S6.5 13.62 6.5 11z"/></svg></span></span></a></li></ul></nav></div></div><div class="metabar metabar--spacer js-metabarSpacer u-tintBgColor  u-height105 u-xs-height95"></div><main role="main"><article class=" u-sizeViewHeightMin100 u-overflowHidden postArticle postArticle--full is-withAccentColors u-marginBottom40"  lang="en"><header class="container u-maxWidth740"><div class="postMetaHeader u-paddingBottom10 row"><div class="col u-size12of12 js-postMetaLockup"><div class="postMetaLockup postMetaLockup--authorWithBio u-flex js-postMetaLockup"><div class="u-flex0"><a class="link avatar u-baseColor--link"  href="https://ayearofai.com/@mckapur?source=post_header_lockup" data-action="show-user-card" data-action-source="post_header_lockup" data-action-value="cb55958ea3bb" data-action-type="hover" data-user-id="cb55958ea3bb" data-collection-slug="a-year-of-artificial-intelligence" dir="auto"><img  src="https://cdn-images-1.medium.com/fit/c/60/60/1*pGDfwp8yLzgCeG1RkPUxgQ.jpeg" class="avatar-image avatar-image--small" alt="Go to the profile of Rohan Kapur"></a></div><div class="u-flex1 u-paddingLeft15 u-overflowHidden"><a class="link link link--darken link--darker u-baseColor--link"  href="https://ayearofai.com/@mckapur?source=post_header_lockup" data-action="show-user-card" data-action-source="post_header_lockup" data-action-value="cb55958ea3bb" data-action-type="hover" data-user-id="cb55958ea3bb" data-collection-slug="a-year-of-artificial-intelligence" dir="auto">Rohan Kapur</a><span class="followState js-followState buttonSet-inner" data-user-id="cb55958ea3bb"><button class="button u-paddingLeft10 u-paddingRight10 u-height19 u-lineHeight13 u-verticalAlignMiddle u-fontSize12 u-uiTextMedium u-noUserSelect button--withChrome u-baseColor--buttonNormal button--withHover button--unblock js-unblockButton u-marginLeft10 u-marginTopNegative2 u-xs-hide"  data-action="sign-in-prompt" data-sign-in-action="toggle-block-user" data-requires-token="true" data-action-source="post_header_lockup"><span class="button-label  button-defaultState">Blocked</span><span class="button-label button-hoverState">Unblock</span></button><button class="button button--primary u-paddingLeft10 u-paddingRight10 u-height19 u-lineHeight13 u-verticalAlignMiddle u-fontSize12 u-uiTextMedium u-noUserSelect button--withChrome u-accentColor--buttonNormal button--follow js-followButton u-marginLeft10 u-marginTopNegative2 u-xs-hide"  data-action="sign-in-prompt" data-sign-in-action="toggle-subscribe-user" data-requires-token="true" data-redirect="https://medium.com/_/subscribe/user/cb55958ea3bb" data-action-source="post_header_lockup_follow"><span class="button-label  button-defaultState js-buttonLabel">Follow</span><span class="button-label button-activeState">Following</span></button></span><div class="postMetaInline u-noWrapWithEllipsis u-xs-normalWrap u-xs-lineClamp2">rohankapur.com</div><div class="postMetaInline js-testPostMetaInlineSupplemental"><time datetime="2017-04-13T08:27:31.153Z">Apr 13</time><span class="middotDivider u-fontSize12"></span><span class="readingTime" title="84 min read"></span></div></div></div></div></div></header><div class="postArticle-content js-postField js-notesSource js-trackedPost"  data-post-id="10300100899b" data-source="post_page" data-collection-id="bb87da25612c" data-tracking-context="postPage"><section name="20d1" class="section section--body section--first"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--fullWidth"><figure name="be8e" id="be8e" class="graf graf--figure graf--layoutFillWidth graf--leading"><div class="aspectRatioPlaceholder is-locked"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 45.4%;"></div><img class="graf-image" data-image-id="1*khIKl9t4XmZGSsKhW_Yg2w.png" data-width="2000" data-height="907" src="https://cdn-images-1.medium.com/max/2000/1*khIKl9t4XmZGSsKhW_Yg2w.png"></div><figcaption class="imageCaption">Sequences upon sequences upon sequences. Sequen-ception.</figcaption></figure></div><div class="section-inner sectionLayout--insetColumn"><h1 name="84b9" id="84b9" class="graf graf--h3 graf-after--figure graf--title">Rohan &amp; Lenny #3: Recurrent Neural Networks &amp;¬†LSTMs</h1><h2 name="9485" id="9485" class="graf graf--h4 graf-after--h3 graf--trailing graf--subtitle">The ultimate guide to machine learning‚Äôs favorite¬†child.</h2></div></div></section><section name="4110" class="section section--body"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><blockquote name="4569" id="4569" class="graf graf--pullquote graf--leading graf--trailing">This is the third group (<a href="https://medium.com/@lennykhazan" data-href="https://medium.com/@lennykhazan" data-anchor-type="2" data-user-id="de8e2540b759" data-action-value="de8e2540b759" data-action="show-user-card" data-action-type="hover" class="markup--user markup--pullquote-user" target="_blank">Lenny</a> and <a href="https://medium.com/@mckapur" data-href="https://medium.com/@mckapur" data-anchor-type="2" data-user-id="cb55958ea3bb" data-action-value="cb55958ea3bb" data-action="show-user-card" data-action-type="hover" class="markup--user markup--pullquote-user" target="_blank">Rohan</a>) entry in our <a href="https://medium.com/a-year-of-artificial-intelligence" data-href="https://medium.com/a-year-of-artificial-intelligence" class="markup--anchor markup--pullquote-anchor" target="_blank">journey</a> to extend our knowledge of artificial intelligence and convey that knowledge in a simple, fun, and accessible manner. Learn more about our motives in this <a href="https://medium.com/a-year-of-artificial-intelligence/0-2016-is-the-year-i-venture-into-artificial-intelligence-d702d65eb919#.bfjoaqxu5" data-href="https://medium.com/a-year-of-artificial-intelligence/0-2016-is-the-year-i-venture-into-artificial-intelligence-d702d65eb919#.bfjoaqxu5" class="markup--anchor markup--pullquote-anchor" target="_blank">introduction</a> post.</blockquote></div></div></section><section name="6590" class="section section--body"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><p name="a16a" id="a16a" class="graf graf--p graf--hasDropCapModel graf--hasDropCap graf--leading"><span class="graf-dropCap">It</span> seems like most of our posts on this blog start with ‚ÄúWe‚Äôre back!‚Äù, so‚Ä¶ you know the drill. It‚Äôs been a while since our last post‚Ää‚Äî‚Ääjust over 5 months‚Ää‚Äî‚Ääbut it certainly doesn‚Äôt feel that way. Whether our articles are more spaced out than we‚Äôd like them to be, well, we haven‚Äôt actually discussed that yet. But I, Rohan, would definitely like to get into a more frequent routine. Since November, we‚Äôve been grinding on school (basically, getting it over and done with), banging out <a href="http://getcontra.com" data-href="http://getcontra.com" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Contra</a> v2, and lazing around more than we should. End of senior year is a fun time.</p><figure name="ac01" id="ac01" class="graf graf--figure graf--layoutOutsetLeft graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 525px; max-height: 358px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 68.2%;"></div><img class="graf-image" data-image-id="1*N5l-Uj5yUVPIDwIuGUvnfg.jpeg" data-width="570" data-height="389" data-action="zoom" data-action-value="1*N5l-Uj5yUVPIDwIuGUvnfg.jpeg" src="https://cdn-images-1.medium.com/max/600/1*N5l-Uj5yUVPIDwIuGUvnfg.jpeg"></div></figure><p name="93ae" id="93ae" class="graf graf--p graf-after--figure">It‚Äôs 2017. We started A <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">Year</em></strong><em class="markup--em markup--p-em"> </em>Of AI in 2016. Last year. Don‚Äôt panic, though. If you‚Äôve read our <a href="https://ayearofai.com/the-goal-of-our-blog-c104d7b6377a#.75o5qyayi" data-href="https://ayearofai.com/the-goal-of-our-blog-c104d7b6377a#.75o5qyayi" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">letter</a>, you‚Äôll know that, despite our name and inception date, we‚Äôre not going anywhere anytime soon. There‚Äôs a good chance we‚Äôll move off Medium, but we‚Äôre still both obsessed with AI and writing these posts to hopefully make other people obsessed, as well.</p><p name="f6d1" id="f6d1" class="graf graf--p graf-after--p">I wrote the first article on this blog just over a year ago, and mentioned that my goal for the year was to be accepted into Stanford University as an undergrad student. A few months ago, I achieved this goal. At Stanford, I‚Äôll probably be studying <a href="https://symsys.stanford.edu/" data-href="https://symsys.stanford.edu/" class="markup--anchor markup--p-anchor" rel="noopener nofollow noopener" target="_blank">Symbolic Systems</a>, which is a program that explores both the humanities and STEM to inform an understanding of artificial intelligence and the nature of minds. Needless to say, A Year of AI will continue to document the new things I learn üòÄ.</p><p name="b842" id="b842" class="graf graf--p graf-after--p">Anyways, you can find plenty of articles on recurrent neural networks (RNNs) online. My <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/" data-href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">favorite one</a>, personally, is from Andrej Karpathy‚Äôs blog. I read it about 1.5 years ago when I was learning about RNNs. We definitely think there‚Äôs space to simplify the topic even more, though. As usual, that‚Äôs our aim for the article‚Ää‚Äî‚Ääto teach you RNNs in a fun, simple manner. We‚Äôre also importantly doing this for completion purposes; we want people to hop onto A Year of AI and be able to work their way up all the way from logistic regression to neural machine translation (don‚Äôt worry, you‚Äôll find out what means soon enough), and thus recurrent neural networks is a vital addition. After this, we want to look at and summarize/simplify a bunch of new super interesting research papers, and for most of them RNNs are a key ingredient. Finally, we think this article contains so much meat and ties together content unlike any other RNN tutorial on the interwebs.</p><p name="60e2" id="60e2" class="graf graf--p graf-after--p">Before we get started, you should try to familiarize yourself with ‚Äúvanilla‚Äù neural networks. If you need a refresher, check out our <a href="https://ayearofai.com/rohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d#.quwnoqtot" data-href="https://ayearofai.com/rohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d#.quwnoqtot" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">neural networks and backpropogation mega-post</a> from earlier this year. This is so you know the basics of machine learning, linear algebra, neural network architecture, cost functions, optimization methods, training/test sets, activation functions/what they do, softmax, etc. Reading our article on <a href="https://ayearofai.com/rohan-lenny-2-convolutional-neural-networks-5f4cd480a60b#.ebwc0kf4z" data-href="https://ayearofai.com/rohan-lenny-2-convolutional-neural-networks-5f4cd480a60b#.ebwc0kf4z" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">convolutional neural networks</a> may also make you more comfortable entering this post, especially because we often reference CNNs. Checking out <a href="https://ayearofai.com/rohan-4-the-vanishing-gradient-problem-ec68f76ffb9b#.58nevwsoa" data-href="https://ayearofai.com/rohan-4-the-vanishing-gradient-problem-ec68f76ffb9b#.58nevwsoa" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">this</a> article I wrote on vanishing gradients will help later on, as well.</p><p name="81ec" id="81ec" class="graf graf--p graf-after--p">Rule of thumb: the more you know, the better!</p><h3 name="3288" id="3288" class="graf graf--h3 graf-after--p">Table of¬†Contents</h3><p name="003f" id="003f" class="graf graf--p graf-after--h3">I can‚Äôt link to each section, but here‚Äôs what we cover in this article (save the intro and conclusion):</p><ol class="postList"><li name="8cfa" id="8cfa" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">What can RNNs do? </strong>Where we look at‚Ä¶ what RNNs can do!</li><li name="7f98" id="7f98" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Why? </strong>Where we talk about the gap that RNNs fill in machine learning‚Äôs suite of algorithms.</li><li name="de84" id="de84" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Show me. </strong>Where we visualize RNNs for the first time.</li><li name="f2f4" id="f2f4" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Formalism. </strong>Where we walk through how an RNN mathematically works with proper notation.</li><li name="c163" id="c163" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">An example? Okay! </strong>Where we walk through, qualitatively, a simple application of RNNs and how the RNN operates in this application, including techniques we can use.</li><li name="394e" id="394e" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Training (or, why vanilla RNNs suck.) </strong>Where we talk about how to train RNNs, and why vanilla RNNs are bad at learning.</li><li name="5756" id="5756" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Fixing the problem with LSTMs (Part I). </strong>Where we introduce the solution to vanilla RNNs‚Äô inability to learn: LSTMs.</li><li name="654d" id="654d" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Fixing the problem with LSTMs (Part II). </strong>Where we analyze on a close, technical level, the reasons LSTMs don‚Äôt suffer from vanishing gradients as much (and why they still do, to an extent). Then we conclude LSTMs with final thoughts on and facts about them.</li><li name="2dfc" id="2dfc" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Yay RNNs! </strong>Where <em class="markup--em markup--li-em">you </em>get to see neat little things RNNs have done!</li><li name="e56c" id="e56c" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">In Practice. </strong>Where we look at more technical and important applications and case studies of RNNs, including other variations of RNNs, especially as relevant in hot/recent research papers.</li><li name="7bc0" id="7bc0" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Building a Vanilla Recurrent Neural Network. </strong>Where you get to code your very first RNN! Woohoo!</li></ol><h3 name="02a8" id="02a8" class="graf graf--h3 graf-after--li">What can RNNs¬†do?</h3><p name="1c1d" id="1c1d" class="graf graf--p graf-after--h3">There are a number of very important tasks that ANNs and CNNs cannot solve, that RNNs are used for instead. Tasks like: image captioning, language translation, sentiment classification, predictive typing, video classification, natural language processing, speech recognition, and a <em class="markup--em markup--p-em">lot </em>more interesting things that have been presented in recent research papers (for example‚Ä¶ <a href="https://arxiv.org/pdf/1606.04474.pdf" data-href="https://arxiv.org/pdf/1606.04474.pdf" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">learning to learn by gradient descent by gradient descent</a>!).</p><figure name="1d0f" id="1d0f" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 393px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 56.10000000000001%;"></div><img class="graf-image" data-image-id="1*X5dk-xGw2yNYsEB3QvHWIA.png" data-width="713" data-height="400" data-action="zoom" data-action-value="1*X5dk-xGw2yNYsEB3QvHWIA.png" src="https://cdn-images-1.medium.com/max/800/1*X5dk-xGw2yNYsEB3QvHWIA.png"></div><figcaption class="imageCaption">Image captioning, taken from CS231n slides: <a href="http://cs231n.stanford.edu/slides/2016/winter1516_lecture10.pdf" data-href="http://cs231n.stanford.edu/slides/2016/winter1516_lecture10.pdf" class="markup--anchor markup--figure-anchor" rel="nofollow noopener noopener noopener" target="_blank">http://cs231n.stanford.edu/slides/2016/winter1516_lecture10.pdf</a></figcaption></figure><p name="02ef" id="02ef" class="graf graf--p graf-after--figure">RNNs are very powerful. Y‚Äôknow how regular neural networks have been proved to be ‚Äúuniversal function approximators‚Äù¬†? If you didn‚Äôt:</p><blockquote name="8206" id="8206" class="graf graf--blockquote graf-after--p">In the mathematical theory of artificial neural networks, the universal approximation theorem states that a feed-forward network with a single hidden layer containing a finite number of neurons can approximate continuous functions on compact subsets of R^n, under mild assumptions on the activation function.</blockquote><p name="314e" id="314e" class="graf graf--p graf-after--blockquote">That‚Äôs pretty confusing. Basically, what this states is that an artificial neural network can compute any function. Even if someone gives you an extremely wiggly, complex looking function, it‚Äôs guaranteed that there exists a neural network that can produce (or at least extremely closely approximate) it. The proof itself is very complex, but <a href="http://neuralnetworksanddeeplearning.com/chap4.html" data-href="http://neuralnetworksanddeeplearning.com/chap4.html" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">this</a> is a brilliant article offering a visual approach as to why<em class="markup--em markup--p-em"> </em>it‚Äôs true.</p><p name="a351" id="a351" class="graf graf--p graf-after--p">So, that‚Äôs great. ANNs are universal function approximators. RNNs take it a step further, though; <a href="http://stats.stackexchange.com/a/221142/98975" data-href="http://stats.stackexchange.com/a/221142/98975" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">they can compute/describe <em class="markup--em markup--p-em">programs</em></a>. In fact, some RNNs with proper weights and architecture qualify as Turing Complete:</p><blockquote name="8f10" id="8f10" class="graf graf--blockquote graf-after--p">A Turing Complete system means a system in which a program can be written that will find an answer (although with no guarantees regarding runtime or memory).</blockquote><blockquote name="c6a2" id="c6a2" class="graf graf--blockquote graf-after--blockquote">So, if somebody says ‚Äúmy new thing is Turing Complete‚Äù that means in principle (although <strong class="markup--strong markup--blockquote-strong">often not in practice</strong>) it could be used to solve any computation problem.</blockquote><blockquote name="1429" id="1429" class="graf graf--blockquote graf-after--blockquote">‚Äî <a href="http://stackoverflow.com/a/7320/1260708" data-href="http://stackoverflow.com/a/7320/1260708" class="markup--anchor markup--blockquote-anchor" rel="nofollow noopener noopener" target="_blank">http://stackoverflow.com/a/7320/1260708</a></blockquote><p name="2ba5" id="2ba5" class="graf graf--p graf-after--blockquote">That‚Äôs cool, isn‚Äôt it? Now, this is all theoretical, and in practice means less than you think, so don‚Äôt get too hyped. Hopefully, though, this gives some more insight into why RNNs are super important for future developments in machine learning‚Ää‚Äî‚Ääand why you should read on.</p><p name="662c" id="662c" class="graf graf--p graf-after--p">At this point, if you weren‚Äôt previously hooked on learning what the heck these things are, you should be now. (If you still aren‚Äôt, just bare with me. Things will get spicy soon.) So, let‚Äôs dive in.</p><h3 name="72f6" id="72f6" class="graf graf--h3 graf-after--p">Why?</h3><p name="54dc" id="54dc" class="graf graf--p graf-after--h3">We took a bit of a detour to talk about how great RNNs are, but haven‚Äôt focused on <em class="markup--em markup--p-em">why</em> ANNs can‚Äôt perform well in the tasks that RNNs can.</p><blockquote name="b88b" id="b88b" class="graf graf--blockquote graf-after--p">Why do we need another neural network model? Why do we need recurrent neural networks when we already have the beloved ANNs (and CNNs) in all their glory?</blockquote><p name="95c9" id="95c9" class="graf graf--p graf-after--blockquote">It boils down to a few things:</p><ul class="postList"><li name="3d40" id="3d40" class="graf graf--li graf-after--p">ANNs can‚Äôt deal with sequential or ‚Äútemporal‚Äù data</li><li name="2a22" id="2a22" class="graf graf--li graf-after--li">ANNs lack memory</li><li name="0d57" id="0d57" class="graf graf--li graf-after--li">ANNs have a fixed architecture</li><li name="fbb9" id="fbb9" class="graf graf--li graf-after--li">RNNs are more ‚Äúbiologically realistic‚Äù because of the recurrent connectivity found in the visual cortex of the brain</li></ul><p name="341a" id="341a" class="graf graf--p graf-after--li">Let‚Äôs address the first three points individually. The first issue refers to the fact that ANNs have a <em class="markup--em markup--p-em">fixed input size </em>and a <em class="markup--em markup--p-em">fixed output size</em>. ANNs have an elaborate list of hyperparameters, and this notably includes the number of neurons in the input layer and output layer. But what if we wanted input data and/or output data of <em class="markup--em markup--p-em">variable</em> size, instead of something that needs to have its size as a preset constant? RNNs allow us to do that. In this aspect, they offer more flexibility than ANNs.</p><figure name="605a" id="605a" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 500px; max-height: 309px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 61.8%;"></div><img class="graf-image" data-image-id="1*BQ0SxdqC9Pl_3ZQtd3e45A.png" data-width="500" data-height="309" src="https://cdn-images-1.medium.com/max/800/1*BQ0SxdqC9Pl_3ZQtd3e45A.png"></div><figcaption class="imageCaption">We might choose this architecture for our ANN, with 4 inputs and 1 output. But that‚Äôs it‚Ää‚Äî‚Ääwe can‚Äôt input a vector with 5 values, for example. <a href="https://qph.ec.quoracdn.net/main-qimg-050d12c5da82f4f97fdd942d7777b8e4" data-href="https://qph.ec.quoracdn.net/main-qimg-050d12c5da82f4f97fdd942d7777b8e4" class="markup--anchor markup--figure-anchor" rel="nofollow noopener noopener" target="_blank">https://qph.ec.quoracdn.net/main-qimg-050d12c5da82f4f97fdd942d7777b8e4</a>.</figcaption></figure><p name="1c8e" id="1c8e" class="graf graf--p graf-after--figure">I‚Äôll give you a couple examples of why this matters.</p><p name="e228" id="e228" class="graf graf--p graf-after--p">It‚Äôs unclear how we could use an ANN by itself to perform a task like image captioning, because the network would need to output a sentence‚Ää‚Äî‚Ääa list of words in a specific order‚Ää‚Äî‚Ääwhich is a<em class="markup--em markup--p-em"> sequence</em>.<em class="markup--em markup--p-em"> </em>It would be a sequence of vectors, because each word would need to be represented numerically. In machine learning and data science, we represent words numerically as vectors; these are called word embeddings. An ANN can only output a <em class="markup--em markup--p-em">single </em>word/label, like in image classification where we treat the output as the label with the highest value in the final vector that is a softmax probability distribution over all classes. The only way to make sentences work with ANNs would be to have billions of output neurons that each map to a single possible sentence in the permutation of all [sensible] sentences that can be formed by the vocabulary we have. And that doesn‚Äôt sound like a good idea.</p><figure name="8f28" id="8f28" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 305px; max-height: 182px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 59.699999999999996%;"></div><img class="graf-image" data-image-id="1*GFVoFpD6cdCY_PGqnjhOlQ.png" data-width="305" data-height="182" src="https://cdn-images-1.medium.com/max/800/1*GFVoFpD6cdCY_PGqnjhOlQ.png"></div><figcaption class="imageCaption">A reminder of what the output of an ANN looks like‚Ää‚Äî‚Ääa probability distribution over classes‚Ää‚Äî‚Ääand how we convert that into a single final result (one-hot encoding): by taking the label with the greatest probability and making it 1, with the rest¬†0.</figcaption></figure><p name="f8ec" id="f8ec" class="graf graf--p graf-after--figure">Wow, that was a lot of words. Nevertheless, I hope it‚Äôs clear that, with ANNs, there‚Äôs no feasible way to output a sequence.</p><p name="f308" id="f308" class="graf graf--p graf-after--p">Now, what about <em class="markup--em markup--p-em">inputting </em>a sequence into an ANN? In other words, ‚Äútemporal‚Äù data: data that varies over time, and is thus a sequence. Take the example of sentiment classification where we input a sentence (sequence of words = sequence of vectors = sequence of set of values where each value goes into an individual neuron) and want to output its sentiment: positive or negative. The output part seems easy, because it‚Äôs just one neuron that‚Äôs either rounded to 1 (positive) or 0 (negative). And, for the input, you might be thinking: couldn‚Äôt we input each ‚Äúset of values‚Äù separately? Input the first word, wait for the neural net to fully feed forward and produce an output, then input the next word, etc. etc.</p><p name="8029" id="8029" class="graf graf--p graf-after--p">Let‚Äôs take the case of this utterly false, and most certainly negative sentence, to evaluate:</p><figure name="6e25" id="6e25" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 625px; max-height: 139px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 22.2%;"></div><img class="graf-image" data-image-id="1*yq_zmka1ssikrmD9GkWmnw.png" data-width="625" data-height="139" src="https://cdn-images-1.medium.com/max/800/1*yq_zmka1ssikrmD9GkWmnw.png"></div><figcaption class="imageCaption">This is just an alternative fact, believe me! Lenny is actually a <em class="markup--em markup--figure-em">great</em><strong class="markup--strong markup--figure-strong"><em class="markup--em markup--figure-em"> </em></strong><em class="markup--em markup--figure-em">coder. The best I know of. The¬†best.</em></figcaption></figure><p name="322d" id="322d" class="graf graf--p graf-after--figure">We‚Äôd input ‚ÄúLenny‚Äù first, then ‚ÄúKhazan‚Äù, then ‚Äúis‚Äù, etc. But, at each feedforward iteration, the output would be completely useless. Why? Because the output would be dependent on <em class="markup--em markup--p-em">only </em>that word. We‚Äôd be finding the sentiment of a single word, which is useless, because we want the sentiment of the entire sentence. Sentiment analysis only makes sense when all the words come together, dependent on each other, to form a sentence.</p><p name="13ff" id="13ff" class="graf graf--p graf-after--p">Think of it this way‚Ää‚Äî‚Ääthis means you‚Äôre essentially running a neural network a bunch of times, just with new data at each separate iteration. Those run-throughs aren‚Äôt linked in any way; they‚Äôre independent. Once you feedforward and fully run the neural network, it forgets everything it just did. This sentence only makes sense and can only be interpretable because it‚Äôs a collection of words put together in a specific order to form <em class="markup--em markup--p-em">meaning</em>. The relevance of each word is dependent on the words that precede it: the context. This is why RNNs are being used heavily in NLP; they retain context by having <em class="markup--em markup--p-em">memory</em>. ANNs have no memory.</p><p name="e71f" id="e71f" class="graf graf--p graf-after--p">I like this quote from another article on RNNs:</p><blockquote name="9cd3" id="9cd3" class="graf graf--blockquote graf-after--p">Humans don‚Äôt start their thinking from scratch every second. As you read this essay, you understand each word based on your understanding of previous words. You don‚Äôt throw everything away and start thinking from scratch again. Your thoughts have persistence.</blockquote><blockquote name="b760" id="b760" class="graf graf--blockquote graf-after--blockquote">‚Äî <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" data-href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" class="markup--anchor markup--blockquote-anchor" rel="nofollow noopener noopener" target="_blank">http://colah.github.io/posts/2015‚Äì08-Understanding-LSTMs/</a></blockquote><p name="e0e4" id="e0e4" class="graf graf--p graf-after--blockquote">(Furthermore, take the case where we had sequential data in <em class="markup--em markup--p-em">both </em>the input and the output. Translating one language to another is a good example of this. Clearly, ANNs aren‚Äôt the answer.)</p><p name="fa2f" id="fa2f" class="graf graf--p graf-after--p">RNNs don‚Äôt just need memory; they need <em class="markup--em markup--p-em">long term</em> memory. Let‚Äôs take the example of predictive typing. Let‚Äôs say we typed the following sentence in an SMS message to 911, and the operating system needs to fill in the blank:</p><figure name="2c67" id="2c67" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 543px; max-height: 90px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 16.6%;"></div><img class="graf-image" data-image-id="1*TugitPvwm_IZqAdAPR-7UA.png" data-width="543" data-height="90" src="https://cdn-images-1.medium.com/max/800/1*TugitPvwm_IZqAdAPR-7UA.png"></div></figure><figure name="3077" id="3077" class="graf graf--figure graf-after--figure"><div class="aspectRatioPlaceholder is-locked" style="max-width: 250px; max-height: 250px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 100%;"></div><img class="graf-image" data-image-id="1*7l0aNgpXDXZnY-P9C8K4IA.jpeg" data-width="250" data-height="250" src="https://cdn-images-1.medium.com/max/800/1*7l0aNgpXDXZnY-P9C8K4IA.jpeg"></div><figcaption class="imageCaption">The face of a criminal?</figcaption></figure><p name="325c" id="325c" class="graf graf--p graf-after--figure">Here, if the RNN wasn‚Äôt able to look back much (ie. before ‚Äúshould‚Äù), then many different options could arise:</p><figure name="da28" id="da28" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 552px; max-height: 266px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 48.199999999999996%;"></div><img class="graf-image" data-image-id="1*aMJu60wscb9m4A-Zn_xyqQ.png" data-width="552" data-height="266" src="https://cdn-images-1.medium.com/max/800/1*aMJu60wscb9m4A-Zn_xyqQ.png"></div><figcaption class="imageCaption">Lenny in the military? Make it into a TV show! I‚Äôd watch¬†it.</figcaption></figure><p name="930e" id="930e" class="graf graf--p graf-after--figure">The word ‚Äúsent‚Äù would indicate to the RNN that a location needs to be outputted. However, if the RNN was able to retain information from all the way back, such as the word ‚Äúcriminal‚Äù, then it would be much more confident that:</p><figure name="0990" id="0990" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 557px; max-height: 102px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 18.3%;"></div><img class="graf-image" data-image-id="1*4CZskdiGqIx29BQylqnYuA.png" data-width="557" data-height="102" src="https://cdn-images-1.medium.com/max/800/1*4CZskdiGqIx29BQylqnYuA.png"></div></figure><p name="c9f1" id="c9f1" class="graf graf--p graf-after--figure">The probability of outputting ‚Äújail‚Äù drastically increases when it sees the word ‚Äúcriminal‚Äù is present. That‚Äôs why context matters, be it predictive typing, image captioning, machine translation, etc. The output or outputs of a recurrent neural network will always be functionally dependent on (meaning, a function of) information from the very beginning, but how much it chooses to ‚Äúforget‚Äù or ‚Äúretain‚Äù (that is, varying degrees of influence from earlier information) depends on the weights that it learns from the training data.</p><p name="eb76" id="eb76" class="graf graf--p graf-after--p">As it turns out, RNNs‚Ää‚Äî‚Ääespecially deep ones‚Ää‚Äî‚Ääare rarely good at retaining much information, due to an issue called the vanishing gradient problem. That‚Äôs where we turn to other variants of RNNs such as LSTMs and GRUs. But, more on that later.</p><p name="c00c" id="c00c" class="graf graf--p graf-after--p">To address the third point, one more constraint with ANNs is that they have a fixed number of computation/processing steps (because, once again, the number of hidden layers is a hyperparameter). With RNNs, we can have much more dynamic processing since we operate over vectors. Each neuron in an RNN is almost like an entire layer in an ANN; this will make more sense as we bring up an illustration for you. <strong class="markup--strong markup--p-strong">Exciting stuff.</strong></p><h3 name="4abf" id="4abf" class="graf graf--h3 graf-after--p">Show me.</h3><p name="0160" id="0160" class="graf graf--p graf-after--h3">OK, that‚Äôs enough teasing. Three sections into the article, and you‚Äôre yet to see what an RNN looks like, or appreciate how it really works. Everything comes in due time, though!</p><p name="d391" id="d391" class="graf graf--p graf-after--p">The first thing I‚Äôm going to do is show you what a normal ANN diagram looks like:</p><figure name="ce30" id="ce30" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 310px; max-height: 220px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 71%;"></div><img class="graf-image" data-image-id="1*GapzcZDrwnVbflhlRoWZ9g.png" data-width="310" data-height="220" src="https://cdn-images-1.medium.com/max/800/1*GapzcZDrwnVbflhlRoWZ9g.png"></div></figure><p name="deda" id="deda" class="graf graf--p graf-after--figure">Each neuron stores a single scalar value. Thus, each layer can be considered a vector.</p><p name="f077" id="f077" class="graf graf--p graf-after--p">Now I‚Äôm going to show you what this ANN looks like in our RNN visual notation:</p><figure name="c5e9" id="c5e9" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 300px; max-height: 65px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 21.7%;"></div><img class="graf-image" data-image-id="1*ntKLnv52DCUnkseNcm91iQ.png" data-width="300" data-height="65" src="https://cdn-images-1.medium.com/max/800/1*ntKLnv52DCUnkseNcm91iQ.png"></div></figure><p name="f1fc" id="f1fc" class="graf graf--p graf-after--figure">The two diagrams above represent the same thing. The latter, obviously, looks more succinct than the former. That‚Äôs because, with our RNN visual notation, each neuron (inputs, hidden(s), and outputs) contains a <strong class="markup--strong markup--p-strong">vector </strong>of information. The term ‚Äúcell‚Äù is also used, and is interchangeable with neuron. (I‚Äôll use the latter instead of the former.) Red is the input neuron, blue is the hidden neuron, and green is the output neuron. Therefore, an entire ANN layer is encapsulated into one neuron with our RNN illustration. All operations in RNNs, like the mapping from one neuron‚Äôs state to another, are over entire vectors, compared to individual scalars that are summed up with ANNs.</p><p name="abee" id="abee" class="graf graf--p graf-after--p">Let‚Äôs flip it the other way:</p><figure name="535f" id="535f" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 50px; max-height: 231px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 462%;"></div><img class="graf-image" data-image-id="1*HewvhrMCcdy-oQcpeeNJ9w.png" data-width="50" data-height="231" src="https://cdn-images-1.medium.com/max/800/1*HewvhrMCcdy-oQcpeeNJ9w.png"></div></figure><p name="8a15" id="8a15" class="graf graf--p graf-after--figure">This is in fact a type of recurrent neural network‚Ää‚Äî‚Ääa <strong class="markup--strong markup--p-strong">one to one</strong> recurrent net, because it maps one input to one output. A one to one recurrent net is equivalent to an artificial neural net.</p><p name="b960" id="b960" class="graf graf--p graf-after--p">We can have a one to <em class="markup--em markup--p-em">many</em> recurrent net, where one input is mapped to multiple outputs. An example of this would be image captioning‚Ää‚Äî‚Ääthe input would be the image in some processed form (usually the result of a CNN analyzing the image), and the output would be a sequence of words. Such an RNN may look like this:</p><figure name="7fff" id="7fff" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 190px; max-height: 230px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 121.10000000000001%;"></div><img class="graf-image" data-image-id="1*-Jv3TxauJBwBgWwjoe_UkA.png" data-width="190" data-height="230" src="https://cdn-images-1.medium.com/max/800/1*-Jv3TxauJBwBgWwjoe_UkA.png"></div><figcaption class="imageCaption">Changed the shades of the green nodes‚Ä¶ hope that‚Äôs¬†OK!</figcaption></figure><p name="ea3c" id="ea3c" class="graf graf--p graf-after--figure">This may be confusing at first, so I‚Äôm going to make sure I walk slowly through it. On the x-axis we have time, and on the y-axis we have depth/layers:</p><figure name="c4eb" id="c4eb" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 323px; max-height: 287px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 88.9%;"></div><img class="graf-image" data-image-id="1*OEyIsiEi5SJ9l3GrB5DpuA.png" data-width="323" data-height="287" src="https://cdn-images-1.medium.com/max/800/1*OEyIsiEi5SJ9l3GrB5DpuA.png"></div></figure><p name="22b2" id="22b2" class="graf graf--p graf-after--figure">When I refer to ‚Äútime‚Äù on the x-axis, I‚Äôm referring to the order at which these operations occur. Time could also be literal for temporal data, where the input is a sequence. When I say ‚Äúdepth‚Äù on the y-axis, I‚Äôm referring to the mapping from the input layer, to the hidden layer(s), to the output layer, where layer number and thus depth increases.</p><p name="f94b" id="f94b" class="graf graf--p graf-after--p">It may look like we have seven neurons now, but we still have three: one input neuron, one hidden neuron, and one output neuron. The difference is that these neurons now experience multiple ‚Äútimesteps‚Äù where they take on different values, which are, again, vectors. The input neuron in our example above doesn‚Äôt, because it‚Äôs not representing sequential data (one to many), but for other architectures it could.</p><p name="e200" id="e200" class="graf graf--p graf-after--p">The hidden neuron will take on the vector value <strong class="markup--strong markup--p-strong">h_1</strong> first, then <strong class="markup--strong markup--p-strong">h_2</strong>, and finally <strong class="markup--strong markup--p-strong">h_3</strong>. At each timestep, the hidden neuron‚Äôs vector <strong class="markup--strong markup--p-strong">h_t </strong>is a function of the vector at the previous timestep <strong class="markup--strong markup--p-strong">h_t-1</strong>, except for <strong class="markup--strong markup--p-strong">h_1 </strong>which is dependent <em class="markup--em markup--p-em">only</em> on the input <strong class="markup--strong markup--p-strong">x_1</strong>. In the diagram above, each hidden vector then gives rise to an output <strong class="markup--strong markup--p-strong">y_t</strong>, and this is how we map one input to multiple outputs. You can visualize these functional dependencies with the arrows, which illustrates flow of information in the network.</p><p name="64eb" id="64eb" class="graf graf--p graf-after--p">As we progress on the x-axis, the current timestep increases. As we progress on the y-axis, the neuron in question changes. Each point on this graph thus represents one neuron‚Ää‚Äî‚Ääbe it input, hidden, or output‚Ää‚Äî‚Ääat some timestep, being fed information from a neuron (be it itself or another) at the previous timestep.</p><p name="5402" id="5402" class="graf graf--p graf-after--p">The RNN would execute like so:</p><ol class="postList"><li name="3623" id="3623" class="graf graf--li graf-after--p">Input <strong class="markup--strong markup--li-strong">x_1</strong></li><li name="1923" id="1923" class="graf graf--li graf-after--li">Compute <strong class="markup--strong markup--li-strong">h_1</strong> based on <strong class="markup--strong markup--li-strong">x_1</strong> (the arrow implies functional dependency)</li><li name="00ff" id="00ff" class="graf graf--li graf-after--li">Compute <strong class="markup--strong markup--li-strong">h_2</strong> based on <strong class="markup--strong markup--li-strong">h_1</strong></li><li name="ca4c" id="ca4c" class="graf graf--li graf-after--li">Compute <strong class="markup--strong markup--li-strong">h_3</strong> based on <strong class="markup--strong markup--li-strong">h_2</strong></li><li name="9fac" id="9fac" class="graf graf--li graf-after--li">Compute <strong class="markup--strong markup--li-strong">y_1</strong> based on <strong class="markup--strong markup--li-strong">h_1</strong></li><li name="649b" id="649b" class="graf graf--li graf-after--li">Compute <strong class="markup--strong markup--li-strong">y_2</strong> based on <strong class="markup--strong markup--li-strong">h_2</strong></li><li name="ba63" id="ba63" class="graf graf--li graf-after--li">Compute <strong class="markup--strong markup--li-strong">y_3</strong> based on <strong class="markup--strong markup--li-strong">h_3</strong></li></ol><p name="bba5" id="bba5" class="graf graf--p graf-after--li">You could compute <strong class="markup--strong markup--p-strong">y_t </strong>either immediately after <strong class="markup--strong markup--p-strong">h_t </strong>has been computed, or, like above, compute all outputs once all hidden states have been computed. I‚Äôm not entirely sure which is more common in practice.</p><p name="6c68" id="6c68" class="graf graf--p graf-after--p">This allows for more complex and interesting networks than ANNs because we can have as many timesteps as we want.</p><p name="e78f" id="e78f" class="graf graf--p graf-after--p">The value of the output neuron at each timestep represents a word in the sentence, in the order the sentence will be constructed. The caption this RNN produces is hence 3 words long. (It‚Äôs actually 2, because the RNN would need to output a period or &lt;END&gt; marker at the final timestep, but we‚Äôll get into that later.)</p><p name="a009" id="a009" class="graf graf--p graf-after--p">In case you don‚Äôt understand yet exactly <em class="markup--em markup--p-em">why </em>RNNs work, I‚Äôll walk through how these functional dependencies come to fruition when you apply it to a one to many scenario such as image captioning.</p><figure name="2856" id="2856" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 422px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 60.199999999999996%;"></div><img class="graf-image" data-image-id="1*iBtLegQFwfsqWZpTVAjrEw.jpeg" data-width="1298" data-height="782" data-action="zoom" data-action-value="1*iBtLegQFwfsqWZpTVAjrEw.jpeg" src="https://cdn-images-1.medium.com/max/800/1*iBtLegQFwfsqWZpTVAjrEw.jpeg"></div><figcaption class="imageCaption">Lenny and I on student scholarship at WWDC 2013. Good¬†times!</figcaption></figure><p name="6b74" id="6b74" class="graf graf--p graf-after--figure">When you combine an RNN and CNN, you‚Ää‚Äî‚Ääin practice‚Ää‚Äî‚Ääget an ‚ÄúLCRN‚Äù. The architecture for LCRNs are more complex than what I‚Äôm going to present in the next paragraph; rather, I‚Äôm going to simplify it to convey my point. We‚Äôll actually get fully into how they work later.</p><p name="e029" id="e029" class="graf graf--p graf-after--p">Imagine an RNN tries to caption this image. An accurate result might be:</p><blockquote name="f14e" id="f14e" class="graf graf--pullquote graf-after--p">Two people happily posing for a photo inside a building.</blockquote><p name="482a" id="482a" class="graf graf--p graf-after--pullquote">The input to the RNN would be the output of a CNN that processes this image. (However, to be pedantic, it would be the output of the CNN without a classification/softmax layer‚Ää‚Äî‚Ääthat is, pulled from the final fully connected layer.) The CNN might pick up on the fact that there are two primary human face-like objects present in the image, which, paired with what the RNN has learned via training, may induce the first hidden state<strong class="markup--strong markup--p-strong">¬π</strong> of the recurrent neural network to be one where the most likely candidate word is ‚Äútwo‚Äù.</p><p name="f5ec" id="f5ec" class="graf graf--p graf-after--p">Pro-tip<strong class="markup--strong markup--p-strong">¬π</strong>: The term ‚Äúhidden state‚Äù refers to the vector of a hidden neuron at a given timestep. ‚ÄúFirst hidden state‚Äù refers to the hidden state at timestep 1.</p><p name="10db" id="10db" class="graf graf--p graf-after--p">The first output, which represents the word ‚Äútwo‚Äù, was functionally dependent on the first hidden state, which in itself was a function of the input to the RNN. Thus, ‚Äútwo‚Äù was ultimately determined from the information that the CNN gave us and the experience/weights of the RNN. Now, the second word, ‚Äúpeople‚Äù, is functionally dependent on the second hidden state. However, note that the second hidden state is just a function of the <em class="markup--em markup--p-em">first </em>hidden state. This means that the word ‚Äúpeople‚Äù was the most likely candidate given the hidden state where ‚Äútwo‚Äù was likely. In other words, the RNN recognized that, given the word ‚Äútwo‚Äù, the word ‚Äúpeople‚Äù should be next, based on the RNN‚Äôs experience from training and the initial image [analysis] we inputted.</p><p name="b004" id="b004" class="graf graf--p graf-after--p">The same will occur for every following word; the nth word will be based on the nth hidden state, which, ultimately, is a function of every hidden state before it, and thus could be interpreted purely as an extremely complex and layered function of the input. The weights do the heavy lifting by making sense of all this information and deducing an output from it.</p><p name="d51d" id="d51d" class="graf graf--p graf-after--p">To put it bluntly, you can boil down what the RNN is ‚Äúthinking‚Äù to this:</p><blockquote name="4a94" id="4a94" class="graf graf--blockquote graf-after--p">Based on what I‚Äôve seen from the input, based on the current timestep I‚Äôm at, and based on what I know from all my training, I need to output: <strong class="markup--strong markup--blockquote-strong">‚Äúx‚Äù.</strong></blockquote><p name="a918" id="a918" class="graf graf--p graf-after--blockquote">Thus, each outputted word is dependent on the words before it, all the way back to the input image data. However, this relationship is indirect. It‚Äôs indirect because the outputs are only dependent on the hidden states, <em class="markup--em markup--p-em">not </em>on each other (ie. the RNN doesn‚Äôt deduce ‚Äúpeople‚Äù from ‚Äútwo‚Äù, it deduces ‚Äúpeople‚Äù, partly, from the information‚Ää‚Äî‚Ääthe hidden state‚Ää‚Äî‚Ääthat <em class="markup--em markup--p-em">gave rise</em> to ‚Äútwo‚Äù). In LCRNs, though, this is explicit instead of implicit; we ‚Äúsample‚Äù the output of one timestep by taking it and literally feeding it back as input into the next timestep. In a sense, LCRNs can hence be interpreted as having many to many architecture.</p><p name="cdc3" id="cdc3" class="graf graf--p graf-after--p">The exact quantitative relationships depend on the RNN‚Äôs weights. But, generally, this is the concept of memory in play. Creating a coherent sentence as we go along is only really possible if we can recall what we said before. And RNNs are able to do exactly that; they remember what they said before and figure out, based on their image captioning expertise, what from this is useful to continue accurately speaking.</p><blockquote name="0d89" id="0d89" class="graf graf--blockquote graf-after--p">Yep, I went to France for a holiday. And I actually learned to speak some &lt;wait, shit, what was the language again? oh yea, ‚ÄúFrance‚Äù‚Ä¶&gt; French!</blockquote><p name="5857" id="5857" class="graf graf--p graf-after--blockquote">Obviously, an RNN needs to be trained and have proper weights for this to all function properly. RNNs aren‚Äôt magic; they only work because trained networks identified and learned patterns in data during training time that they now look for during prediction.</p><p name="34be" id="34be" class="graf graf--p graf-after--p">Perhaps this was a bit over-explaining on my part, but hopefully I nailed down some important and core ideas about how RNNs function.</p><p name="05f6" id="05f6" class="graf graf--p graf-after--p">So far we‚Äôve looked at one to one and one to many recurrent networks. We can also have <em class="markup--em markup--p-em">many to one</em>:</p><figure name="6c8d" id="6c8d" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 190px; max-height: 228px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 120%;"></div><img class="graf-image" data-image-id="1*KjYQyc-JD_zs5ERQypM9EA.png" data-width="190" data-height="228" src="https://cdn-images-1.medium.com/max/800/1*KjYQyc-JD_zs5ERQypM9EA.png"></div></figure><p name="da0a" id="da0a" class="graf graf--p graf-after--figure">With many to one (and many to many), the input is in the form of a sequence, and so the hidden states are functionally dependent on <em class="markup--em markup--p-em">both </em>the input at that timestep and the previous hidden state. This is different to one to many, where the hidden state after <strong class="markup--strong markup--p-strong">h_1 </strong>is only dependent on the previous hidden state. That‚Äôs why, in the image above, the second hidden state has two arrows directed at it.</p><p name="4715" id="4715" class="graf graf--p graf-after--p">Only one output exists in many to one architecture. An example application is sentiment classification, where the input is a sentence (sequence of words) and the output is a probability indicating that the inputted sentence was positive.</p><p name="a55f" id="a55f" class="graf graf--p graf-after--p">The final type of recurrent net is many to many, where both the input and output are sequential:</p><figure name="86b5" id="86b5" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 310px; max-height: 214px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 69%;"></div><img class="graf-image" data-image-id="1*MPpLCBI1J6r6VmsDm7G4_g.png" data-width="310" data-height="214" src="https://cdn-images-1.medium.com/max/800/1*MPpLCBI1J6r6VmsDm7G4_g.png"></div></figure><p name="7038" id="7038" class="graf graf--p graf-after--figure">A use case would be machine translation where a sequence of words in one language needs to be translated to a sequence of words in another.</p><p name="2538" id="2538" class="graf graf--p graf-after--p">We can also go deeper and have multiple hidden layers, and/or a greater number of timesteps:</p><figure name="7896" id="7896" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 313px; max-height: 305px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 97.39999999999999%;"></div><img class="graf-image" data-image-id="1*vfUWsAgW-c5hUntaPhb1aQ.png" data-width="313" data-height="305" src="https://cdn-images-1.medium.com/max/800/1*vfUWsAgW-c5hUntaPhb1aQ.png"></div><figcaption class="imageCaption">We‚Äôre getting deeper and¬†deeper!</figcaption></figure><p name="2703" id="2703" class="graf graf--p graf-after--figure">Really, this could be considered as <em class="markup--em markup--p-em">multiple RNNs</em>. Technically, you can consider each ‚Äúhidden layer‚Äù as an RNN itself, given each neuron operates on vectors and updates through time; in ANN context, that volume of operations would be considered an entire network. So this is like stacking RNNs on top of each other. However, in this article I‚Äôll refer to it as multiple hidden layers; different papers and lecturers may take different approaches.</p><p name="a0dc" id="a0dc" class="graf graf--p graf-after--p">When we have many timesteps (usually hundreds) and multiple hidden layers, the architecture of the network becomes much more complex and interesting. One feature of this RNN, in particular, is that all the outputs, including the first, depend on not just the input up to that timestep, but all of the inputs. (You can see this because the green neuron is only introduced <em class="markup--em markup--p-em">after</em> the final input timestep.) If this RNN was to translate English to Chinese, the first word of translated Chinese isn‚Äôt just dependent on the first word of the inputted English; it‚Äôs dependent on the <em class="markup--em markup--p-em">entire </em>sentence.</p><p name="98d9" id="98d9" class="graf graf--p graf-after--p">One way to demonstrate why this matters is to use Google Translate:</p><figure name="af3c" id="af3c" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 680px; max-height: 145px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 21.3%;"></div><img class="graf-image" data-image-id="1*mptNrzbgaDT3YuQL-tpEOw.png" data-width="680" data-height="145" src="https://cdn-images-1.medium.com/max/800/1*mptNrzbgaDT3YuQL-tpEOw.png"></div><figcaption class="imageCaption">One of my favorite <strong class="markup--strong markup--figure-strong">Green Day</strong> lyrics, from the song ‚ÄúFashion Victim‚Äù on WARNING:. Side-note: Based on my experience with Google Translate in Chinese class over the last 8 years, this translation is <em class="markup--em markup--figure-em">probably¬†</em>off.</figcaption></figure><p name="3229" id="3229" class="graf graf--p graf-after--figure">Now I‚Äôll input ‚ÄúHe‚Äôs a victim‚Äù and ‚Äúof his own time‚Äù separately. You‚Äôll notice that when you join the two translated outputs, this won‚Äôt be equal to the corresponding phrase in the first translation:</p><figure name="aef0" id="aef0" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 276px; max-height: 245px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 88.8%;"></div><img class="graf-image" data-image-id="1*lO5oCsSXTy4Loic3SASMlw.png" data-width="276" data-height="245" src="https://cdn-images-1.medium.com/max/800/1*lO5oCsSXTy4Loic3SASMlw.png"></div><figcaption class="imageCaption">What happens if we break up the English into different parts, translate, and join together the translated Chinese¬†parts?</figcaption></figure><figure name="44aa" id="44aa" class="graf graf--figure graf-after--figure"><div class="aspectRatioPlaceholder is-locked" style="max-width: 551px; max-height: 110px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 20%;"></div><img class="graf-image" data-image-id="1*oJRFlstyAI-MkBguW6otfA.png" data-width="551" data-height="110" src="https://cdn-images-1.medium.com/max/800/1*oJRFlstyAI-MkBguW6otfA.png"></div><figcaption class="imageCaption"><strong class="markup--strong markup--figure-strong">They‚Äôre not¬†equal.</strong></figcaption></figure><p name="a661" id="a661" class="graf graf--p graf-after--figure">What gives? Well, the way sentences are constructed in languages can differ in varying scenarios. Some words in English may also map to multiple different words in Chinese, depending on how it‚Äôs used. It all depends on the <strong class="markup--strong markup--p-strong">context </strong>and the entire sentence as a whole‚Ää‚Äî‚Ääthe meaning you‚Äôre trying to convey. This is the exact approach a human translator would take.</p><p name="fad3" id="fad3" class="graf graf--p graf-after--p">Another type of many to many architecture exists where each neuron has a state at every timestep, in a ‚Äúsynchronized‚Äù fashion. Here, each output is only dependent on the inputs that were fed in during or before it. Because of this, synchronized many to many probably wouldn‚Äôt be suitable for translation.</p><figure name="5a39" id="5a39" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 190px; max-height: 231px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 121.6%;"></div><img class="graf-image" data-image-id="1*84IkP_dqLUfImZ5SyZLwjA.png" data-width="190" data-height="231" src="https://cdn-images-1.medium.com/max/800/1*84IkP_dqLUfImZ5SyZLwjA.png"></div></figure><p name="cef1" id="cef1" class="graf graf--p graf-after--figure">An application for this could be video classification where each frame needs to be mapped to some sort of class or label. Interesting note‚Ää‚Äî‚Ääan RNN is better at this task than CNNs are because what‚Äôs going on in a scene is much easier to understand if you‚Äôve watched the video up to that point and thus can contextualize it. <strong class="markup--strong markup--p-strong">That‚Äôs what humans do!</strong></p><p name="f0a6" id="f0a6" class="graf graf--p graf-after--p">Quick note: we can ‚Äúwrap‚Äù the RNN into a much more succinct form, where we collapse the depth and time properties, like so:</p><figure name="e5dd" id="e5dd" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 190px; max-height: 250px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 131.6%;"></div><img class="graf-image" data-image-id="1*7POP9GXAsRlbRRrsrhr-jA.png" data-width="190" data-height="250" src="https://cdn-images-1.medium.com/max/800/1*7POP9GXAsRlbRRrsrhr-jA.png"></div></figure><p name="fc78" id="fc78" class="graf graf--p graf-after--figure">This notation demonstrates that RNNs take input, process that input through multiple timesteps and hidden layers, and produce output. The arrow both leaving and entering the RNN conveys that an RNN hidden state is functionally dependent on the hidden state at the preceding timestep; it‚Äôs sort of like a loop that feeds itself.</p><p name="3ba7" id="3ba7" class="graf graf--p graf-after--p">When you ever read about ‚Äúunrolling‚Äù an RNN into a feedforward network that looks like it‚Äôs in the same collapsed format as the diagram above, this means we expand it to show all timesteps and hidden layers like we did before.</p><p name="0d76" id="0d76" class="graf graf--p graf-after--p">Another quick note: when somebody or a research paper mentions that they are using ‚Äú512 RNN units‚Äù, this translates to: ‚Äú1 RNN neuron that outputs a 512-wide vector‚Äù; that is, a vector with 512 values. At first, I thought this meant that maybe at each timestep there were 512 separate neurons somehow working in conjunction, but nope, it‚Äôs luckily much simpler than that‚Ä¶ albeit strangely worded.</p><p name="2ae1" id="2ae1" class="graf graf--p graf-after--p">Furthermore, one ‚ÄúRNN unit‚Äù usually refers to an RNN with one hidden layer; thus, instead of defining RNN as something that is multilayer inherently, we often see people use the phrase like: ‚Äústacking RNNs on top of each other‚Äù. Each RNN will have its on weights, but connecting them gives rise to an overarching multilayer RNN. In this article, we treat recurrent neural networks as a model that can have variable timesteps <strong class="markup--strong markup--p-strong">t </strong>and fixed layers <strong class="markup--strong markup--p-strong">‚Ñì</strong>, just make sure you understand that this is not always the case. Our formalism, especially for weights, will slightly differ.</p><h3 name="e5b5" id="e5b5" class="graf graf--h3 graf-after--p">Formalism</h3><p name="a93d" id="a93d" class="graf graf--p graf-after--h3">So, now, let‚Äôs walk through the formal mathematical notation involved in RNNs.</p><p name="ad6a" id="ad6a" class="graf graf--p graf-after--p">If an input or output neuron has a value at timestep <strong class="markup--strong markup--p-strong">t</strong>, we denote the vector as:</p><figure name="a4f8" id="a4f8" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 141px; max-height: 77px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 54.6%;"></div><img class="graf-image" data-image-id="1*_D9bOLepOSSbC2zgK7wreQ.png" data-width="141" data-height="77" src="https://cdn-images-1.medium.com/max/800/1*_D9bOLepOSSbC2zgK7wreQ.png"></div></figure><p name="a6fd" id="a6fd" class="graf graf--p graf-after--figure">For the hidden neurons it‚Äôs a bit different; since we can have multiple hidden layers, we denote the hidden state vector at timestep <strong class="markup--strong markup--p-strong">t </strong>and hidden layer <strong class="markup--strong markup--p-strong">‚Ñì </strong>as:</p><figure name="7987" id="7987" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 157px; max-height: 41px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 26.1%;"></div><img class="graf-image" data-image-id="1*QJFWCdOVxGAge0ZT17hw1g.png" data-width="157" data-height="41" src="https://cdn-images-1.medium.com/max/800/1*QJFWCdOVxGAge0ZT17hw1g.png"></div></figure><p name="7ba4" id="7ba4" class="graf graf--p graf-after--figure">The input is obviously some preset values that we know. The outputs and hidden states are<em class="markup--em markup--p-em"> not</em>; they are calculated.</p><p name="982f" id="982f" class="graf graf--p graf-after--p">Let‚Äôs start with hidden states. First, we‚Äôll revisit the most complex recurrent net we came across earlier‚Ää‚Äî‚Ääthe many to many architecture:</p><figure name="44bd" id="44bd" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 250px; max-height: 206px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 82.39999999999999%;"></div><img class="graf-image" data-image-id="1*TJxcM6GI8dMHEq6sK3Ky8Q.png" data-width="250" data-height="206" src="https://cdn-images-1.medium.com/max/800/1*TJxcM6GI8dMHEq6sK3Ky8Q.png"></div><figcaption class="imageCaption">Many to many, non-synchronized.</figcaption></figure><p name="5f3c" id="5f3c" class="graf graf--p graf-after--figure">This RNN has: sequential input, sequential output, multiple timesteps, and multiple hidden layers. The formula we derive for this RNN should generalize for all others.</p><p name="af44" id="af44" class="graf graf--p graf-after--p">First, let‚Äôs list out the possible functional dependencies for a given hidden state, based on the arrows and flow of information in the diagram:</p><ul class="postList"><li name="5021" id="5021" class="graf graf--li graf-after--p">An input</li><li name="8970" id="8970" class="graf graf--li graf-after--li">Hidden state at the previous timestep, same layer</li><li name="429d" id="429d" class="graf graf--li graf-after--li">Hidden state at the current timestep, previous layer</li></ul><p name="dc21" id="dc21" class="graf graf--p graf-after--li">A hidden state can have two functional dependencies at max. Just by looking at the diagram, the only impossible combination is to be dependent on both the input and a hidden state at the current timestep but previous layer. This is because the only hidden states that are dependent on input exist in the first hidden layer, where no such previous layer exists.</p><p name="b721" id="b721" class="graf graf--p graf-after--p">If this is all difficult to follow, make sure once again to look at and trace back the arrows in the RNN that illustrate flow of information throughout the network.</p><p name="5a0a" id="5a0a" class="graf graf--p graf-after--p">Because of the impossible combination, we define two separate equations: an equation for the hidden state at hidden layer 1, and for layers after 1.</p><figure name="8095" id="8095" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 400px; max-height: 107px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 26.8%;"></div><img class="graf-image" data-image-id="1*n5QR9Q9ZGnWFRf7pROtliA.png" data-width="400" data-height="107" src="https://cdn-images-1.medium.com/max/800/1*n5QR9Q9ZGnWFRf7pROtliA.png"></div></figure><p name="7d12" id="7d12" class="graf graf--p graf-after--figure">This probably looks a bit confusing; let me break it down for you. The function <strong class="markup--strong markup--p-strong">∆íw </strong>computes the numeric hidden state vector for timestep <strong class="markup--strong markup--p-strong">t </strong>and layer <strong class="markup--strong markup--p-strong">‚Ñì</strong>; it contains the ‚Äúactivation function‚Äù you‚Äôre used to hearing about with ANNs. <strong class="markup--strong markup--p-strong">W </strong>are the weights of the recurrent net, and thus <strong class="markup--strong markup--p-strong">∆í </strong>is conditioned on <strong class="markup--strong markup--p-strong">W</strong>. We haven‚Äôt exactly defined <strong class="markup--strong markup--p-strong">∆í</strong> just yet, but what‚Äôs important to note is the two parameters it takes. Once you do, this notation simply states what we have stated before in plain English:</p><blockquote name="a42e" id="a42e" class="graf graf--blockquote graf-after--p">Where <strong class="markup--strong markup--blockquote-strong">‚Ñì </strong>= 1, the hidden state at time <strong class="markup--strong markup--blockquote-strong">t </strong>and layer <strong class="markup--strong markup--blockquote-strong">‚Ñì </strong>is a function of the hidden state vector at time <strong class="markup--strong markup--blockquote-strong">t-1 </strong>and layer <strong class="markup--strong markup--blockquote-strong">‚Ñì </strong>as well as the input vector at time <strong class="markup--strong markup--blockquote-strong">t</strong>. Where <strong class="markup--strong markup--blockquote-strong">‚Ñì &gt; 1</strong>, this hidden state is a function of the hidden state vector at time <strong class="markup--strong markup--blockquote-strong">t-1 </strong>and layer <strong class="markup--strong markup--blockquote-strong">‚Ñì </strong>as well as the hidden state vector at time <strong class="markup--strong markup--blockquote-strong">t</strong>, layer <strong class="markup--strong markup--blockquote-strong">‚Ñì-1</strong>.</blockquote><p name="7713" id="7713" class="graf graf--p graf-after--blockquote">You might notice that we have a couple issues:</p><ul class="postList"><li name="69f7" id="69f7" class="graf graf--li graf-after--p">When <strong class="markup--strong markup--li-strong">t = 1</strong>‚Ää‚Äî‚Ääthat is, when each neuron is at the initial timestep‚Ää‚Äî‚Ääthen no previous timestep exists. However, we still attempt to pass <strong class="markup--strong markup--li-strong">h_0 </strong>as a parameter to <strong class="markup--strong markup--li-strong">∆íw</strong>.</li><li name="7b09" id="7b09" class="graf graf--li graf-after--li">If no input exists at time <strong class="markup--strong markup--li-strong">t</strong>‚Ää‚Äî‚Ääthus, <strong class="markup--strong markup--li-strong">x_t </strong>does not exist‚Ää‚Äî‚Ääthen we still attempt to pass <strong class="markup--strong markup--li-strong">x_t </strong>as a parameter.</li></ul><p name="2062" id="2062" class="graf graf--p graf-after--li">Our respective solutions follow:</p><ul class="postList"><li name="c5c1" id="c5c1" class="graf graf--li graf-after--p">Define <strong class="markup--strong markup--li-strong">h_0 </strong>for any layer as 0</li><li name="68f6" id="68f6" class="graf graf--li graf-after--li">Consider <strong class="markup--strong markup--li-strong">x_t </strong>where no input exists at timestep <strong class="markup--strong markup--li-strong">t</strong> as 0</li></ul><p name="0ed3" id="0ed3" class="graf graf--p graf-after--li">If these are 0, then the invalid functional dependency stops existing, and our formal notation still holds up.</p><p name="cf77" id="cf77" class="graf graf--p graf-after--p">We actually have five different types of weight matrices:</p><figure name="61fa" id="61fa" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 390px; max-height: 233px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 59.699999999999996%;"></div><img class="graf-image" data-image-id="1*r09EQtFlEA1kIiOJD2aZ6g.png" data-width="390" data-height="233" src="https://cdn-images-1.medium.com/max/800/1*r09EQtFlEA1kIiOJD2aZ6g.png"></div></figure><p name="b66b" id="b66b" class="graf graf--p graf-after--figure">Pro-tip: The indices for each weight matrix tell you what they are used for in the recurrent net. <strong class="markup--strong markup--p-strong">W_xh </strong>maps an input vector <strong class="markup--strong markup--p-strong">x </strong>to a hidden state vector <strong class="markup--strong markup--p-strong">h</strong>. <strong class="markup--strong markup--p-strong">W_hht </strong>maps a hidden state vector <strong class="markup--strong markup--p-strong">h</strong> to another hidden state vector <strong class="markup--strong markup--p-strong">h </strong>along the time axis, ie. from <strong class="markup--strong markup--p-strong">h_t-1 </strong>to <strong class="markup--strong markup--p-strong">h_t</strong>. On the other hand, <strong class="markup--strong markup--p-strong">W_hhd </strong>maps a hidden state vector <strong class="markup--strong markup--p-strong">h </strong>to another hidden state vector <strong class="markup--strong markup--p-strong">h </strong>along the depth axis, ie. from <strong class="markup--strong markup--p-strong">h^(‚Ñì-1)_t </strong>to <strong class="markup--strong markup--p-strong">h^‚Ñì_t</strong>. <strong class="markup--strong markup--p-strong">W_hy </strong>maps a hidden state vector <strong class="markup--strong markup--p-strong">h </strong>to an output vector <strong class="markup--strong markup--p-strong">y</strong>.</p><p name="283a" id="283a" class="graf graf--p graf-after--p">Like with ANNs, we also learn and add a constant bias vector, denoted <strong class="markup--strong markup--p-strong">b_h</strong>, that can vertically shift what we pass to the activation function. We can also shift our outputs with <strong class="markup--strong markup--p-strong">b_y</strong>. More about bias units <a href="https://ayearofai.com/rohan-5-what-are-bias-units-828d942b4f52" data-href="https://ayearofai.com/rohan-5-what-are-bias-units-828d942b4f52" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">here</a>.</p><p name="246f" id="246f" class="graf graf--p graf-after--p">For both <strong class="markup--strong markup--p-strong">b_h </strong>and <strong class="markup--strong markup--p-strong">W_hht/W_hhd</strong>, we actually have multiple weight matrices depending on the value of <strong class="markup--strong markup--p-strong">‚Ñì</strong>, as indicated by the superscript. This is because each hidden layer can have a different set of weights (the network would be extremely uninteresting if this wasn‚Äôt the case), including the bias vector. However, inside a single hidden layer, all timesteps share the same weight matrix. This is important because the number of timesteps is a variable; we may train on sequences with up to 20 values, but in practice output sequences with up to 30 values‚Ää‚Äî‚Ää10 extra timesteps. If each timestep had an independent weight to learn, those last 10 timesteps wouldn‚Äôt have anything to use.</p><p name="6da9" id="6da9" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">W_hy </strong>is just one matrix because only the final layer gives rise to the outputs denoted <strong class="markup--strong markup--p-strong">y</strong>. At the final hidden layer <strong class="markup--strong markup--p-strong">‚Ñì</strong>, we could suggest that <strong class="markup--strong markup--p-strong">W_hhd </strong>will not exist because <strong class="markup--strong markup--p-strong">W_hy </strong>will be in its place.</p><p name="e3ed" id="e3ed" class="graf graf--p graf-after--p">Now we‚Äôll define the function <strong class="markup--strong markup--p-strong">∆íw</strong>:</p><figure name="d692" id="d692" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 404px; max-height: 226px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 55.900000000000006%;"></div><img class="graf-image" data-image-id="1*9YGuqXdNiknmZR2HScMDKw.png" data-width="404" data-height="226" src="https://cdn-images-1.medium.com/max/800/1*9YGuqXdNiknmZR2HScMDKw.png"></div></figure><p name="e06a" id="e06a" class="graf graf--p graf-after--figure">The function is very similar to the ANN hidden function you‚Äôve seen before; it applies the correct weights to the corresponding parameters, adds the bias, and passes this weighted sum through an activation or ‚Äúsquashing‚Äù function to introduce non-linearities. The key difference, though, is that this is not a weighted sum<em class="markup--em markup--p-em"> </em>but rather a weighted sum <em class="markup--em markup--p-em">vector</em>; any <strong class="markup--strong markup--p-strong">W ‚ãÖ h</strong>, along with the bias,<strong class="markup--strong markup--p-strong"> </strong>will have the dimensions of a vector. The <strong class="markup--strong markup--p-strong">tanh </strong>function will thus simply output a vector where each value is the tanh<strong class="markup--strong markup--p-strong"> </strong>of what it was in the inputted vector (sort of like an element-wise tanh). Remember, this contrasts ANNs because RNNs operate over vectors versus scalars.</p><p name="d904" id="d904" class="graf graf--p graf-after--p">If you‚Äôve followed our blog so far, you most likely know about two activation functions: sigmoid and ReLU. tanh is another such function. We mostly use the tanh function with RNNs. This is, I think, mostly because of their role in LSTMs (a variant of RNNs that are used more than RNNs‚Ää‚Äî‚Äämore on that later), the fact that they produce gradients with a greater range, and that their second derivative don‚Äôt die off as quickly.</p><p name="25ba" id="25ba" class="graf graf--p graf-after--p">Similar to sigmoid, the tanh function has two horizontal asymptotes and a smooth S-shape. The main difference is that the tanh function asymptotes at y = -1 instead of y = 0, intercepting the y-axis at y = 0 instead of y = 0.5. Thus, the tanh function has a greater range than the sigmoid.</p><figure name="0f69" id="0f69" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 324px; max-height: 166px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 51.2%;"></div><img class="graf-image" data-image-id="1*NPI9iLLVlYLQ2gu9A9xp0A.png" data-width="324" data-height="166" src="https://cdn-images-1.medium.com/max/800/1*NPI9iLLVlYLQ2gu9A9xp0A.png"></div></figure><p name="9d21" id="9d21" class="graf graf--p graf-after--figure">If interested, the tanh equation follows (though I won‚Äôt walk you through it):</p><figure name="a172" id="a172" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 241px; max-height: 84px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 34.9%;"></div><img class="graf-image" data-image-id="1*w7LV9vY1hCAXcLk2K_peEg.png" data-width="241" data-height="84" src="https://cdn-images-1.medium.com/max/800/1*w7LV9vY1hCAXcLk2K_peEg.png"></div></figure><p name="85eb" id="85eb" class="graf graf--p graf-after--figure">The final equation is mapping a hidden state to an output.</p><figure name="f531" id="f531" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 177px; max-height: 44px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 24.9%;"></div><img class="graf-image" data-image-id="1*n7simJp73WxCRx_Bz4dXwg.png" data-width="177" data-height="44" src="https://cdn-images-1.medium.com/max/800/1*n7simJp73WxCRx_Bz4dXwg.png"></div></figure><p name="93b1" id="93b1" class="graf graf--p graf-after--figure">This is <em class="markup--em markup--p-em">one</em> such possible equation. Depending on the context, we might also remove the bias vector, apply a non-linearity like sigmoid/softmax (for example if the output needs to be a probability distribution), etc.</p><p name="3584" id="3584" class="graf graf--p graf-after--p">And that‚Äôs how we express recurrent nets, mathematically!</p><p name="2650" id="2650" class="graf graf--p graf-after--p">Quick note: Notation may and will differ between various lectures, research paper, articles, etc. For example‚Ää‚Äî‚Ääsome research papers may start indexing at 0 instead of 1. More drastically, most RNN notation is <em class="markup--em markup--p-em">much </em>more general than mine to promote simplicity, ie. doesn‚Äôt cover edge cases like I did or obfuscates certain indices like <strong class="markup--strong markup--p-strong">‚Ñì </strong>with hidden to hidden weight matrices. So, just keep note that specifics don‚Äôt always transfer over and avoid being confused by this. The reason I was meticulous about notation in this article is that I wanted to ensure you understood exactly how RNNs work, fueled by my frustration with the very same problem ~1.5 years ago.</p><h3 name="ed09" id="ed09" class="graf graf--h3 graf-after--p">An example?¬†Okay!</h3><p name="e53b" id="e53b" class="graf graf--p graf-after--h3">Let‚Äôs take a look at a quick example of an RNN in action. I‚Äôm going to adapt a super dumbed down one from Andrej Karpathy‚Äôs Stanford CS231n <a href="https://www.youtube.com/watch?v=cO0a0QYmFm8&amp;index=10&amp;list=PLlJy-eBtNFt6EuMxFYRiNRS07MCWN5UIA" data-href="https://www.youtube.com/watch?v=cO0a0QYmFm8&amp;index=10&amp;list=PLlJy-eBtNFt6EuMxFYRiNRS07MCWN5UIA" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">RNN lecture</a>, where a one to many ‚Äúcharacter level language model‚Äù single layer recurrent neural network needs to output ‚Äúhello‚Äù. We‚Äôll kick it of by giving the RNN the letter ‚Äúh‚Äù¬†, such that it needs to complete the word by outputting the other four letters.</p><p name="1901" id="1901" class="graf graf--p graf-after--p">Sidenote: this model nicknamed ‚Äúchar-rnn‚Äù‚Ää‚Äî‚Ääremember it for later, where we get to code our own!</p><p name="fc9c" id="fc9c" class="graf graf--p graf-after--p">The neural network has the vocabulary: h, e, l¬†, o. That is, it only knows these four characters; exactly enough to produce the word ‚Äúhello‚Äù. We will input the first character, ‚Äúh‚Äù, and from there expect the output at the following timesteps to be: ‚Äúe‚Äù, ‚Äúl‚Äù, ‚Äúl‚Äù, and ‚Äúo‚Äù respectively, to form:</p><blockquote name="56af" id="56af" class="graf graf--pullquote graf-after--p">hello</blockquote><p name="9f5f" id="9f5f" class="graf graf--p graf-after--pullquote">We can represent input and output via one hot encoding, where each character is a vector with a 1 at the corresponding character position and otherwise all 0s. For example, since our vocabulary is [h, e, l, o], we can represent characters using a vector with four values, where a 1 in the first, second, third, and fourth position would represent ‚Äúh‚Äù, ‚Äúe‚Äù, ‚Äúl‚Äù, and ‚Äúo‚Äù respectively.</p><figure name="28cd" id="28cd" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 440px; max-height: 192px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 43.6%;"></div><img class="graf-image" data-image-id="1*pgWSPyximAFHqZtUkiLeKg.png" data-width="440" data-height="192" src="https://cdn-images-1.medium.com/max/800/1*pgWSPyximAFHqZtUkiLeKg.png"></div><figcaption class="imageCaption">This is called ‚Äúone-hot encoding‚Äù, because only one of the values in the vector is equal to 1 and thus on (or¬†‚Äúhot‚Äù).</figcaption></figure><p name="c682" id="c682" class="graf graf--p graf-after--figure">This is what we‚Äôd expect with a trained RNN:</p><figure name="0f80" id="0f80" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 250px; max-height: 227px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 90.8%;"></div><img class="graf-image" data-image-id="1*mmuQb8msqqQLtz580_lpvw.png" data-width="250" data-height="227" src="https://cdn-images-1.medium.com/max/800/1*mmuQb8msqqQLtz580_lpvw.png"></div></figure><p name="17a3" id="17a3" class="graf graf--p graf-after--figure">As you can see, we input the first letter and the word is completed. We don‚Äôt know exactly what the hidden states will be‚Ää‚Äî‚Ääthat‚Äôs why they‚Äôre hidden!</p><p name="04ad" id="04ad" class="graf graf--p graf-after--p">One interesting technique would be to sample the output at each timestep and feed it into the next as input:</p><figure name="9eb9" id="9eb9" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 250px; max-height: 226px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 90.4%;"></div><img class="graf-image" data-image-id="1*KyVSttLGcexQWDLvSWD0Lg.png" data-width="250" data-height="226" src="https://cdn-images-1.medium.com/max/800/1*KyVSttLGcexQWDLvSWD0Lg.png"></div></figure><p name="08da" id="08da" class="graf graf--p graf-after--figure">When we ‚Äúsample‚Äù from a distribution, we select a random character probabilistically following the distribution. For example, in the diagram above, the character with the highest likeliness is ‚Äúe‚Äù at the first timestep‚Äôs output. Let‚Äôs say this likeliness is, concretely, 0.9. Now, when we sample into the next timestep‚Äôs input, there‚Äôs a 90% chance we select ‚Äúe‚Äù; <em class="markup--em markup--p-em">most</em> of the time we will pick the most likely character, but not every time. This adds a level of randomness so you don‚Äôt end up in a loop where you keep sampling the same letter or sequence of letters over and over again.</p><p name="0880" id="0880" class="graf graf--p graf-after--p">As mentioned earlier, this is used pretty heavily with LCRNs. It‚Äôs even more effective than only relying on the memory of the RNN to output the correct letter at the future timesteps. In a sense, this makes the recurrent net many to many. (Though, not really, because we still only have one preset input.)</p><p name="9cac" id="9cac" class="graf graf--p graf-after--p">However, to be clear, this does not mean that the RNN can <em class="markup--em markup--p-em">only </em>rely on these sampled inputs. For example, at timestep 3 the input is ‚Äúl‚Äù and the expected output is also ‚Äúl‚Äù. However, at timestep 4, the input is again ‚Äúl‚Äù but the output is now ‚Äúo‚Äù, to complete the word. Memory is still needed to make a distinction like this.</p><p name="db16" id="db16" class="graf graf--p graf-after--p">In numerical form, it would look something like this:</p><figure name="5acf" id="5acf" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 280px; max-height: 371px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 132.5%;"></div><img class="graf-image" data-image-id="1*AguGRuRZg6e6RZ7Ctvn2Ww.png" data-width="280" data-height="371" src="https://cdn-images-1.medium.com/max/800/1*AguGRuRZg6e6RZ7Ctvn2Ww.png"></div></figure><p name="c815" id="c815" class="graf graf--p graf-after--figure">Of course, we won‚Äôt get a one-hot vector output during prediction mode; rather, we will get a probability distribution over each letter (so we‚Äôd apply softmax to the output), and will sample from this distribution to get a single character output.</p><figure name="afba" id="afba" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 300px; max-height: 574px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 191.3%;"></div><img class="graf-image" data-image-id="1*6067M6Oqz2zNoyyyQC1Suw.png" data-width="300" data-height="574" src="https://cdn-images-1.medium.com/max/800/1*6067M6Oqz2zNoyyyQC1Suw.png"></div></figure><p name="9419" id="9419" class="graf graf--p graf-after--figure">Each hidden state would contain a similar sort of vector, though not necessarily something we could interpret like we can for the output.</p><p name="58bf" id="58bf" class="graf graf--p graf-after--p">The RNN is saying: given ‚Äúh‚Äù, ‚Äúe‚Äù is most likely to be the next character. Given ‚Äúhe‚Äù, ‚Äúl‚Äù is the next likely character. With ‚Äúhel‚Äù, ‚Äúl‚Äù should be next, and with ‚Äúhell‚Äù, the final character should be ‚Äúo‚Äù.</p><p name="b7ec" id="b7ec" class="graf graf--p graf-after--p">But, if the neural network wasn‚Äôt trained on the word ‚Äúhello‚Äù, and thus didn‚Äôt have optimal weights (ie. just randomly initialized weights), then we‚Äôd have garble like ‚Äúhleol‚Äù coming out.</p><p name="399a" id="399a" class="graf graf--p graf-after--p">One more important thing to note: <strong class="markup--strong markup--p-strong">start and end tokens</strong>. They signify when input begins and when output ends. For example, when the final character is outputted (‚Äúo‚Äù), we can sample this back as input and expect that the ‚Äú&lt;END&gt;‚Äù token (however we choose to represent it‚Ää‚Äî‚Ääcould also use a period) will be outputted at the next timestep; this is the RNN telling us that it has completed the word and its processing as a whole. The use case isn‚Äôt as obvious in this fabricated example, because we know when ‚Äúhello‚Äù has been completed, but consider a real-life scenario where we don‚Äôt: image captioning. In image captioning, the caption could be 1, 2, 3, or n words long, given a reasonable upper limit of n. The end token tells us when the caption has been completed, so we can halt the RNN and complete the prediction loop (which would keep going forever if we were using <code class="markup--code markup--p-code">while</code> or stop after the upper limit/max possible preset constant value of n is reached).</p><p name="a127" id="a127" class="graf graf--p graf-after--p">Start tokens are more used for generating content from complete scratch. For example, imagine an RNN read and learned from a bunch of Shakespeare. (This is an actual funny application of character level language models that Karpathy implemented, and we‚Äôll see it in action on a later section.) Now, based on what the RNN learned, we want it to create a brand new Shakespearean sonnet! Feeding in a ‚Äú&lt;START&gt;‚Äù token enables it to kick this process off and begin writing without us giving the network some arbitrary pre-determined initial word or character.</p><p name="85be" id="85be" class="graf graf--p graf-after--p">I‚Äôve also noticed that another potential use case of start tokens is when we have some other sort of <em class="markup--em markup--p-em">initial </em>input, like CNN produced image data with image captioning, that doesn‚Äôt ‚Äúfit‚Äù what we‚Äôll normally use for input at timesteps after <strong class="markup--strong markup--p-strong">t=1 </strong>(the word outputted at the previous timestep via sampling). As a result, we feed this data directly to the first hidden state and set the input as ‚Äú&lt;START&gt;‚Äù instead.</p><p name="2aca" id="2aca" class="graf graf--p graf-after--p">Now, just to be clear, the RNN doesn‚Äôt magically output these end tokens and recognize the start tokens. We have to add them, along with start tokens, to the training data and vocabulary such that they can be outputted by the recurrent net during prediction time.</p><p name="1c00" id="1c00" class="graf graf--p graf-after--p">This is how we can get RNNs to ‚Äúwrite‚Äù! More on some examples of text RNNs have actually generated, Shakespeare most certainly included, in a later section.</p><h3 name="6b00" id="6b00" class="graf graf--h3 graf-after--p">Training (or, why vanilla RNNs¬†suck.)</h3><p name="58b5" id="58b5" class="graf graf--p graf-after--h3">For a recurrent net to be useful, it needs to learn proper weights via training. That‚Äôs no surprise.</p><p name="8a33" id="8a33" class="graf graf--p graf-after--p">Recall this snippet from earlier:</p><blockquote name="f72f" id="f72f" class="graf graf--blockquote graf-after--p">But, if the neural network wasn‚Äôt trained on the word ‚Äúhello‚Äù, and thus didn‚Äôt have optimal weights (ie. just randomly initialized weights), then we‚Äôd have garble like ‚Äúhleol‚Äù coming out.</blockquote><p name="f9d1" id="f9d1" class="graf graf--p graf-after--blockquote">This is, of course, because we initialize the <strong class="markup--strong markup--p-strong">W </strong>weights randomly at first, so random stuff will come out.</p><p name="c7a6" id="c7a6" class="graf graf--p graf-after--p">But, through multiple iterations of training with a first-order optimization algorithm like gradient descent, we perturb the weights such that the probability of each correct character being outputted at their respective timestep increases. The actual output would be ‚Äúhello‚Äù in one-hot encoding form, and we‚Äôd compute the discrepancy between this output and what the recurrent net predicts (we‚Äôd get the error at each timestep and then add this up) as the total error to then calculate the gradient/update value.</p><p name="2ded" id="2ded" class="graf graf--p graf-after--p">So, each output contributes to the error somehow. If the error is an addition of the outputs, then, if we had something like <strong class="markup--strong markup--p-strong">Y</strong> outputs, we‚Äôd need to backpropagate them individually and add these up. This is because derivatives are distributed evenly when we‚Äôre differentiating a sum:</p><figure name="6145" id="6145" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 651px; max-height: 79px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 12.1%;"></div><img class="graf-image" data-image-id="1*d5mzuu-EmcZz0IFukW6XsQ.png" data-width="651" data-height="79" src="https://cdn-images-1.medium.com/max/800/1*d5mzuu-EmcZz0IFukW6XsQ.png"></div><figcaption class="imageCaption">For any arbitrary weight¬†<strong class="markup--strong markup--figure-strong">W</strong>.</figcaption></figure><p name="bb6e" id="bb6e" class="graf graf--p graf-after--figure">But, you should know that, with artificial neural networks, calculating these gradients isn‚Äôt that easy. We have so many weights contributing to the output, and thus need to figure out exactly how much these weights contribute, and by how much we modify them to decrease overall error. To do this, we use the backpropagation algorithm; this algorithm propagates the error between the predicted output of a recurrent net and the actual output in the dataset all the way back to the beginning of the network. Using the chain rule from differential calculus, backprop helps us calculate the gradients of the output error w.r.t. each individual weight (sort of like the error of each individual weight).</p><p name="c4f4" id="c4f4" class="graf graf--p graf-after--p">Once we have those gradients, we have to use an optimization algorithm to calculate the update values and make the updates. We can use the vanilla gradient descent algorithm to do this, but there are many other possible, better variants as well; learn about them by reading <a href="http://sebastianruder.com/optimizing-gradient-descent/" data-href="http://sebastianruder.com/optimizing-gradient-descent/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">this article</a>, if you want. (I think we‚Äôre long overdue for our own mega-post on optimization!)</p><p name="0be4" id="0be4" class="graf graf--p graf-after--p">Backpropagation with RNNs is called ‚ÄúBackpropagation Through Time‚Äù (short for BPTT), since it operates on sequences in time. But don‚Äôt be fooled‚Ää‚Äî‚Ääthere‚Äôs not much difference between normal backprop and BPTT; when it comes down to it, BPTT is just backprop, but on RNNs! Remember that when you ‚Äúunroll‚Äù an RNN, it essentially becomes a feedforward network; not an ANN, but a feedforward network in the sense that we can visualize where all the information is flowing and observe the activations at each neuron and timestep, all the way from the input to the final output. Like ANNs, RNNs have functional dependencies that link the entire network together; it‚Äôs just that RNNs operate over vectors instead (yay for matrix calculus?) and extend in depth <em class="markup--em markup--p-em">as well as </em>time. There‚Äôs more work to do to compute the gradients, but it‚Äôs no surprise that backprop works pretty much the same way for recurrent nets that it would for normal ones. Because of this, I‚Äôm not going to walk through all the math and show the derivatives etc. Read our backprop mega-post for all that jazz.</p><p name="224b" id="224b" class="graf graf--p graf-after--p">One thing to note is that, since we have multiple timesteps in our RNN, each timestep in a single layer will want to change the weight in a different way and have different gradients. However, remember that each hidden layer uses only one weight matrix because the number of timesteps is a variable. Thus, we just average or sum the weight updates between these timesteps and apply this as an update to the <strong class="markup--strong markup--p-strong">W_hh </strong>for that entire layer. Also, a general practice is to train on shorter sequences first and then gradually increase sequence size as we train on more and more data.</p><p name="1fee" id="1fee" class="graf graf--p graf-after--p">Now, if you haven‚Äôt already, make sure to read this article that I wrote on vanishing and exploding gradients before proceeding:</p><div name="c672" id="c672" class="graf graf--mixtapeEmbed graf-after--p"><a href="https://ayearofai.com/rohan-4-the-vanishing-gradient-problem-ec68f76ffb9b" data-href="https://ayearofai.com/rohan-4-the-vanishing-gradient-problem-ec68f76ffb9b" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://ayearofai.com/rohan-4-the-vanishing-gradient-problem-ec68f76ffb9b"><strong class="markup--strong markup--mixtapeEmbed-strong">Rohan #4: The vanishing gradient problem</strong><br><em class="markup--em markup--mixtapeEmbed-em">Oh no‚Ää‚Äî‚Ääan obstacle to deep learning!</em>ayearofai.com</a><a href="https://ayearofai.com/rohan-4-the-vanishing-gradient-problem-ec68f76ffb9b" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="bb894bd0a8e1cfee65aea593ec3751b3" data-thumbnail-img-id="1*m2gDBT_nc-iE7R4AM3sHBQ.jpeg" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/1*m2gDBT_nc-iE7R4AM3sHBQ.jpeg);"></a></div><p name="be11" id="be11" class="graf graf--p graf-after--mixtapeEmbed">You may be thinking: how does this issue apply to RNNs? Well, RNNs are very deep models; on top of often having multiple hidden layers, each hidden layer in practice can have <em class="markup--em markup--p-em">hundreds</em> of timesteps. That‚Äôs like an ANN with hundreds of entire hidden layers! That‚Äôs <em class="markup--em markup--p-em">deep</em>.<em class="markup--em markup--p-em"> </em>(Well, it‚Äôs more <em class="markup--em markup--p-em">long </em>because we‚Äôre dealing with the time axis here, but you know what I mean.) tanh derivatives are very similar to sigmoid derivatives in range, so the problem of vanishing gradients is thus even more drastic with RNNs than with ANNs, and training them becomes almost impossible.</p><p name="4878" id="4878" class="graf graf--p graf-after--p">Imagine trying to propagate the error to the 1st timestep in an RNN with <strong class="markup--strong markup--p-strong">k</strong> timesteps. The derivative would look something like this:</p><figure name="44b3" id="44b3" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 346px; max-height: 79px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 22.8%;"></div><img class="graf-image" data-image-id="1*gKbRtQfPwGK2d7jnKZdv5w.png" data-width="346" data-height="79" src="https://cdn-images-1.medium.com/max/800/1*gKbRtQfPwGK2d7jnKZdv5w.png"></div></figure><p name="0e72" id="0e72" class="graf graf--p graf-after--figure">With a tanh activation function, that‚Äôs freaking crazy. Then, for getting the derivative of the error with respect to a weight matrix <strong class="markup--strong markup--p-strong">W_hh</strong>, we‚Äôd add‚Ää‚Äî‚Ääor, as mentioned before, we could average as well‚Ää‚Äî‚Ääeach of these hidden state error gradients, then multiplied by the derivative of the hidden state with respect to the weight, such that we can backprop from the error to the weight:</p><figure name="0639" id="0639" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 72px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 10.299999999999999%;"></div><img class="graf-image" data-image-id="1*jf52uXcsAX6Nn8ghLYoJWQ.png" data-width="773" data-height="80" data-action="zoom" data-action-value="1*jf52uXcsAX6Nn8ghLYoJWQ.png" src="https://cdn-images-1.medium.com/max/800/1*jf52uXcsAX6Nn8ghLYoJWQ.png"></div><figcaption class="imageCaption">Assuming our sequence is of length¬†<strong class="markup--strong markup--figure-strong">k</strong>.</figcaption></figure><p name="9baa" id="9baa" class="graf graf--p graf-after--figure">So we‚Äôd be effectively adding together a bunch of terms that have vanished‚Ää‚Äî‚Ääthe exception being very late gradients with a small number of terms‚Ää‚Äî‚Ääand so <strong class="markup--strong markup--p-strong">dJ/dWhh </strong>would only capture gradient signals from the last few timesteps. (Or, for exploding gradients, it would become infinity).</p><p name="fd7e" id="fd7e" class="graf graf--p graf-after--p">But, you might be asking, instead of tanh‚Ää‚Äî‚Ääwhich is bounded between -1 and 1, and has a similar problem to sigmoid where the peak of the derivative is smaller than 1‚Ää‚Äî‚Ääwhy don‚Äôt we just use ReLUs? Don‚Äôt ReLUs, or perhaps leaky ReLUs, solve the vanishing gradient problem?</p><p name="6a76" id="6a76" class="graf graf--p graf-after--p">Well, not entirely; it‚Äôs not enough to solve the problem. With RNNs, the problem really lies in the architecture. Even though we could use ReLU to ensure many of the values in the gradient computation are not between -1, 0, and 1 such that they vanish‚Ää‚Äî‚Ääor vice-versa, explode‚Ää‚Äî‚Ääwe do still indeed have a lot of other variables other than the activation function derivative in the gradient computation such as the weights; you can revisit the mega-post on backprop we wrote to confirm this. Since weights are also normally randomly initialized in the range -1 to 1, and RNNs are like super deep ANNs, these weights keep multiplying on top of each other and potentially cause the gradients to vanish.</p><p name="4faf" id="4faf" class="graf graf--p graf-after--p">This is more my suspicion though‚Ää‚Äî‚ÄäI‚Äôm yet to confirm this is the case by testing. I was curious so I asked this exact question on Quora:</p><figure name="6eaa" id="6eaa" class="graf graf--figure graf--iframe graf-after--p"><div class="aspectRatioPlaceholder is-locked"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 100%;"></div><div class="iframeContainer"><IFRAME data-width="560" data-height="560" width="560" height="560" src="/media/aa3fb6f891aba6d55742cf7dabf3f7f7?postId=10300100899b" data-media-id="aa3fb6f891aba6d55742cf7dabf3f7f7" data-thumbnail="https://i.embed.ly/1/image?url=https%3A%2F%2Fwww.quora.com%2Fstatic%2Fimages%2Flogo%2Fwordmark_default.png&amp;key=4fce0568f2ce49e8b54624ef71a8a5bd" allowfullscreen frameborder="0"></IFRAME></div></div></figure><p name="e537" id="e537" class="graf graf--p graf-after--figure">From this, something interesting I learned is that: since ReLUs are <em class="markup--em markup--p-em">un</em>bounded (it‚Äôs not restricted to be between -1 and 1 or 0 and 1) unlike sigmoid/tanh, and RNNs are very deep, the activations, especially later ones, can become too big. This is because hidden states have a multiplicative relationship; one hidden state is a multiple of the previous ones, where that multiple specifically is a weight. If we use ReLU, then the hidden state isn‚Äôt limited by any range, and we could have a bunch of numbers bigger than 1 multiplying by each other.</p><p name="b4fc" id="b4fc" class="graf graf--p graf-after--p">It ends up being sort of like the exploding gradient problem, but with the values inside the neurons, not gradients. This is also what <em class="markup--em markup--p-em">then</em> causes the gradients to explode: large activations ‚Üí large gradients ‚Üí large change in weights ‚Üí even bigger activations, because updating the weights in the wrong direction ever so slightly can cause the entire network to explode. This makes learning unstable:</p><blockquote name="2c04" id="2c04" class="graf graf--blockquote graf-after--p">This means that the computation within the RNN can potentially blow up to infinity without sensible weights. This makes learning VERY unstable because a slight shift in the weights in the wrong direction during backprop can blow up the activations during the forward pass. So that‚Äôs why you see most people using sigmoid/tanh units, despite the vanishing gradient descent problem.</blockquote><p name="4f2a" id="4f2a" class="graf graf--p graf-after--blockquote">Also well said:</p><blockquote name="e86a" id="e86a" class="graf graf--blockquote graf-after--p">With RNN‚Äôs, the problem is that you are repeatedly applying your RNN to itself, which tends to [mostly] cause exponential blowup or [rarely, but sometimes] shrinkage.</blockquote><p name="4cb6" id="4cb6" class="graf graf--p graf-after--blockquote">Other issues with ReLU functions are discussed in the article I wrote, and they similarly apply to RNNs. Generally speaking, though, they just don‚Äôt work that well, especially compared to other options we have. Making RNNs perform well with ReLUs is actually a pretty hot topic of research right now, but until someone figures out something genius, RNNs are a lost cause.</p><p name="4a29" id="4a29" class="graf graf--p graf-after--p">And that‚Äôs why <strong class="markup--strong markup--p-strong">vanilla RNNs suck</strong>. Seriously. In practice, nobody uses them. Even if you didn‚Äôt fully grasp this section on how the vanishing and exploding gradient/activation problem is applicable to them, it doesn‚Äôt matter anyways. Because, everything you‚Äôve read up to this point so far‚Ä¶ throw it all away. Forget about it.</p><p name="044a" id="044a" class="graf graf--p graf-after--p">Just kidding. Don‚Äôt do that.</p><h3 name="1d2b" id="1d2b" class="graf graf--h3 graf-after--p">Fixing the problem with LSTMs (Part¬†I)</h3><p name="daad" id="daad" class="graf graf--p graf-after--h3">You shouldn‚Äôt do that because RNNs actually <em class="markup--em markup--p-em">aren‚Äôt </em>a lost cause. They‚Äôre far from it. We just need to make a few‚Ä¶ modifications.</p><p name="9491" id="9491" class="graf graf--p graf-after--p">Enter the LSTM.</p><figure name="32fa" id="32fa" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 298px; max-height: 170px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 56.99999999999999%;"></div><img class="graf-image" data-image-id="1*JuC5afKk7QIntsvyEn-IFA.png" data-width="298" data-height="170" src="https://cdn-images-1.medium.com/max/800/1*JuC5afKk7QIntsvyEn-IFA.png"></div></figure><p name="731a" id="731a" class="graf graf--p graf-after--figure">Makes sense, no?</p><figure name="ea5b" id="ea5b" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 350px; max-height: 357px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 102%;"></div><img class="graf-image" data-image-id="1*tB6QdzunJV08wyep0ZhVMA.png" data-width="350" data-height="357" src="https://cdn-images-1.medium.com/max/800/1*tB6QdzunJV08wyep0ZhVMA.png"></div></figure><p name="03ff" id="03ff" class="graf graf--p graf-after--figure">How about this?</p><figure name="0c36" id="0c36" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 400px; max-height: 314px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 78.5%;"></div><img class="graf-image" data-image-id="1*Oin8uuuQzp_wqtHAX1yyjQ.png" data-width="400" data-height="314" src="https://cdn-images-1.medium.com/max/800/1*Oin8uuuQzp_wqtHAX1yyjQ.png"></div></figure><p name="0d8c" id="0d8c" class="graf graf--p graf-after--figure">OK. Clearly something‚Äôs not registering here. But that‚Äôs fine; LSTM diagrams are frikin‚Äô difficult for beginners to grasp. I too remember when I first searched up ‚ÄúLSTM‚Äù on Google to encounter something similar to the works of art above. I reacted like this:</p><figure name="16e7" id="16e7" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 195px; max-height: 229px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 117.39999999999999%;"></div><img class="graf-image" data-image-id="1*S7ABwK33X7no_MP3epry6A.gif" data-width="195" data-height="229" src="https://cdn-images-1.medium.com/max/800/1*S7ABwK33X7no_MP3epry6A.gif"></div><figcaption class="imageCaption">MRW first Google Image-ing LSTMs.</figcaption></figure><p name="7cd4" id="7cd4" class="graf graf--p graf-after--figure">In this section, I‚Äôm going to embark on a mission to design the first simple, comprehensible, and beautiful LSTM diagram. Wish me luck, because I‚Äôll probably fail.</p><p name="b288" id="b288" class="graf graf--p graf-after--p graf--trailing">With that being said, let‚Äôs dive into <strong class="markup--strong markup--p-strong">Long Short-Term Memory networks</strong>. (Yes, that‚Äôs what LSTM stands for.)</p></div></div></section><section name="da40" class="section section--body"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><p name="bb27" id="bb27" class="graf graf--p graf--leading">With RNNs, the real ‚Äúsubstance‚Äù of the model were the hidden neurons; these were the units that did processing on the input, through time, to produce the outputs. Specifically, at each timestep, a hidden neuron embodies a hidden state that is computed by feeding the weighted sum vector of the input and/or previous hidden states with an added bias vector through a tanh squashing function. We can have multiple hidden neurons, each of which when unrolled forms an entire hidden ‚Äúlayer‚Äù.</p><p name="66c2" id="66c2" class="graf graf--p graf-after--p">If you need a refresher on this, look through the ‚ÄúFormalism‚Äù section once again.</p><p name="cd18" id="cd18" class="graf graf--p graf-after--p">With LSTMs, we still have hidden states, but they‚Äôre computed through a much more complex mechanism: LSTM cells. Instead of computing each hidden state as a direct function of inputs and other hidden states, we compute it as a function of the LSTM cell‚Äôs value (the ‚Äúcell state‚Äù) at that timestep. Each cell state is in turn functionally dependent on the previous<em class="markup--em markup--p-em"> </em>cell state and any available input or previous hidden states. That‚Äôs right‚Ää‚Äî‚Äähidden states are computed from cell states, and cell states are (in part) computed from older and/or shallower hidden states.</p><p name="095f" id="095f" class="graf graf--p graf-after--p">The cell state at a specific timestep <strong class="markup--strong markup--p-strong">t </strong>is denoted <strong class="markup--strong markup--p-strong">c_t</strong>. Like a hidden state, a cell state is just a vector.</p><figure name="e92a" id="e92a" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 500px; max-height: 245px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 49%;"></div><img class="graf-image" data-image-id="1*sr8XQzvr-WTNSbgWwI9qbQ.png" data-width="500" data-height="245" src="https://cdn-images-1.medium.com/max/800/1*sr8XQzvr-WTNSbgWwI9qbQ.png"></div><figcaption class="imageCaption">For simplicity‚Äôs sake, I‚Äôve obfuscated layer index¬†<strong class="markup--strong markup--figure-strong">‚Ñì</strong>.</figcaption></figure><p name="68f5" id="68f5" class="graf graf--p graf-after--figure">If the diagram above seems a bit trippy, let me break it down for you.</p><p name="8bd2" id="8bd2" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">c_t</strong>, as highlighted by the three arrows pointing towards it, has multiple potential functional dependencies. Four to be exact, though only a maximum of three can exist at once. These are:</p><ul class="postList"><li name="4f8f" id="4f8f" class="graf graf--li graf-after--p">The previous hidden state in time: <strong class="markup--strong markup--li-strong">h_t-1</strong>. Again, if <strong class="markup--strong markup--li-strong">t = 1</strong>, then this won‚Äôt exist. If it does, this would be the first arrow pointing into the left side of <strong class="markup--strong markup--li-strong">c_t</strong>.</li><li name="f283" id="f283" class="graf graf--li graf-after--li">The previous cell state: <strong class="markup--strong markup--li-strong">c_t-1</strong>. If <strong class="markup--strong markup--li-strong">t = 1</strong>, the dependency obviously won‚Äôt exist. This refers to the second arrow pointing into the left side of <strong class="markup--strong markup--li-strong">c_t</strong>.</li><li name="7237" id="7237" class="graf graf--li graf-after--li">Input at the current timestep: <strong class="markup--strong markup--li-strong">x_t</strong>. There may very well be no input available, for example if we are at a hidden layer <strong class="markup--strong markup--li-strong">‚Ñì &gt; 1</strong>. So this dependency doesn‚Äôt always exist. When it does, it‚Äôs the arrow pointing into the bottom of <strong class="markup--strong markup--li-strong">c_t</strong>.</li><li name="83f3" id="83f3" class="graf graf--li graf-after--li">The previous hidden state in depth: <strong class="markup--strong markup--li-strong">h^(‚Ñì-1)_t</strong>. This applies for any hidden layer <strong class="markup--strong markup--li-strong">‚Ñì &gt; 1</strong>. In such case, it would‚Ää‚Äî‚Äälike the input <strong class="markup--strong markup--li-strong">x_t</strong>‚Ää‚Äî‚Ääbe the arrow pointing into the bottom.</li></ul><p name="2a71" id="2a71" class="graf graf--p graf-after--li">Only three can exist at once because the last two are mutually exclusive.</p><p name="5160" id="5160" class="graf graf--p graf-after--p">From there, we pass information to the next cell state <strong class="markup--strong markup--p-strong">c_t+1</strong> and compute <strong class="markup--strong markup--p-strong">h_t</strong>. As you can hopefully see, <strong class="markup--strong markup--p-strong">h_t</strong> then goes on to also influence <strong class="markup--strong markup--p-strong">c_t+1 </strong>(as indicated by the horizontal arrow), along with higher level cell states or final outputs (the vertical arrow).</p><p name="0f0f" id="0f0f" class="graf graf--p graf-after--p">Right now the cells are a black box‚Ä¶ literally; we know what is inputted to them and what they output, but we don‚Äôt know their internal process. So‚Ä¶ what‚Äôs inside these cells? What do they do? What are the exact computations involved? How have the equations changed?</p><p name="ee47" id="ee47" class="graf graf--p graf-after--p">To help answer the question, I want you to imagine something: a factory. Inside this factory we have workers who perform their own tasks. Those tasks are, specifically, operating on some sort of product that runs down a <strong class="markup--strong markup--p-strong">conveyer belt</strong>. Think of, hell, I don‚Äôt know‚Ää‚Äî‚Äächicken nuggets! The first worker cuts an appropriately sized piece, the second worker applies egg wash, the third worker adds breadcrumbs, the fourth worker chucks it in the fryer, etc.</p><figure name="5a97" id="5a97" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 490px; max-height: 600px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 122.39999999999999%;"></div><img class="graf-image" data-image-id="1*4avrG18SFOMJGI4CpDIsoA.png" data-width="490" data-height="600" src="https://cdn-images-1.medium.com/max/800/1*4avrG18SFOMJGI4CpDIsoA.png"></div><figcaption class="imageCaption">I‚Äôm not sure what product this conveyer belt carries, but it certainly doesn‚Äôt look appetizing (or like chicken nuggets).</figcaption></figure><p name="02ea" id="02ea" class="graf graf--p graf-after--figure">You‚Äôre thinking: ‚ÄúOK Rohan, but how does this relate to LSTMs?‚Äù. Good question.</p><p name="9004" id="9004" class="graf graf--p graf-after--p">Basically, think of the conveyer belt as the cell state, the chicken-nugget-in-progress as information flowing through the cell, and the workers as operations we apply to this information. The final product is the finished chicken nugget‚Ää‚Äî‚Ääor, the cell state value.</p><figure name="b54e" id="b54e" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 459px; max-height: 95px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 20.7%;"></div><img class="graf-image" data-image-id="1*qNUGFMhlnl0-mNLIVvyGAg.png" data-width="459" data-height="95" src="https://cdn-images-1.medium.com/max/800/1*qNUGFMhlnl0-mNLIVvyGAg.png"></div><figcaption class="imageCaption">Chicken. Nugget.</figcaption></figure><p name="bca7" id="bca7" class="graf graf--p graf-after--figure">The reason we use the analogy of a conveyer belt is because information can flow through a cell super super easily. It‚Äôs theoretically possible for information to speed past a single cell state without being modified at all. In fact, I think the term ‚Äòmodified‚Äô is a really strong one here. With vanilla RNNs, each hidden state takes all the information it has from before and fully <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">transforms</em></strong> it by applying a function over it. LSTM cells instead take information and make minor <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">modifications</em></strong> (like additions or multiplications) to it while it flows through.</p><figure name="eca6" id="eca6" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 330px; max-height: 241px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 73%;"></div><img class="graf-image" data-image-id="1*I_nQdhxdoDa7KrZBTFeHSQ.png" data-width="330" data-height="241" src="https://cdn-images-1.medium.com/max/800/1*I_nQdhxdoDa7KrZBTFeHSQ.png"></div><figcaption class="imageCaption">Ew. Vanilla¬†RNNs.</figcaption></figure><p name="72ea" id="72ea" class="graf graf--p graf-after--figure">Vanilla RNNs look something like this. And it‚Äôs why the vanishing gradient problem exists; during backprop, gradients cannot flow back past these transformations easily, because the tanh derivatives and weights chain up and multiply together and tend to zero. We then add up or average all these gradients that are basically zero, and we get zero.</p><figure name="9b8e" id="9b8e" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 450px; max-height: 276px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 61.3%;"></div><img class="graf-image" data-image-id="1*360GYNV8kyF5ATWefrSasA.png" data-width="450" data-height="276" src="https://cdn-images-1.medium.com/max/800/1*360GYNV8kyF5ATWefrSasA.png"></div><figcaption class="imageCaption">LSTMs üí¶ üí¶¬†üí¶</figcaption></figure><p name="0a23" id="0a23" class="graf graf--p graf-after--figure">This is an extreme a simplification‚Ää‚Äî‚Ääand I‚Äôll go on to fill in the blanks later‚Ää‚Äî‚Ääbut it‚Äôs sort of what an LSTM looks like. The previous timestep‚Äôs cell state value flows through and instead of transforming the information, we tweak it by <em class="markup--em markup--p-em">adding </em>(another vector) to it. The added term is some function <strong class="markup--strong markup--p-strong">∆íw </strong>of previous information, but this is <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">not</em></strong> the same function as with vanilla RNNs‚Ää‚Äî‚Ääit‚Äôs heavily changed to make sense in this context (more on that soon), do more interesting things, and also reduce the vanishing gradient problem.</p><p name="ba34" id="ba34" class="graf graf--p graf-after--p">Another neat way to think of it is like a live circuit: there are two paths where information, like current, can flow through. After the information passes through<strong class="markup--strong markup--p-strong"> ∆íw</strong>, it‚Äôs added to the information flowing towards <strong class="markup--strong markup--p-strong">c_t</strong>. Thus, in equation form it could look something like this:</p><figure name="ea17" id="ea17" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 208px; max-height: 39px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 18.8%;"></div><img class="graf-image" data-image-id="1*qGqSrpJmO5h6ZGeIT7RK3w.png" data-width="208" data-height="39" src="https://cdn-images-1.medium.com/max/800/1*qGqSrpJmO5h6ZGeIT7RK3w.png"></div><figcaption class="imageCaption">Again‚Ä¶ sort of. We‚Äôll get into the <strong class="markup--strong markup--figure-strong">actual</strong><em class="markup--em markup--figure-em"> equations soon. This is a good proxy to convey my¬†point.</em></figcaption></figure><p name="c99b" id="c99b" class="graf graf--p graf-after--figure">With a bit of substitution, we can expand this to:</p><figure name="2268" id="2268" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 444px; max-height: 39px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 8.799999999999999%;"></div><img class="graf-image" data-image-id="1*OZs7rDSty0VhDhzTLH4Dgg.png" data-width="444" data-height="39" src="https://cdn-images-1.medium.com/max/800/1*OZs7rDSty0VhDhzTLH4Dgg.png"></div><figcaption class="imageCaption">Technically, this could expand even more, if you did some sort of recursive substitution to fully simplify the unrolled recurrence formula. You could express <strong class="markup--strong markup--figure-strong">c_t </strong>for some large value of <strong class="markup--strong markup--figure-strong">t </strong>as a really really really really long function of, ultimately, <strong class="markup--strong markup--figure-strong">c_1</strong>.</figcaption></figure><p name="48be" id="48be" class="graf graf--p graf-after--figure">Why is this better? Well, if you have basic differentiation knowledge, you‚Äôll know that addition distributes gradients equally. When we take the derivative of this whole expression, it‚Äôll become a long addition of the derivatives of individual terms. As Andrej Karpathy puts it, this additive interaction creates ‚Äúgradient super-highways‚Äù, where gradients can flow back super easily.</p><figure name="740d" id="740d" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 224px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 31.900000000000002%;"></div><img class="graf-image" data-image-id="1*n26drGfEkc-Xnmqc2Lw7cw.png" data-width="2161" data-height="690" data-action="zoom" data-action-value="1*n26drGfEkc-Xnmqc2Lw7cw.png" src="https://cdn-images-1.medium.com/max/800/1*n26drGfEkc-Xnmqc2Lw7cw.png"></div><figcaption class="imageCaption">Look‚Ää‚Äî‚Ääit‚Äôs a long conveyer belt! (In a sense, we can use this conveyor belt analogy for the <strong class="markup--strong markup--figure-strong"><em class="markup--em markup--figure-em">whole unrolled LSTM</em></strong> as well. Each cell state is a subsection of the conveyer¬†belt.)</figcaption></figure><figure name="2d87" id="2d87" class="graf graf--figure graf-after--figure"><div class="aspectRatioPlaceholder is-locked" style="max-width: 610px; max-height: 193px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 31.6%;"></div><img class="graf-image" data-image-id="1*szBIWNdr0O0doBI8rfGjzw.png" data-width="610" data-height="193" src="https://cdn-images-1.medium.com/max/800/1*szBIWNdr0O0doBI8rfGjzw.png"></div><figcaption class="imageCaption">Look‚Ää‚Äî‚Ääit‚Äôs an outdated machine learning algorithm!</figcaption></figure><p name="68da" id="68da" class="graf graf--p graf-after--figure">In the former, gradients are always added together, never multiplied. In the latter, gradients are always multiplied. Thus, in the former, when we inject a gradient at the end, it‚Äôll easily flow back all the way to the beginning. Contributions by the <strong class="markup--strong markup--p-strong">∆íw </strong>function will be made to this gradient flowing on the bottom conveyer belt as well.</p><p name="779d" id="779d" class="graf graf--p graf-after--p">This is what gradient flow would look like:</p><figure name="8074" id="8074" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 248px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 35.4%;"></div><img class="graf-image" data-image-id="1*dqOCXyepO590ORWV3VgBKw.png" data-width="740" data-height="262" data-action="zoom" data-action-value="1*dqOCXyepO590ORWV3VgBKw.png" src="https://cdn-images-1.medium.com/max/800/1*dqOCXyepO590ORWV3VgBKw.png"></div></figure><p name="7ea0" id="7ea0" class="graf graf--p graf-after--figure">Before, we discussed that when multiplicative interaction exists between gradients, the gradients either vanish (if they are mostly &lt; 1, as is usually the case for us) or explode (if they are mostly &gt; 1). Here‚Äôs some real calculus to demonstrate this:</p><figure name="6f86" id="6f86" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 287px; max-height: 288px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 100.29999999999998%;"></div><img class="graf-image" data-image-id="1*09oGK1btsVezIoMyBAwqbw.png" data-width="287" data-height="288" src="https://cdn-images-1.medium.com/max/800/1*09oGK1btsVezIoMyBAwqbw.png"></div><figcaption class="imageCaption">Former is akin to RNNs. Latter is akin to¬†LSTMs.</figcaption></figure><p name="0175" id="0175" class="graf graf--p graf-after--figure">Imagine <strong class="markup--strong markup--p-strong">f </strong>being any sort of function, like our <strong class="markup--strong markup--p-strong">∆íw</strong>. When we apply a function to itself repeatedly, the chain rule shows that the overall derivative is the multiplication of multiple different derivative terms. But, when we add functions together, the derivative is simply the addition of the individual derivatives. This won‚Äôt vanish or explode quickly, so our LSTMs won‚Äôt vanish or explode quickly. Yay!</p><p name="4e50" id="4e50" class="graf graf--p graf-after--p">Furthermore, if some of our gradients vanish‚Ää‚Äî‚Ääfor whatever reason‚Ää‚Äî‚Ääthen it should still be OK. It won‚Äôt be optimal, but since our gradient terms add together, if some of them vanish it doesn‚Äôt mean the whole thing will vanish (versus if they were multiplied together). Look: 2 + 0 = 2 but 2 √ó 0 = 0.</p><figure name="bbb5" id="bbb5" class="graf graf--figure graf-after--p graf--trailing"><div class="aspectRatioPlaceholder is-locked" style="max-width: 466px; max-height: 240px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 51.5%;"></div><img class="graf-image" data-image-id="1*K7rYONPTfcpCb0xTvO3ydw.jpeg" data-width="466" data-height="240" src="https://cdn-images-1.medium.com/max/800/1*K7rYONPTfcpCb0xTvO3ydw.jpeg"></div><figcaption class="imageCaption">A gradient super highway? Sounds good to me! <a href="http://www.dyoung.com/assets/images/Articles%20images/article4_PPH.jpg" data-href="http://www.dyoung.com/assets/images/Articles%20images/article4_PPH.jpg" class="markup--anchor markup--figure-anchor" rel="nofollow noopener noopener" target="_blank">http://www.dyoung.com/assets/images/Articles%20images/article4_PPH.jpg</a></figcaption></figure></div></div></section><section name="5df4" class="section section--body"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><p name="a58a" id="a58a" class="graf graf--p graf--leading">So far, we haven‚Äôt <em class="markup--em markup--p-em">really </em>explored LSTMs. We‚Äôve more setup a foundation for them. And there‚Äôs one glaring issue with our foundation: if we just keep adding information to cell state, it could just grow and grow and grow, and essentially act as a counter that only increments. This is not very useful, and could regularly lead to explosion. We want more fine and rich control over memory. Well, worry not, because this is exactly what LSTMs are capable of doing.</p><p name="7ba6" id="7ba6" class="graf graf--p graf-after--p">LSTM cells handle memory in a very intelligent way, enabling them to learn long-term dependencies and perform well. How, exactly? Well, the cell is sort of like an internal memory state that allows for context; it ‚Äúforgets‚Äù, a.k.a. resets, information it doesn‚Äôt find useful from the previous cell state, ‚Äúwrites‚Äù in new information it <em class="markup--em markup--p-em">does</em> find useful from the current input and/or previous hidden state, and similarly only ‚Äúreads‚Äù out part of its information‚Ää‚Äî‚Ääthe good stuff‚Ää‚Äî‚Ääin the computation of <strong class="markup--strong markup--p-strong">h_t</strong>. This respectively corresponds to the concepts of: resetting memory, writing to memory, and reading from memory. Very similar to how a modern computer system works, and we often describe an LSTM cell as a ‚Äúmemory cell‚Äù.</p><p name="6706" id="6706" class="graf graf--p graf-after--p">The ‚Äúwriting to memory‚Äù part is additive‚Ää‚Äî‚Ääit‚Äôs what I showed you in the initial diagrams. Information flows through and we add stuff we think is relevant to it. The ‚Äúresetting memory‚Äù part is multiplicative, and occurs before writing to memory; when information from the previous cell state initially flows in, we multiply it by a vector with values between 0 and 1 to reset or retain parts of it we find useless and useful respectively. The ‚Äúreading from memory‚Äù part is also multiplicative with a similar 0‚Äì1 range vector, but it doesn‚Äôt modify the information flowing through the cell states. Rather, it modifies the information flowing into the hidden states and thus decides what the hidden state is influenced by.</p><p name="6813" id="6813" class="graf graf--p graf-after--p">Both of these multiplications are <em class="markup--em markup--p-em">element wise</em>, like so:</p><figure name="2255" id="2255" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 167px; max-height: 82px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 49.1%;"></div><img class="graf-image" data-image-id="1*YuIuYxt0oYEvGMoTz_J59g.png" data-width="167" data-height="82" src="https://cdn-images-1.medium.com/max/800/1*YuIuYxt0oYEvGMoTz_J59g.png"></div></figure><p name="c869" id="c869" class="graf graf--p graf-after--figure">In this equation, when <strong class="markup--strong markup--p-strong">a = 0 </strong>the information of <strong class="markup--strong markup--p-strong">c </strong>is lost. This is what resetting does, and retaining is the vice versa. I also imagine that values such as 0.5 could be used to diminish the importance of certain information, but not completely wipe it out.</p><p name="c920" id="c920" class="graf graf--p graf-after--p">Our (unfinished) cell state computational graph now looks like this:</p><figure name="f4a0" id="f4a0" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 419px; max-height: 158px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 37.7%;"></div><img class="graf-image" data-image-id="1*_mbUA8vdaTbYreXpdPJccA.png" data-width="419" data-height="158" src="https://cdn-images-1.medium.com/max/800/1*_mbUA8vdaTbYreXpdPJccA.png"></div><figcaption class="imageCaption">This is closer to what an LSTM looks like, though we‚Äôre not exactly there¬†yet.</figcaption></figure><p name="ba90" id="ba90" class="graf graf--p graf-after--figure">Sidenote: don‚Äôt be scared whenever you see the word ‚Äúmultiplicative‚Äù and don‚Äôt immediately think of ‚Äúvanishing‚Äù or ‚Äúexploding‚Äù. It depends on the context. Here, as I‚Äôll show mathematically in a bit, it‚Äôs fine.</p><p name="9705" id="9705" class="graf graf--p graf-after--p">This concept in general is known as <strong class="markup--strong markup--p-strong">gating</strong>, because we ‚Äúgate‚Äù what can flow in and out of the LSTM cell. What we actually multiply and add by to reset, write, and read are known as the ‚Äúgates‚Äù. There are four such gates:</p><ul class="postList"><li name="dafe" id="dafe" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">f</strong>: <em class="markup--em markup--li-em">forget gate. </em>This is the ‚Äúreset‚Äù tool that wipes out, diminishes, or retains information from the previous cell state. It‚Äôs the first interaction we make, and it‚Äôs multiplicative. That is, we multiply it with the cell state. The sigmoid function is used to compute the forget gate such that its values can be in the range 0 to 1. When a value is 1, we ‚Äúremember‚Äù something, and when it is 0 we ‚Äúforget‚Äù. We might choose to forget, for example, when see a period or some sort of end of sentence marker. This is counterintuitive‚Ä¶ I guess it should really be called the ‚Äú<em class="markup--em markup--li-em">remember gate</em>‚Äù!</li><li name="aeb2" id="aeb2" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">g</strong>:<strong class="markup--strong markup--li-strong"><em class="markup--em markup--li-em">¬†</em></strong><em class="markup--em markup--li-em">?.</em><strong class="markup--strong markup--li-strong"><em class="markup--em markup--li-em"> </em></strong>This gate doesn‚Äôt really have a name, but it‚Äôs partly responsible for the ‚Äúwrite‚Äù process. It stores a value between -1 and 1 that represents how much we want to add to the cell state by, and represents the input to the cell state. It‚Äôs computed with the tanh function. We apply a bounded function to it such that the cell state acts as a stable counter, and it also introduces more complexity. (And it works well.)</li><li name="0612" id="0612" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">i</strong>:<strong class="markup--strong markup--li-strong"><em class="markup--em markup--li-em"> </em></strong><em class="markup--em markup--li-em">input gate.</em> This is the other gate responsible for the ‚Äúwrite‚Äù process. It controls how much of <strong class="markup--strong markup--li-strong"><em class="markup--em markup--li-em">g</em></strong> we ‚Äúlet in‚Äù, and is thus between 0 and 1, computed with sigmoid. It‚Äôs similar to the forget gate in this sense, in that it blocks input like the forget gate blocks the incoming cell state. We multiply <strong class="markup--strong markup--li-strong"><em class="markup--em markup--li-em">i</em> </strong>by <strong class="markup--strong markup--li-strong"><em class="markup--em markup--li-em">g </em></strong>and add this to the cell state.<strong class="markup--strong markup--li-strong"> </strong>Since <strong class="markup--strong markup--li-strong"><em class="markup--em markup--li-em">i </em></strong>is in the range 0 to 1, and <strong class="markup--strong markup--li-strong"><em class="markup--em markup--li-em">g</em></strong> is in the range -1 to 1, we add a value between -1 and 1 to the cell state. Intuitively, this sort of acts as decrementing or incrementing the counter.</li><li name="57eb" id="57eb" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">o</strong>:<em class="markup--em markup--li-em"> output gate</em>.<strong class="markup--strong markup--li-strong"><em class="markup--em markup--li-em"> </em></strong>This is also passed through sigmoid, and is a number between 0 and 1 that modulates which aspects the hidden state can draw from the cell state. It enables the ‚Äúread from memory‚Äù operation. It multiplies with the tanh<strong class="markup--strong markup--li-strong"><em class="markup--em markup--li-em"> </em></strong>of the cell state to compute the hidden state. So, I didn‚Äôt bring this up before, but the cell state leaks into a tanh before <strong class="markup--strong markup--li-strong">h_t</strong> is computed.</li></ul><p name="5cb0" id="5cb0" class="graf graf--p graf-after--li">Here‚Äôs our updated computational graph for the cell state:</p><figure name="d768" id="d768" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 533px; max-height: 251px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 47.099999999999994%;"></div><img class="graf-image" data-image-id="1*3xq3p-nVgxQXXPSXueVWdw.png" data-width="533" data-height="251" src="https://cdn-images-1.medium.com/max/800/1*3xq3p-nVgxQXXPSXueVWdw.png"></div></figure><p name="01f6" id="01f6" class="graf graf--p graf-after--figure">Looks like I‚Äôm starting to create a complex diagram of my own. Damn. üòû I guess LSTMs and immediately interpretable diagrams just weren‚Äôt meant to be!</p><p name="7eb5" id="7eb5" class="graf graf--p graf-after--p">Basically, <strong class="markup--strong markup--p-strong">f </strong>interacts with the cell state through a multiplication. <strong class="markup--strong markup--p-strong">i </strong>interacts with <strong class="markup--strong markup--p-strong">g </strong>through a multiplication as well, the result of which interacts with the cell state through an addition. Finally, the cell state leaks into a tanh (that‚Äôs the shape of the tanh function in the circle), the result of which then interacts with <strong class="markup--strong markup--p-strong">o </strong>through multiplication to compute <strong class="markup--strong markup--p-strong">h_t</strong>. This does not disrupt the cell state, which flows to the next timestep. <strong class="markup--strong markup--p-strong">h_t </strong>then flows forward (and it could flow upward as well).</p><p name="e614" id="e614" class="graf graf--p graf-after--p">Here‚Äôs the equation form:</p><figure name="94fe" id="94fe" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 222px; max-height: 84px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 37.8%;"></div><img class="graf-image" data-image-id="1*B9Qd1pW1kYM_zcg0IhPfUA.png" data-width="222" data-height="84" src="https://cdn-images-1.medium.com/max/800/1*B9Qd1pW1kYM_zcg0IhPfUA.png"></div><figcaption class="imageCaption">Each gate should actually be indexed by timestep <strong class="markup--strong markup--figure-strong">t‚Ää</strong>‚Äî‚Ääwe‚Äôll see why¬†soon.</figcaption></figure><p name="2518" id="2518" class="graf graf--p graf-after--figure">As you can see, our cell state has no activation function; the activation function is simply the identity function! Yet, the cell state usually doesn‚Äôt explode‚Ää‚Äî‚Ääit stays stable by ‚Äúforgetting‚Äù and ‚Äúwriting‚Äù, and does interesting things with this gating to promote context, fine control over memory, and long-term dependency learning.</p><p name="b30a" id="b30a" class="graf graf--p graf-after--p">So, how are the gates calculated? Well, all of these gates have their own learnable weights and are functions of the last timestep‚Äôs <em class="markup--em markup--p-em">hidden state </em>flowing in and any current timestep inputs, not the cell state (contrary to what I may have implied earlier with the gradient flow diagrams). This should make sense when you think about it; I mean, firstly, the <strong class="markup--strong markup--p-strong">g </strong>and <strong class="markup--strong markup--p-strong">i </strong>gates literally represent input, so they better be functionally dependent on hidden states and input data! On an intuitive level, the gates help us modify the cell state, and we modify the cell state based on our current context. External stimulus that provide context should be used to compute these gates, and since context = input + hidden states our gates are functionally dependent on input and hidden states.</p><p name="27fb" id="27fb" class="graf graf--p graf-after--p">Since every gate has a different value at each timestep, we index by timestep <strong class="markup--strong markup--p-strong">t </strong>just like for hidden states, cell states, or something similar.</p><p name="6fb5" id="6fb5" class="graf graf--p graf-after--p">We could generalize for multiple hidden layers as well:</p><figure name="53d0" id="53d0" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 290px; max-height: 41px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 14.099999999999998%;"></div><img class="graf-image" data-image-id="1*BWz6E_IFi6UTLkSNSoq1Yg.png" data-width="290" data-height="41" src="https://cdn-images-1.medium.com/max/800/1*BWz6E_IFi6UTLkSNSoq1Yg.png"></div></figure><p name="1ae0" id="1ae0" class="graf graf--p graf-after--figure">But, for simplicity‚Äôs sake, let‚Äôs assume we are at the first hidden layer, or that there is only one hidden layer in the LSTM. This way, we can obfuscate the <strong class="markup--strong markup--p-strong">‚Ñì </strong>term and ignore influence from hidden states in the previous depth. We‚Äôll also forget about edge cases and assume input exists at the current timestep. In practice, we obviously can‚Äôt make said assumptions, but for the sake of demonstrating the equations it becomes too tedious otherwise.</p><p name="c042" id="c042" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Sidenote: we make this assumption for the rest of the discussion on LSTMs in this article.</strong></p><figure name="5a86" id="5a86" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 384px; max-height: 180px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 46.9%;"></div><img class="graf-image" data-image-id="1*hP6I692iv7oc6AkcWINDaw.png" data-width="384" data-height="180" src="https://cdn-images-1.medium.com/max/800/1*hP6I692iv7oc6AkcWINDaw.png"></div></figure><p name="033c" id="033c" class="graf graf--p graf-after--figure">Like with the RNN hidden state, the index of each weight matrix is descriptive; for example, <strong class="markup--strong markup--p-strong">W_xf </strong>are the weights that map input <strong class="markup--strong markup--p-strong">x </strong>to the forget gate <strong class="markup--strong markup--p-strong">f</strong>. Each gate has weight matrices that map input and hidden states to itself, including biases.</p><p name="6989" id="6989" class="graf graf--p graf-after--p">And this is the beauty of LSTMs; the whole thing is end-to-end differentiable. These gates can <em class="markup--em markup--p-em">learn </em>when to allow data to flow and what data should flow depending on the context it sees (the input and the hidden states). It learns this based on patterns it sees while training. In this sense, it‚Äôs sort of like how a CNN learns feature detectors for images, but the patterns are way more complex and less human interpretable with LSTMs. This is why they perform so well.</p><figure name="1be4" id="1be4" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 551px; max-height: 321px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 58.3%;"></div><img class="graf-image" data-image-id="1*VJL6ONtLK77GpO2XmFCH7g.png" data-width="551" data-height="321" src="https://cdn-images-1.medium.com/max/800/1*VJL6ONtLK77GpO2XmFCH7g.png"></div><figcaption class="imageCaption"><strong class="markup--strong markup--figure-strong">üò® </strong>üò± üò∞¬†: perhaps your immediate reaction.</figcaption></figure><p name="b263" id="b263" class="graf graf--p graf-after--figure">Okay, this looks scarier, but it‚Äôs actually not much different to what we had before, especially once you look past the intimidating web of arrows. One notable change is that we‚Äôre showing the previous hidden state in time and the current input flowing in. This diagram makes the assumption that we‚Äôre in the first layer and at some timestep &gt; 1 where input exists. We then show how the <strong class="markup--strong markup--p-strong">f</strong>, <strong class="markup--strong markup--p-strong">i</strong>, <strong class="markup--strong markup--p-strong">g, </strong>and <strong class="markup--strong markup--p-strong">o</strong> gates are computed from this information‚Ää‚Äî‚Ääthe hidden state and inputs are fed into an activation function like sigmoid (or, for <strong class="markup--strong markup--p-strong">g</strong>, a tanh; you can tell because it‚Äôs double the height of the others)‚Ää‚Äî‚Ääand it‚Äôs expressed through the web of arrows. It‚Äôs implied that we weight the two terms entering our activation functions, adding them up with a bias vector, but it‚Äôs not necessarily explicit in the diagram.</p><p name="b56c" id="b56c" class="graf graf--p graf-after--p">Let‚Äôs embed this into our overall LSTM diagram for a single timestep:</p><figure name="689a" id="689a" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 317px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 45.300000000000004%;"></div><img class="graf-image" data-image-id="1*0h88NXeFxkb-xD1rBq4lgA.png" data-width="770" data-height="349" data-action="zoom" data-action-value="1*0h88NXeFxkb-xD1rBq4lgA.png" src="https://cdn-images-1.medium.com/max/800/1*0h88NXeFxkb-xD1rBq4lgA.png"></div></figure><p name="147c" id="147c" class="graf graf--p graf-after--figure">Now let‚Äôs zoom out and view our entire unrolled single layer, three timestep LSTM:</p></div><div class="section-inner sectionLayout--fullWidth"><figure name="4127" id="4127" class="graf graf--figure graf--layoutFillWidth graf-after--p"><div class="aspectRatioPlaceholder is-locked"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 28.499999999999996%;"></div><img class="graf-image" data-image-id="1*-lhIk-yAsXk88gcPvEeIRQ.png" data-width="4225" data-height="1204" src="https://cdn-images-1.medium.com/max/2000/1*-lhIk-yAsXk88gcPvEeIRQ.png"></div></figure></div><div class="section-inner sectionLayout--insetColumn"><p name="c9d0" id="c9d0" class="graf graf--p graf-after--figure">It‚Äôs beautiful, isn‚Äôt it? The full screen width size just adds to the effect! <a href="https://drive.google.com/file/d/0BwbWRPtraa2zQUsydXRKSkd3YUU/view?usp=sharing" data-href="https://drive.google.com/file/d/0BwbWRPtraa2zQUsydXRKSkd3YUU/view?usp=sharing" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Here‚Äôs</a> a link to the full res version.</p><p name="c678" id="c678" class="graf graf--p graf-after--p">The only thing that would look more beautiful would be multiple LSTM cells that stack on top of each other (multiple hidden layers)! üòç</p><h3 name="e117" id="e117" class="graf graf--h3 graf-after--p">Fixing the problem with LSTMs (Part¬†II)</h3><p name="f89a" id="f89a" class="graf graf--p graf-after--h3">You‚Äôve come a long way, young padawan. But there‚Äôs still a bit left to go. Part I focused on the motivation for LSTMs, how they work, and a bit on why they reduce the vanishing gradient problem. Now, having a full understanding of LSTMs, Part II will hone in on the latter part‚Äîanalyzing on a more close, technical level why our gradients stop vanishing as quickly. You won‚Äôt find a lot of this information online easily; I had to search and ask left and right to find an explanation better and more comprehensive than what you‚Äôll find in other current tutorials.</p><p name="a182" id="a182" class="graf graf--p graf-after--p">Firstly, truncated BPTT is often used with LSTMs; it‚Äôs a method to speed up training. In particular, note that if we input a sequence of length 1000 into an LSTM, and want to train it, it‚Äôs equivalent to training a 1000 layer neural network. Doing forward and backwards passes into this is very memory and time consuming, especially while backpropagating the error when we need to compute a derivative like this:</p><figure name="7285" id="7285" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 87px; max-height: 79px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 90.8%;"></div><img class="graf-image" data-image-id="1*UqC4IRIfcDfoiwD8zvqW2A.png" data-width="87" data-height="79" src="https://cdn-images-1.medium.com/max/800/1*UqC4IRIfcDfoiwD8zvqW2A.png"></div></figure><p name="06fe" id="06fe" class="graf graf--p graf-after--figure">‚Ä¶which would include a <em class="markup--em markup--p-em">lot</em> of terms.</p><p name="792f" id="792f" class="graf graf--p graf-after--p">When we backprop the error, and add all the gradients up, this is what we get:</p><figure name="b325" id="b325" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 675px; max-height: 79px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 11.700000000000001%;"></div><img class="graf-image" data-image-id="1*ucOvP6wOs9MHzH8WKiykbQ.png" data-width="675" data-height="79" src="https://cdn-images-1.medium.com/max/800/1*ucOvP6wOs9MHzH8WKiykbQ.png"></div></figure><p name="9c5f" id="9c5f" class="graf graf--p graf-after--figure">Truncated BPTT does two things:</p><ul class="postList"><li name="5bfb" id="5bfb" class="graf graf--li graf-after--p">Instead of doing a forward pass on the whole sequence and then doing a backwards pass, we process the sequence timestep by timestep and do a backwards pass ‚Äúevery so often‚Äù. That is‚Ää‚Äî‚Ääwe compute <strong class="markup--strong markup--li-strong">h_1 </strong>and <strong class="markup--strong markup--li-strong">c_1</strong>, then <strong class="markup--strong markup--li-strong">h_2 </strong>and <strong class="markup--strong markup--li-strong">c_2</strong>, then <strong class="markup--strong markup--li-strong">h_3</strong> and <strong class="markup--strong markup--li-strong">c_3</strong>, and then at some point in time, quantified by <strong class="markup--strong markup--li-strong">k1, </strong>we do a backwards pass. Every <strong class="markup--strong markup--li-strong">k1 </strong>timesteps, we perform BPTT; if <strong class="markup--strong markup--li-strong">k1 = 10</strong>, for example, then once we compute <strong class="markup--strong markup--li-strong">h_10 </strong>and <strong class="markup--strong markup--li-strong">c_10 </strong>we perform BPTT. Same for <strong class="markup--strong markup--li-strong">h_20 </strong>and <strong class="markup--strong markup--li-strong">c_20</strong>, and so on so forth. When we perform the backwards pass, our error <strong class="markup--strong markup--li-strong">J </strong>won‚Äôt be the same as if we did a full forwards pass and full backwards pass, since we haven‚Äôt observed all the outputs yet‚Äîwe wouldn‚Äôt have even computed all the potential outputs yet! Instead, the error describes what we‚Äôve observed and computed so far, because we process the sequence timestep by timestep. Intuitively, it‚Äôs like we train on a small subset of the training sequence, and this subset increases in length each time, which enables us to continue learning long-term dependencies. We could denote the error at timestep <strong class="markup--strong markup--li-strong">t</strong> ‚Äîwhere <strong class="markup--strong markup--li-strong">t </strong>is a multiple of <strong class="markup--strong markup--li-strong">k1</strong>‚Ää‚Äî‚Ääwith truncated backprop as<strong class="markup--strong markup--li-strong"> J^t</strong>. So:</li></ul><figure name="dd7e" id="dd7e" class="graf graf--figure graf-after--li"><div class="aspectRatioPlaceholder is-locked" style="max-width: 445px; max-height: 80px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 18%;"></div><img class="graf-image" data-image-id="1*Br6EoWvUmGTNoX3NqkZVpA.png" data-width="445" data-height="80" src="https://cdn-images-1.medium.com/max/800/1*Br6EoWvUmGTNoX3NqkZVpA.png"></div></figure><p name="eb5a" id="eb5a" class="graf graf--p graf-after--figure">For example, if <strong class="markup--strong markup--p-strong">t = 20</strong> and <strong class="markup--strong markup--p-strong">k1 = 10</strong>, our <em class="markup--em markup--p-em">second</em> (because 20 √∑ 10 = 2) round of BPTT would be:</p><figure name="6f7d" id="6f7d" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 667px; max-height: 80px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 12%;"></div><img class="graf-image" data-image-id="1*pg91-TmNosH9B7Py0wjCdQ.png" data-width="667" data-height="80" src="https://cdn-images-1.medium.com/max/800/1*pg91-TmNosH9B7Py0wjCdQ.png"></div></figure><ul class="postList"><li name="f441" id="f441" class="graf graf--li graf-after--figure">On top of this, instead of backpropagating from <strong class="markup--strong markup--li-strong">J^t </strong>all the way to the first timestep <strong class="markup--strong markup--li-strong">c_1</strong>, we set a cut-off point. This cut-off point, quantified by <strong class="markup--strong markup--li-strong">k2</strong>, is the timestep at which our cell states stop contributing to the overall gradient. For example, if <strong class="markup--strong markup--li-strong">k2 = 10</strong>, and we‚Äôre backpropagating at <strong class="markup--strong markup--li-strong">t = 20</strong>, then <strong class="markup--strong markup--li-strong">c_10 </strong>is the final cell state to contribute to the overall gradient. Everything before <strong class="markup--strong markup--li-strong">c_10 </strong>will have no say. This is designed such that we avoid computing derivatives between cell states far apart in time, which would include a huge number of terms (as mentioned earlier). The equation is now:</li></ul><figure name="73ca" id="73ca" class="graf graf--figure graf-after--li"><div class="aspectRatioPlaceholder is-locked" style="max-width: 515px; max-height: 80px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 15.5%;"></div><img class="graf-image" data-image-id="1*GmP4nvsdBTyyRwo7ffRW2g.png" data-width="515" data-height="80" src="https://cdn-images-1.medium.com/max/800/1*GmP4nvsdBTyyRwo7ffRW2g.png"></div></figure><p name="956c" id="956c" class="graf graf--p graf-after--figure">So, with <strong class="markup--strong markup--p-strong">t = 20</strong>, <strong class="markup--strong markup--p-strong">k2 = 10</strong>, and <strong class="markup--strong markup--p-strong">k1 = 10</strong>, our second round of BPTT would follow:</p><figure name="12a1" id="12a1" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 674px; max-height: 80px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 11.899999999999999%;"></div><img class="graf-image" data-image-id="1*WyHlRZljjmHEaFKsGg0JQg.png" data-width="674" data-height="80" src="https://cdn-images-1.medium.com/max/800/1*WyHlRZljjmHEaFKsGg0JQg.png"></div></figure><p name="a297" id="a297" class="graf graf--p graf-after--figure">Both <strong class="markup--strong markup--p-strong">k1</strong> and <strong class="markup--strong markup--p-strong">k2</strong> are hyperparameters. <strong class="markup--strong markup--p-strong">k1 </strong>does not have to equal <strong class="markup--strong markup--p-strong">k2</strong>.</p><p name="93ed" id="93ed" class="graf graf--p graf-after--p">These two techniques combined enables truncated BPTT to not lose the ability to learn long term dependencies. Here‚Äôs a formal definition:</p><blockquote name="72ee" id="72ee" class="graf graf--blockquote graf-after--p">[Truncated BPTT] processes the sequence one timestep at a time, and every <strong class="markup--strong markup--blockquote-strong">k1</strong> timesteps, it runs BPTT for <strong class="markup--strong markup--blockquote-strong">k2</strong> timesteps, so a parameter update can be cheap if <strong class="markup--strong markup--blockquote-strong">k2</strong> is small. Consequently, its hidden states have been exposed to many timesteps and so may contain useful information about the far past, which would be opportunistically exploited.</blockquote><blockquote name="ae91" id="ae91" class="graf graf--blockquote graf-after--blockquote">‚Äî <a href="http://www.cs.utoronto.ca/~ilya/pubs/ilya_sutskever_phd_thesis.pdf" data-href="http://www.cs.utoronto.ca/~ilya/pubs/ilya_sutskever_phd_thesis.pdf" class="markup--anchor markup--blockquote-anchor" rel="noopener" target="_blank">‚ÄúTraining Recurrent Neural Networks‚Äù</a>, 2.8.6, Page 23</blockquote><p name="28a4" id="28a4" class="graf graf--p graf-after--blockquote">The same paper gives nice pseudocode for truncated BPTT:</p><figure name="f5d9" id="f5d9" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 413px; max-height: 113px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 27.400000000000002%;"></div><img class="graf-image" data-image-id="1*0SnUb2iYt1RNa7JsGG-7gQ.png" data-width="413" data-height="113" src="https://cdn-images-1.medium.com/max/800/1*0SnUb2iYt1RNa7JsGG-7gQ.png"></div></figure><p name="399f" id="399f" class="graf graf--p graf-after--figure">The rest of the math in this section will not be in the context of using truncated backprop, because it‚Äôs a technique vs. something rooted in the mathematical foundation of LSTMs.</p><p name="d546" id="d546" class="graf graf--p graf-after--p">Moving on‚Ää‚Äî‚Ääbefore, we saw this diagram:</p><figure name="dc69" id="dc69" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 224px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 31.900000000000002%;"></div><img class="graf-image" data-image-id="1*n26drGfEkc-Xnmqc2Lw7cw.png" data-width="2161" data-height="690" data-action="zoom" data-action-value="1*n26drGfEkc-Xnmqc2Lw7cw.png" src="https://cdn-images-1.medium.com/max/800/1*n26drGfEkc-Xnmqc2Lw7cw.png"></div></figure><p name="48f4" id="48f4" class="graf graf--p graf-after--figure">In this context, <strong class="markup--strong markup--p-strong">∆íw = i ‚äô g</strong>, because it‚Äôs the value we‚Äôre adding to the cell state.</p><p name="db0e" id="db0e" class="graf graf--p graf-after--p">But this diagram is a bit of a lie. Why? It ignores forget gates. So, does the presence of forget gates affect the vanishing gradient problem? Quite significantly, actually. How? Let‚Äôs bring up our cell state equation to see:</p><figure name="4484" id="4484" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 236px; max-height: 39px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 16.5%;"></div><img class="graf-image" data-image-id="1*i6rbrX0k9mKLXewD4korCw.png" data-width="236" data-height="39" src="https://cdn-images-1.medium.com/max/800/1*i6rbrX0k9mKLXewD4korCw.png"></div></figure><p name="09ae" id="09ae" class="graf graf--p graf-after--figure">With the forget gate, we now include a multiplicative interaction. Our new diagram will look like this:</p><figure name="931c" id="931c" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 283px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 40.5%;"></div><img class="graf-image" data-image-id="1*UVx1vL6ADQGTBeKSaWX7bw.png" data-width="2436" data-height="986" data-action="zoom" data-action-value="1*UVx1vL6ADQGTBeKSaWX7bw.png" src="https://cdn-images-1.medium.com/max/800/1*UVx1vL6ADQGTBeKSaWX7bw.png"></div><figcaption class="imageCaption">Do not confuse forget gate <strong class="markup--strong markup--figure-strong">∆í</strong> with function <strong class="markup--strong markup--figure-strong">∆íw</strong> in this diagram. I know, it‚Äôs confusing‚Ä¶ üò¢</figcaption></figure><p name="bf2c" id="bf2c" class="graf graf--p graf-after--figure">When our gradients flow back, they will be affected by this multiplicative interaction. So, let‚Äôs compute the new derivative:</p><figure name="0504" id="0504" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 113px; max-height: 79px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 69.89999999999999%;"></div><img class="graf-image" data-image-id="1*60XFfJvc0t9a0ekdMTAp_Q.png" data-width="113" data-height="79" src="https://cdn-images-1.medium.com/max/800/1*60XFfJvc0t9a0ekdMTAp_Q.png"></div></figure><p name="f174" id="f174" class="graf graf--p graf-after--figure">This seems super neat, actually. <em class="markup--em markup--p-em">Obviously</em> the gradient will be <strong class="markup--strong markup--p-strong">f</strong>, because <strong class="markup--strong markup--p-strong">f </strong>acts as a blocker and controls how much <strong class="markup--strong markup--p-strong">c_t-1 </strong>influences <strong class="markup--strong markup--p-strong">c_t</strong>; it‚Äôs the gate that you can fully or partially open and close that lets information from <strong class="markup--strong markup--p-strong">c_t-1 </strong>flow through! It‚Äôs just intuitive that it would propagate back perfectly.</p><p name="d859" id="d859" class="graf graf--p graf-after--p">But, if you‚Äôve payed close attention so far, you might be asking: ‚Äú<em class="markup--em markup--p-em">wait, what happened to </em><strong class="markup--strong markup--p-strong">∆íw</strong><em class="markup--em markup--p-em">‚Äôs contribution to the gradient?‚Äù</em><strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em"> </em></strong>If you‚Äôre a hardcore mathematician, you might also be worried that we‚Äôre content with leaving the gradient as just <strong class="markup--strong markup--p-strong">f</strong>. This is because the gates <strong class="markup--strong markup--p-strong">f</strong>,<strong class="markup--strong markup--p-strong"> i</strong>, and <strong class="markup--strong markup--p-strong">g </strong>are all functions of <strong class="markup--strong markup--p-strong">c_t-1</strong>; they are functions of <strong class="markup--strong markup--p-strong">h_t-1</strong>, which is, in turn, a function of <strong class="markup--strong markup--p-strong">c_t-1</strong>! The diagram shows this visually, as well. It seems we‚Äôre failing to apply calculus properly. We‚Äôd need to backprop through <strong class="markup--strong markup--p-strong">f</strong> and through <strong class="markup--strong markup--p-strong">i ‚äô g </strong>to complete the derivative.</p><p name="3f0c" id="3f0c" class="graf graf--p graf-after--p">Let‚Äôs walk through the differentiation to show why you‚Äôre actually not wrong<strong class="markup--strong markup--p-strong">,<em class="markup--em markup--p-em"> </em></strong>but neither am I:</p><figure name="f0c6" id="f0c6" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 380px; max-height: 162px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 42.6%;"></div><img class="graf-image" data-image-id="1*x1mvDnbZOmZ1CnHJo23tZg.png" data-width="380" data-height="162" src="https://cdn-images-1.medium.com/max/800/1*x1mvDnbZOmZ1CnHJo23tZg.png"></div></figure><p name="80e0" id="80e0" class="graf graf--p graf-after--figure">Now, with the first derivative, we need to apply product rule. Why? Because we‚Äôre differentiating the product of two functions of <strong class="markup--strong markup--p-strong">c_t-1</strong>. The former being the forget gate, and the latter being just <strong class="markup--strong markup--p-strong">c_t-1</strong>. Let‚Äôs do it:</p><figure name="b39b" id="b39b" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 271px; max-height: 124px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 45.800000000000004%;"></div><img class="graf-image" data-image-id="1*TwjnkG6vtuIke1pkAzTzzA.png" data-width="271" data-height="124" src="https://cdn-images-1.medium.com/max/800/1*TwjnkG6vtuIke1pkAzTzzA.png"></div></figure><p name="00fd" id="00fd" class="graf graf--p graf-after--figure">Then, from product rule:</p><figure name="416d" id="416d" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 381px; max-height: 164px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 43%;"></div><img class="graf-image" data-image-id="1*cgq4UnWxun6rQ6H00gDzWg.png" data-width="381" data-height="164" src="https://cdn-images-1.medium.com/max/800/1*cgq4UnWxun6rQ6H00gDzWg.png"></div></figure><p name="7dd1" id="7dd1" class="graf graf--p graf-after--figure">That‚Äôs the first derivative done. We purposely choose not to compute the derivative of the forget gate with respect to the previous cell state on previous. You‚Äôll see why in a bit.</p><p name="e8f2" id="e8f2" class="graf graf--p graf-after--p">Now let‚Äôs tackle the second one:</p><figure name="be20" id="be20" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 150px; max-height: 77px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 51.300000000000004%;"></div><img class="graf-image" data-image-id="1*1T0iaNg6vY4pEOz-ybkjuw.png" data-width="150" data-height="77" src="https://cdn-images-1.medium.com/max/800/1*1T0iaNg6vY4pEOz-ybkjuw.png"></div></figure><p name="10b0" id="10b0" class="graf graf--p graf-after--figure">You‚Äôll notice that it‚Äôs also two functions of <strong class="markup--strong markup--p-strong">c_t-1 </strong>multiplied together, so we use the product rule again:</p><figure name="f85e" id="f85e" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 320px; max-height: 124px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 38.800000000000004%;"></div><img class="graf-image" data-image-id="1*tVfrKCc1T7eRgypliDe19A.png" data-width="320" data-height="124" src="https://cdn-images-1.medium.com/max/800/1*tVfrKCc1T7eRgypliDe19A.png"></div></figure><p name="618f" id="618f" class="graf graf--p graf-after--figure">So:</p><figure name="c8bf" id="c8bf" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 361px; max-height: 164px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 45.4%;"></div><img class="graf-image" data-image-id="1*s2nnva6Yhb2AuZDYYALbEA.png" data-width="361" data-height="164" src="https://cdn-images-1.medium.com/max/800/1*s2nnva6Yhb2AuZDYYALbEA.png"></div><figcaption class="imageCaption">Once again, we purposely do not simplify the gate derivative terms.</figcaption></figure><p name="41e0" id="41e0" class="graf graf--p graf-after--figure">Thus, our overall derivative becomes:</p><figure name="7b20" id="7b20" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 461px; max-height: 79px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 17.1%;"></div><img class="graf-image" data-image-id="1*o0dzU_s9WxoTYfOkQo1a0A.png" data-width="461" data-height="79" src="https://cdn-images-1.medium.com/max/800/1*o0dzU_s9WxoTYfOkQo1a0A.png"></div><figcaption class="imageCaption">Notice that the first term in this derivative is our forget¬†gate.</figcaption></figure><p name="9307" id="9307" class="graf graf--p graf-after--figure">Pay attention to the caption of the diagram.</p><p name="4712" id="4712" class="graf graf--p graf-after--p">This is actually our <em class="markup--em markup--p-em">real </em>derivative. Modern LSTM implementations just use an auto differentiation library to compute derivatives, so they‚Äôll probably come up with this. However, <em class="markup--em markup--p-em">effectively</em>, our gradient is just the forget gate, because the other three terms tend towards zero. Yup‚Ää‚Äî‚Ääthey vanish. Why?</p><p name="b5a4" id="b5a4" class="graf graf--p graf-after--p">When we backprop error in LSTMs, we backprop through cell states to propagate the error from the outputs to the cell state we want. For example, if we want to backprop the error from the output at time <strong class="markup--strong markup--p-strong">t</strong> down <strong class="markup--strong markup--p-strong">k </strong>timesteps, then we need to compute the derivative of the cell state at time <strong class="markup--strong markup--p-strong">t </strong>to the cell state at time <strong class="markup--strong markup--p-strong">t-k</strong>. Look what happens when we do that:</p><figure name="db33" id="db33" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 642px; max-height: 79px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 12.3%;"></div><img class="graf-image" data-image-id="1*dBFbl6NCqp94Lnyb0taFWg.png" data-width="642" data-height="79" src="https://cdn-images-1.medium.com/max/800/1*dBFbl6NCqp94Lnyb0taFWg.png"></div></figure><p name="57e3" id="57e3" class="graf graf--p graf-after--figure">We didn‚Äôt simplify the gate w.r.t. cell state derivatives for a reason; as we backpropagate through time, they begin to vanish. Thus, whatever they multiplied with is killed off from making contributions to the gradient, too. So, effectively:</p><figure name="08ea" id="08ea" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 458px; max-height: 71px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 15.5%;"></div><img class="graf-image" data-image-id="1*Q9BJ7JxQ08YvfBW_OsJabw.png" data-width="458" data-height="71" src="https://cdn-images-1.medium.com/max/800/1*Q9BJ7JxQ08YvfBW_OsJabw.png"></div></figure><p name="e607" id="e607" class="graf graf--p graf-after--figure">The rationale behind this is pretty simple, and we don‚Äôt need math for it; these gates are the outputs of non-linearities eg. sigmoid and tanh. If we were to get the derivative of them in getting our cell state derivative, then this derivative would contain the derivatives of sigmoid/tanh in them. But, just because we don‚Äôt <em class="markup--em markup--p-em">need</em> to use math to show this, doesn‚Äôt mean we don‚Äôt <em class="markup--em markup--p-em">want </em>to üòè¬†:</p><figure name="0dbb" id="0dbb" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 541px; max-height: 79px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 14.6%;"></div><img class="graf-image" data-image-id="1*-mUDovQ8ovejmWPNoFSI1g.png" data-width="541" data-height="79" src="https://cdn-images-1.medium.com/max/800/1*-mUDovQ8ovejmWPNoFSI1g.png"></div><figcaption class="imageCaption">I obfuscated the input to the sigmoid function for the input gate, just for simplicity.</figcaption></figure><p name="f6be" id="f6be" class="graf graf--p graf-after--figure">Recall from our vanishing gradient article that the max output of sigmoid‚Äôs first order derivative is 0.25, and it‚Äôs something similar for tanh. This becomes textbook vanishing gradient problem. As we backprop through more and more cell states, the gradient terms become longer and longer, and this will definitely vanish. When they don‚Äôt vanish, they‚Äôll be super minor contributions, so we can just leave them out for brevity.</p><blockquote name="95ec" id="95ec" class="graf graf--blockquote graf-after--p">Sidenote: one person reached out to me unsure of why gradients with long terms‚Ää‚Äî‚Ääaka, that are equal to the product of a lot of terms‚Ää‚Äî‚Ääusually vanishes/explodes. Here‚Äôs what I said in response:</blockquote><blockquote name="eabd" id="eabd" class="graf graf--blockquote graf--startsWithDoubleQuote graf-after--blockquote">‚ÄúIf you have long gradient terms, you probably have the vanishing gradient problem, unless you can guarantee those terms are around 1 each. If they‚Äôre not, it‚Äôll explode or vanish. And, given the nature of the problems where this is an issue, it‚Äôs very unlikely they‚Äôll be around 1 each. Especially if they are the output some non-linear function like sigmoid/tanh or their derivatives.</blockquote><blockquote name="944e" id="944e" class="graf graf--blockquote graf-after--blockquote">For example, let‚Äôs say the gradient term = k_1 √ó k_2 √ó k_3 √ó¬†‚Ä¶ √ó k_100. 100 terms in this product.</blockquote><blockquote name="1583" id="1583" class="graf graf--blockquote graf-after--blockquote">If each of these terms is, let‚Äôs say, around 0.5, then you have 0.5¬π‚Å∞‚Å∞ = some absurdly low number. If you have each term be arond 1.5, then you have 1.5¬π‚Å∞‚Å∞ which is some absurdly high number.<br><br>When we introduce tanh/sigmoid and/or their derivatives in these huge products, you can guarantee that they‚Äôll saturate and die off. As mentioned, the max for sigmoid‚Äôs first order derivative is 0.25, so just imagine something like 0.25¬π‚Å∞‚Å∞.</blockquote><p name="59c9" id="59c9" class="graf graf--p graf-after--blockquote">Ultimately, the reason I obfuscate these terms that vanish in the derivative is because I would like to show the effect of the forget gate on gradient flow now. If I included the other terms, the same implications would be present, but the math would just take longer to type out and render.</p><p name="e361" id="e361" class="graf graf--p graf-after--p">Because <strong class="markup--strong markup--p-strong">∆íw = i ‚äô g</strong>, we can redraw our diagram showing that <strong class="markup--strong markup--p-strong">∆íw </strong>won‚Äôt make any contributions to the gradient flow back. Again‚Ää‚Äî‚Ää<strong class="markup--strong markup--p-strong">∆íw</strong> does, but it‚Äôs effectively negligible, so we can just exclude it from our updated gradient flow diagram, which follows:</p><figure name="3217" id="3217" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 290px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 41.4%;"></div><img class="graf-image" data-image-id="1*RsHULZCgY6p-5bKkE99Q-Q.png" data-width="1095" data-height="453" data-action="zoom" data-action-value="1*RsHULZCgY6p-5bKkE99Q-Q.png" src="https://cdn-images-1.medium.com/max/800/1*RsHULZCgY6p-5bKkE99Q-Q.png"></div></figure><p name="8811" id="8811" class="graf graf--p graf-after--figure">But wait! This doesn‚Äôt look good; the gradients have to multiply by this <strong class="markup--strong markup--p-strong">f_t</strong> gate at each timestep. Before, they didn‚Äôt have to multiply by anything (or, in other words, they multiplied by 1) and flowed past super easily.</p><p name="9feb" id="9feb" class="graf graf--p graf-after--p">Machine learning researchers coined a name for the type of function we had before we introduced the forget gate where the derivative of one cell state w.r.t. the previous is <strong class="markup--strong markup--p-strong">1.0</strong>: ‚ÄúConstant Error Carousel‚Äù (CEC). With our new function, the derivative is equal to <strong class="markup--strong markup--p-strong">f</strong>. You‚Äôll see this referred to as a ‚Äúlinear carousel‚Äù in papers.</p><p name="fdbb" id="fdbb" class="graf graf--p graf-after--p">Before we introduced a forget gate‚Ää‚Äî‚Ääwhere all we had was the additive interaction from <strong class="markup--strong markup--p-strong">∆íw‚Ää</strong>‚Äî‚Ääour cell state function was a CEC:</p><figure name="2bf9" id="2bf9" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 208px; max-height: 84px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 40.400000000000006%;"></div><img class="graf-image" data-image-id="1*9O__qOVOK1wxJDFy6m4YAQ.png" data-width="208" data-height="84" src="https://cdn-images-1.medium.com/max/800/1*9O__qOVOK1wxJDFy6m4YAQ.png"></div><figcaption class="imageCaption">A CEC‚Ää‚Äî‚Ääsame as before, but no forget¬†gate.</figcaption></figure><p name="61b2" id="61b2" class="graf graf--p graf-after--figure">The derivative of this cell state w.r.t. the previous one, again as long as we don‚Äôt backprop through the <strong class="markup--strong markup--p-strong">i </strong>and <strong class="markup--strong markup--p-strong">g</strong> gates, is just 1. That‚Äôs why gradients flow back super comfortably, without vanishing at all. Basically, for a CEC to exist in this context, the coefficient of <strong class="markup--strong markup--p-strong">c_t-1 </strong>needs to be 1.</p><p name="7ab2" id="7ab2" class="graf graf--p graf-after--p">Once we introduced this multiplicative interaction (for good reason), we got a linear carousel; the coefficient of <strong class="markup--strong markup--p-strong">c_t-1 </strong>is <strong class="markup--strong markup--p-strong">f</strong>. So, in our case, when <strong class="markup--strong markup--p-strong">f = 1</strong> (when we‚Äôre not going to forget) our function becomes a CEC, and our gradients will pretty much never vanish. If it‚Äôs close to 0, though, the gradient term will immediately die. Gradients will stay on the carousel for a while until the forget gate is triggered; the effect on the gradient is like a step function, in that it‚Äôs constant with a value of 1 and then drops off to zero/dies when we have <strong class="markup--strong markup--p-strong">f ‚âà 0</strong>.</p><p name="3337" id="3337" class="graf graf--p graf-after--p">Intuitively, this seems problematic. Let‚Äôs do some math to investigate:</p><figure name="0955" id="0955" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 372px; max-height: 164px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 44.1%;"></div><img class="graf-image" data-image-id="1*UbqEhAyW7bMv_tDf-cvuWg.png" data-width="372" data-height="164" src="https://cdn-images-1.medium.com/max/800/1*UbqEhAyW7bMv_tDf-cvuWg.png"></div></figure><p name="2abc" id="2abc" class="graf graf--p graf-after--figure">The derivative of a cell state to the previous is <strong class="markup--strong markup--p-strong">f_t</strong>. The derivative of a cell state to two prior cell states is <strong class="markup--strong markup--p-strong">f_t ‚äô f_t-1</strong>. Thus:</p><figure name="1596" id="1596" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 603px; max-height: 83px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 13.8%;"></div><img class="graf-image" data-image-id="1*p9OndETS7tR-zUU-1TuaTw.png" data-width="603" data-height="83" src="https://cdn-images-1.medium.com/max/800/1*p9OndETS7tR-zUU-1TuaTw.png"></div></figure><p name="8238" id="8238" class="graf graf--p graf-after--figure">As we backpropagate through time, these forget gates keep chaining up and multiplying together to form the overall gradient term.</p><p name="6f12" id="6f12" class="graf graf--p graf-after--p">Now, imagine an LSTM with 100 timesteps. If we wanted to get the derivative of the error w.r.t. a weight like <strong class="markup--strong markup--p-strong">W_xi</strong>, to optimize it, remember that with BPTT we add up or average all the gradients from the different timesteps:</p><figure name="d867" id="d867" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 692px; max-height: 101px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 14.6%;"></div><img class="graf-image" data-image-id="1*rhb_2DO5MulJvzV9QxasMg.png" data-width="692" data-height="101" src="https://cdn-images-1.medium.com/max/800/1*rhb_2DO5MulJvzV9QxasMg.png"></div></figure><p name="a73a" id="a73a" class="graf graf--p graf-after--figure">OK. Now let‚Äôs look at an early (in time) term, like the gradient propagated from the error to the third cell:</p><figure name="9f34" id="9f34" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 106px; max-height: 79px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 74.5%;"></div><img class="graf-image" data-image-id="1*fAaYlJPgsPjRGJGoyc8XCw.png" data-width="106" data-height="79" src="https://cdn-images-1.medium.com/max/800/1*fAaYlJPgsPjRGJGoyc8XCw.png"></div></figure><p name="9972" id="9972" class="graf graf--p graf-after--figure">Remember that <strong class="markup--strong markup--p-strong">J </strong>is an addition of errors from<strong class="markup--strong markup--p-strong"> Y </strong>individual outputs, so we backpropagate through each of the outputs first:</p><figure name="7c21" id="7c21" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 609px; max-height: 164px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 26.900000000000002%;"></div><img class="graf-image" data-image-id="1*vsybuvtlGl-cQqUI1Pqbag.png" data-width="609" data-height="164" src="https://cdn-images-1.medium.com/max/800/1*vsybuvtlGl-cQqUI1Pqbag.png"></div></figure><p name="4355" id="4355" class="graf graf--p graf-after--figure">The first few terms, where we backprop <strong class="markup--strong markup--p-strong">y_k </strong>to <strong class="markup--strong markup--p-strong">c_3 </strong>where <strong class="markup--strong markup--p-strong">k &lt; 3</strong>, would just be equal to zero because <strong class="markup--strong markup--p-strong">c_3 </strong>only exists after these outputs have been computed.</p><p name="e686" id="e686" class="graf graf--p graf-after--p">Let‚Äôs assume that <strong class="markup--strong markup--p-strong">Y = 100 </strong>and continue with our assumption that <strong class="markup--strong markup--p-strong">t = 100 </strong>(so each timestep gives rise to an output), for simplicity. With this, let‚Äôs now look at the last term in this sum.</p><figure name="53d1" id="53d1" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 501px; max-height: 79px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 15.8%;"></div><img class="graf-image" data-image-id="1*JJIQxpb1mHjDn5KaoOKQkA.png" data-width="501" data-height="79" src="https://cdn-images-1.medium.com/max/800/1*JJIQxpb1mHjDn5KaoOKQkA.png"></div></figure><p name="6208" id="6208" class="graf graf--p graf-after--figure">That‚Äôs a lot of forget gates chained together. <strong class="markup--strong markup--p-strong">If one of these forget gates is [approximately] zero, the whole gradient dies</strong>. If these also tend to be a small number between 0 and 1, the whole thing will vanish, and <strong class="markup--strong markup--p-strong">c_3 </strong>won‚Äôt make any contributions to the gradient here.</p><p name="fb13" id="fb13" class="graf graf--p graf-after--p">This isn‚Äôt <em class="markup--em markup--p-em">intrinsically </em>an issue though! Because, when a forget gate is zero, it means that cell is no longer making any contributions past that point. If <strong class="markup--strong markup--p-strong">f_4 </strong>is zero, then any <strong class="markup--strong markup--p-strong">y</strong> outputs at/past timestep 4 won‚Äôt be influenced by <strong class="markup--strong markup--p-strong">c_3 </strong>(as well as <strong class="markup--strong markup--p-strong">c_2 </strong>and <strong class="markup--strong markup--p-strong">c_1</strong>) because we ‚Äúerased‚Äù it from memory. Therefore that particular gradient should be zero. If <strong class="markup--strong markup--p-strong">y_80</strong> is zero, then any outputs at/past timestep 80 won‚Äôt be influenced by <strong class="markup--strong markup--p-strong">c_1, c_2,¬†‚Ä¶¬†, c_79</strong>. Same story here. If these forget gates are between 0 and 1, then the influence of our cell decays over time anyways, and our gradients will be very small, so they‚Äôll reflect that. <a href="https://pdfs.semanticscholar.org/1154/0131eae85b2e11d53df7f1360eeb6476e7f4.pdf" data-href="https://pdfs.semanticscholar.org/1154/0131eae85b2e11d53df7f1360eeb6476e7f4.pdf" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">Gers 1999</a> calls this ‚Äúreleasing resources‚Äù.</p><p name="b6f8" id="b6f8" class="graf graf--p graf-after--p">Cell <strong class="markup--strong markup--p-strong">c_3 </strong>will still contribute to the overall gradient, though. For example, take this term:</p><figure name="0851" id="0851" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 474px; max-height: 79px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 16.7%;"></div><img class="graf-image" data-image-id="1*KZjK3wcZpYG_qnjkMB1HkA.png" data-width="474" data-height="79" src="https://cdn-images-1.medium.com/max/800/1*KZjK3wcZpYG_qnjkMB1HkA.png"></div></figure><p name="8736" id="8736" class="graf graf--p graf-after--figure">Here, we‚Äôre looking at <strong class="markup--strong markup--p-strong">y_12 </strong>instead of <strong class="markup--strong markup--p-strong">y_100</strong>. Chances are that, if you have a sequence of length 100, your 100th cell state isn‚Äôt drawing from your 3rd; the forget gate would have been triggered at some point by then. However, the 12th cell state probably will still be drawing from the ones before it.</p><p name="5cd8" id="5cd8" class="graf graf--p graf-after--p">If we decide not to forget in the first 12 timesteps, ie. <strong class="markup--strong markup--p-strong">f_1¬†‚Ä¶ f_12 </strong>are each not far from 1, then <strong class="markup--strong markup--p-strong">c_3 </strong>would have more influence over <strong class="markup--strong markup--p-strong">y_12 </strong>and the error that stems from <strong class="markup--strong markup--p-strong">y_12</strong>. Thus, the gradient would not vanish and <strong class="markup--strong markup--p-strong">c_3</strong> still contributes to update <strong class="markup--strong markup--p-strong">W_xi</strong>, it just doesn‚Äôt contribute a gradient where it‚Äôs not warranted to (that is, where it doesn‚Äôt actually contribute to any activation, because it‚Äôs been forgotten). To summarize: one activated forget gate will indeed kill off gradient flow to cell(s), but that is a good thing because the network is learning that that gradient from the future has no benefit and is completely irrelevant to those particular cell(s), since those cells have been forgotten by then. In practice, different cells learn different ranges of context, some short, some long. This is a <em class="markup--em markup--p-em">benefit </em>for LSTMs.</p><p name="7b24" id="7b24" class="graf graf--p graf-after--p">So, given a gradient between two cell states in time, when all of these forget gates are [approximately] equal to 1, the gradient signal will remain stable, because we‚Äôre multiplying by 1 at each timestep‚Ää‚Äî‚Ääeffectively, not multiplying by anything at all. In such a case, our gradient flow diagram would look like this:</p><figure name="9b4b" id="9b4b" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 222px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 31.7%;"></div><img class="graf-image" data-image-id="1*rBJm9F6zz8drQnDlWd7wvQ.png" data-width="2190" data-height="694" data-action="zoom" data-action-value="1*rBJm9F6zz8drQnDlWd7wvQ.png" src="https://cdn-images-1.medium.com/max/800/1*rBJm9F6zz8drQnDlWd7wvQ.png"></div><figcaption class="imageCaption">It‚Äôs‚Ä¶ it‚Äôs beautiful!</figcaption></figure><p name="f12e" id="f12e" class="graf graf--p graf-after--figure">The gradient will have literally zero interactions or disturbances, and will just flow through like it‚Äôs driving 150 mph on an empty countryside America highway. The beauty of CECs is that they‚Äôre <em class="markup--em markup--p-em">always </em>like this.</p><p name="87cf" id="87cf" class="graf graf--p graf-after--p">But, let‚Äôs get back to reality. LSTMs aren‚Äôt CECs. One disadvantage of these forget gates chaining together is that it could block learning. That is, when we set out to train our LSTM, the forget gates have not been learned; we have to learn them while we learn everything else. So, if they all start around 0, no gradients will flow through our cell states when we perform BPTT, and learning won‚Äôt happen at all.</p><p name="3db3" id="3db3" class="graf graf--p graf-after--p">The obvious solution is to set the forget gate bias to a very large value when training, so it starts at 1 instead of 0 (because <strong class="markup--strong markup--p-strong">y = 1</strong> is to the far right of the sigmoid function, so adding to the input will ensure ~1 will be the output). In early stages of training, the forget gates equalling/approximating 1 will result in learning not being blocked. So many papers do this and mention it explicitly such that this forget gate bias could even be considered a hyperparameter.</p><p name="d763" id="d763" class="graf graf--p graf-after--p graf--trailing">By introducing forget gates, we stray from CECs and thus the guarantee that our gradients will never ever vanish. But, again, we do it for good reason. And when gradients vanish it‚Äôs because we chose to forget that cell‚Ää‚Äî‚Ääso it‚Äôs not necessarily a bad thing. We just need to make sure the forget gates don‚Äôt block learning in initial stages of training.</p></div></div></section><section name="ccfc" class="section section--body"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><p name="e1c7" id="e1c7" class="graf graf--p graf--leading">We can try computing some more derivatives, just for fun! Let‚Äôs sub in real values for the timesteps, backprop across more than one timestep, and do it for a gate this time.</p><p name="62f7" id="62f7" class="graf graf--p graf-after--p">We‚Äôll expand <strong class="markup--strong markup--p-strong">c_4 </strong>and express it in terms of our gates only. In the process, each <strong class="markup--strong markup--p-strong">c_t, </strong>except <strong class="markup--strong markup--p-strong">c_1</strong>, will collapse into a few interactions between the <strong class="markup--strong markup--p-strong">f</strong>, <strong class="markup--strong markup--p-strong">i</strong>,<strong class="markup--strong markup--p-strong"> </strong>and<strong class="markup--strong markup--p-strong"> g </strong>gate:</p><figure name="26eb" id="26eb" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 669px; max-height: 166px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 24.8%;"></div><img class="graf-image" data-image-id="1*2LZxa4YAMGCJqOrirI1-_w.png" data-width="669" data-height="166" src="https://cdn-images-1.medium.com/max/800/1*2LZxa4YAMGCJqOrirI1-_w.png"></div></figure><p name="5092" id="5092" class="graf graf--p graf-after--figure">Now, let‚Äôs get the derivative of <strong class="markup--strong markup--p-strong">c_4</strong> with respect to one of the earliest possible gates, like <strong class="markup--strong markup--p-strong">g_2</strong>. In the expression above, this turns out to just be the coefficient of <strong class="markup--strong markup--p-strong">g_2</strong>:</p><figure name="c936" id="c936" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 205px; max-height: 79px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 38.5%;"></div><img class="graf-image" data-image-id="1*rHDurdaN9SKnChWfyVk38w.png" data-width="205" data-height="79" src="https://cdn-images-1.medium.com/max/800/1*rHDurdaN9SKnChWfyVk38w.png"></div></figure><p name="708a" id="708a" class="graf graf--p graf-after--figure">We experience the same neatness here as with the cell state backprop! It makes complete sense that the gradient would be <strong class="markup--strong markup--p-strong">i_2 ‚äô</strong> <strong class="markup--strong markup--p-strong">f_3 ‚äô f_4</strong>, since <strong class="markup--strong markup--p-strong">i_2 </strong>controls what influence <strong class="markup--strong markup--p-strong">g_2 </strong>has over <strong class="markup--strong markup--p-strong">c_2</strong>, <strong class="markup--strong markup--p-strong">f_3 </strong>controls what influence <strong class="markup--strong markup--p-strong">c_2 </strong>has on <strong class="markup--strong markup--p-strong">c_3</strong>, and <strong class="markup--strong markup--p-strong">f_4 </strong>controls what influence <strong class="markup--strong markup--p-strong">c_3 </strong>has over <strong class="markup--strong markup--p-strong">c_4</strong>. Notice the chaining up of the forget gates üëª; everything about the carousels I just talked about‚Ää‚Äî‚Ääand what they imply about vanishing gradients‚Ää‚Äî‚Ääapplies here.</p><p name="6229" id="6229" class="graf graf--p graf-after--p">I‚Äôll leave it up to you to derive something similar for the other gates.</p><p name="f15d" id="f15d" class="graf graf--p graf-after--p">And that‚Äôs it! That‚Äôs why LSTMs rock their socks off when it comes to keeping their gradients in check,.</p><p name="627a" id="627a" class="graf graf--p graf-after--p">Here‚Äôs a neat GIF showing a visual representation of the gradients that exist at each timestep, starting from timestep 128 and going all the way to the first, during backprop. More noise represents greater values:</p></div><div class="section-inner sectionLayout--outsetColumn"><figure name="2414" id="2414" class="graf graf--figure graf--iframe graf--layoutOutsetCenter graf-after--p"><div class="aspectRatioPlaceholder is-locked"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 56.2%;"></div><div class="iframeContainer"><IFRAME data-width="854" data-height="480" width="980" height="551" src="/media/c239248e2e0b9a4aadc7b43d8c08ca12?postId=10300100899b" data-media-id="c239248e2e0b9a4aadc7b43d8c08ca12" data-thumbnail="https://i.embed.ly/1/image?url=http%3A%2F%2Fi.imgur.com%2FvaNahKEh.jpg&amp;key=4fce0568f2ce49e8b54624ef71a8a5bd" allowfullscreen frameborder="0"></IFRAME></div></div><figcaption class="imageCaption">Super highway indeed. <a href="http://imgur.com/gallery/vaNahKE" data-href="http://imgur.com/gallery/vaNahKE" class="markup--anchor markup--figure-anchor" rel="noopener" target="_blank">imgur.com/gallery/vaNahKE</a>.</figcaption></figure></div><div class="section-inner sectionLayout--insetColumn"><p name="868a" id="868a" class="graf graf--p graf-after--figure">As you can see, the vanilla RNN‚Äôs gradients die off way quicker than the LSTM‚Äôs. The RNN is almost immediate in comparison. LSTMs seem like a super highway indeed, although it does seem that they do vanish. In this diagram, it can be suggested that the gradients perhaps die for the LSTM eventually because we chose to forget early cell states; again, this depends on the application at hand, and is <em class="markup--em markup--p-em">learnable</em>. (I‚Äôm not sure if this GIF uses truncated backprop, so that could be another thing. In general, I don‚Äôt know the context of this GIF.) Also, part of the gradient signal definitely vanishes‚Äîit‚Äôs the signals that pass through the <strong class="markup--strong markup--p-strong">f/i/g </strong>gates that we looked at earlier and obfuscated from the cell state‚Üícell state derivative. We showed they would vanish because of tanh/sigmoid derivatives; initially, these signals will make a fairly significant contribution, but over time they‚Äôll get smaller and smaller. That‚Äôs the explanation for this GIF.</p><p name="2b66" id="2b66" class="graf graf--p graf-after--p">Exploding gradients is still an issue, though. Recall that when we have a bunch of gradient terms added together, if some of them vanish it doesn‚Äôt mean the whole thing will vanish (versus if they were multiplied together). However, if some of the gradients explode, the whole thing explodes; x + 0 = x, but x + <strong class="markup--strong markup--p-strong">‚àû</strong> = <strong class="markup--strong markup--p-strong">‚àû</strong>. If cell states become unstable and grow too much in some rare scenario, then our gradients could explode. In such a case we‚Äôd need to implement gradient clipping, which is where we choose some arbitrary threshold that gradients cannot be larger than; so, <code class="markup--code markup--p-code">grad = min(grad, clip_threshold)</code>. This would enable the LSTM to deal with such cases without essentially collapsing. Many successful LSTM applications use gradient clipping.</p><p name="69b6" id="69b6" class="graf graf--p graf-after--p graf--trailing">Usually, though, exploding gradients are avoided because sooner or later the forget gate in the carousel is triggered and we reset the memory.</p></div></div></section><section name="70a1" class="section section--body section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><p name="3679" id="3679" class="graf graf--p graf--leading">There are variants of LSTMs. People have tried modifying the model, like computing the hidden state without using tanh activation (so <strong class="markup--strong markup--p-strong">h_t = o ‚äô c_t</strong>) or ditching the <strong class="markup--strong markup--p-strong">i </strong>input gate and only using <strong class="markup--strong markup--p-strong">g</strong>, since that would still satisfy the -1 to 1 range. The results didn‚Äôt change by much.</p><p name="dc3e" id="dc3e" class="graf graf--p graf-after--p">In fact, some researchers even applied evolutionary algorithms to spawn and test a bunch of variants on the standard LSTM equations. Most of the good ones just worked roughly the same.</p><p name="c7f8" id="c7f8" class="graf graf--p graf-after--p">This highlights an issue with LSTMs‚Ää‚Äî‚Ääthey are definitely fairly handwavy. We use them because their architecture allows us to prevent gradients from vanishing such that we can learn long-term dependencies, but there‚Äôs not much theoretical or empirical backing for them. ANNs and RNNs make sense in that they‚Äôre biologically inspired and that they‚Äôre essentially just deep composite functions that have parameters we can optimize. LSTMs stray so far from statistical methods and introduce complex concepts/architectures that work but aren‚Äôt necessarily justified from the get-go. Fully understanding why LSTMs work so well and coming up with better/simpler architectures is a hot topic of research right now.</p><p name="4bca" id="4bca" class="graf graf--p graf-after--p">There are also other variants of RNNs, similar to LSTMs, like <a href="https://en.wikipedia.org/wiki/Gated_recurrent_unit" data-href="https://en.wikipedia.org/wiki/Gated_recurrent_unit" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">GRUs</a> (Gated Recurrent Units). LSTM is still the king, but GRUs in particular have grown in popularity and are seen in many recent, well-respected research papers. It‚Äôs a must learn next to LSTMs, but this article would get too bloated with it. TL;DR: GRUs have a less complex architecture than LSTMs but achieves similar results, and they can control the flow of information without requiring a memory cell. And, they‚Äôre fairly new. (See, told you ‚Äúcoming up with better/simpler architectures is a hot topic of research right now‚Äù is true!)</p><h3 name="c57a" id="c57a" class="graf graf--h3 graf-after--p">Yay RNNs!</h3><p name="8a09" id="8a09" class="graf graf--p graf-after--h3">Wanna see a couple cool things? Practical applications were talked about in the first section, and the next section will walk through more technical applications of RNNs ie. in recent research papers. This section, instead, contains some fun things RNNs have done or produced that‚Äôll ease yourself from the hell that was actually understanding them. As simply as we try to convey things on this blog, LSTMs be LSTMs.</p><p name="7337" id="7337" class="graf graf--p graf-after--p">Sidenote: now, don‚Äôt be frightened by ‚ÄúRNNs‚Äù. <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">Do </em></strong>be frightened by ‚Äúvanilla RNNs‚Äù, in most contexts, but we use RNN as an umbrella term, and it most often refers to an LSTM or a similar variant like GRU.</p><p name="f30f" id="f30f" class="graf graf--p graf-after--p">Many if not all of these are taken from Andrej Karpathy‚Äôs <a href="https://www.youtube.com/watch?v=cO0a0QYmFm8&amp;index=10&amp;list=PLlJy-eBtNFt6EuMxFYRiNRS07MCWN5UIA" data-href="https://www.youtube.com/watch?v=cO0a0QYmFm8&amp;index=10&amp;list=PLlJy-eBtNFt6EuMxFYRiNRS07MCWN5UIA" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">CS231n lecture</a>, or his blog post on the same subject:</p><div name="d19b" id="d19b" class="graf graf--mixtapeEmbed graf-after--p"><a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/" data-href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/" class="markup--anchor markup--mixtapeEmbed-anchor" title="http://karpathy.github.io/2015/05/21/rnn-effectiveness/"><strong class="markup--strong markup--mixtapeEmbed-strong">The Unreasonable Effectiveness of Recurrent Neural Networks</strong><br><em class="markup--em markup--mixtapeEmbed-em">Musings of a Computer Scientist.</em>karpathy.github.io</a><a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="3019eda2afc61d3398ef9b0a1762edf9" data-thumbnail-img-id="0*7sIxt7RqO7deGldw." style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*7sIxt7RqO7deGldw.);"></a></div><p name="83d4" id="83d4" class="graf graf--p graf-after--mixtapeEmbed">You should <strong class="markup--strong markup--p-strong">most certainly</strong> visit either his blog post or lecture for info on exactly how these experiments were conducted, and for more interpretation on their results. Taking a look at the ‚ÄòVisualizing the predictions and the ‚Äúneuron‚Äù firings in the RNN‚Äô section would also be helpful to gain more insight and intuition on how RNNs work and learn over time.</p><p name="c051" id="c051" class="graf graf--p graf-after--p">A recurrent neural network generated this body of text, after it ‚Äúread‚Äù a bunch of Shakespeare:</p><figure name="802f" id="802f" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 574px; max-height: 619px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 107.80000000000001%;"></div><img class="graf-image" data-image-id="1*BkvFHx8nYL1-NHmzZ_BbCQ.png" data-width="574" data-height="619" src="https://cdn-images-1.medium.com/max/800/1*BkvFHx8nYL1-NHmzZ_BbCQ.png"></div></figure><p name="62d6" id="62d6" class="graf graf--p graf-after--figure">Similarly, Karpathy gave an LSTM a lot of Paul Graham‚Äôs startup advice and life wisdom to read, and it produced this:</p><blockquote name="3654" id="3654" class="graf graf--pullquote graf--startsWithDoubleQuote graf-after--p">‚ÄúThe surprised in investors weren‚Äôt going to raise money. I‚Äôm not the company with the time there are all interesting quickly, don‚Äôt have to get off the same programmers. There‚Äôs a super-angel round fundraising, why do you can do. If you have a different physical investment are become in people who reduced in a startup with the way to argument the acquirer could see them just that you‚Äôre also the founders will part of users‚Äô affords that and an alternation to the idea. [2] Don‚Äôt work at first member to see the way kids will seem in advance of a bad successful startup. And if you have to act the big company¬†too.‚Äù</blockquote><p name="ee9f" id="ee9f" class="graf graf--p graf-after--pullquote">A lot of relevant terminology, but it doesn‚Äôt really‚Ä¶ come together üòñ.</p><p name="1fc3" id="1fc3" class="graf graf--p graf-after--p">An LSTM can even generate valid XML, after reading Wikipedia!:</p><pre name="89df" id="89df" class="graf graf--pre graf-after--p"><code class="markup--code markup--pre-code">&lt;page&gt;<br>  &lt;title&gt;Antichrist&lt;/title&gt;<br>  &lt;id&gt;865&lt;/id&gt;<br>  &lt;revision&gt;<br>    &lt;id&gt;15900676&lt;/id&gt;<br>    &lt;timestamp&gt;2002-08-03T18:14:12Z&lt;/timestamp&gt;<br>    &lt;contributor&gt;<br>      &lt;username&gt;Paris&lt;/username&gt;<br>      &lt;id&gt;23&lt;/id&gt;<br>    &lt;/contributor&gt;<br>    &lt;minor /&gt;<br>    &lt;comment&gt;Automated conversion&lt;/comment&gt;<br>    &lt;text xml:space=&quot;preserve&quot;&gt;#REDIRECT [[Christianity]]&lt;/text&gt;<br>  &lt;/revision&gt;<br>&lt;/page&gt;</code></pre><p name="7056" id="7056" class="graf graf--p graf-after--pre">After a similar LSTM read the raw Latex version of an advanced algebra textbook, it came up with this‚Ää‚Äî‚Ääput frankly‚Ää‚Äî‚Ääfancy looking bogus. Let‚Äôs be real, you could definitely believe this was actual math üòú:</p><figure name="221d" id="221d" class="graf graf--figure graf--iframe graf-after--p"><div class="aspectRatioPlaceholder is-locked"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 130%;"></div><div class="iframeContainer"><IFRAME data-width="600" data-height="780" width="600" height="780" src="/media/32e75fb2f7f388775689a155c5c27d86?postId=10300100899b" data-media-id="32e75fb2f7f388775689a155c5c27d86" allowfullscreen frameborder="0"></IFRAME></div></div></figure><p name="e129" id="e129" class="graf graf--p graf-after--figure">An LSTM also read the Linux source code, and tried to write some code of its own:</p><pre name="ce46" id="ce46" class="graf graf--pre graf-after--p"><code class="markup--code markup--pre-code u-paddingRight0 u-marginRight0"><em class="markup--em markup--pre-em">/*<br> * Increment the size file of the new incorrect UI_FILTER group information<br> * of the size generatively.<br> */</em><br><strong class="markup--strong markup--pre-strong">static</strong> <strong class="markup--strong markup--pre-strong">int</strong> <strong class="markup--strong markup--pre-strong">indicate_policy</strong>(<strong class="markup--strong markup--pre-strong">void</strong>)<br>{<br>  <strong class="markup--strong markup--pre-strong">int</strong> error;<br>  <strong class="markup--strong markup--pre-strong">if</strong> (fd <strong class="markup--strong markup--pre-strong">==</strong> MARN_EPT) {<br>    <em class="markup--em markup--pre-em">/*<br>     * The kernel blank will coeld it to userspace.<br>     */</em><br>    <strong class="markup--strong markup--pre-strong">if</strong> (ss<strong class="markup--strong markup--pre-strong">-&gt;</strong>segment <strong class="markup--strong markup--pre-strong">&lt;</strong> mem_total)<br>      unblock_graph_and_set_blocked();<br>    <strong class="markup--strong markup--pre-strong">else</strong><br>      ret <strong class="markup--strong markup--pre-strong">=</strong> 1;<br>    <strong class="markup--strong markup--pre-strong">goto</strong> bail;<br>  }<br>  segaddr <strong class="markup--strong markup--pre-strong">=</strong> in_SB(in.addr);<br>  selector <strong class="markup--strong markup--pre-strong">=</strong> seg <strong class="markup--strong markup--pre-strong">/</strong> 16;<br>  setup_works <strong class="markup--strong markup--pre-strong">=</strong> true;<br>  <strong class="markup--strong markup--pre-strong">for</strong> (i <strong class="markup--strong markup--pre-strong">=</strong> 0; i <strong class="markup--strong markup--pre-strong">&lt;</strong> blocks; i<strong class="markup--strong markup--pre-strong">++</strong>) {<br>    seq <strong class="markup--strong markup--pre-strong">=</strong> buf[i<strong class="markup--strong markup--pre-strong">++</strong>];<br>    bpf <strong class="markup--strong markup--pre-strong">=</strong> bd<strong class="markup--strong markup--pre-strong">-&gt;</strong>bd.next <strong class="markup--strong markup--pre-strong">+</strong> i <strong class="markup--strong markup--pre-strong">*</strong> search;<br>    <strong class="markup--strong markup--pre-strong">if</strong> (fd) {<br>      current <strong class="markup--strong markup--pre-strong">=</strong> blocked;<br>    }<br>  }<br>  rw<strong class="markup--strong markup--pre-strong">-&gt;</strong>name <strong class="markup--strong markup--pre-strong">=</strong> &quot;Getjbbregs&quot;;<br>  bprm_self_clearl(<strong class="markup--strong markup--pre-strong">&amp;</strong>iv<strong class="markup--strong markup--pre-strong">-&gt;</strong>version);<br>  regs<strong class="markup--strong markup--pre-strong">-&gt;</strong>new <strong class="markup--strong markup--pre-strong">=</strong> blocks[(BPF_STATS <strong class="markup--strong markup--pre-strong">&lt;&lt;</strong> info<strong class="markup--strong markup--pre-strong">-&gt;</strong>historidac)] <strong class="markup--strong markup--pre-strong">|</strong> PFMR_CLOBATHINC_SECONDS <strong class="markup--strong markup--pre-strong">&lt;&lt;</strong> 12;<br>  <strong class="markup--strong markup--pre-strong">return</strong> segtable;<br>}</code></pre><p name="da41" id="da41" class="graf graf--p graf-after--pre"><strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">SUPERINTELLIGENCE MUCH‚ÄΩ SELF-RECURSIVE IMPROVEMENT MUCH‚ÄΩ THE END OF THE UNIVERSE MUCH‚ÄΩ</em></strong></p><p name="7094" id="7094" class="graf graf--p graf-after--p">Nope. Just some code doesn‚Äôt compile or make any sense. It even has its own bogus comments!</p><p name="b3fc" id="b3fc" class="graf graf--p graf-after--p">Generating music? Easy! A fun watch:</p><figure name="eae8" id="eae8" class="graf graf--figure graf--iframe graf-after--p"><div class="aspectRatioPlaceholder is-locked"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 56.2%;"></div><div class="iframeContainer"><IFRAME data-width="854" data-height="480" width="700" height="393" src="/media/8de70b7fa3e5cb979099278112052953?postId=10300100899b" data-media-id="8de70b7fa3e5cb979099278112052953" data-thumbnail="https://i.embed.ly/1/image?url=https%3A%2F%2Fi.ytimg.com%2Fvi%2FA2gyidoFsoI%2Fhqdefault.jpg&amp;key=4fce0568f2ce49e8b54624ef71a8a5bd" allowfullscreen frameborder="0"></IFRAME></div></div></figure><p name="c80a" id="c80a" class="graf graf--p graf-after--figure">A more informative watch:</p><figure name="565f" id="565f" class="graf graf--figure graf--iframe graf-after--p"><div class="aspectRatioPlaceholder is-locked"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 56.2%;"></div><div class="iframeContainer"><IFRAME data-width="854" data-height="480" width="700" height="393" src="/media/3f992ef4ac506dafa8d2d8badfc31dc2?postId=10300100899b" data-media-id="3f992ef4ac506dafa8d2d8badfc31dc2" data-thumbnail="https://i.embed.ly/1/image?url=https%3A%2F%2Fi.ytimg.com%2Fvi%2FaSr8_QQYpYM%2Fhqdefault.jpg&amp;key=4fce0568f2ce49e8b54624ef71a8a5bd" allowfullscreen frameborder="0"></IFRAME></div></div></figure><p name="6518" id="6518" class="graf graf--p graf-after--figure">Something even cooler and‚Ä¶ creepier (seriously, the results after the first couple iterations of training are so unsettling):</p><figure name="7c5c" id="7c5c" class="graf graf--figure graf--iframe graf-after--p"><div class="aspectRatioPlaceholder is-locked"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 56.2%;"></div><div class="iframeContainer"><IFRAME data-width="854" data-height="480" width="700" height="393" src="/media/b5f70a5d514e61ad4646217c70974843?postId=10300100899b" data-media-id="b5f70a5d514e61ad4646217c70974843" data-thumbnail="https://i.embed.ly/1/image?url=https%3A%2F%2Fi.ytimg.com%2Fvi%2FNG-LATBZNBs%2Fhqdefault.jpg&amp;key=4fce0568f2ce49e8b54624ef71a8a5bd" allowfullscreen frameborder="0"></IFRAME></div></div></figure><h3 name="7eb1" id="7eb1" class="graf graf--h3 graf-after--figure">In Practice</h3><p name="a21c" id="a21c" class="graf graf--p graf-after--h3">So we‚Äôve seen how RNNs work in theory; now where do they fit in in practice?</p><p name="e928" id="e928" class="graf graf--p graf-after--p">As it turns out, recurrent neural networks can do a whole lot. I‚Äôll try to cover a few of the important, significant, and interesting uses that have cropped up over the last few years.</p><h4 name="e30a" id="e30a" class="graf graf--h4 graf-after--p">Bidirectional Recurrent Neural¬†Networks</h4><p name="5985" id="5985" class="graf graf--p graf-after--h4"><strong class="markup--strong markup--p-strong">The Problem</strong>: Giving the network access to a sequence of vectors is fine and dandy, but what if we want our output at time <strong class="markup--strong markup--p-strong">t</strong> to be conditioned on an input vector that comes at a later timestep? Take the example of speech recognition, where our input vectors are some kind of audio features at time <strong class="markup--strong markup--p-strong">t</strong> and the output is the predicted <a href="https://en.wikipedia.org/wiki/Phoneme" data-href="https://en.wikipedia.org/wiki/Phoneme" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">phoneme</a><em class="markup--em markup--p-em"> </em>at that time. In our traditional RNN architecture, the output at time <strong class="markup--strong markup--p-strong">t</strong> is conditioned <em class="markup--em markup--p-em">only</em> on input vectors <strong class="markup--strong markup--p-strong">1..t</strong>, but as it turns out future information might be useful too. The sounds at time step <strong class="markup--strong markup--p-strong">t+1 </strong>(and maybe <strong class="markup--strong markup--p-strong">t+2</strong>, <strong class="markup--strong markup--p-strong">t+3</strong>,¬†‚Ä¶) are likely part of the same phoneme, and therefore could help us make more accurate predictions. But our network won‚Äôt have access to them until we already output a prediction at time <strong class="markup--strong markup--p-strong">t</strong>. That‚Äôs bad.</p><p name="60ef" id="60ef" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">The Solution</strong>: We essentially ‚Äúdouble up‚Äù each RNN neuron into two independent neurons‚Ää‚Äî‚Ääa ‚Äúforward‚Äù neuron and a ‚Äúbackward‚Äù neuron. The forward neuron is the same as a regular RNN neuron, which gets inputs <strong class="markup--strong markup--p-strong">0..T</strong> sequentially, updating its internal state and outputting some value at each time step along the way. The backward neuron follows the same general principle, but it sees the input vectors in reverse order.</p><p name="e2ef" id="e2ef" class="graf graf--p graf-after--p">We‚Äôll look at an example to make sense of all this.</p><figure name="2749" id="2749" class="graf graf--figure graf--layoutOutsetLeft graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 404px; max-height: 504px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 124.8%;"></div><img class="graf-image" data-image-id="1*Vsvw39SW0xEwRLijLRb3qg.png" data-width="404" data-height="504" src="https://cdn-images-1.medium.com/max/600/1*Vsvw39SW0xEwRLijLRb3qg.png"></div><figcaption class="imageCaption">This is a typical recurrent neural network: at each timestep, the hidden state is updated based on the latest¬†input.</figcaption></figure><figure name="89e5" id="89e5" class="graf graf--figure graf--layoutOutsetLeft graf-after--figure"><div class="aspectRatioPlaceholder is-locked" style="max-width: 525px; max-height: 437px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 83.2%;"></div><img class="graf-image" data-image-id="1*JZNjmHjYFVcHrPKTDmvoXQ.png" data-width="606" data-height="504" data-action="zoom" data-action-value="1*JZNjmHjYFVcHrPKTDmvoXQ.png" src="https://cdn-images-1.medium.com/max/600/1*JZNjmHjYFVcHrPKTDmvoXQ.png"></div><figcaption class="imageCaption">This is a bidirectional recurrent neural network. There are two neurons: one that takes inputs like normal, and one that takes them in reverse. Their output is combined to produce one¬†output.</figcaption></figure><p name="ea03" id="ea03" class="graf graf--p graf-after--figure">Let‚Äôs walk through this timestep-by-timestep. At <strong class="markup--strong markup--p-strong">t=0</strong>, our vanilla RNN cell takes the input, updates its hidden state, and outputs a value. Now let‚Äôs look at the BiRNN: the ‚Äúforward‚Äù half of our BiRNN neuron does exactly the same thing, but the ‚Äúbackward‚Äù half looks through all of our inputs‚Ää‚Äî‚Ääin reverse order, <strong class="markup--strong markup--p-strong">t=T..0</strong>‚Ää‚Äî‚Ääand updates its hidden state with each one. Then when we get to the <strong class="markup--strong markup--p-strong">t=0</strong> input vector, it updates its hidden state one last time and outputs a final value. We then take this final output value and combine it with the ‚Äúforward‚Äù half (‚Äúcombine‚Äù is pretty loosely-defined, usually just by concatenation or addition). Moving on to <strong class="markup--strong markup--p-strong">t=1</strong>, our ‚Äúforward‚Äù part reads in the next input, updates state, and outputs another value. Combined with the second-to-last output of our ‚Äúbackward‚Äù counterpart, and we have the second output of our BiRNN neuron. Rinse and repeat.</p><p name="1bf8" id="1bf8" class="graf graf--p graf-after--p">And that‚Äôs the general idea. Neat, right? BiRNNs (and their more adept cousin, BiLSTMs) are used all over the place. Maybe we‚Äôll see them popping up in some of the other case studies that we‚Äôll be looking at.</p><h4 name="7549" id="7549" class="graf graf--h4 graf-after--p">Autoencoders</h4><p name="cdb3" id="cdb3" class="graf graf--p graf-after--h4">Remember when we talked about <a href="https://ayearofai.com/lenny-2-autoencoders-and-word-embeddings-oh-my-576403b0113a#.oevzdnnnp" data-href="https://ayearofai.com/lenny-2-autoencoders-and-word-embeddings-oh-my-576403b0113a#.oevzdnnnp" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">autoencoders</a>? Turns out we can use RNNs there too!</p><p name="107c" id="107c" class="graf graf--p graf-after--p">Let‚Äôs refresh: what is an autoencoder? Put simply, it‚Äôs a clever way of tricking a neural network to learn a useful representation of some data. Let‚Äôs say we have a dataset of images of faces, and we want to compress the thousands of numbers representing RGB values of pixels down into a 500-dimensional latent vector. We construct a network as such, where the middle layer has 500 neurons:</p><figure name="08e3" id="08e3" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 677px; max-height: 506px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 74.7%;"></div><img class="graf-image" data-image-id="0*M1bVZtZ6UPTyXoiy." data-width="677" data-height="506" src="https://cdn-images-1.medium.com/max/800/0*M1bVZtZ6UPTyXoiy."></div><figcaption class="imageCaption"><a href="https://en.wikipedia.org/wiki/Autoencoder#/media/File:Autoencoder_structure.png" data-href="https://en.wikipedia.org/wiki/Autoencoder#/media/File:Autoencoder_structure.png" class="markup--anchor markup--figure-anchor" rel="nofollow noopener noopener noopener" target="_blank">https://en.wikipedia.org/wiki/Autoencoder#/media/File:Autoencoder_structure.png</a></figcaption></figure><p name="fe44" id="fe44" class="graf graf--p graf-after--figure">‚Ä¶and train it to reproduce the input in the output.</p><p name="46d5" id="46d5" class="graf graf--p graf-after--p">Let‚Äôs explore this idea a little further. Imagine that data is flowing through our network, starting with the input layer and through each subsequent layer. We can view each layer as performing a transformation, converting our input to another vector, and then that vector into another, until we get our output. If we train our network to reproduce the input, that means that each intermediate vector must still represent the same information as the input, in some form or another. Essentially, the activations of each layer are a new representation of our input vector. If our network trains well, we can convert a 10,000-dimensional vector of pixel values into a 500-dimensional vector of image features which can be converted back into a 10,000-dimensional vector of pixel values that approximates what the input would have been.</p><figure name="f1d3" id="f1d3" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 201px; max-height: 34px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 16.900000000000002%;"></div><img class="graf-image" data-image-id="0*RP5VZyqDJ9JI5wBk." data-width="201" data-height="34" src="https://cdn-images-1.medium.com/max/800/0*RP5VZyqDJ9JI5wBk."></div></figure><figure name="1008" id="1008" class="graf graf--figure graf-after--figure"><div class="aspectRatioPlaceholder is-locked" style="max-width: 200px; max-height: 34px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 17%;"></div><img class="graf-image" data-image-id="0*Tc8mc_NWMmQZ15ND." data-width="200" data-height="34" src="https://cdn-images-1.medium.com/max/800/0*Tc8mc_NWMmQZ15ND."></div></figure><p name="c1de" id="c1de" class="graf graf--p graf-after--figure">Let‚Äôs make this a tad more concrete. We have two functions, <strong class="markup--strong markup--p-strong">f</strong> and <strong class="markup--strong markup--p-strong">g</strong>. <strong class="markup--strong markup--p-strong">f</strong> is our encoder, mapping from an <strong class="markup--strong markup--p-strong">n</strong>-long vector to an <strong class="markup--strong markup--p-strong">m</strong>-long vector. (<strong class="markup--strong markup--p-strong">n</strong> is the size of our input, <strong class="markup--strong markup--p-strong">m</strong> is the size of our latent representation.) <strong class="markup--strong markup--p-strong">g</strong> is our decoder, which maps back from an <strong class="markup--strong markup--p-strong">m</strong>-long vector to an <strong class="markup--strong markup--p-strong">n</strong>-long vector. In the normal autoencoder setting, both <strong class="markup--strong markup--p-strong">f</strong> and <strong class="markup--strong markup--p-strong">g</strong> are neural networks trained jointly (or different parts of a single network, same thing really) to reconstruct x.</p><figure name="f029" id="f029" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 182px; max-height: 37px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 20.3%;"></div><img class="graf-image" data-image-id="0*1R_heM-ujpUvlbGM." data-width="182" data-height="37" src="https://cdn-images-1.medium.com/max/800/0*1R_heM-ujpUvlbGM."></div></figure><p name="1680" id="1680" class="graf graf--p graf-after--figure">So, where do RNNs fit in? Let‚Äôs say our inputs are now sequences of vectors instead of a single vector. We can use a similar concept, with both the encoder and decoder represented using an RNN. Here‚Äôs how it works: we feed our input sequence into the <em class="markup--em markup--p-em">encoder RNN</em>. With each input vector of the sequence, this <em class="markup--em markup--p-em">encoder</em> updates its internal state. Eventually, once it has seen the entire input, we have some final network internal state which represents our entire input sequence. Neat! Now, we make the hidden state of our <em class="markup--em markup--p-em">decoder RNN</em> the initial hidden state of our <em class="markup--em markup--p-em">encoder</em>, and ask it to spit out a sequence. Ideally, it spits out something close to what the initial sequence was.</p><p name="b8c2" id="b8c2" class="graf graf--p graf-after--p">Going back to our math-y definitions, we see that it basically fits in to the same framework, except we have <strong class="markup--strong markup--p-strong">q</strong> n-long vectors going into <strong class="markup--strong markup--p-strong">f</strong> and coming out of <strong class="markup--strong markup--p-strong">g</strong>. So <strong class="markup--strong markup--p-strong">q</strong> <strong class="markup--strong markup--p-strong">n</strong>-long vectors go in to <strong class="markup--strong markup--p-strong">f</strong>, and a single <strong class="markup--strong markup--p-strong">m</strong>-long vector comes out. We then give this <strong class="markup--strong markup--p-strong">m</strong>-long vector back to <strong class="markup--strong markup--p-strong">g</strong>, which spits out <strong class="markup--strong markup--p-strong">q</strong> <strong class="markup--strong markup--p-strong">n</strong>-long vectors.</p><p name="5a64" id="5a64" class="graf graf--p graf-after--p">That was a lot of letters, but you get the idea (I hope).</p><p name="b829" id="b829" class="graf graf--p graf-after--p">Like much of deep learning, the concept itself is pretty simple, but the implications are pretty cool. We can take any sequence‚Ää‚Äî‚Ääa variable-length sequence, mind you‚Ää‚Äî‚Ääand convert it into a fixed-size vector. And then convert that back to a variable-length sequence.</p><p name="5546" id="5546" class="graf graf--p graf-after--p">It turns out this model is actually incredibly powerful, so let‚Äôs take a look at one particularly useful (and successful) application: machine translation.</p><h4 name="4be2" id="4be2" class="graf graf--h4 graf-after--p">Neural Machine Translation</h4><p name="9d22" id="9d22" class="graf graf--p graf-after--h4">Let‚Äôs take these ideas we just learned about sequence-to-sequence (or seq2seq, for short) RNNs and apply them to machine translation. We throw in a sequence of words in one language, and it outputs a sequence of words in another. Simple enough, right?</p><p name="cd0e" id="cd0e" class="graf graf--p graf-after--p">The model we‚Äôre going to look at specifically is Google‚Äôs implementation of NMT. You can read all the gory details <a href="https://arxiv.org/pdf/1609.08144.pdf" data-href="https://arxiv.org/pdf/1609.08144.pdf" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">in their paper</a>, but for now why don‚Äôt I give you the watered-down version.</p><p name="3f1b" id="3f1b" class="graf graf--p graf-after--p">At it‚Äôs core, the GNMT architecture is just another seq2seq model. We have an encoder, consisting of 8 LSTM layers with skip connections (the first layer is bidirectional). We also have a decoder, once again containing 8 LSTM layers with skip connections. (A skip connection in a neural network is a connection which skips a layer and connects to the next available layer.) The decoder network outputs a probability distribution of words (well, sort of‚Ää‚Äî‚Ääwe‚Äôll talk more about that later), which we sample from to get our [translated] sentence. üéâ</p><p name="afc6" id="afc6" class="graf graf--p graf-after--p">Here‚Äôs a scary diagram from the paper:</p><figure name="6a00" id="6a00" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 656px; max-height: 363px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 55.300000000000004%;"></div><img class="graf-image" data-image-id="0*mk1BeF8ANMbAVOzD." data-width="656" data-height="363" src="https://cdn-images-1.medium.com/max/800/0*mk1BeF8ANMbAVOzD."></div><figcaption class="imageCaption"><a href="https://arxiv.org/abs/1609.08144" data-href="https://arxiv.org/abs/1609.08144" class="markup--anchor markup--figure-anchor" rel="nofollow noopener noopener noopener" target="_blank">https://arxiv.org/abs/1609.08144</a></figcaption></figure><p name="a450" id="a450" class="graf graf--p graf-after--figure">But there are a few other aspects to the GNMT that are important to note (there‚Äôs actually lots of interesting stuff going on in this architecture, so I really recommend you do <a href="https://arxiv.org/pdf/1609.08144.pdf" data-href="https://arxiv.org/pdf/1609.08144.pdf" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">read the paper</a>).</p><p name="1fbc" id="1fbc" class="graf graf--p graf-after--p">Let‚Äôs turn our <em class="markup--em markup--p-em">attention</em> to the center of the above diagram. This is a critical part of the GNMT architecture (and GNMT is certainly not the first to use attention) which allows the decoder to focus on certain parts of the encoder‚Äôs output as it produces output. Specifically, the GNMT architecture differs from the traditional seq2seq model in that our encoder does <em class="markup--em markup--p-em">not</em><strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em"> </em></strong>produce a single fixed-width vector (the final hidden state) representing the entire output. Instead, we actually look at the output from each time step, and each time step gives us some latent representation. While decoding, we combine all of these hidden vectors into one <em class="markup--em markup--p-em">context</em> vector using something called <em class="markup--em markup--p-em">soft attention</em>.</p><figure name="369f" id="369f" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 243px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 34.699999999999996%;"></div><img class="graf-image" data-image-id="0*ua03RdgdNWPw1_Jd." data-width="704" data-height="244" data-action="zoom" data-action-value="0*ua03RdgdNWPw1_Jd." src="https://cdn-images-1.medium.com/max/800/0*ua03RdgdNWPw1_Jd."></div><figcaption class="imageCaption"><a href="https://arxiv.org/abs/1609.08144" data-href="https://arxiv.org/abs/1609.08144" class="markup--anchor markup--figure-anchor" rel="nofollow noopener noopener noopener" target="_blank">https://arxiv.org/abs/1609.08144</a></figcaption></figure><p name="ddb5" id="ddb5" class="graf graf--p graf-after--figure">More concretely, that works like this (at every decoder time step). We first look at the output of the first decoder layer from the <em class="markup--em markup--p-em">last</em> time step. Following the notation from the paper, we‚Äôll call that <strong class="markup--strong markup--p-strong">yi-1</strong>. We also have a series of encoder outputs, <strong class="markup--strong markup--p-strong">x1‚Ä¶xM</strong>, one for each encoder timestep. For each <em class="markup--em markup--p-em">encoder</em> timestep, we give our special attention function <strong class="markup--strong markup--p-strong">yi-1 </strong>and <strong class="markup--strong markup--p-strong">xt</strong> and get back a <em class="markup--em markup--p-em">single fixed-size vector</em> <strong class="markup--strong markup--p-strong">st</strong>, which we then run through a softmax. So, we‚Äôve converted our encoder information from that timestep (and some decoder information) into a single attention vector‚Ää‚Äî‚Ääthis attention vector tells us which parts of the encoder output we should look at more closely. We multiply this attention vector by our encoder output <strong class="markup--strong markup--p-strong">xt</strong>, which has the effect of ‚Äúfocusing‚Äù more on certain values and less on others. Finally, we take the sum of those ‚Äúfocused‚Äù vectors over each encoder timestep to produce our attention context for this timestep <strong class="markup--strong markup--p-strong">ai</strong>, which is fed to every decoder layer.</p><p name="8681" id="8681" class="graf graf--p graf-after--p">Oh yeah, that attention function? That‚Äôs just yet <em class="markup--em markup--p-em">another</em> neural network.</p><p name="145a" id="145a" class="graf graf--p graf-after--p">Attention mechanisms like this one are pretty common in many deep learning architectures. This is an example of soft attention: we learn a distribution over our inputs and compute a weighted sum. This process is fully-differentiated, so we can use standard backpropogation to figure out how to train our attention model. Another possible mechanism is called <em class="markup--em markup--p-em">hard attention</em>, in which we select just one of the possible inputs and ‚Äúfocus‚Äù solely on that input. This process is not differentiable, so we need to use some other algorithm (usually some kind of reinforcement learning) to train a hard attention algorithm.</p><p name="10b2" id="10b2" class="graf graf--p graf-after--p">GNMT combines all kinds of other cool ideas to achieve state-of-the-art results, including a wordpiece model which segments words into smaller ‚Äúwordpieces‚Äù to help translate rarer words and neat parallelization techniques that let them train this monstrosity of an architecture in reasonable time.</p><p name="818e" id="818e" class="graf graf--p graf-after--p">A few months ago, <a href="https://blog.google/products/translate/found-translation-more-accurate-fluent-sentences-google-translate/" data-href="https://blog.google/products/translate/found-translation-more-accurate-fluent-sentences-google-translate/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Google put their GNMT model into production</a>. Cutting-edge research is being implemented in the real world at an incredible rapid pace within the field of machine learning, and this is just one of countless examples.</p><h4 name="0c13" id="0c13" class="graf graf--h4 graf-after--p">Long-Term Recurrent Convolutional Networks</h4><p name="c3b8" id="c3b8" class="graf graf--p graf-after--h4">(Not to be confused with LCRNs.)</p><p name="e750" id="e750" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">The Problem</strong>: We have a sequence of images that we need to make predictions for. CNNs are good at processing images, RNNs are good at processing sequences‚Ä¶how do we put the two together?</p><p name="606b" id="606b" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">The Solution</strong>: The solution proposed in <a href="https://arxiv.org/pdf/1411.4389.pdf" data-href="https://arxiv.org/pdf/1411.4389.pdf" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">this paper</a> is as straightforward as you would expect: take your image, extract features using a CNN, and feed this feature vector to your LSTM.</p><figure name="fdf8" id="fdf8" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 355px; max-height: 319px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 89.9%;"></div><img class="graf-image" data-image-id="0*qiQ7DvCkHydXAFZ1." data-width="355" data-height="319" src="https://cdn-images-1.medium.com/max/800/0*qiQ7DvCkHydXAFZ1."></div><figcaption class="imageCaption"><a href="https://arxiv.org/abs/1411.4389" data-href="https://arxiv.org/abs/1411.4389" class="markup--anchor markup--figure-anchor" rel="nofollow noopener noopener noopener" target="_blank">https://arxiv.org/abs/1411.4389</a></figcaption></figure><p name="1c9f" id="1c9f" class="graf graf--p graf-after--figure">That‚Äôs really all there is to it, and the reason it works is because (<a href="https://ayearofai.com/rohan-lenny-2-convolutional-neural-networks-5f4cd480a60b?gi=25a4c415cc58" data-href="https://ayearofai.com/rohan-lenny-2-convolutional-neural-networks-5f4cd480a60b?gi=25a4c415cc58" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">as we‚Äôve seen before</a>) CNNs are incredibly adept at converting raw pixel data to a more meaningful representation. This saves the LSTMs the problem of parsing through the pixels to figure out what‚Äôs going on in the image and allows the LSTM weights to focus on converting a vector of image features into some meaningful sequence (say, a caption). It‚Äôs the same reason that using a word embedding is often preferred to a one-hot vector when feeding in words to an NLP model: the more meaningful your representation is, the easier it is to make further predictions with it.</p><h4 name="b189" id="b189" class="graf graf--h4 graf-after--p">Image Captioning</h4><p name="3e15" id="3e15" class="graf graf--p graf-after--h4">(To be confused with LCRNs!)</p><p name="7c86" id="7c86" class="graf graf--p graf-after--p">So there has been a lot of really impressive work on image captioning lately, but I wanna give a special shout-out to <a href="http://cs.stanford.edu/people/karpathy/cvpr2015.pdf" data-href="http://cs.stanford.edu/people/karpathy/cvpr2015.pdf" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">this 2015 paper</a> from Karpathy <em class="markup--em markup--p-em">et al</em>. It was one of the first ML papers I had ever read, and really got me excited about the field. And, it uses RNNs, so that‚Äôs cool too.</p><p name="ca5d" id="ca5d" class="graf graf--p graf-after--p">The idea behind image captioning is kind of self-explanatory, but I‚Äôll explain it anyway. You give the model an image, it gives you a caption. Which is kind of insane if you think about it‚Ää‚Äî‚Ääa computer can go from pixels to interpreting what it‚Äôs seeing, and from that generate real and grammatical sentences to explain what it sees. I still can‚Äôt really believe stuff like this actually works, but somehow it does.</p><p name="3370" id="3370" class="graf graf--p graf-after--p">The model described in this specific paper combines our old pal convolutional neural networks with our newly-discovered RNNs. Step 1 is to pass our image through a convolutional neural network and extract some features from the last fully-connected layer. This lets us convert our pixel representation of the image into something that‚Äôs hopefully a bit more meaningful. We take this image feature vector and use it to initialize the hidden state of our RNN.</p><p name="de23" id="de23" class="graf graf--p graf-after--p">This is where it gets cool. We feed our network a start token, and it gives us a word (more accurately, a distribution of words, which we sample to get the first word of our caption). We feed this word back as the next input, and sample another word from the output. And again, and again, and again, until we finally sample an end token and have a complete caption.</p><p name="0330" id="0330" class="graf graf--p graf-after--p">It‚Äôs not strictly necessary to feed the word that we sampled back to the network, but that‚Äôs pretty common practice to help the network condition its output on the previous word (the hidden state is critical for this too, of course). The results from this particular paper were pretty cool, you can see some of the results <a href="http://cs.stanford.edu/people/karpathy/deepimagesent/" data-href="http://cs.stanford.edu/people/karpathy/deepimagesent/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">here</a>.</p><h4 name="6ec4" id="6ec4" class="graf graf--h4 graf-after--p">Neural Machine Translation, Again</h4><p name="aba2" id="aba2" class="graf graf--p graf-after--h4">Yes, NMTs are just that cool that I need to talk about them again.</p><p name="7a0e" id="7a0e" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">The Problem</strong>: With our good ol‚Äô GNMT architecture, we can train a massive model to convert from language A to language B. That‚Äôs great‚Ää‚Äî‚Ääexcept, if we support more than a hundred languages, we need to train more than <strong class="markup--strong markup--p-strong">10,000</strong> different language-pair models, each of which can take months to converge. That‚Äôs no good, and it‚Äôs the reason that when Google put GNMT in production, they only did so for eight language-pairs (still a monumental achievement). But‚Ä¶what if we didn‚Äôt need to train a separate model for each language pair? What if we could train one model for all the language pairs‚Ää‚Äî‚Ääimpossible, right?</p><p name="1717" id="1717" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">The Solution</strong>: Apparently it‚Äôs not impossible, and to make things even crazier, <strong class="markup--strong markup--p-strong">we can use the original GNMT architecture without modification</strong>. The only real change is that we prepend a special language token to the beginning of each sequence telling it what language to translate to. (We also use one shared wordpiece model for all language, instead of one per language pair.)</p><p name="90b7" id="90b7" class="graf graf--p graf-after--p">So we‚Äôve condensed tens of thousands of NMT models into a single model that is able to translate to and from any language it was trained on. <a href="https://arxiv.org/pdf/1611.04558.pdf" data-href="https://arxiv.org/pdf/1611.04558.pdf" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">The paper</a> elaborates on the implications and benefits of this more than I will, but to summarize:</p><ul class="postList"><li name="2db1" id="2db1" class="graf graf--li graf-after--p">One model instead of tens of thousands. Months of training time saved, simpler production deployment, fewer parameters‚Ää‚Äî‚Ääsimplicity wins out over complexity.</li><li name="66a7" id="66a7" class="graf graf--li graf-after--li">We might have more training data for some language pairs than others. When we have separate models for each language pair, this means that the pairs with less data will have significantly poorer performance. If we put them all into one model, the language pairs with less data can still benefit from all of the data in the other language pairs, because all of the language pairs share weights (since they all use the same model).</li><li name="2a4f" id="2a4f" class="graf graf--li graf-after--li">This one is absolutely nuts. If we train our network to translate English ‚Üí Spanish and Spanish ‚Üí French, <strong class="markup--strong markup--li-strong">our network automatically knows how to translate English ‚Üí French</strong> (reasonably well).</li></ul><p name="0314" id="0314" class="graf graf--p graf-after--li">Expanding on that last point some more: the authors of the paper even found evidence of an <em class="markup--em markup--p-em">interlingua</em>, or an intermediate representation that is shared by multiple languages. Being able to learn an interlingua is the ideal end goal to create a fully generalized multilingual NMT: we learn an encoder/decoder to convert to/from the interlingua for each language, and we immediately know how to translate to and from that language. We aren‚Äôt <em class="markup--em markup--p-em">quite</em> there yet, but this is a major step in that direction. Creating a larger multilingual NMT model and giving it even more data could be all it takes to achieve new state-of-the-art translation results.</p><h4 name="e74d" id="e74d" class="graf graf--h4 graf-after--p">So, yeah</h4><p name="1de9" id="1de9" class="graf graf--p graf-after--h4">RNNs are pretty awesome. There are new RNN papers published literally every day and it‚Äôs impossible to cover everything‚Ää‚Äî‚Ääif you think I missed something important, definitely <a href="https://twitter.com/LennyKhazan" data-href="https://twitter.com/LennyKhazan" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">let me know</a>. (From Rohan: Except Neural Turing Machines and Learning to Learn. Those are dope, we know it, and we‚Äôre going to be covering them soon!)</p><h3 name="449c" id="449c" class="graf graf--h3 graf-after--p">Building a Vanilla Recurrent Neural¬†Network</h3><p name="55e8" id="55e8" class="graf graf--p graf-after--h3">Let‚Äôs get practical for a minute and see how we can build one of these things in practice. We‚Äôll stick with char-rnn (the single layer character level language model we talked about much earlier) with raw numpy so we can see the nitty-gritty details, but if you‚Äôre using one of these in practice <em class="markup--em markup--p-em">there are much better solutions!</em> For out-of-the-box functional deep learning models <a href="https://keras.io/" data-href="https://keras.io/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Keras</a> is the de facto framework that people seem to use. For more creative models and all kinds of other fancy stuff I‚Äôm a fan of the newly-released <a href="http://pytorch.org/" data-href="http://pytorch.org/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">PyTorch</a>, or the ‚Äúolder‚Äù <a href="https://www.tensorflow.org/" data-href="https://www.tensorflow.org/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">TensorFlow</a>.</p><p name="5997" id="5997" class="graf graf--p graf-after--p">I‚Äôm going to walk us through <a href="https://gist.github.com/karpathy/d4dee566867f8291f086" data-href="https://gist.github.com/karpathy/d4dee566867f8291f086" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">this implementation</a> line by line so we can see exactly what‚Äôs going on. It‚Äôs really well-commented, so feel free to peruse it on your own too.</p><p name="8745" id="8745" class="graf graf--p graf-after--p">Afterwards, I challenge you to code an LSTM!</p><figure name="4989" id="4989" class="graf graf--figure graf--iframe graf-after--p"><div class="aspectRatioPlaceholder is-locked"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 35.699999999999996%;"></div><div class="iframeContainer"><IFRAME width="700" height="250" src="/media/515164c3f20643d9534d745371d34b9f?postId=10300100899b" data-media-id="515164c3f20643d9534d745371d34b9f" data-thumbnail="https://i.embed.ly/1/image?url=https%3A%2F%2Favatars0.githubusercontent.com%2Fu%2F241138%3Fv%3D3%26s%3D400&amp;key=4fce0568f2ce49e8b54624ef71a8a5bd" allowfullscreen frameborder="0"></IFRAME></div></div></figure><p name="8a37" id="8a37" class="graf graf--p graf-after--figure"><code class="markup--code markup--p-code">import numpy as np</code></p><p name="54ad" id="54ad" class="graf graf--p graf-after--p">Well, duh.</p><p name="a14a" id="a14a" class="graf graf--p graf-after--p"><code class="markup--code markup--p-code">data = open(‚Äòinput.txt‚Äô, ‚Äòr‚Äô).read()<br>chars = list(set(data))<br>data_size, vocab_size = len(data), len(chars)<br>print ‚Äòdata has %d characters, %d unique.‚Äô % (data_size, vocab_size)<br>char_to_ix = { ch:i for i,ch in enumerate(chars) }<br>ix_to_char = { i:ch for i,ch in enumerate(chars) }</code></p><p name="3f08" id="3f08" class="graf graf--p graf-after--p">We load in our data and get a list of all of the characters that appear in it. We set up two dictionaries: one mapping characters to an index, and one for the reverse. We‚Äôll use this when converting characters to/from a one-hot encoding later on.</p><p name="f0a4" id="f0a4" class="graf graf--p graf-after--p"><code class="markup--code markup--p-code">hidden_size = 100<br>seq_length = 25<br>learning_rate = 1e-1</code></p><p name="b15a" id="b15a" class="graf graf--p graf-after--p">Typical hyperparam stuff. Our RNN layer will have a hidden size of 100, and we‚Äôll train our network on batches of 25 characters at a time. Since we‚Äôll be training our network with BPTT, we need to make sure the sequences are sufficiently short that we can unroll the network all the way and keep everything in memory. Finally, set the learning rate to¬†.1.</p><p name="a836" id="a836" class="graf graf--p graf-after--p"><code class="markup--code markup--p-code">Wxh = np.random.randn(hidden_size, vocab_size)*0.01 # input to hidden</code><br><code class="markup--code markup--p-code">Whh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden<br>Why = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output<br>bh = np.zeros((hidden_size, 1)) # hidden bias<br>by = np.zeros((vocab_size, 1)) # output bias</code></p><p name="7dce" id="7dce" class="graf graf--p graf-after--p">We set up our parameters‚Ää‚Äî‚Äänote that this is just a typical RNN, no fancy LSTM cells. We have weight matrices for updating our hidden state with each input, updating our hidden state with each timestep, and producing an output (and biases for our hidden state + output). We could be doing some fancy weight initialization here, but some normally-distributed randomness is sufficient for breaking symmetry.</p><p name="c19b" id="c19b" class="graf graf--p graf-after--p">Now let‚Äôs talk loss function. We start by computing the forward pass, then computing the backward pass, just like with any neural network.</p><p name="08d8" id="08d8" class="graf graf--p graf-after--p"><code class="markup--code markup--p-code">xs, hs, ys, ps = {}, {}, {}, {}<br>hs[-1] = np.copy(hprev)<br>loss = 0</code></p><p name="7017" id="7017" class="graf graf--p graf-after--p">We start off by just setting up some variables to store our one-hot inputs, hidden states, outputs, and softmax probabilities.</p><p name="03ad" id="03ad" class="graf graf--p graf-after--p"><code class="markup--code markup--p-code">for t in xrange(len(inputs)):</code></p><p name="7bca" id="7bca" class="graf graf--p graf-after--p">Go through each timestep, and for each timestep‚Ä¶</p><p name="d87e" id="d87e" class="graf graf--p graf-after--p"><code class="markup--code markup--p-code">xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation<br>xs[t][inputs[t]] = 1</code></p><p name="0962" id="0962" class="graf graf--p graf-after--p">Convert our input character at this timestep to a one-hot vector.</p><p name="ea9f" id="ea9f" class="graf graf--p graf-after--p"><code class="markup--code markup--p-code">hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state</code></p><p name="de7c" id="de7c" class="graf graf--p graf-after--p">Update our hidden state. We saw this formula already‚Ää‚Äî‚Ääuse our <strong class="markup--strong markup--p-strong">Wxh</strong> and <strong class="markup--strong markup--p-strong">Whh</strong> matrices to update our hidden state based on the last state and our input, and add a bias.</p><p name="15c1" id="15c1" class="graf graf--p graf-after--p"><code class="markup--code markup--p-code">ys[t] = np.dot(Why, hs[t]) + by</code></p><p name="26df" id="26df" class="graf graf--p graf-after--p">Compute our output‚Ä¶</p><p name="4308" id="4308" class="graf graf--p graf-after--p"><code class="markup--code markup--p-code">ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars</code></p><p name="46cd" id="46cd" class="graf graf--p graf-after--p">‚Ä¶and convert it to a probability distribution with a softmax.</p><p name="b3bb" id="b3bb" class="graf graf--p graf-after--p"><code class="markup--code markup--p-code">loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)</code></p><p name="011a" id="011a" class="graf graf--p graf-after--p">Accumulate the loss for this time step as the negative log of the predicted probability. Ideally, we would have a probability of 1 for the <em class="markup--em markup--p-em">actual</em> next character. If it is 1, the loss is 0, log(1) = 0. As the predicted probability approaches 0, the loss approaches inf, because log(0) = -inf.</p><p name="d366" id="d366" class="graf graf--p graf-after--p">That‚Äôs it for the forward pass (not bad, right? Boiled down, it‚Äôs like six lines of code. Piece of cake).</p><p name="b1db" id="b1db" class="graf graf--p graf-after--p"><code class="markup--code markup--p-code">dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)</code><br><code class="markup--code markup--p-code"> dbh, dby = np.zeros_like(bh), np.zeros_like(by)</code><br><code class="markup--code markup--p-code"> dhnext = np.zeros_like(hs[0])</code></p><p name="cdb0" id="cdb0" class="graf graf--p graf-after--p">Setting up some variables for our backward pass‚Ää‚Äî‚Ääthe gradients of our weight matrices, the gradients for our biases, and the gradients from the next timestep (we‚Äôll see how that works in a bit).</p><p name="2fd8" id="2fd8" class="graf graf--p graf-after--p"><code class="markup--code markup--p-code">for t in reversed(xrange(len(inputs))):</code></p><p name="14bf" id="14bf" class="graf graf--p graf-after--p">Go through our sequence in reverse as we back up the gradients.</p><p name="9148" id="9148" class="graf graf--p graf-after--p"><code class="markup--code markup--p-code u-paddingRight0 u-marginRight0">dy = np.copy(ps[t])<br>¬†dy[targets[t]] -= 1 # backprop into y. see <a href="http://cs231n.github.io/neural-networks-case-study/#grad" data-href="http://cs231n.github.io/neural-networks-case-study/#grad" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">http://cs231n.github.io/neural-networks-case-study/#grad</a> if confused here</code></p><p name="3fb3" id="3fb3" class="graf graf--p graf-after--p">First, get the gradient of the output, dy. <a href="http://cs231n.github.io/neural-networks-case-study/#grad" data-href="http://cs231n.github.io/neural-networks-case-study/#grad" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">As it turns out</a>, the gradient of the cross-entropy loss is really as copying over the distribution and subtracting 1 from the correct class.</p><p name="b8b8" id="b8b8" class="graf graf--p graf-after--p"><a href="https://ayearofai.com/rohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d" data-href="https://ayearofai.com/rohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Remember backpropogation</a>? When we have a weighted sum, the gradient of the weights is just the corresponding value that it is being multiplied by, because the other terms drop out and that one weight is treated as a constant. So, computing the gradient of our Why matrix is super simple: just multiply the gradient of loss w.r.t. the output (dy) by the derivative of the output w.r.t. Why (which is just the hidden state at our given timestep), and we get the derivative of the loss w.r.t. Why.</p><figure name="9790" id="9790" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 426px; max-height: 86px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 20.200000000000003%;"></div><img class="graf-image" data-image-id="0*TVvKSJJqaM9CDjlk." data-width="426" data-height="86" src="https://cdn-images-1.medium.com/max/800/0*TVvKSJJqaM9CDjlk."></div></figure><p name="247c" id="247c" class="graf graf--p graf-after--figure"><code class="markup--code markup--p-code">dWhy += np.dot(dy, hs[t].T)</code></p><p name="2ff0" id="2ff0" class="graf graf--p graf-after--p">Like the other gradients (except dy, of course) we accumulate these gradients over all timesteps and apply them at the end.</p><figure name="c397" id="c397" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 321px; max-height: 86px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 26.8%;"></div><img class="graf-image" data-image-id="0*QKDwSXVEJ9fHQ4hT." data-width="321" data-height="86" src="https://cdn-images-1.medium.com/max/800/0*QKDwSXVEJ9fHQ4hT."></div></figure><p name="1c4f" id="1c4f" class="graf graf--p graf-after--figure"><code class="markup--code markup--p-code">dby += dy</code></p><p name="a781" id="a781" class="graf graf--p graf-after--p">The derivative of loss w.r.t. output (dy) multiplied by the derivative of our output w.r.t. the bias (which is 1) gives us the derivative of our output w.r.t. the bias. So far so good.</p><p name="6890" id="6890" class="graf graf--p graf-after--p"><code class="markup--code markup--p-code">dh = np.dot(Why.T, dy) + dhnext # backprop into h</code></p><figure name="9602" id="9602" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 265px; max-height: 74px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 27.900000000000002%;"></div><img class="graf-image" data-image-id="1*cVr1t2s7gsC4Sd6vowSkHA.png" data-width="265" data-height="74" src="https://cdn-images-1.medium.com/max/800/1*cVr1t2s7gsC4Sd6vowSkHA.png"></div></figure><p name="6dbe" id="6dbe" class="graf graf--p graf-after--figure">We compute dL/dh using the chain rule, and accumulate it over all timesteps (hence <code class="markup--code markup--p-code">+ dhnext</code>). We‚Äôll need this for the next step.</p><p name="6d9e" id="6d9e" class="graf graf--p graf-after--p"><code class="markup--code markup--p-code">dhraw = (1‚Ää‚Äî‚Äähs[t] * hs[t]) * dh # backprop through tanh nonlinearity</code></p><p name="796c" id="796c" class="graf graf--p graf-after--p">This computes the derivative of the <code class="markup--code markup--p-code">np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh)</code> line from earlier.</p><p name="7fa1" id="7fa1" class="graf graf--p graf-after--p"><code class="markup--code markup--p-code">dbh += dhraw</code></p><p name="f74d" id="f74d" class="graf graf--p graf-after--p">Which is also our bh derivative, for the same reason that the by derivative was just dy.</p><p name="a0f2" id="a0f2" class="graf graf--p graf-after--p"><code class="markup--code markup--p-code">dWxh += np.dot(dhraw, xs[t].T)<br>dWhh += np.dot(dhraw, hs[t-1].T)</code></p><p name="cea8" id="cea8" class="graf graf--p graf-after--p">We accumulate our weight gradients.</p><figure name="1bb0" id="1bb0" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 286px; max-height: 77px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 26.900000000000002%;"></div><img class="graf-image" data-image-id="1*uf-YEbf0258UhbDc5QZLRw.png" data-width="286" data-height="77" src="https://cdn-images-1.medium.com/max/800/1*uf-YEbf0258UhbDc5QZLRw.png"></div></figure><figure name="d79b" id="d79b" class="graf graf--figure graf-after--figure"><div class="aspectRatioPlaceholder is-locked" style="max-width: 287px; max-height: 77px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 26.8%;"></div><img class="graf-image" data-image-id="1*Vl1LVzPJSZKDplA9J5cElg.png" data-width="287" data-height="77" src="https://cdn-images-1.medium.com/max/800/1*Vl1LVzPJSZKDplA9J5cElg.png"></div></figure><p name="012f" id="012f" class="graf graf--p graf-after--figure"><code class="markup--code markup--p-code">dhnext = np.dot(Whh.T, dhraw)</code></p><p name="bb22" id="bb22" class="graf graf--p graf-after--p">And finally, store dh for this timestep so we can use it for the previous one.</p><p name="1ea0" id="1ea0" class="graf graf--p graf-after--p"><code class="markup--code markup--p-code">for dparam in [dWxh, dWhh, dWhy, dbh, dby]:<br>np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients</code></p><p name="a901" id="a901" class="graf graf--p graf-after--p">Last but not least, a little gradient clipping so we don‚Äôt get no exploding gradients.</p><p name="8392" id="8392" class="graf graf--p graf-after--p"><code class="markup--code markup--p-code">return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]</code></p><p name="4dc5" id="4dc5" class="graf graf--p graf-after--p">And then return all the gradients so we can apply an optimizer step. And that‚Äôs it for the backprop code; not <em class="markup--em markup--p-em">too</em> bad, right?</p><p name="4eaf" id="4eaf" class="graf graf--p graf-after--p"><code class="markup--code markup--p-code">def sample(h, seed_ix, n):</code></p><p name="ed15" id="ed15" class="graf graf--p graf-after--p">This method is used for sampling a generated sequence from the network, starting with state <code class="markup--code markup--p-code">h</code>, first letter <code class="markup--code markup--p-code">seed_ix</code>, with length <code class="markup--code markup--p-code">n</code>.</p><p name="d445" id="d445" class="graf graf--p graf-after--p"><code class="markup--code markup--p-code">x = np.zeros((vocab_size, 1))¬†<br>x[seed_ix] = 1</code></p><p name="a8f6" id="a8f6" class="graf graf--p graf-after--p">Set up our one-hot encoded input vector based on the seed character.</p><p name="8a82" id="8a82" class="graf graf--p graf-after--p"><code class="markup--code markup--p-code">ixes = []</code></p><p name="7e2b" id="7e2b" class="graf graf--p graf-after--p">And an array to keep track of our sequence.</p><p name="3cae" id="3cae" class="graf graf--p graf-after--p"><code class="markup--code markup--p-code">for t in xrange(n):</code></p><p name="c8e6" id="c8e6" class="graf graf--p graf-after--p">To generate each character in our sequence‚Ä¶</p><p name="18cf" id="18cf" class="graf graf--p graf-after--p"><code class="markup--code markup--p-code">h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)</code></p><p name="6d86" id="6d86" class="graf graf--p graf-after--p">Update our hidden state! We saw this formula in the last function, too.</p><p name="ce6f" id="ce6f" class="graf graf--p graf-after--p"><code class="markup--code markup--p-code">y = np.dot(Why, h) + by<br>p = np.exp(y) / np.sum(np.exp(y))</code></p><p name="d108" id="d108" class="graf graf--p graf-after--p">Generate our output and run it through a softmax. Again, straight from the last function.</p><p name="62ae" id="62ae" class="graf graf--p graf-after--p"><code class="markup--code markup--p-code">ix = np.random.choice(range(vocab_size), p=p.ravel())</code></p><p name="fd39" id="fd39" class="graf graf--p graf-after--p">Sample from our output distribution using some numpy magic.</p><p name="cec1" id="cec1" class="graf graf--p graf-after--p"><code class="markup--code markup--p-code">x = np.zeros((vocab_size, 1))<br>x[ix] = 1<br>ixes.append(ix)</code></p><p name="e8b3" id="e8b3" class="graf graf--p graf-after--p">Convert the sampled value into a one-hot encoding and append it to the array.</p><p name="4928" id="4928" class="graf graf--p graf-after--p"><code class="markup--code markup--p-code">return ixes</code></p><p name="e074" id="e074" class="graf graf--p graf-after--p">‚Ä¶and of course, return the final sequence when we‚Äôre done.</p><p name="6968" id="6968" class="graf graf--p graf-after--p"><code class="markup--code markup--p-code">n, p = 0, 0</code></p><p name="2eb1" id="2eb1" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">n</strong> is the number of training iterations we‚Äôve done. <strong class="markup--strong markup--p-strong">p</strong> is the index into our training data for where we are now.</p><p name="ee12" id="ee12" class="graf graf--p graf-after--p"><code class="markup--code markup--p-code">mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)</code></p><p name="9951" id="9951" class="graf graf--p graf-after--p">Set up memory variables for the Adagrad algorithm (out of scope of this post, maybe next time‚Ää‚Äî‚Ääit‚Äôs just a variant on gradient descent).</p><p name="ffc4" id="ffc4" class="graf graf--p graf-after--p"><code class="markup--code markup--p-code">while True:</code></p><p name="bcde" id="bcde" class="graf graf--p graf-after--p">Training loop.</p><p name="a04e" id="a04e" class="graf graf--p graf-after--p"><code class="markup--code markup--p-code">if p+seq_length+1 &gt;= len(data) or n == 0:</code></p><p name="3444" id="3444" class="graf graf--p graf-after--p">This is a little check to see if we need to reset our memory because we‚Äôre starting back at the beginning of our data.</p><p name="a0be" id="a0be" class="graf graf--p graf-after--p"><code class="markup--code markup--p-code">hprev = np.zeros((hidden_size,1)) # reset RNN memory</code></p><p name="038a" id="038a" class="graf graf--p graf-after--p">‚Ä¶and if we are, reset the memory.</p><p name="9c3d" id="9c3d" class="graf graf--p graf-after--p"><code class="markup--code markup--p-code">p = 0</code></p><p name="bd23" id="bd23" class="graf graf--p graf-after--p">And reset the data pointer.</p><p name="0161" id="0161" class="graf graf--p graf-after--p"><code class="markup--code markup--p-code">inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]<br>targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]</code></p><p name="70c2" id="70c2" class="graf graf--p graf-after--p">We grab a <code class="markup--code markup--p-code">seq_length</code>-long piece of the data as our input to the network. At each timestep, we want to predict the next character; this means that our ‚Äútargets‚Äù will be the next character for each input. We get a snippet of the input the same length as the input, but offset by 1, for the target.</p><p name="3f42" id="3f42" class="graf graf--p graf-after--p"><code class="markup--code markup--p-code">if n % 100 == 0:<br>sample_ix = sample(hprev, inputs[0], 200)<br>txt = ‚Äò‚Äô.join(ix_to_char[ix] for ix in sample_ix)<br>print ‚Äò‚Ää‚Äî‚Ää‚Äî \n %s \n‚Ää‚Äî‚Ää‚Äî ‚Äò % (txt, )</code></p><p name="a7eb" id="a7eb" class="graf graf--p graf-after--p">Here we just print to the terminal a sample every 100 training steps so we can see how its doing. Ideally, this will print out gibberish the first few times, before gradually printing out more and more reasonable language.</p><p name="e412" id="e412" class="graf graf--p graf-after--p"><code class="markup--code markup--p-code">loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)</code></p><p name="538d" id="538d" class="graf graf--p graf-after--p">Do a forward pass, backward pass, and get the gradients.</p><p name="9d99" id="9d99" class="graf graf--p graf-after--p"><code class="markup--code markup--p-code">smooth_loss = smooth_loss * 0.999 + loss * 0.001</code></p><p name="4ab1" id="4ab1" class="graf graf--p graf-after--p">Adagrad stuff.</p><p name="3823" id="3823" class="graf graf--p graf-after--p"><code class="markup--code markup--p-code">if n % 100 == 0: print ‚Äòiter %d, loss: %f‚Äô % (n, smooth_loss) # print progress</code></p><p name="b38c" id="b38c" class="graf graf--p graf-after--p">Keep up with progress.</p><p name="995f" id="995f" class="graf graf--p graf-after--p"><code class="markup--code markup--p-code">for param, dparam, mem in zip([Wxh, Whh, Why, bh, by], [dWxh, dWhh, dWhy, dbh, dby], [mWxh, mWhh, mWhy, mbh, mby]):<br>mem += dparam * dparam<br>param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update</code></p><p name="9509" id="9509" class="graf graf--p graf-after--p">More Adagrad. We should really do an article on optimization algorithms.</p><p name="a2fc" id="a2fc" class="graf graf--p graf-after--p"><code class="markup--code markup--p-code">p += seq_length # move data pointer<br>n += 1 # iteration counter</code></p><p name="a889" id="a889" class="graf graf--p graf-after--p">Annnddd finally, we update our data pointer and iteration counter.</p><p name="7850" id="7850" class="graf graf--p graf-after--p">And that‚Äôs it. We have an RNN. Neat-o. Reminder: your challenge is to code an LSTM‚Ä¶ and TensorFlow doesn‚Äôt count!</p><h3 name="5a53" id="5a53" class="graf graf--h3 graf-after--p">Conclusion</h3><p name="8a00" id="8a00" class="graf graf--p graf-after--h3">Wow. That was a <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">lot</em></strong>. If you came in knowing nothing or very little about recurrent neural nets, you sure as hell know a lot now. And you don‚Äôt just know about something cool; you know about something <em class="markup--em markup--p-em">very important</em>‚Ää‚Äî‚Ääsomething that can equip you to read and understand some of the most prominent and hottest recent research papers in machine learning.</p><p name="eec5" id="eec5" class="graf graf--p graf-after--p">Something this article didn‚Äôt do so good at was making sure the calculus and derivatives were in the context of operating on vectors (because, remember, RNNs/LSTMs operate over vectors). In many cases the derivatives were in the 1-D context. It‚Äôs not something you need to worry about, but you might want to look into.</p><p name="f6a4" id="f6a4" class="graf graf--p graf-after--p">We‚Äôre finally at the point where we can focus our energies on this blog towards cooler stuff including hot research papers like Neural Turing Machines or Learning to Learn, case studies eg. AlphaGo, other parts of machine learning and artificial intelligence (I‚Äôm, Rohan, personally looking forward to optimization!), or different algorithms like GANs. There‚Äôs very little compulsory content or ‚Äúgroundwork‚Äù we need to cover anymore. So, now, we‚Äôre officially onto the cool stuff.</p><p name="4f33" id="4f33" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">That‚Äôs right.</strong> A Year Of AI is officially‚Ä¶ <em class="markup--em markup--p-em">cool</em>.</p><figure name="f41b" id="f41b" class="graf graf--figure graf-after--p graf--trailing"><div class="aspectRatioPlaceholder is-locked" style="max-width: 80px; max-height: 80px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 100%;"></div><img class="graf-image" data-image-id="1*necEBipfgD-Z3_9R9usFgg.png" data-width="80" data-height="80" src="https://cdn-images-1.medium.com/max/800/1*necEBipfgD-Z3_9R9usFgg.png"></div></figure></div></div></section></div><footer class="u-paddingTop10"><div class="container u-maxWidth740"><div class="row"><div class="col u-size12of12"></div></div><div class="row"><div class="col u-size12of12 js-postTags"><div class="u-paddingBottom10"><ul class="tags tags--postTags tags--borderless"><li><a class="link u-baseColor--link"  href="https://ayearofai.com/tagged/machine-learning?source=post" data-action-source="post" data-collection-slug="a-year-of-artificial-intelligence">Machine Learning</a></li><li><a class="link u-baseColor--link"  href="https://ayearofai.com/tagged/artificial-intelligence?source=post" data-action-source="post" data-collection-slug="a-year-of-artificial-intelligence">Artificial Intelligence</a></li><li><a class="link u-baseColor--link"  href="https://ayearofai.com/tagged/data-science?source=post" data-action-source="post" data-collection-slug="a-year-of-artificial-intelligence">Data Science</a></li><li><a class="link u-baseColor--link"  href="https://ayearofai.com/tagged/deep-learning?source=post" data-action-source="post" data-collection-slug="a-year-of-artificial-intelligence">Deep Learning</a></li><li><a class="link u-baseColor--link"  href="https://ayearofai.com/tagged/algorithms?source=post" data-action-source="post" data-collection-slug="a-year-of-artificial-intelligence">Algorithms</a></li></ul></div></div></div><div class="row js-postActionsFooter"><div class="postActions col u-size12of12"><div class="u-floatLeft buttonSet buttonSet--withLabels"><div class="buttonSet-inner"><div class="js-actionRecommend" data-post-id="10300100899b" data-is-icon-29px="true" data-has-recommend-list="true" data-source="post_actions_footer"><button class="button button--primary button--large button--chromeless is-touchIconFadeInPulse u-accentColor--buttonNormal button--withIcon button--withSvgIcon u-accentColor--iconLight js-actionRecommendButton"  title="Recommend to share this article with your followers and let the author know you liked it" aria-label="Recommend to share this article with your followers and let the author know you liked it" data-action="sign-in-prompt" data-sign-in-action="upvote" data-requires-token="true" data-redirect="https://medium.com/_/vote/p/10300100899b" data-action-source="post_actions_footer"><span class="button-defaultState"><span class="svgIcon svgIcon--heart svgIcon--29px"><svg class="svgIcon-use" width="29" height="29" viewBox="0 0 29 29" ><path d="M16.215 23.716c-.348.288-.984.826-1.376 1.158a.526.526 0 0 1-.68 0c-.36-.307-.92-.78-1.22-1.03C9.22 20.734 3 15.527 3 10.734 3 7.02 5.916 4 9.5 4c1.948 0 3.77.898 5 2.434C15.73 4.898 17.552 4 19.5 4c3.584 0 6.5 3.02 6.5 6.734 0 4.9-6.125 9.96-9.785 12.982zM19.5 5.2c-1.774 0-3.423.923-4.41 2.468a.699.699 0 0 1-.59.323.706.706 0 0 1-.59-.32c-.988-1.54-2.637-2.47-4.41-2.47-2.922 0-5.3 2.49-5.3 5.54 0 4.23 6.19 9.41 9.517 12.19.217.18.566.48.783.66l.952-.79c3.496-2.88 9.348-7.72 9.348-12.05 0-3.05-2.378-5.53-5.3-5.53z"/></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--heartFilled svgIcon--29px"><svg class="svgIcon-use" width="29" height="29" viewBox="0 0 29 29" ><path d="M19.5 4c-1.948 0-3.77.898-5 2.434C13.27 4.898 11.448 4 9.5 4 5.916 4 3 7.02 3 10.734c0 4.793 6.227 10 9.95 13.11.296.25.853.723 1.212 1.03.196.166.48.166.677 0 .39-.332 1.02-.87 1.37-1.158 3.66-3.022 9.79-8.08 9.79-12.982C26 7.02 23.08 4 19.5 4z" fill-rule="evenodd"/></svg></span></span></button><button class="button button--chromeless u-baseColor--buttonNormal"  data-action="show-recommends" data-action-value="10300100899b">88</button></div></div><div class="buttonSet-inner"><button class="button button--large button--dark button--chromeless is-touchIconBlackPulse u-baseColor--buttonDark button--withIcon button--withSvgIcon"  data-action="scroll-to-responses" data-action-source="post_actions_footer"><span class="svgIcon svgIcon--response svgIcon--29px"><svg class="svgIcon-use" width="29" height="29" viewBox="0 0 29 29" ><path d="M21.27 20.058c1.89-1.826 2.754-4.17 2.754-6.674C24.024 8.21 19.67 4 14.1 4 8.53 4 4 8.21 4 13.384c0 5.175 4.53 9.385 10.1 9.385 1.007 0 2-.14 2.95-.41.285.25.592.49.918.7 1.306.87 2.716 1.31 4.19 1.31.276-.01.494-.14.6-.36a.625.625 0 0 0-.052-.65c-.61-.84-1.042-1.71-1.282-2.58a5.417 5.417 0 0 1-.154-.75zm-3.85 1.324l-.083-.28-.388.12a9.72 9.72 0 0 1-2.85.424c-4.96 0-8.99-3.706-8.99-8.262 0-4.556 4.03-8.263 8.99-8.263 4.95 0 8.77 3.71 8.77 8.27 0 2.25-.75 4.35-2.5 5.92l-.24.21v.32c0 .07 0 .19.02.37.03.29.1.6.19.92.19.7.49 1.4.89 2.08-.93-.14-1.83-.49-2.67-1.06-.34-.22-.88-.48-1.16-.74z"/></svg></span></button><button class="button button--chromeless u-baseColor--buttonNormal"  data-action="scroll-to-responses">4</button></div></div><div class="u-floatRight buttonSet buttonSet--narrow"><button class="button button--large button--dark button--chromeless is-touchIconBlackPulse u-baseColor--buttonDark button--withIcon button--withSvgIcon"  title="Share on Twitter" aria-label="Share on Twitter" data-action="share-on-twitter" data-action-source="post_actions_footer"><span class="svgIcon svgIcon--twitter svgIcon--29px"><svg class="svgIcon-use" width="29" height="29" viewBox="0 0 29 29" ><path d="M21.967 11.8c.018 5.93-4.607 11.18-11.177 11.18-2.172 0-4.25-.62-6.047-1.76l-.268.422-.038.5.186.013.168.012c.3.02.44.032.6.046 2.06-.026 3.95-.686 5.49-1.86l1.12-.85-1.4-.048c-1.57-.055-2.92-1.08-3.36-2.51l-.48.146-.05.5c.22.03.48.05.75.08.48-.02.87-.07 1.25-.15l2.33-.49-2.32-.49c-1.68-.35-2.91-1.83-2.91-3.55 0-.05 0-.01-.01.03l-.49-.1-.25.44c.63.36 1.35.57 2.07.58l1.7.04L7.4 13c-.978-.662-1.59-1.79-1.618-3.047a4.08 4.08 0 0 1 .524-1.8l-.825.07a12.188 12.188 0 0 0 8.81 4.515l.59.033-.06-.59v-.02c-.05-.43-.06-.63-.06-.87a3.617 3.617 0 0 1 6.27-2.45l.2.21.28-.06c1.01-.22 1.94-.59 2.73-1.09l-.75-.56c-.1.36-.04.89.12 1.36.23.68.58 1.13 1.17.85l-.21-.45-.42-.27c-.52.8-1.17 1.48-1.92 2L22 11l.016.28c.013.2.014.35 0 .52v.04zm.998.038c.018-.22.017-.417 0-.66l-.498.034.284.41a8.183 8.183 0 0 0 2.2-2.267l.97-1.48-1.6.755c.17-.08.3-.02.34.03a.914.914 0 0 1-.13-.292c-.1-.297-.13-.64-.1-.766l.36-1.254-1.1.695c-.69.438-1.51.764-2.41.963l.48.15a4.574 4.574 0 0 0-3.38-1.484 4.616 4.616 0 0 0-4.61 4.613c0 .29.02.51.08.984l.01.02.5-.06.03-.5c-3.17-.18-6.1-1.7-8.08-4.15l-.48-.56-.36.64c-.39.69-.62 1.48-.65 2.28.04 1.61.81 3.04 2.06 3.88l.3-.92c-.55-.02-1.11-.17-1.6-.45l-.59-.34-.14.67c-.02.08-.02.16 0 .24-.01 2.12 1.55 4.01 3.69 4.46l.1-.49-.1-.49c-.33.07-.67.12-1.03.14-.18-.02-.43-.05-.64-.07l-.76-.09.23.73c.57 1.84 2.29 3.14 4.28 3.21l-.28-.89a8.252 8.252 0 0 1-4.85 1.66c-.12-.01-.26-.02-.56-.05l-.17-.01-.18-.01L2.53 21l1.694 1.07a12.233 12.233 0 0 0 6.58 1.917c7.156 0 12.2-5.73 12.18-12.18l-.002.04z"/></svg></span></button><button class="button button--large button--dark button--chromeless is-touchIconBlackPulse u-baseColor--buttonDark button--withIcon button--withSvgIcon"  title="Share on Facebook" aria-label="Share on Facebook" data-action="share-on-facebook" data-action-source="post_actions_footer"><span class="svgIcon svgIcon--facebook svgIcon--29px"><svg class="svgIcon-use" width="29" height="29" viewBox="0 0 29 29" ><path d="M16.39 23.61v-5.808h1.846a.55.55 0 0 0 .546-.48l.36-2.797a.551.551 0 0 0-.547-.62H16.39V12.67c0-.67.12-.813.828-.813h1.474a.55.55 0 0 0 .55-.55V8.803a.55.55 0 0 0-.477-.545c-.436-.06-1.36-.116-2.22-.116-2.5 0-4.13 1.62-4.13 4.248v1.513H10.56a.551.551 0 0 0-.55.55v2.797c0 .304.248.55.55.55h1.855v5.76c-4.172-.96-7.215-4.7-7.215-9.1 0-5.17 4.17-9.36 9.31-9.36 5.14 0 9.31 4.19 9.31 9.36 0 4.48-3.155 8.27-7.43 9.15M14.51 4C8.76 4 4.1 8.684 4.1 14.46c0 5.162 3.75 9.523 8.778 10.32a.55.55 0 0 0 .637-.543v-6.985a.551.551 0 0 0-.55-.55H11.11v-1.697h1.855a.55.55 0 0 0 .55-.55v-2.063c0-2.02 1.136-3.148 3.03-3.148.567 0 1.156.027 1.597.06v1.453h-.924c-1.363 0-1.93.675-1.93 1.912v1.78c0 .3.247.55.55.55h2.132l-.218 1.69H15.84c-.305 0-.55.24-.55.55v7.02c0 .33.293.59.623.54 5.135-.7 9.007-5.11 9.007-10.36C24.92 8.68 20.26 4 14.51 4"/></svg></span></button></div></div></div></div><div class="js-postPromotionWrapper postPromotionWrapper" data-location-id="footer_above_post_attribution"></div><div class="container u-maxWidth740 js-postAttributionFooterContainer u-paddingTop20 u-paddingBottom20 u-marginTop10 u-borderTopLightest u-xs-paddingTop10 u-xs-paddingBottom10"><div class="row js-postFooterInfo"><div class="col u-size6of12 u-xs-size12of12"><li class="u-block u-paddingBottom18 js-cardUser"><div class="u-marginLeft20 u-floatRight"><span class="followState js-followState buttonSet-inner" data-user-id="cb55958ea3bb"><button class="button button--small u-noUserSelect button--withChrome u-baseColor--buttonNormal button--withHover button--unblock js-unblockButton"  data-action="sign-in-prompt" data-sign-in-action="toggle-block-user" data-requires-token="true" data-action-source="footer_card"><span class="button-label  button-defaultState">Blocked</span><span class="button-label button-hoverState">Unblock</span></button><button class="button button--primary button--small u-noUserSelect button--withChrome u-accentColor--buttonNormal button--follow js-followButton"  data-action="sign-in-prompt" data-sign-in-action="toggle-subscribe-user" data-requires-token="true" data-redirect="https://medium.com/_/subscribe/user/cb55958ea3bb" data-action-source="footer_card_follow"><span class="button-label  button-defaultState js-buttonLabel">Follow</span><span class="button-label button-activeState">Following</span></button></span></div><div class="u-tableCell "><a class="link avatar u-baseColor--link"  href="https://ayearofai.com/@mckapur?source=footer_card" title="Go to the profile of Rohan Kapur" aria-label="Go to the profile of Rohan Kapur" data-action-source="footer_card" data-user-id="cb55958ea3bb" data-collection-slug="a-year-of-artificial-intelligence" dir="auto"><img  src="https://cdn-images-1.medium.com/fit/c/60/60/1*pGDfwp8yLzgCeG1RkPUxgQ.jpeg" class="avatar-image avatar-image--small" alt="Go to the profile of Rohan Kapur"></a></div><div class="u-tableCell u-verticalAlignMiddle u-breakWord u-paddingLeft15"><h3 class="u-fontSize18 u-lineHeightTighter u-marginBottom4"><a class="link link--primary u-accentColor--hoverTextNormal"  href="https://ayearofai.com/@mckapur" property="cc:attributionName" title="Go to the profile of Rohan Kapur" aria-label="Go to the profile of Rohan Kapur" rel="author cc:attributionUrl" data-user-id="cb55958ea3bb" data-collection-slug="a-year-of-artificial-intelligence" dir="auto">Rohan Kapur</a></h3><p class="u-fontSize14 u-lineHeightBaseSans u-textColorDark u-marginBottom4">rohankapur.com</p></div></li></div><div class="col u-size6of12 u-xs-size12of12 u-xs-marginTop30"><li class="u-block u-paddingBottom18 js-cardCollection"><div class="u-marginLeft20 u-floatRight"><button class="button button--primary button--small u-noUserSelect button--withChrome u-accentColor--buttonNormal js-relationshipButton"  data-action="sign-in-prompt" data-sign-in-action="toggle-follow-collection" data-requires-token="true" data-redirect="https://medium.com/_/subscribe/collection/a-year-of-artificial-intelligence" data-collection-id="bb87da25612c"><span class="button-label  js-buttonLabel">Follow</span></button></div><div class="u-tableCell "><a class="link avatar avatar--roundedRectangle u-baseColor--link"  href="https://ayearofai.com?source=footer_card" title="Go to A Year of Artificial Intelligence" aria-label="Go to A Year of Artificial Intelligence" data-action-source="footer_card" data-collection-slug="a-year-of-artificial-intelligence"><img src="https://cdn-images-1.medium.com/fit/c/60/60/1*NZsNSuNxe_O2YW1ybboOvA.jpeg" class="avatar-image u-size60x60" alt="A Year of Artificial Intelligence"></a></div><div class="u-tableCell u-verticalAlignMiddle u-breakWord u-paddingLeft15"><h3 class="u-fontSize18 u-lineHeightTighter u-marginBottom4"><a class="link link--primary u-accentColor--hoverTextNormal"  href="https://ayearofai.com?source=footer_card" rel="collection" data-action-source="footer_card" data-collection-slug="a-year-of-artificial-intelligence">A Year of Artificial Intelligence</a></h3><p class="u-fontSize14 u-lineHeightBaseSans u-textColorDark u-marginBottom4">Our ongoing effort to make the mathematics, science, linguistics, and philosophy of artificial intelligence fun and simple.</p><div class="buttonSet"></div></div></li></div></div></div><div class="js-postFooterPlacements"></div><div class="u-padding0 u-clearfix u-backgroundGrayLightest u-print-hide supplementalPostContent js-responsesWrapper"></div><div class="supplementalPostContent js-readNext"></div><div class="supplementalPostContent js-heroPromo"></div></footer></article></main><div class="u-marginAuto u-maxWidth1000"><div class="js-postShareWidget u-foreground u-sm-hide u-transition--fadeOut300 u-fixed"><ul><li class="u-uiTextSemibold u-textAlignCenter u-textColorNormal u-fontSize12 u-textUppercase">Share</li><li class="u-textAlignCenter"><div class="js-actionRecommend" data-post-id="10300100899b" data-is-icon-29px="true" data-is-vertical="true" data-has-recommend-list="true" data-source="post_share_widget"><button class="button button--primary button--large button--chromeless is-touchIconFadeInPulse u-accentColor--buttonNormal button--withIcon button--withSvgIcon u-accentColor--iconLight js-actionRecommendButton"  title="Recommend to share this article with your followers and let the author know you liked it" aria-label="Recommend to share this article with your followers and let the author know you liked it" data-action="sign-in-prompt" data-sign-in-action="upvote" data-requires-token="true" data-redirect="https://medium.com/_/vote/p/10300100899b" data-action-source="post_share_widget"><span class="button-defaultState"><span class="svgIcon svgIcon--heart svgIcon--29px"><svg class="svgIcon-use" width="29" height="29" viewBox="0 0 29 29" ><path d="M16.215 23.716c-.348.288-.984.826-1.376 1.158a.526.526 0 0 1-.68 0c-.36-.307-.92-.78-1.22-1.03C9.22 20.734 3 15.527 3 10.734 3 7.02 5.916 4 9.5 4c1.948 0 3.77.898 5 2.434C15.73 4.898 17.552 4 19.5 4c3.584 0 6.5 3.02 6.5 6.734 0 4.9-6.125 9.96-9.785 12.982zM19.5 5.2c-1.774 0-3.423.923-4.41 2.468a.699.699 0 0 1-.59.323.706.706 0 0 1-.59-.32c-.988-1.54-2.637-2.47-4.41-2.47-2.922 0-5.3 2.49-5.3 5.54 0 4.23 6.19 9.41 9.517 12.19.217.18.566.48.783.66l.952-.79c3.496-2.88 9.348-7.72 9.348-12.05 0-3.05-2.378-5.53-5.3-5.53z"/></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--heartFilled svgIcon--29px"><svg class="svgIcon-use" width="29" height="29" viewBox="0 0 29 29" ><path d="M19.5 4c-1.948 0-3.77.898-5 2.434C13.27 4.898 11.448 4 9.5 4 5.916 4 3 7.02 3 10.734c0 4.793 6.227 10 9.95 13.11.296.25.853.723 1.212 1.03.196.166.48.166.677 0 .39-.332 1.02-.87 1.37-1.158 3.66-3.022 9.79-8.08 9.79-12.982C26 7.02 23.08 4 19.5 4z" fill-rule="evenodd"/></svg></span></span></button><button class="button button--chromeless u-baseColor--buttonNormal  u-block u-marginAuto u-marginTopNegative5"  data-action="show-recommends" data-action-value="10300100899b">88</button></div></li><li class="u-textAlignCenter"><button class="button button--large button--dark button--chromeless is-touchIconBlackPulse u-baseColor--buttonDark button--withIcon button--withSvgIcon"  title="Share on Twitter" aria-label="Share on Twitter" data-action="share-on-twitter" data-action-source="post_share_widget"><span class="svgIcon svgIcon--twitter svgIcon--29px"><svg class="svgIcon-use" width="29" height="29" viewBox="0 0 29 29" ><path d="M21.967 11.8c.018 5.93-4.607 11.18-11.177 11.18-2.172 0-4.25-.62-6.047-1.76l-.268.422-.038.5.186.013.168.012c.3.02.44.032.6.046 2.06-.026 3.95-.686 5.49-1.86l1.12-.85-1.4-.048c-1.57-.055-2.92-1.08-3.36-2.51l-.48.146-.05.5c.22.03.48.05.75.08.48-.02.87-.07 1.25-.15l2.33-.49-2.32-.49c-1.68-.35-2.91-1.83-2.91-3.55 0-.05 0-.01-.01.03l-.49-.1-.25.44c.63.36 1.35.57 2.07.58l1.7.04L7.4 13c-.978-.662-1.59-1.79-1.618-3.047a4.08 4.08 0 0 1 .524-1.8l-.825.07a12.188 12.188 0 0 0 8.81 4.515l.59.033-.06-.59v-.02c-.05-.43-.06-.63-.06-.87a3.617 3.617 0 0 1 6.27-2.45l.2.21.28-.06c1.01-.22 1.94-.59 2.73-1.09l-.75-.56c-.1.36-.04.89.12 1.36.23.68.58 1.13 1.17.85l-.21-.45-.42-.27c-.52.8-1.17 1.48-1.92 2L22 11l.016.28c.013.2.014.35 0 .52v.04zm.998.038c.018-.22.017-.417 0-.66l-.498.034.284.41a8.183 8.183 0 0 0 2.2-2.267l.97-1.48-1.6.755c.17-.08.3-.02.34.03a.914.914 0 0 1-.13-.292c-.1-.297-.13-.64-.1-.766l.36-1.254-1.1.695c-.69.438-1.51.764-2.41.963l.48.15a4.574 4.574 0 0 0-3.38-1.484 4.616 4.616 0 0 0-4.61 4.613c0 .29.02.51.08.984l.01.02.5-.06.03-.5c-3.17-.18-6.1-1.7-8.08-4.15l-.48-.56-.36.64c-.39.69-.62 1.48-.65 2.28.04 1.61.81 3.04 2.06 3.88l.3-.92c-.55-.02-1.11-.17-1.6-.45l-.59-.34-.14.67c-.02.08-.02.16 0 .24-.01 2.12 1.55 4.01 3.69 4.46l.1-.49-.1-.49c-.33.07-.67.12-1.03.14-.18-.02-.43-.05-.64-.07l-.76-.09.23.73c.57 1.84 2.29 3.14 4.28 3.21l-.28-.89a8.252 8.252 0 0 1-4.85 1.66c-.12-.01-.26-.02-.56-.05l-.17-.01-.18-.01L2.53 21l1.694 1.07a12.233 12.233 0 0 0 6.58 1.917c7.156 0 12.2-5.73 12.18-12.18l-.002.04z"/></svg></span></button></li><li class="u-textAlignCenter"><button class="button button--large button--dark button--chromeless is-touchIconBlackPulse u-baseColor--buttonDark button--withIcon button--withSvgIcon"  title="Share on Facebook" aria-label="Share on Facebook" data-action="share-on-facebook" data-action-source="post_share_widget"><span class="svgIcon svgIcon--facebook svgIcon--29px"><svg class="svgIcon-use" width="29" height="29" viewBox="0 0 29 29" ><path d="M16.39 23.61v-5.808h1.846a.55.55 0 0 0 .546-.48l.36-2.797a.551.551 0 0 0-.547-.62H16.39V12.67c0-.67.12-.813.828-.813h1.474a.55.55 0 0 0 .55-.55V8.803a.55.55 0 0 0-.477-.545c-.436-.06-1.36-.116-2.22-.116-2.5 0-4.13 1.62-4.13 4.248v1.513H10.56a.551.551 0 0 0-.55.55v2.797c0 .304.248.55.55.55h1.855v5.76c-4.172-.96-7.215-4.7-7.215-9.1 0-5.17 4.17-9.36 9.31-9.36 5.14 0 9.31 4.19 9.31 9.36 0 4.48-3.155 8.27-7.43 9.15M14.51 4C8.76 4 4.1 8.684 4.1 14.46c0 5.162 3.75 9.523 8.778 10.32a.55.55 0 0 0 .637-.543v-6.985a.551.551 0 0 0-.55-.55H11.11v-1.697h1.855a.55.55 0 0 0 .55-.55v-2.063c0-2.02 1.136-3.148 3.03-3.148.567 0 1.156.027 1.597.06v1.453h-.924c-1.363 0-1.93.675-1.93 1.912v1.78c0 .3.247.55.55.55h2.132l-.218 1.69H15.84c-.305 0-.55.24-.55.55v7.02c0 .33.293.59.623.54 5.135-.7 9.007-5.11 9.007-10.36C24.92 8.68 20.26 4 14.51 4"/></svg></span></button></li><li class="u-textAlignCenter"><button class="button button--large button--dark button--chromeless is-touchIconFadeInPulse u-baseColor--buttonDark button--withIcon button--withSvgIcon button--bookmark js-bookmarkButton"  title="Bookmark this story to read later" aria-label="Bookmark this story to read later" data-action="sign-in-prompt" data-sign-in-action="add-to-bookmarks" data-requires-token="true" data-redirect="https://medium.com/_/bookmark/p/10300100899b"><span class="button-defaultState"><span class="svgIcon svgIcon--bookmark svgIcon--29px"><svg class="svgIcon-use" width="29" height="29" viewBox="0 0 29 29" ><path d="M19.385 4h-9.77A2.623 2.623 0 0 0 7 6.615V23.01a1.022 1.022 0 0 0 1.595.847l5.905-4.004 5.905 4.004A1.022 1.022 0 0 0 22 23.011V6.62A2.625 2.625 0 0 0 19.385 4zM21 23l-5.91-3.955-.148-.107a.751.751 0 0 0-.884 0l-.147.107L8 23V6.615C8 5.725 8.725 5 9.615 5h9.77C20.275 5 21 5.725 21 6.615V23z" fill-rule="evenodd"/></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--bookmarkFilled svgIcon--29px"><svg class="svgIcon-use" width="29" height="29" viewBox="0 0 29 29" ><path d="M19.385 4h-9.77A2.623 2.623 0 0 0 7 6.615V23.01a1.022 1.022 0 0 0 1.595.847l5.905-4.004 5.905 4.004A1.022 1.022 0 0 0 22 23.011V6.62A2.625 2.625 0 0 0 19.385 4z" fill-rule="evenodd"/></svg></span></span></button></li></ul></div></div><div class="u-fixed u-bottom0 u-sizeFullWidth u-backgroundWhite u-boxShadowTop u-borderBox u-paddingTop10 u-paddingBottom10 u-zIndexMetabar u-xs-paddingLeft10 u-xs-paddingRight10 js-stickyFooter"><div class="u-maxWidth700 u-marginAuto u-flexCenter"><div class="u-fontSize16 u-flex1 u-flexCenter"><div class="u-flex0 u-inlineBlock u-paddingRight20 u-xs-paddingRight10"><a class="link avatar avatar--roundedRectangle u-baseColor--link"  href="https://ayearofai.com" title="Go to A Year of Artificial Intelligence" aria-label="Go to A Year of Artificial Intelligence" data-collection-slug="a-year-of-artificial-intelligence"><img src="https://cdn-images-1.medium.com/fit/c/40/40/1*NZsNSuNxe_O2YW1ybboOvA.jpeg" class="avatar-image avatar-image--smaller" alt="A Year of Artificial Intelligence"></a></div><div class="u-flex1 u-inlineBlock"><div class="u-xs-hide">Never miss a story from<strong> A Year of Artificial Intelligence</strong>, when you sign up for Medium. <a class="link link--accent u-accentColor--textNormal u-accentColor--textDarken u-baseColor--link"  href="https://medium.com/@Medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg" data-action-source="sticky_footer">Learn more</a></div><div class="u-xs-show">Never miss a story from<strong> A Year of Artificial Intelligence</strong></div></div></div><div class="u-marginLeft50 u-xs-marginAuto"><button class="button button--primary button--dark is-active u-noUserSelect button--withChrome u-accentColor--buttonDark u-uiTextSemibold u-textUppercase u-fontSize12 button--followCollection js-followCollectionButton"  data-action="sign-in-prompt" data-sign-in-action="toggle-subscribe-collection" data-requires-token="true" data-redirect="https://medium.com/_/subscribe/collection/a-year-of-artificial-intelligence" data-action-source="sticky_footer_collection_follow"><span class="button-label  button-defaultState js-buttonLabel">Get updates</span><span class="button-label button-activeState">Get updates</span></button></div></div></div><style class="js-collectionStyle">
.u-accentColor--borderLight {border-color: #868484 !important;}
.u-accentColor--borderNormal {border-color: #868484 !important;}
.u-accentColor--borderDark {border-color: #737171 !important;}
.u-accentColor--iconLight .svgIcon,.u-accentColor--iconLight.svgIcon {fill: #868484 !important;}
.u-accentColor--iconNormal .svgIcon,.u-accentColor--iconNormal.svgIcon {fill: #868484 !important;}
.u-accentColor--iconDark .svgIcon,.u-accentColor--iconDark.svgIcon {fill: #737171 !important;}
.u-accentColor--textNormal {color: #737171 !important;}
.u-accentColor--hoverTextNormal:hover {color: #737171 !important;}
.u-accentColor--textNormal.u-accentColor--textDarken:hover {color: #696867 !important;}
.u-accentColor--textDark {color: #696867 !important;}
.u-accentColor--backgroundLight {background-color: #868484 !important;}
.u-accentColor--backgroundNormal {background-color: #868484 !important;}
.u-accentColor--backgroundDark {background-color: #737171 !important;}
.u-accentColor--buttonDark {border-color: #737171 !important; color: #696867 !important;}
.u-accentColor--buttonDark:hover {border-color: #696867 !important;}
.u-accentColor--buttonDark .icon:before,.u-accentColor--buttonDark .svgIcon{color: #737171 !important; fill: #737171 !important;}
.u-accentColor--buttonNormal {border-color: #868484 !important; color: #737171 !important;}
.u-accentColor--buttonNormal:hover {border-color: #737171 !important;}
.u-accentColor--buttonNormal .icon:before,.u-accentColor--buttonNormal .svgIcon{color: #868484 !important; fill: #868484 !important;}
.u-accentColor--buttonNormal.button--filled .icon:before,.u-accentColor--buttonNormal.button--filled .svgIcon{color: rgba(255, 255, 255, 1) !important; fill: rgba(255, 255, 255, 1) !important;}
.u-accentColor--buttonDark.button--filled,.u-accentColor--buttonDark.button--withChrome.is-active,.u-accentColor--fillWhenActive.is-active {background-color: #737171 !important; border-color: #737171 !important; color: rgba(255, 255, 255, 1) !important; fill: rgba(255, 255, 255, 1) !important;}
.u-accentColor--buttonNormal.button--filled,.u-accentColor--buttonNormal.button--withChrome.is-active {background-color: #868484 !important; border-color: #868484 !important; color: rgba(255, 255, 255, 1) !important; fill: rgba(255, 255, 255, 1) !important;}
.postArticle.is-withAccentColors .markup--user,.postArticle.is-withAccentColors .markup--query {color: #737171 !important;}.u-tintBgColor {background-color: rgba(0, 0, 0, 1) !important;}.u-tintBgColor .u-fadeLeft:before {background-image: linear-gradient(to right, rgba(0, 0, 0, 1) 0%, rgba(0, 0, 0, 0) 100%) !important;}.u-tintBgColor .u-fadeRight:after {background-image: linear-gradient(to right, rgba(0, 0, 0, 0) 0%, rgba(0, 0, 0, 1) 100%) !important;}
.u-tintSpectrum .u-baseColor--borderLight {border-color: #868484 !important;}
.u-tintSpectrum .u-baseColor--borderNormal {border-color: #B1AEAE !important;}
.u-tintSpectrum .u-baseColor--borderDark {border-color: #D9D6D6 !important;}
.u-tintSpectrum .u-baseColor--iconLight .svgIcon,.u-tintSpectrum .u-baseColor--iconLight.svgIcon {fill: #868484 !important;}
.u-tintSpectrum .u-baseColor--iconNormal .svgIcon,.u-tintSpectrum .u-baseColor--iconNormal.svgIcon {fill: #B1AEAE !important;}
.u-tintSpectrum .u-baseColor--iconDark .svgIcon,.u-tintSpectrum .u-baseColor--iconDark.svgIcon {fill: #D9D6D6 !important;}
.u-tintSpectrum .u-baseColor--textNormal {color: #B1AEAE !important;}
.u-tintSpectrum .u-baseColor--textNormal.u-baseColor--textDarken:hover {color: #ECE9E9 !important;}
.u-tintSpectrum .u-baseColor--textDark {color: #ECE9E9 !important;}
.u-tintSpectrum .u-baseColor--backgroundLight {background-color: #868484 !important;}
.u-tintSpectrum .u-baseColor--backgroundNormal {background-color: #B1AEAE !important;}
.u-tintSpectrum .u-baseColor--backgroundDark {background-color: #D9D6D6 !important;}
.u-tintSpectrum .u-baseColor--buttonLight {border-color: #868484 !important; color: #868484 !important;}
.u-tintSpectrum .u-baseColor--buttonLight:hover {border-color: #868484 !important;}
.u-tintSpectrum .u-baseColor--buttonLight .icon:before,.u-tintSpectrum .u-baseColor--buttonLight .svgIcon {color: #868484 !important; fill: #868484 !important;}
.u-tintSpectrum .u-baseColor--buttonDark {border-color: #D9D6D6 !important; color: #ECE9E9 !important;}
.u-tintSpectrum .u-baseColor--buttonDark:hover {border-color: #ECE9E9 !important;}
.u-tintSpectrum .u-baseColor--buttonDark .icon:before,.u-tintSpectrum .u-baseColor--buttonDark .svgIcon {color: #D9D6D6 !important; fill: #D9D6D6 !important;}
.u-tintSpectrum .u-baseColor--buttonNormal {border-color: #B1AEAE !important; color: #B1AEAE !important;}
.u-tintSpectrum .u-baseColor--buttonNormal:hover {border-color: #D9D6D6 !important;}
.u-tintSpectrum .u-baseColor--buttonNormal .icon:before,.u-tintSpectrum .u-baseColor--buttonNormal .svgIcon {color: #B1AEAE !important; fill: #B1AEAE !important;}
.u-tintSpectrum .u-baseColor--buttonDark.button--filled,.u-tintSpectrum .u-baseColor--buttonDark.button--withChrome.is-active {background-color: #D9D6D6 !important; border-color: #D9D6D6 !important; color: rgba(0, 0, 0, 1) !important; fill: rgba(0, 0, 0, 1) !important;}
.u-tintSpectrum .u-baseColor--buttonNormal.button--filled,.u-tintSpectrum .u-baseColor--buttonNormal.button--withChrome.is-active {background-color: #B1AEAE !important; border-color: #B1AEAE !important; color: rgba(0, 0, 0, 1) !important; fill: rgba(0, 0, 0, 1) !important;}
.u-tintSpectrum .u-baseColor--link {color: #B1AEAE !important;}
.u-tintSpectrum .u-baseColor--link.link--darkenOnHover:hover {color: #ECE9E9 !important;}
.u-tintSpectrum .u-baseColor--link.link--darken:hover,.u-tintSpectrum .u-baseColor--link.link--darken:focus,.u-tintSpectrum .u-baseColor--link.link--darken:active {color: #ECE9E9 !important;}
.u-tintSpectrum .u-baseColor--link.link--dark {color: #ECE9E9 !important;}
.u-tintSpectrum .u-baseColor--link.link--dark.link--darken:hover,.u-tintSpectrum .u-baseColor--link.link--dark.link--darken:focus,.u-tintSpectrum .u-baseColor--link.link--dark.link--darken:active {color: #ECE9E9 !important;}
.u-tintSpectrum .u-baseColor--link.link--darker {color: #ECE9E9 !important;}
.u-tintSpectrum .u-baseColor--placeholderNormal ::-webkit-input-placeholder {color: #868484;}
.u-tintSpectrum .u-baseColor--placeholderNormal ::-moz-placeholder {color: #868484;}
.u-tintSpectrum .u-baseColor--placeholderNormal :-ms-input-placeholder {color: #868484;}
.u-tintSpectrum .svgIcon--logoNew path:nth-child(1) {stroke: none !important; fill: #3C3B3B !important;}
.u-tintSpectrum .svgIcon--logoNew path:nth-child(2) {stroke: none !important; fill: #565555 !important;}
.u-tintSpectrum .svgIcon--logoNew path:nth-child(3) {stroke: none !important; fill: #868484 !important;}
.u-tintSpectrum .svgIcon--logoNew path:nth-child(4) {stroke: none !important; fill: #B1AEAE !important;}
.u-tintSpectrum .u-accentColor--borderLight {border-color: #868484 !important;}
.u-tintSpectrum .u-accentColor--borderNormal {border-color: #B1AEAE !important;}
.u-tintSpectrum .u-accentColor--borderDark {border-color: #D9D6D6 !important;}
.u-tintSpectrum .u-accentColor--iconLight .svgIcon,.u-tintSpectrum .u-accentColor--iconLight.svgIcon {fill: #868484 !important;}
.u-tintSpectrum .u-accentColor--iconNormal .svgIcon,.u-tintSpectrum .u-accentColor--iconNormal.svgIcon {fill: #B1AEAE !important;}
.u-tintSpectrum .u-accentColor--iconDark .svgIcon,.u-tintSpectrum .u-accentColor--iconDark.svgIcon {fill: #D9D6D6 !important;}
.u-tintSpectrum .u-accentColor--textNormal {color: #B1AEAE !important;}
.u-tintSpectrum .u-accentColor--hoverTextNormal:hover {color: #B1AEAE !important;}
.u-tintSpectrum .u-accentColor--textNormal.u-accentColor--textDarken:hover {color: #ECE9E9 !important;}
.u-tintSpectrum .u-accentColor--textDark {color: #ECE9E9 !important;}
.u-tintSpectrum .u-accentColor--backgroundLight {background-color: #868484 !important;}
.u-tintSpectrum .u-accentColor--backgroundNormal {background-color: #B1AEAE !important;}
.u-tintSpectrum .u-accentColor--backgroundDark {background-color: #D9D6D6 !important;}
.u-tintSpectrum .u-accentColor--buttonDark {border-color: #D9D6D6 !important; color: #ECE9E9 !important;}
.u-tintSpectrum .u-accentColor--buttonDark:hover {border-color: #ECE9E9 !important;}
.u-tintSpectrum .u-accentColor--buttonDark .icon:before,.u-tintSpectrum .u-accentColor--buttonDark .svgIcon{color: #D9D6D6 !important; fill: #D9D6D6 !important;}
.u-tintSpectrum .u-accentColor--buttonNormal {border-color: #B1AEAE !important; color: #B1AEAE !important;}
.u-tintSpectrum .u-accentColor--buttonNormal:hover {border-color: #D9D6D6 !important;}
.u-tintSpectrum .u-accentColor--buttonNormal .icon:before,.u-tintSpectrum .u-accentColor--buttonNormal .svgIcon{color: #B1AEAE !important; fill: #B1AEAE !important;}
.u-tintSpectrum .u-accentColor--buttonNormal.button--filled .icon:before,.u-tintSpectrum .u-accentColor--buttonNormal.button--filled .svgIcon{color: rgba(0, 0, 0, 1) !important; fill: rgba(0, 0, 0, 1) !important;}
.u-tintSpectrum .u-accentColor--buttonDark.button--filled,.u-tintSpectrum .u-accentColor--buttonDark.button--withChrome.is-active,.u-tintSpectrum .u-accentColor--fillWhenActive.is-active {background-color: #D9D6D6 !important; border-color: #D9D6D6 !important; color: rgba(0, 0, 0, 1) !important; fill: rgba(0, 0, 0, 1) !important;}
.u-tintSpectrum .u-accentColor--buttonNormal.button--filled,.u-tintSpectrum .u-accentColor--buttonNormal.button--withChrome.is-active {background-color: #B1AEAE !important; border-color: #B1AEAE !important; color: rgba(0, 0, 0, 1) !important; fill: rgba(0, 0, 0, 1) !important;}
.u-tintSpectrum .postArticle.is-withAccentColors .markup--user,.u-tintSpectrum .postArticle.is-withAccentColors .markup--query {color: #B1AEAE !important;}
.u-accentColor--highlightFaint {background-color: rgba(243, 240, 239, 1) !important;}
.u-accentColor--highlightStrong.is-active .svgIcon {fill: rgba(226, 223, 222, 1) !important;}
.postArticle.is-withAccentColors .markup--quote.is-other {background-color: rgba(243, 240, 239, 1) !important;}
body.is-withMagicUnderlines .postArticle.is-withAccentColors .markup--quote.is-other {background-color: transparent !important; background-image: linear-gradient(to bottom, rgba(243, 240, 239, 1), rgba(243, 240, 239, 1));}
.postArticle.is-withAccentColors .markup--quote.is-me {background-color: rgba(233, 230, 230, 1) !important;}
body.is-withMagicUnderlines .postArticle.is-withAccentColors .markup--quote.is-me {background-color: transparent !important; background-image: linear-gradient(to bottom, rgba(233, 230, 230, 1), rgba(233, 230, 230, 1));}
.postArticle.is-withAccentColors .markup--quote.is-targeted {background-color: rgba(226, 223, 222, 1) !important;}
body.is-withMagicUnderlines .postArticle.is-withAccentColors .markup--quote.is-targeted {background-color: transparent !important; background-image: linear-gradient(to bottom, rgba(226, 223, 222, 1), rgba(226, 223, 222, 1));}
.postArticle.is-withAccentColors .markup--quote.is-selected {background-color: rgba(226, 223, 222, 1) !important;}
body.is-withMagicUnderlines .postArticle.is-withAccentColors .markup--quote.is-selected {background-color: transparent !important; background-image: linear-gradient(to bottom, rgba(226, 223, 222, 1), rgba(226, 223, 222, 1));}
.postArticle.is-withAccentColors .markup--highlight {background-color: rgba(226, 223, 222, 1) !important;}
body.is-withMagicUnderlines .postArticle.is-withAccentColors .markup--highlight {background-color: transparent !important; background-image: linear-gradient(to bottom, rgba(226, 223, 222, 1), rgba(226, 223, 222, 1));}</style><style class="js-collectionStyleConstant">.u-imageBgColor {background-color: rgba(0, 0, 0, 0.24705882352941178);}
.u-imageSpectrum .u-baseColor--borderLight {border-color: rgba(255, 255, 255, 0.6980392156862745) !important;}
.u-imageSpectrum .u-baseColor--borderNormal {border-color: rgba(255, 255, 255, 0.8980392156862745) !important;}
.u-imageSpectrum .u-baseColor--borderDark {border-color: rgba(255, 255, 255, 0.9490196078431372) !important;}
.u-imageSpectrum .u-baseColor--iconLight .svgIcon,.u-imageSpectrum .u-baseColor--iconLight.svgIcon {fill: rgba(255, 255, 255, 0.8) !important;}
.u-imageSpectrum .u-baseColor--iconNormal .svgIcon,.u-imageSpectrum .u-baseColor--iconNormal.svgIcon {fill: rgba(255, 255, 255, 0.9490196078431372) !important;}
.u-imageSpectrum .u-baseColor--iconDark .svgIcon,.u-imageSpectrum .u-baseColor--iconDark.svgIcon {fill: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-baseColor--textNormal {color: rgba(255, 255, 255, 0.9490196078431372) !important;}
.u-imageSpectrum .u-baseColor--textNormal.u-baseColor--textDarken:hover {color: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-baseColor--textDark {color: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-baseColor--backgroundLight {background-color: rgba(255, 255, 255, 0.8980392156862745) !important;}
.u-imageSpectrum .u-baseColor--backgroundNormal {background-color: rgba(255, 255, 255, 0.9490196078431372) !important;}
.u-imageSpectrum .u-baseColor--backgroundDark {background-color: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-baseColor--buttonLight {border-color: rgba(255, 255, 255, 0.6980392156862745) !important; color: rgba(255, 255, 255, 0.8) !important;}
.u-imageSpectrum .u-baseColor--buttonLight:hover {border-color: rgba(255, 255, 255, 0.6980392156862745) !important;}
.u-imageSpectrum .u-baseColor--buttonLight .icon:before,.u-imageSpectrum .u-baseColor--buttonLight .svgIcon {color: rgba(255, 255, 255, 0.8) !important; fill: rgba(255, 255, 255, 0.8) !important;}
.u-imageSpectrum .u-baseColor--buttonDark {border-color: rgba(255, 255, 255, 0.9490196078431372) !important; color: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-baseColor--buttonDark:hover {border-color: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-baseColor--buttonDark .icon:before,.u-imageSpectrum .u-baseColor--buttonDark .svgIcon {color: rgba(255, 255, 255, 1) !important; fill: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-baseColor--buttonNormal {border-color: rgba(255, 255, 255, 0.8980392156862745) !important; color: rgba(255, 255, 255, 0.9490196078431372) !important;}
.u-imageSpectrum .u-baseColor--buttonNormal:hover {border-color: rgba(255, 255, 255, 0.9490196078431372) !important;}
.u-imageSpectrum .u-baseColor--buttonNormal .icon:before,.u-imageSpectrum .u-baseColor--buttonNormal .svgIcon {color: rgba(255, 255, 255, 0.9490196078431372) !important; fill: rgba(255, 255, 255, 0.9490196078431372) !important;}
.u-imageSpectrum .u-baseColor--buttonDark.button--filled,.u-imageSpectrum .u-baseColor--buttonDark.button--withChrome.is-active {background-color: rgba(255, 255, 255, 1) !important; border-color: rgba(255, 255, 255, 1) !important; color: rgba(0, 0, 0, 0.24705882352941178) !important; fill: rgba(0, 0, 0, 0.24705882352941178) !important;}
.u-imageSpectrum .u-baseColor--buttonNormal.button--filled,.u-imageSpectrum .u-baseColor--buttonNormal.button--withChrome.is-active {background-color: rgba(255, 255, 255, 0.9490196078431372) !important; border-color: rgba(255, 255, 255, 0.9490196078431372) !important; color: rgba(0, 0, 0, 0.24705882352941178) !important; fill: rgba(0, 0, 0, 0.24705882352941178) !important;}
.u-imageSpectrum .u-baseColor--link {color: rgba(255, 255, 255, 0.9490196078431372) !important;}
.u-imageSpectrum .u-baseColor--link.link--darkenOnHover:hover {color: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-baseColor--link.link--darken:hover,.u-imageSpectrum .u-baseColor--link.link--darken:focus,.u-imageSpectrum .u-baseColor--link.link--darken:active {color: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-baseColor--link.link--dark {color: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-baseColor--link.link--dark.link--darken:hover,.u-imageSpectrum .u-baseColor--link.link--dark.link--darken:focus,.u-imageSpectrum .u-baseColor--link.link--dark.link--darken:active {color: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-baseColor--link.link--darker {color: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-baseColor--placeholderNormal ::-webkit-input-placeholder {color: rgba(255, 255, 255, 0.8);}
.u-imageSpectrum .u-baseColor--placeholderNormal ::-moz-placeholder {color: rgba(255, 255, 255, 0.8);}
.u-imageSpectrum .u-baseColor--placeholderNormal :-ms-input-placeholder {color: rgba(255, 255, 255, 0.8);}
.u-imageSpectrum .svgIcon--logoNew path:nth-child(1) {stroke: none !important; fill: rgba(255, 255, 255, 0.4) !important;}
.u-imageSpectrum .svgIcon--logoNew path:nth-child(2) {stroke: none !important; fill: rgba(255, 255, 255, 0.4980392156862745) !important;}
.u-imageSpectrum .svgIcon--logoNew path:nth-child(3) {stroke: none !important; fill: rgba(255, 255, 255, 0.6980392156862745) !important;}
.u-imageSpectrum .svgIcon--logoNew path:nth-child(4) {stroke: none !important; fill: rgba(255, 255, 255, 0.8980392156862745) !important;}
.u-imageSpectrum .u-accentColor--borderLight {border-color: rgba(255, 255, 255, 0.6980392156862745) !important;}
.u-imageSpectrum .u-accentColor--borderNormal {border-color: rgba(255, 255, 255, 0.8980392156862745) !important;}
.u-imageSpectrum .u-accentColor--borderDark {border-color: rgba(255, 255, 255, 0.9490196078431372) !important;}
.u-imageSpectrum .u-accentColor--iconLight .svgIcon,.u-imageSpectrum .u-accentColor--iconLight.svgIcon {fill: rgba(255, 255, 255, 0.8) !important;}
.u-imageSpectrum .u-accentColor--iconNormal .svgIcon,.u-imageSpectrum .u-accentColor--iconNormal.svgIcon {fill: rgba(255, 255, 255, 0.9490196078431372) !important;}
.u-imageSpectrum .u-accentColor--iconDark .svgIcon,.u-imageSpectrum .u-accentColor--iconDark.svgIcon {fill: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-accentColor--textNormal {color: rgba(255, 255, 255, 0.9490196078431372) !important;}
.u-imageSpectrum .u-accentColor--hoverTextNormal:hover {color: rgba(255, 255, 255, 0.9490196078431372) !important;}
.u-imageSpectrum .u-accentColor--textNormal.u-accentColor--textDarken:hover {color: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-accentColor--textDark {color: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-accentColor--backgroundLight {background-color: rgba(255, 255, 255, 0.8980392156862745) !important;}
.u-imageSpectrum .u-accentColor--backgroundNormal {background-color: rgba(255, 255, 255, 0.9490196078431372) !important;}
.u-imageSpectrum .u-accentColor--backgroundDark {background-color: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-accentColor--buttonDark {border-color: rgba(255, 255, 255, 0.9490196078431372) !important; color: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-accentColor--buttonDark:hover {border-color: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-accentColor--buttonDark .icon:before,.u-imageSpectrum .u-accentColor--buttonDark .svgIcon{color: rgba(255, 255, 255, 1) !important; fill: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-accentColor--buttonNormal {border-color: rgba(255, 255, 255, 0.8980392156862745) !important; color: rgba(255, 255, 255, 0.9490196078431372) !important;}
.u-imageSpectrum .u-accentColor--buttonNormal:hover {border-color: rgba(255, 255, 255, 0.9490196078431372) !important;}
.u-imageSpectrum .u-accentColor--buttonNormal .icon:before,.u-imageSpectrum .u-accentColor--buttonNormal .svgIcon{color: rgba(255, 255, 255, 0.9490196078431372) !important; fill: rgba(255, 255, 255, 0.9490196078431372) !important;}
.u-imageSpectrum .u-accentColor--buttonNormal.button--filled .icon:before,.u-imageSpectrum .u-accentColor--buttonNormal.button--filled .svgIcon{color: rgba(0, 0, 0, 0.24705882352941178) !important; fill: rgba(0, 0, 0, 0.24705882352941178) !important;}
.u-imageSpectrum .u-accentColor--buttonDark.button--filled,.u-imageSpectrum .u-accentColor--buttonDark.button--withChrome.is-active,.u-imageSpectrum .u-accentColor--fillWhenActive.is-active {background-color: rgba(255, 255, 255, 1) !important; border-color: rgba(255, 255, 255, 1) !important; color: rgba(0, 0, 0, 0.24705882352941178) !important; fill: rgba(0, 0, 0, 0.24705882352941178) !important;}
.u-imageSpectrum .u-accentColor--buttonNormal.button--filled,.u-imageSpectrum .u-accentColor--buttonNormal.button--withChrome.is-active {background-color: rgba(255, 255, 255, 0.9490196078431372) !important; border-color: rgba(255, 255, 255, 0.9490196078431372) !important; color: rgba(0, 0, 0, 0.24705882352941178) !important; fill: rgba(0, 0, 0, 0.24705882352941178) !important;}
.u-imageSpectrum .postArticle.is-withAccentColors .markup--user,.u-imageSpectrum .postArticle.is-withAccentColors .markup--query {color: rgba(255, 255, 255, 0.9490196078431372) !important;}
.u-imageSpectrum .u-accentColor--highlightFaint {background-color: rgba(255, 255, 255, 0.2) !important;}
.u-imageSpectrum .u-accentColor--highlightStrong.is-active .svgIcon {fill: rgba(255, 255, 255, 0.6) !important;}
.postArticle.is-withAccentColors .u-imageSpectrum .markup--quote.is-other {background-color: rgba(255, 255, 255, 0.2) !important;}
body.is-withMagicUnderlines .postArticle.is-withAccentColors .u-imageSpectrum .markup--quote.is-other {background-color: transparent !important; background-image: linear-gradient(to bottom, rgba(255, 255, 255, 0.2), rgba(255, 255, 255, 0.2));}
.postArticle.is-withAccentColors .u-imageSpectrum .markup--quote.is-me {background-color: rgba(255, 255, 255, 0.4) !important;}
body.is-withMagicUnderlines .postArticle.is-withAccentColors .u-imageSpectrum .markup--quote.is-me {background-color: transparent !important; background-image: linear-gradient(to bottom, rgba(255, 255, 255, 0.4), rgba(255, 255, 255, 0.4));}
.postArticle.is-withAccentColors .u-imageSpectrum .markup--quote.is-targeted {background-color: rgba(255, 255, 255, 0.6) !important;}
body.is-withMagicUnderlines .postArticle.is-withAccentColors .u-imageSpectrum .markup--quote.is-targeted {background-color: transparent !important; background-image: linear-gradient(to bottom, rgba(255, 255, 255, 0.6), rgba(255, 255, 255, 0.6));}
.postArticle.is-withAccentColors .u-imageSpectrum .markup--quote.is-selected {background-color: rgba(255, 255, 255, 0.6) !important;}
body.is-withMagicUnderlines .postArticle.is-withAccentColors .u-imageSpectrum .markup--quote.is-selected {background-color: transparent !important; background-image: linear-gradient(to bottom, rgba(255, 255, 255, 0.6), rgba(255, 255, 255, 0.6));}
.postArticle.is-withAccentColors .u-imageSpectrum .markup--highlight {background-color: rgba(255, 255, 255, 0.6) !important;}
body.is-withMagicUnderlines .postArticle.is-withAccentColors .u-imageSpectrum .markup--highlight {background-color: transparent !important; background-image: linear-gradient(to bottom, rgba(255, 255, 255, 0.6), rgba(255, 255, 255, 0.6));}.u-resetSpectrum .u-tintBgColor {background-color: rgba(255, 255, 255, 1) !important;}.u-resetSpectrum .u-tintBgColor .u-fadeLeft:before {background-image: linear-gradient(to right, rgba(255, 255, 255, 1) 0%, rgba(255, 255, 255, 0) 100%) !important;}.u-resetSpectrum .u-tintBgColor .u-fadeRight:after {background-image: linear-gradient(to right, rgba(255, 255, 255, 0) 0%, rgba(255, 255, 255, 1) 100%) !important;}
.u-resetSpectrum .u-baseColor--borderLight {border-color: rgba(0, 0, 0, 0.2980392156862745) !important;}
.u-resetSpectrum .u-baseColor--borderNormal {border-color: rgba(0, 0, 0, 0.4980392156862745) !important;}
.u-resetSpectrum .u-baseColor--borderDark {border-color: rgba(0, 0, 0, 0.6) !important;}
.u-resetSpectrum .u-baseColor--iconLight .svgIcon,.u-resetSpectrum .u-baseColor--iconLight.svgIcon {fill: rgba(0, 0, 0, 0.2980392156862745) !important;}
.u-resetSpectrum .u-baseColor--iconNormal .svgIcon,.u-resetSpectrum .u-baseColor--iconNormal.svgIcon {fill: rgba(0, 0, 0, 0.4980392156862745) !important;}
.u-resetSpectrum .u-baseColor--iconDark .svgIcon,.u-resetSpectrum .u-baseColor--iconDark.svgIcon {fill: rgba(0, 0, 0, 0.6) !important;}
.u-resetSpectrum .u-baseColor--textNormal {color: rgba(0, 0, 0, 0.4980392156862745) !important;}
.u-resetSpectrum .u-baseColor--textNormal.u-baseColor--textDarken:hover {color: rgba(0, 0, 0, 0.6) !important;}
.u-resetSpectrum .u-baseColor--textDark {color: rgba(0, 0, 0, 0.6) !important;}
.u-resetSpectrum .u-baseColor--backgroundLight {background-color: rgba(0, 0, 0, 0.09803921568627451) !important;}
.u-resetSpectrum .u-baseColor--backgroundNormal {background-color: rgba(0, 0, 0, 0.2) !important;}
.u-resetSpectrum .u-baseColor--backgroundDark {background-color: rgba(0, 0, 0, 0.2980392156862745) !important;}
.u-resetSpectrum .u-baseColor--buttonLight {border-color: rgba(0, 0, 0, 0.2980392156862745) !important; color: rgba(0, 0, 0, 0.2980392156862745) !important;}
.u-resetSpectrum .u-baseColor--buttonLight:hover {border-color: rgba(0, 0, 0, 0.2980392156862745) !important;}
.u-resetSpectrum .u-baseColor--buttonLight .icon:before,.u-resetSpectrum .u-baseColor--buttonLight .svgIcon {color: rgba(0, 0, 0, 0.2980392156862745) !important; fill: rgba(0, 0, 0, 0.2980392156862745) !important;}
.u-resetSpectrum .u-baseColor--buttonDark {border-color: rgba(0, 0, 0, 0.6) !important; color: rgba(0, 0, 0, 0.6) !important;}
.u-resetSpectrum .u-baseColor--buttonDark:hover {border-color: rgba(0, 0, 0, 0.8) !important;}
.u-resetSpectrum .u-baseColor--buttonDark .icon:before,.u-resetSpectrum .u-baseColor--buttonDark .svgIcon {color: rgba(0, 0, 0, 0.6) !important; fill: rgba(0, 0, 0, 0.6) !important;}
.u-resetSpectrum .u-baseColor--buttonNormal {border-color: rgba(0, 0, 0, 0.4980392156862745) !important; color: rgba(0, 0, 0, 0.4980392156862745) !important;}
.u-resetSpectrum .u-baseColor--buttonNormal:hover {border-color: rgba(0, 0, 0, 0.6) !important;}
.u-resetSpectrum .u-baseColor--buttonNormal .icon:before,.u-resetSpectrum .u-baseColor--buttonNormal .svgIcon {color: rgba(0, 0, 0, 0.4980392156862745) !important; fill: rgba(0, 0, 0, 0.4980392156862745) !important;}
.u-resetSpectrum .u-baseColor--buttonDark.button--filled,.u-resetSpectrum .u-baseColor--buttonDark.button--withChrome.is-active {background-color: rgba(0, 0, 0, 0.2980392156862745) !important; border-color: rgba(0, 0, 0, 0.2980392156862745) !important; color: rgba(255, 255, 255, 1) !important; fill: rgba(255, 255, 255, 1) !important;}
.u-resetSpectrum .u-baseColor--buttonNormal.button--filled,.u-resetSpectrum .u-baseColor--buttonNormal.button--withChrome.is-active {background-color: rgba(0, 0, 0, 0.2) !important; border-color: rgba(0, 0, 0, 0.2) !important; color: rgba(255, 255, 255, 1) !important; fill: rgba(255, 255, 255, 1) !important;}
.u-resetSpectrum .u-baseColor--link {color: rgba(0, 0, 0, 0.4980392156862745) !important;}
.u-resetSpectrum .u-baseColor--link.link--darkenOnHover:hover {color: rgba(0, 0, 0, 0.6) !important;}
.u-resetSpectrum .u-baseColor--link.link--darken:hover,.u-resetSpectrum .u-baseColor--link.link--darken:focus,.u-resetSpectrum .u-baseColor--link.link--darken:active {color: rgba(0, 0, 0, 0.6) !important;}
.u-resetSpectrum .u-baseColor--link.link--dark {color: rgba(0, 0, 0, 0.6) !important;}
.u-resetSpectrum .u-baseColor--link.link--dark.link--darken:hover,.u-resetSpectrum .u-baseColor--link.link--dark.link--darken:focus,.u-resetSpectrum .u-baseColor--link.link--dark.link--darken:active {color: rgba(0, 0, 0, 0.8) !important;}
.u-resetSpectrum .u-baseColor--link.link--darker {color: rgba(0, 0, 0, 0.8) !important;}
.u-resetSpectrum .u-baseColor--placeholderNormal ::-webkit-input-placeholder {color: rgba(0, 0, 0, 0.2980392156862745);}
.u-resetSpectrum .u-baseColor--placeholderNormal ::-moz-placeholder {color: rgba(0, 0, 0, 0.2980392156862745);}
.u-resetSpectrum .u-baseColor--placeholderNormal :-ms-input-placeholder {color: rgba(0, 0, 0, 0.2980392156862745);}
.u-resetSpectrum .svgIcon--logoNew path:nth-child(1) {stroke: none !important; fill: rgba(0, 0, 0, 0.2) !important;}
.u-resetSpectrum .svgIcon--logoNew path:nth-child(2) {stroke: none !important; fill: rgba(0, 0, 0, 0.2980392156862745) !important;}
.u-resetSpectrum .svgIcon--logoNew path:nth-child(3) {stroke: none !important; fill: rgba(0, 0, 0, 0.4) !important;}
.u-resetSpectrum .svgIcon--logoNew path:nth-child(4) {stroke: none !important; fill: rgba(0, 0, 0, 0.4980392156862745) !important;}
.u-resetSpectrum .u-accentColor--borderLight {border-color: rgba(2, 184, 117, 1) !important;}
.u-resetSpectrum .u-accentColor--borderNormal {border-color: rgba(2, 184, 117, 1) !important;}
.u-resetSpectrum .u-accentColor--borderDark {border-color: rgba(0, 171, 107, 1) !important;}
.u-resetSpectrum .u-accentColor--iconLight .svgIcon,.u-resetSpectrum .u-accentColor--iconLight.svgIcon {fill: rgba(2, 184, 117, 1) !important;}
.u-resetSpectrum .u-accentColor--iconNormal .svgIcon,.u-resetSpectrum .u-accentColor--iconNormal.svgIcon {fill: rgba(0, 171, 107, 1) !important;}
.u-resetSpectrum .u-accentColor--iconDark .svgIcon,.u-resetSpectrum .u-accentColor--iconDark.svgIcon {fill: rgba(28, 153, 99, 1) !important;}
.u-resetSpectrum .u-accentColor--textNormal {color: rgba(0, 171, 107, 1) !important;}
.u-resetSpectrum .u-accentColor--hoverTextNormal:hover {color: rgba(0, 171, 107, 1) !important;}
.u-resetSpectrum .u-accentColor--textNormal.u-accentColor--textDarken:hover {color: rgba(28, 153, 99, 1) !important;}
.u-resetSpectrum .u-accentColor--textDark {color: rgba(28, 153, 99, 1) !important;}
.u-resetSpectrum .u-accentColor--backgroundLight {background-color: rgba(2, 184, 117, 1) !important;}
.u-resetSpectrum .u-accentColor--backgroundNormal {background-color: rgba(0, 171, 107, 1) !important;}
.u-resetSpectrum .u-accentColor--backgroundDark {background-color: rgba(28, 153, 99, 1) !important;}
.u-resetSpectrum .u-accentColor--buttonDark {border-color: rgba(0, 171, 107, 1) !important; color: rgba(28, 153, 99, 1) !important;}
.u-resetSpectrum .u-accentColor--buttonDark:hover {border-color: rgba(28, 153, 99, 1) !important;}
.u-resetSpectrum .u-accentColor--buttonDark .icon:before,.u-resetSpectrum .u-accentColor--buttonDark .svgIcon{color: rgba(28, 153, 99, 1) !important; fill: rgba(28, 153, 99, 1) !important;}
.u-resetSpectrum .u-accentColor--buttonNormal {border-color: rgba(2, 184, 117, 1) !important; color: rgba(0, 171, 107, 1) !important;}
.u-resetSpectrum .u-accentColor--buttonNormal:hover {border-color: rgba(0, 171, 107, 1) !important;}
.u-resetSpectrum .u-accentColor--buttonNormal .icon:before,.u-resetSpectrum .u-accentColor--buttonNormal .svgIcon{color: rgba(0, 171, 107, 1) !important; fill: rgba(0, 171, 107, 1) !important;}
.u-resetSpectrum .u-accentColor--buttonNormal.button--filled .icon:before,.u-resetSpectrum .u-accentColor--buttonNormal.button--filled .svgIcon{color: rgba(255, 255, 255, 1) !important; fill: rgba(255, 255, 255, 1) !important;}
.u-resetSpectrum .u-accentColor--buttonDark.button--filled,.u-resetSpectrum .u-accentColor--buttonDark.button--withChrome.is-active,.u-resetSpectrum .u-accentColor--fillWhenActive.is-active {background-color: rgba(28, 153, 99, 1) !important; border-color: rgba(28, 153, 99, 1) !important; color: rgba(255, 255, 255, 1) !important; fill: rgba(255, 255, 255, 1) !important;}
.u-resetSpectrum .u-accentColor--buttonNormal.button--filled,.u-resetSpectrum .u-accentColor--buttonNormal.button--withChrome.is-active {background-color: rgba(0, 171, 107, 1) !important; border-color: rgba(0, 171, 107, 1) !important; color: rgba(255, 255, 255, 1) !important; fill: rgba(255, 255, 255, 1) !important;}
.u-resetSpectrum .postArticle.is-withAccentColors .markup--user,.u-resetSpectrum .postArticle.is-withAccentColors .markup--query {color: rgba(0, 171, 107, 1) !important;}</style></div></div></div><div class="loadingBar"></div><script>// <![CDATA[
window["obvInit"] = function (opt_embedded) {window["obvInit"]["embedded"] = opt_embedded; window["obvInit"]["ready"] = true;}
// ]]></script><script>// <![CDATA[
var GLOBALS = {"audioUrl":"https://d1fcbxp97j4nb2.cloudfront.net","baseUrl":"https://ayearofai.com","buildLabel":"28586-6143d83","currentUser":{"userId":"lo_ced97fb2944f","isVerified":false,"subscriberEmail":""},"currentUserHasUnverifiedEmail":false,"isAuthenticated":false,"isCurrentUserVerified":false,"mediumTwitterScreenName":"medium","miroUrl":"https://cdn-images-1.medium.com","moduleUrls":{"base":"https://cdn-static-1.medium.com/_/fp/gen-js/main-base.bundle.2nXtJ8XknWJqOjh2ZNHRVw.js","notes":"https://cdn-static-1.medium.com/_/fp/gen-js/main-notes.bundle.AouowiblLn4SBKZF6cWDtA.js","posters":"https://cdn-static-1.medium.com/_/fp/gen-js/main-posters.bundle.4rYp3nph7xNXmlmak5XWxQ.js","common-async":"https://cdn-static-1.medium.com/_/fp/gen-js/main-common-async.bundle.R35emyz9xZL7YdHbIl-SvQ.js","stats":"https://cdn-static-1.medium.com/_/fp/gen-js/main-stats.bundle.XUyD-cfC9DG2nc9iq5x7GA.js","home-screens":"https://cdn-static-1.medium.com/_/fp/gen-js/main-home-screens.bundle.0oM8iNH9LgWeJY00O_ao2Q.js","misc-screens":"https://cdn-static-1.medium.com/_/fp/gen-js/main-misc-screens.bundle.caQfKUCZ18Wg2IbFBoaXiA.js"},"previewConfig":{"weightThreshold":1,"weightImageParagraph":0.51,"weightIframeParagraph":0.8,"weightTextParagraph":0.08,"weightEmptyParagraph":0,"weightP":0.003,"weightH":0.005,"weightBq":0.003,"minPTextLength":60,"truncateBoundaryChars":20,"detectTitle":true,"detectTitleLevThreshold":0.15},"productName":"Medium","supportsEdit":false,"termsUrl":"//medium.com/policy/9db0094a1e0f","textshotHost":"textshot.medium.com","transactionId":"1492393969082:201c27e9570c","useragent":{"browser":"python requests","family":"","os":"","version":2.2,"supportsDesktopEdit":false,"supportsInteract":false,"supportsView":true,"isMobile":false,"isTablet":false,"isNative":false,"supportsFileAPI":false,"isTier1":false,"clientVersion":"","unknownParagraphsBad":false,"clientChannel":"","supportsRealScrollEvents":false,"supportsVhUnits":false,"ruinsViewportSections":false,"supportsHtml5Video":false,"supportsMagicUnderlines":false,"isWebView":false,"isFacebookWebView":false,"supportsProgressiveMedia":false,"supportsPromotedPosts":true,"isBot":false,"isNativeIphone":false,"supportsCssVariables":false,"supportsScrollableMetabar":false},"variants":{"allow_access":true,"allow_signup":true,"allow_test_auth":"disallow","signin_services":"twitter,facebook,google,email,google-fastidv","signup_services":"twitter,facebook,google,email,google-fastidv","android_rating_prompt_recommend_threshold":5,"google_sign_in_android":true,"enable_onboarding":true,"ios_custom_miro_url":"https://cdn-images-1.medium.com","reengagement_notification_duration":3,"enable_adsnative_integration":true,"browsable_stream_config_bucket":"curated-topics","ios_small_post_preview_truncation_length":5.5,"ios_large_post_preview_truncation_length":5.5,"disable_ios_catalog_badging":true,"enable_series_creation":true,"enable_your_series_pages":true,"enable_productionized_series":true,"enable_dedicated_series_tab_api_ios":true,"enable_clap_milestone_notifications":true,"enable_series_stats_page":true,"enable_prepublish_share_settings":true,"enable_direct_auth_connect":true,"enable_post_import":true,"enable_sponsored_post_labelling":true,"enable_logged_in_follow_on_collection_post":true,"promoted_story_placement_locations":"POST_PAGE_FOOTER","show_topics":true,"enable_search_collection_by_tag_recency_filter":true,"search_collection_by_tag_filter_min_votes":10,"enable_sms_app_promo":true,"enable_export_members":true,"enable_series_card_background_creation":true,"can_see_subscription_branding":true,"enable_subscriptions_landing_page":true,"enable_partner_program_landing_page":true,"enable_hide_broken_links":true,"enable_pay_for_custom_domain":true,"enable_promos_from_dynamo":true,"enable_promos_in_placement":true,"enable_series_promo_in_email":true,"enable_sms":true,"enable_series_in_user_profiles":true,"enable_new_logged_out_bento_operation":true,"enable_topic_brief_relations":true},"xsrfToken":"","iosAppId":"828256236","supportEmail":"yourfriends@medium.com","teamName":"Team Medium","fp":{"/icons/favicon.svg":"https://cdn-static-1.medium.com/_/fp/icons/favicon.KjTfUJo7yJH_fCoUzzH3cg.svg","/icons/favicon-dev-editor.ico":"https://cdn-static-1.medium.com/_/fp/icons/favicon-dev-editor.YKKRxBO8EMvIqhyCwIiJeQ.ico","/icons/favicon-hatch-editor.ico":"https://cdn-static-1.medium.com/_/fp/icons/favicon-hatch-editor.BuEyHIqlyh2s_XEk4Rl32Q.ico","/icons/favicon-medium-editor.ico":"https://cdn-static-1.medium.com/_/fp/icons/favicon-medium-editor.PiakrZWB7Yb80quUVQWM6g.ico"},"authBaseUrl":"https://medium.com","imageUploadSizeMb":25,"isAuthDomainRequest":false,"domainCollectionSlug":"a-year-of-artificial-intelligence","algoliaApiEndpoint":"https://MQ57UUUQZ2-dsn.algolia.net","algoliaAppId":"MQ57UUUQZ2","algoliaSearchOnlyApiKey":"394474ced050e3911ae2249ecc774921","iosAppStoreUrl":"https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8","iosAppLinkBaseUrl":"medium:","algoliaIndexPrefix":"medium_","androidPlayStoreUrl":"https://play.google.com/store/apps/details?id=com.medium.reader","googleClientId":"216296035834-k1k6qe060s2tp2a2jam4ljdcms00sttg.apps.googleusercontent.com","androidPackage":"com.medium.reader","androidPlayStoreMarketScheme":"market://details?id=com.medium.reader","googleAuthUri":"https://accounts.google.com/o/oauth2/auth","androidScheme":"medium","layoutData":{"useDynamicScripts":false,"googleAnalyticsTrackingCode":"UA-24232453-2","jsShivUrl":"https://cdn-static-1.medium.com/_/fp/js/shiv.RI2ePTZ5gFmMgLzG5bEVAA.js","useDynamicCss":false,"faviconUrl":"https://cdn-static-1.medium.com/_/fp/icons/favicon-medium.TAS6uQ-Y7kcKgi0xjcYHXw.ico","faviconImageId":"1*W0nmth_X8nFKjn6BZ388UQ.png","fontSets":[{"id":1,"url":"https://cdn-static-1.medium.com/_/fp/css/fonts-base.by5Oi_VbnwEIvhnWIsuUjA.css"},{"id":4,"url":"https://cdn-static-1.medium.com/_/fp/css/fonts-lazy-base.g08Jj5TZPAiuPWj5YNUsSg.css"},{"id":6,"url":"https://cdn-static-1.medium.com/_/fp/css/fonts-latin-base.141WxxXgxGxNcfeza73H7Q.css"},{"id":7,"url":"https://cdn-static-1.medium.com/_/fp/css/fonts-lazy-latin-base.jMU532QDmysQMOINr-cr2A.css"}],"editorFaviconUrl":"https://cdn-static-1.medium.com/_/fp/icons/favicon-medium-editor.PiakrZWB7Yb80quUVQWM6g.ico"},"authBaseUrlRev":"moc.muidem//:sptth","isDnt":false,"stripePublishableKey":"pk_live_7FReX44VnNIInZwrIIx6ghjl","archiveUploadSizeMb":100,"paymentData":{"currencies":{"1":{"label":"US Dollar","external":"usd"}},"countries":{"1":{"label":"United States of America","external":"US"}},"accountTypes":{"1":{"label":"Individual","external":"individual"},"2":{"label":"Company","external":"company"}}},"previewConfig2":{"weightThreshold":1,"weightImageParagraph":0.05,"raiseImage":true,"enforceHeaderHierarchy":true,"isImageInsetRight":true},"isAmp":false,"iosScheme":"medium","isSwBoot":false,"lightstep":{"accessToken":"ce5be895bef60919541332990ac9fef2","carrier":"{\"ot-tracer-spanid\":\"00478ee86baae7c5\",\"ot-tracer-traceid\":\"368051c4509483c0\",\"ot-tracer-sampled\":\"true\"}","host":"collector-medium.lightstep.com"},"facebook":{"key":"542599432471018","secret":"c14df7146e9052a1131f3c900c1f0644","token":"542599432471018|1JqjIwxSfY9jOt_KwjWEl1R7T6I","namespace":"medium-com","scope":{"default":["public_profile","email","user_friends"],"connect":["public_profile","email","user_friends"],"login":["public_profile","email","user_friends"],"share":["public_profile","email","user_friends","publish_actions"]},"smartPublishWhitelistedPublications":["bcc38c8f6edf","f3726e2a5878","828a270689e","81c7d351c056","f30e42fd7ff8","8bf1d7d3081b","d16afa0ae7c","d8f3f6ad9c31","e74de0cedea9","15f753907972","c8c6a6b01ebd","3412b9729488","2ce4bbcf83bb","544c7006046e","7bfcdbc6b30a","a268fd916824","458a773bccd2"],"instantArticles":{"published":true,"developmentMode":false}},"mailingListArchiveUploadSizeMb":2,"availableMembershipPlans":[],"editorsPicksTopicId":"3985d2a191c5","popularOnMediumTopicId":"9d34e48ecf94","memberContentTopicId":"13d7efd82fb2","isDoNotAuth":false,"goldfinchUrl":"https://goldfinch.medium.com"}
// ]]></script><script charset="UTF-8" src="https://cdn-static-1.medium.com/_/fp/gen-js/main-base.bundle.2nXtJ8XknWJqOjh2ZNHRVw.js" async></script><script>// <![CDATA[
window["obvInit"]({"value":{"id":"10300100899b","versionId":"4e632fdafd3b","creatorId":"cb55958ea3bb","creator":{"userId":"cb55958ea3bb","name":"Rohan Kapur","username":"mckapur","createdAt":1383813905817,"lastPostCreatedAt":1492253036060,"imageId":"1*pGDfwp8yLzgCeG1RkPUxgQ.jpeg","backgroundImageId":"","bio":"rohankapur.com","twitterScreenName":"mckapur","socialStats":{"userId":"cb55958ea3bb","usersFollowedCount":204,"usersFollowedByCount":592,"type":"SocialStats"},"social":{"userId":"lo_6d640aa2d2ff","targetUserId":"cb55958ea3bb","type":"Social"},"facebookAccountId":"1004843339565980","allowNotes":1,"type":"User"},"homeCollection":{"id":"bb87da25612c","name":"A Year of Artificial Intelligence","slug":"a-year-of-artificial-intelligence","tags":["DATA SCIENCE","ARTIFICIAL INTELLIGENCE","MACHINE LEARNING","TECHNOLOGY","COMPUTER SCIENCE"],"creatorId":"cb55958ea3bb","description":"Our ongoing effort to make the mathematics, science, linguistics, and philosophy of artificial intelligence fun and simple.","shortDescription":"Our ongoing effort to make the mathematics, science‚Ä¶","image":{"imageId":"1*NZsNSuNxe_O2YW1ybboOvA.jpeg","filter":"","backgroundSize":"","originalWidth":0,"originalHeight":0,"strategy":"resample","height":0,"width":0},"metadata":{"followerCount":521,"postCount":6,"activeAt":1492072051871},"virtuals":{"permissions":{"canPublish":false,"canPublishAll":false,"canRepublish":false,"canRemove":false,"canManageAll":false,"canSubmit":false,"canEditPosts":false,"canAddWriters":false,"canViewStats":false,"canSendNewsletter":false,"canViewLockedPosts":false,"canViewCloaked":false,"canEditOwnPosts":false,"canBeAssignedAuthor":false},"isSubscribed":false,"isNewsletterSubscribed":false,"memberOfMembershipPlanId":""},"logo":{"imageId":"1*NZsNSuNxe_O2YW1ybboOvA.jpeg","filter":"","backgroundSize":"","originalWidth":1500,"originalHeight":1000,"strategy":"resample","height":0,"width":0},"twitterUsername":"mckapur","facebookPageName":"mckapur","publicEmail":"me@rohankapur.com","domain":"ayearofai.com","sections":[{"type":2,"collectionHeaderMetadata":{"title":"A Year of Artificial Intelligence","description":"Where we make the mathematics, science, linguistics, and philosophy of artificial intelligence fun and simple.","backgroundImage":{"id":"1*UlHnUWTtTHWOrOuJdHjVww.png","originalWidth":2000,"originalHeight":1333,"focusPercentX":51.785714285714285,"focusPercentY":43.31550802139037},"logoImage":{"id":"1*NZsNSuNxe_O2YW1ybboOvA.jpeg","originalWidth":1500,"originalHeight":1000,"alt":"A Year of Artificial Intelligence"},"alignment":1,"layout":6}},{"type":1,"postListMetadata":{"source":1,"layout":2,"number":1,"postIds":[],"sectionHeader":"New"}},{"type":1,"postListMetadata":{"source":1,"layout":4,"number":14,"postIds":[]}}],"tintColor":"#FF000000","lightText":true,"favicon":{"imageId":"","filter":"","backgroundSize":"","originalWidth":0,"originalHeight":0,"strategy":"resample","height":0,"width":0},"colorPalette":{"defaultBackgroundSpectrum":{"colorPoints":[{"color":"#FF868484","point":0},{"color":"#FF7C7B7A","point":0.1},{"color":"#FF737171","point":0.2},{"color":"#FF696867","point":0.3},{"color":"#FF5F5E5E","point":0.4},{"color":"#FF555454","point":0.5},{"color":"#FF4A4949","point":0.6},{"color":"#FF3F3E3E","point":0.7},{"color":"#FF343333","point":0.8},{"color":"#FF272727","point":0.9},{"color":"#FF1A1A1A","point":1}],"backgroundColor":"#FFFFFFFF"},"tintBackgroundSpectrum":{"colorPoints":[{"color":"#FF000000","point":0},{"color":"#FF1E1D1D","point":0.1},{"color":"#FF3C3B3B","point":0.2},{"color":"#FF565555","point":0.3},{"color":"#FF6F6D6D","point":0.4},{"color":"#FF868484","point":0.5},{"color":"#FF9C9A99","point":0.6},{"color":"#FFB1AEAE","point":0.7},{"color":"#FFC5C3C2","point":0.8},{"color":"#FFD9D6D6","point":0.9},{"color":"#FFECE9E9","point":1}],"backgroundColor":"#FF000000"},"highlightSpectrum":{"colorPoints":[{"color":"#FFF5F2F1","point":0},{"color":"#FFF3F0EF","point":0.1},{"color":"#FFF1EEED","point":0.2},{"color":"#FFEFECEC","point":0.3},{"color":"#FFEDEAEA","point":0.4},{"color":"#FFEBE8E8","point":0.5},{"color":"#FFE9E6E6","point":0.6},{"color":"#FFE7E5E4","point":0.7},{"color":"#FFE5E3E2","point":0.8},{"color":"#FFE4E1E0","point":0.9},{"color":"#FFE2DFDE","point":1}],"backgroundColor":"#FFFFFFFF"}},"navItems":[{"type":1,"title":"Algorithms","tagSlug":"algorithms","url":"https://ayearofai.com/tagged/algorithms","source":"tagSlug"},{"type":1,"title":"Today I Learned","tagSlug":"today-i-learned","url":"https://ayearofai.com/tagged/today-i-learned","source":"tagSlug"},{"type":1,"title":"Case Studies","tagSlug":"case-studies","url":"https://ayearofai.com/tagged/case-studies","source":"tagSlug"},{"type":1,"title":"Philosophical","tagSlug":"philosophical","url":"https://ayearofai.com/tagged/philosophical","source":"tagSlug"},{"type":1,"title":"Meta","tagSlug":"meta","url":"https://ayearofai.com/tagged/meta","source":"tagSlug"}],"colorBehavior":2,"instantArticlesState":0,"acceleratedMobilePagesState":0,"ampLogo":{"imageId":"","filter":"","backgroundSize":"","originalWidth":0,"originalHeight":0,"strategy":"resample","height":0,"width":0},"header":{"title":"A Year of Artificial Intelligence","description":"Where we make the mathematics, science, linguistics, and philosophy of artificial intelligence fun and simple.","backgroundImage":{"id":"1*UlHnUWTtTHWOrOuJdHjVww.png","originalWidth":2000,"originalHeight":1333,"focusPercentX":51.785714285714285,"focusPercentY":43.31550802139037},"logoImage":{"id":"1*NZsNSuNxe_O2YW1ybboOvA.jpeg","originalWidth":1500,"originalHeight":1000,"alt":"A Year of Artificial Intelligence"},"alignment":1,"layout":6},"type":"Collection"},"homeCollectionId":"bb87da25612c","title":"Rohan & Lenny #3: Recurrent Neural Networks & LSTMs","detectedLanguage":"en","latestVersion":"4e632fdafd3b","latestPublishedVersion":"4e632fdafd3b","hasUnpublishedEdits":false,"latestRev":4674,"createdAt":1492065760544,"updatedAt":1492389083311,"acceptedAt":0,"firstPublishedAt":1492072051153,"latestPublishedAt":1492389083311,"vote":false,"experimentalCss":"","displayAuthor":"","content":{"subtitle":"The ultimate guide to machine learning‚Äôs favorite child.","bodyModel":{"paragraphs":[{"name":"be8e","type":4,"text":"Sequences upon sequences upon sequences. Sequen-ception.","markups":[],"layout":5,"metadata":{"id":"1*khIKl9t4XmZGSsKhW_Yg2w.png","originalWidth":2000,"originalHeight":907}},{"name":"84b9","type":3,"text":"Rohan & Lenny #3: Recurrent Neural Networks & LSTMs","markups":[]},{"name":"9485","type":13,"text":"The ultimate guide to machine learning‚Äôs favorite child.","markups":[]},{"name":"4569","type":7,"text":"This is the third group (Lenny and Rohan) entry in our journey to extend our knowledge of artificial intelligence and convey that knowledge in a simple, fun, and accessible manner. Learn more about our motives in this introduction post.","markups":[{"type":3,"start":25,"end":30,"anchorType":2,"userId":"de8e2540b759"},{"type":3,"start":35,"end":40,"anchorType":2,"userId":"cb55958ea3bb"},{"type":3,"start":55,"end":62,"href":"https://medium.com/a-year-of-artificial-intelligence","title":"","rel":"","anchorType":0},{"type":3,"start":218,"end":230,"href":"https://medium.com/a-year-of-artificial-intelligence/0-2016-is-the-year-i-venture-into-artificial-intelligence-d702d65eb919#.bfjoaqxu5","title":"","rel":"","anchorType":0}]},{"name":"a16a","type":1,"text":"It seems like most of our posts on this blog start with ‚ÄúWe‚Äôre back!‚Äù, so‚Ä¶ you know the drill. It‚Äôs been a while since our last post ‚Äî just over 5 months ‚Äî but it certainly doesn‚Äôt feel that way. Whether our articles are more spaced out than we‚Äôd like them to be, well, we haven‚Äôt actually discussed that yet. But I, Rohan, would definitely like to get into a more frequent routine. Since November, we‚Äôve been grinding on school (basically, getting it over and done with), banging out Contra v2, and lazing around more than we should. End of senior year is a fun time.","markups":[{"type":3,"start":485,"end":491,"href":"http://getcontra.com","title":"","rel":"noopener","anchorType":0}],"hasDropCap":true},{"name":"ac01","type":4,"text":"","markups":[],"layout":4,"metadata":{"id":"1*N5l-Uj5yUVPIDwIuGUvnfg.jpeg","originalWidth":570,"originalHeight":389}},{"name":"93ae","type":1,"text":"It‚Äôs 2017. We started A Year Of AI in 2016. Last year. Don‚Äôt panic, though. If you‚Äôve read our letter, you‚Äôll know that, despite our name and inception date, we‚Äôre not going anywhere anytime soon. There‚Äôs a good chance we‚Äôll move off Medium, but we‚Äôre still both obsessed with AI and writing these posts to hopefully make other people obsessed, as well.","markups":[{"type":3,"start":95,"end":101,"href":"https://ayearofai.com/the-goal-of-our-blog-c104d7b6377a#.75o5qyayi","title":"","rel":"noopener","anchorType":0},{"type":1,"start":24,"end":28},{"type":2,"start":24,"end":29}]},{"name":"f6d1","type":1,"text":"I wrote the first article on this blog just over a year ago, and mentioned that my goal for the year was to be accepted into Stanford University as an undergrad student. A few months ago, I achieved this goal. At Stanford, I‚Äôll probably be studying Symbolic Systems, which is a program that explores both the humanities and STEM to inform an understanding of artificial intelligence and the nature of minds. Needless to say, A Year of AI will continue to document the new things I learn üòÄ.","markups":[{"type":3,"start":249,"end":265,"href":"https://symsys.stanford.edu/","title":"","rel":"noopener nofollow","anchorType":0}]},{"name":"b842","type":1,"text":"Anyways, you can find plenty of articles on recurrent neural networks (RNNs) online. My favorite one, personally, is from Andrej Karpathy‚Äôs blog. I read it about 1.5 years ago when I was learning about RNNs. We definitely think there‚Äôs space to simplify the topic even more, though. As usual, that‚Äôs our aim for the article ‚Äî to teach you RNNs in a fun, simple manner. We‚Äôre also importantly doing this for completion purposes; we want people to hop onto A Year of AI and be able to work their way up all the way from logistic regression to neural machine translation (don‚Äôt worry, you‚Äôll find out what means soon enough), and thus recurrent neural networks is a vital addition. After this, we want to look at and summarize/simplify a bunch of new super interesting research papers, and for most of them RNNs are a key ingredient. Finally, we think this article contains so much meat and ties together content unlike any other RNN tutorial on the interwebs.","markups":[{"type":3,"start":88,"end":100,"href":"http://karpathy.github.io/2015/05/21/rnn-effectiveness/","title":"","rel":"noopener","anchorType":0}]},{"name":"60e2","type":1,"text":"Before we get started, you should try to familiarize yourself with ‚Äúvanilla‚Äù neural networks. If you need a refresher, check out our neural networks and backpropogation mega-post from earlier this year. This is so you know the basics of machine learning, linear algebra, neural network architecture, cost functions, optimization methods, training/test sets, activation functions/what they do, softmax, etc. Reading our article on convolutional neural networks may also make you more comfortable entering this post, especially because we often reference CNNs. Checking out this article I wrote on vanishing gradients will help later on, as well.","markups":[{"type":3,"start":133,"end":178,"href":"https://ayearofai.com/rohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d#.quwnoqtot","title":"","rel":"noopener","anchorType":0},{"type":3,"start":430,"end":459,"href":"https://ayearofai.com/rohan-lenny-2-convolutional-neural-networks-5f4cd480a60b#.ebwc0kf4z","title":"","rel":"noopener","anchorType":0},{"type":3,"start":572,"end":576,"href":"https://ayearofai.com/rohan-4-the-vanishing-gradient-problem-ec68f76ffb9b#.58nevwsoa","title":"","rel":"noopener","anchorType":0}]},{"name":"81ec","type":1,"text":"Rule of thumb: the more you know, the better!","markups":[]},{"name":"3288","type":3,"text":"Table of Contents","markups":[]},{"name":"003f","type":1,"text":"I can‚Äôt link to each section, but here‚Äôs what we cover in this article (save the intro and conclusion):","markups":[]},{"name":"8cfa","type":10,"text":"What can RNNs do? Where we look at‚Ä¶ what RNNs can do!","markups":[{"type":1,"start":0,"end":18}]},{"name":"7f98","type":10,"text":"Why? Where we talk about the gap that RNNs fill in machine learning‚Äôs suite of algorithms.","markups":[{"type":1,"start":0,"end":5}]},{"name":"de84","type":10,"text":"Show me. Where we visualize RNNs for the first time.","markups":[{"type":1,"start":0,"end":9}]},{"name":"f2f4","type":10,"text":"Formalism. Where we walk through how an RNN mathematically works with proper notation.","markups":[{"type":1,"start":0,"end":11}]},{"name":"c163","type":10,"text":"An example? Okay! Where we walk through, qualitatively, a simple application of RNNs and how the RNN operates in this application, including techniques we can use.","markups":[{"type":1,"start":0,"end":18}]},{"name":"394e","type":10,"text":"Training (or, why vanilla RNNs suck.) Where we talk about how to train RNNs, and why vanilla RNNs are bad at learning.","markups":[{"type":1,"start":0,"end":38}]},{"name":"5756","type":10,"text":"Fixing the problem with LSTMs (Part I). Where we introduce the solution to vanilla RNNs‚Äô inability to learn: LSTMs.","markups":[{"type":1,"start":0,"end":40}]},{"name":"654d","type":10,"text":"Fixing the problem with LSTMs (Part II). Where we analyze on a close, technical level, the reasons LSTMs don‚Äôt suffer from vanishing gradients as much (and why they still do, to an extent). Then we conclude LSTMs with final thoughts on and facts about them.","markups":[{"type":1,"start":0,"end":41}]},{"name":"2dfc","type":10,"text":"Yay RNNs! Where you get to see neat little things RNNs have done!","markups":[{"type":1,"start":0,"end":10},{"type":2,"start":16,"end":20}]},{"name":"e56c","type":10,"text":"In Practice. Where we look at more technical and important applications and case studies of RNNs, including other variations of RNNs, especially as relevant in hot/recent research papers.","markups":[{"type":1,"start":0,"end":13}]},{"name":"7bc0","type":10,"text":"Building a Vanilla Recurrent Neural Network. Where you get to code your very first RNN! Woohoo!","markups":[{"type":1,"start":0,"end":45}]},{"name":"02a8","type":3,"text":"What can RNNs do?","markups":[]},{"name":"1c1d","type":1,"text":"There are a number of very important tasks that ANNs and CNNs cannot solve, that RNNs are used for instead. Tasks like: image captioning, language translation, sentiment classification, predictive typing, video classification, natural language processing, speech recognition, and a lot more interesting things that have been presented in recent research papers (for example‚Ä¶ learning to learn by gradient descent by gradient descent!).","markups":[{"type":3,"start":375,"end":432,"href":"https://arxiv.org/pdf/1606.04474.pdf","title":"","rel":"noopener","anchorType":0},{"type":2,"start":282,"end":286}]},{"name":"1d0f","type":4,"text":"Image captioning, taken from CS231n slides: http://cs231n.stanford.edu/slides/2016/winter1516_lecture10.pdf","markups":[{"type":3,"start":44,"end":107,"href":"http://cs231n.stanford.edu/slides/2016/winter1516_lecture10.pdf","title":"","rel":"nofollow noopener noopener","anchorType":0}],"layout":1,"metadata":{"id":"1*X5dk-xGw2yNYsEB3QvHWIA.png","originalWidth":713,"originalHeight":400}},{"name":"02ef","type":1,"text":"RNNs are very powerful. Y‚Äôknow how regular neural networks have been proved to be ‚Äúuniversal function approximators‚Äù ? If you didn‚Äôt:","markups":[]},{"name":"8206","type":6,"text":"In the mathematical theory of artificial neural networks, the universal approximation theorem states that a feed-forward network with a single hidden layer containing a finite number of neurons can approximate continuous functions on compact subsets of R^n, under mild assumptions on the activation function.","markups":[]},{"name":"314e","type":1,"text":"That‚Äôs pretty confusing. Basically, what this states is that an artificial neural network can compute any function. Even if someone gives you an extremely wiggly, complex looking function, it‚Äôs guaranteed that there exists a neural network that can produce (or at least extremely closely approximate) it. The proof itself is very complex, but this is a brilliant article offering a visual approach as to why it‚Äôs true.","markups":[{"type":3,"start":343,"end":347,"href":"http://neuralnetworksanddeeplearning.com/chap4.html","title":"","rel":"noopener","anchorType":0},{"type":2,"start":407,"end":408}]},{"name":"a351","type":1,"text":"So, that‚Äôs great. ANNs are universal function approximators. RNNs take it a step further, though; they can compute/describe programs. In fact, some RNNs with proper weights and architecture qualify as Turing Complete:","markups":[{"type":3,"start":98,"end":132,"href":"http://stats.stackexchange.com/a/221142/98975","title":"","rel":"noopener","anchorType":0},{"type":2,"start":124,"end":132}]},{"name":"8f10","type":6,"text":"A Turing Complete system means a system in which a program can be written that will find an answer (although with no guarantees regarding runtime or memory).","markups":[]},{"name":"c6a2","type":6,"text":"So, if somebody says ‚Äúmy new thing is Turing Complete‚Äù that means in principle (although often not in practice) it could be used to solve any computation problem.","markups":[{"type":1,"start":89,"end":110}]},{"name":"1429","type":6,"text":"‚Äî http://stackoverflow.com/a/7320/1260708","markups":[{"type":3,"start":2,"end":41,"href":"http://stackoverflow.com/a/7320/1260708","title":"","rel":"nofollow noopener","anchorType":0}]},{"name":"2ba5","type":1,"text":"That‚Äôs cool, isn‚Äôt it? Now, this is all theoretical, and in practice means less than you think, so don‚Äôt get too hyped. Hopefully, though, this gives some more insight into why RNNs are super important for future developments in machine learning ‚Äî and why you should read on.","markups":[]},{"name":"662c","type":1,"text":"At this point, if you weren‚Äôt previously hooked on learning what the heck these things are, you should be now. (If you still aren‚Äôt, just bare with me. Things will get spicy soon.) So, let‚Äôs dive in.","markups":[]},{"name":"72f6","type":3,"text":"Why?","markups":[]},{"name":"54dc","type":1,"text":"We took a bit of a detour to talk about how great RNNs are, but haven‚Äôt focused on why ANNs can‚Äôt perform well in the tasks that RNNs can.","markups":[{"type":2,"start":83,"end":86}]},{"name":"b88b","type":6,"text":"Why do we need another neural network model? Why do we need recurrent neural networks when we already have the beloved ANNs (and CNNs) in all their glory?","markups":[]},{"name":"95c9","type":1,"text":"It boils down to a few things:","markups":[]},{"name":"3d40","type":9,"text":"ANNs can‚Äôt deal with sequential or ‚Äútemporal‚Äù data","markups":[]},{"name":"2a22","type":9,"text":"ANNs lack memory","markups":[]},{"name":"0d57","type":9,"text":"ANNs have a fixed architecture","markups":[]},{"name":"fbb9","type":9,"text":"RNNs are more ‚Äúbiologically realistic‚Äù because of the recurrent connectivity found in the visual cortex of the brain","markups":[]},{"name":"341a","type":1,"text":"Let‚Äôs address the first three points individually. The first issue refers to the fact that ANNs have a fixed input size and a fixed output size. ANNs have an elaborate list of hyperparameters, and this notably includes the number of neurons in the input layer and output layer. But what if we wanted input data and/or output data of variable size, instead of something that needs to have its size as a preset constant? RNNs allow us to do that. In this aspect, they offer more flexibility than ANNs.","markups":[{"type":2,"start":103,"end":120},{"type":2,"start":126,"end":143},{"type":2,"start":333,"end":341}]},{"name":"605a","type":4,"text":"We might choose this architecture for our ANN, with 4 inputs and 1 output. But that‚Äôs it ‚Äî we can‚Äôt input a vector with 5 values, for example. https://qph.ec.quoracdn.net/main-qimg-050d12c5da82f4f97fdd942d7777b8e4.","markups":[{"type":3,"start":143,"end":213,"href":"https://qph.ec.quoracdn.net/main-qimg-050d12c5da82f4f97fdd942d7777b8e4","title":"","rel":"nofollow noopener","anchorType":0}],"layout":1,"metadata":{"id":"1*BQ0SxdqC9Pl_3ZQtd3e45A.png","originalWidth":500,"originalHeight":309}},{"name":"1c8e","type":1,"text":"I‚Äôll give you a couple examples of why this matters.","markups":[]},{"name":"e228","type":1,"text":"It‚Äôs unclear how we could use an ANN by itself to perform a task like image captioning, because the network would need to output a sentence ‚Äî a list of words in a specific order ‚Äî which is a sequence. It would be a sequence of vectors, because each word would need to be represented numerically. In machine learning and data science, we represent words numerically as vectors; these are called word embeddings. An ANN can only output a single word/label, like in image classification where we treat the output as the label with the highest value in the final vector that is a softmax probability distribution over all classes. The only way to make sentences work with ANNs would be to have billions of output neurons that each map to a single possible sentence in the permutation of all [sensible] sentences that can be formed by the vocabulary we have. And that doesn‚Äôt sound like a good idea.","markups":[{"type":2,"start":190,"end":199},{"type":2,"start":200,"end":201},{"type":2,"start":436,"end":443}]},{"name":"8f28","type":4,"text":"A reminder of what the output of an ANN looks like ‚Äî a probability distribution over classes ‚Äî and how we convert that into a single final result (one-hot encoding): by taking the label with the greatest probability and making it 1, with the rest 0.","markups":[],"layout":1,"metadata":{"id":"1*GFVoFpD6cdCY_PGqnjhOlQ.png","originalWidth":305,"originalHeight":182}},{"name":"f8ec","type":1,"text":"Wow, that was a lot of words. Nevertheless, I hope it‚Äôs clear that, with ANNs, there‚Äôs no feasible way to output a sequence.","markups":[]},{"name":"f308","type":1,"text":"Now, what about inputting a sequence into an ANN? In other words, ‚Äútemporal‚Äù data: data that varies over time, and is thus a sequence. Take the example of sentiment classification where we input a sentence (sequence of words = sequence of vectors = sequence of set of values where each value goes into an individual neuron) and want to output its sentiment: positive or negative. The output part seems easy, because it‚Äôs just one neuron that‚Äôs either rounded to 1 (positive) or 0 (negative). And, for the input, you might be thinking: couldn‚Äôt we input each ‚Äúset of values‚Äù separately? Input the first word, wait for the neural net to fully feed forward and produce an output, then input the next word, etc. etc.","markups":[{"type":2,"start":16,"end":26}]},{"name":"8029","type":1,"text":"Let‚Äôs take the case of this utterly false, and most certainly negative sentence, to evaluate:","markups":[]},{"name":"6e25","type":4,"text":"This is just an alternative fact, believe me! Lenny is actually a great coder. The best I know of. The best.","markups":[{"type":1,"start":71,"end":72},{"type":2,"start":66,"end":108}],"layout":1,"metadata":{"id":"1*yq_zmka1ssikrmD9GkWmnw.png","originalWidth":625,"originalHeight":139}},{"name":"322d","type":1,"text":"We‚Äôd input ‚ÄúLenny‚Äù first, then ‚ÄúKhazan‚Äù, then ‚Äúis‚Äù, etc. But, at each feedforward iteration, the output would be completely useless. Why? Because the output would be dependent on only that word. We‚Äôd be finding the sentiment of a single word, which is useless, because we want the sentiment of the entire sentence. Sentiment analysis only makes sense when all the words come together, dependent on each other, to form a sentence.","markups":[{"type":2,"start":179,"end":184}]},{"name":"13ff","type":1,"text":"Think of it this way ‚Äî this means you‚Äôre essentially running a neural network a bunch of times, just with new data at each separate iteration. Those run-throughs aren‚Äôt linked in any way; they‚Äôre independent. Once you feedforward and fully run the neural network, it forgets everything it just did. This sentence only makes sense and can only be interpretable because it‚Äôs a collection of words put together in a specific order to form meaning. The relevance of each word is dependent on the words that precede it: the context. This is why RNNs are being used heavily in NLP; they retain context by having memory. ANNs have no memory.","markups":[{"type":2,"start":436,"end":443},{"type":2,"start":606,"end":612}]},{"name":"e71f","type":1,"text":"I like this quote from another article on RNNs:","markups":[]},{"name":"9cd3","type":6,"text":"Humans don‚Äôt start their thinking from scratch every second. As you read this essay, you understand each word based on your understanding of previous words. You don‚Äôt throw everything away and start thinking from scratch again. Your thoughts have persistence.","markups":[]},{"name":"b760","type":6,"text":"‚Äî http://colah.github.io/posts/2015‚Äì08-Understanding-LSTMs/","markups":[{"type":3,"start":2,"end":59,"href":"http://colah.github.io/posts/2015-08-Understanding-LSTMs/","title":"","rel":"nofollow noopener","anchorType":0}]},{"name":"e0e4","type":1,"text":"(Furthermore, take the case where we had sequential data in both the input and the output. Translating one language to another is a good example of this. Clearly, ANNs aren‚Äôt the answer.)","markups":[{"type":2,"start":60,"end":65}]},{"name":"fa2f","type":1,"text":"RNNs don‚Äôt just need memory; they need long term memory. Let‚Äôs take the example of predictive typing. Let‚Äôs say we typed the following sentence in an SMS message to 911, and the operating system needs to fill in the blank:","markups":[{"type":2,"start":39,"end":48}]},{"name":"2c67","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*TugitPvwm_IZqAdAPR-7UA.png","originalWidth":543,"originalHeight":90}},{"name":"3077","type":4,"text":"The face of a criminal?","markups":[],"layout":1,"metadata":{"id":"1*7l0aNgpXDXZnY-P9C8K4IA.jpeg","originalWidth":250,"originalHeight":250}},{"name":"325c","type":1,"text":"Here, if the RNN wasn‚Äôt able to look back much (ie. before ‚Äúshould‚Äù), then many different options could arise:","markups":[]},{"name":"da28","type":4,"text":"Lenny in the military? Make it into a TV show! I‚Äôd watch it.","markups":[],"layout":1,"metadata":{"id":"1*aMJu60wscb9m4A-Zn_xyqQ.png","originalWidth":552,"originalHeight":266}},{"name":"930e","type":1,"text":"The word ‚Äúsent‚Äù would indicate to the RNN that a location needs to be outputted. However, if the RNN was able to retain information from all the way back, such as the word ‚Äúcriminal‚Äù, then it would be much more confident that:","markups":[]},{"name":"0990","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*4CZskdiGqIx29BQylqnYuA.png","originalWidth":557,"originalHeight":102}},{"name":"c9f1","type":1,"text":"The probability of outputting ‚Äújail‚Äù drastically increases when it sees the word ‚Äúcriminal‚Äù is present. That‚Äôs why context matters, be it predictive typing, image captioning, machine translation, etc. The output or outputs of a recurrent neural network will always be functionally dependent on (meaning, a function of) information from the very beginning, but how much it chooses to ‚Äúforget‚Äù or ‚Äúretain‚Äù (that is, varying degrees of influence from earlier information) depends on the weights that it learns from the training data.","markups":[]},{"name":"eb76","type":1,"text":"As it turns out, RNNs ‚Äî especially deep ones ‚Äî are rarely good at retaining much information, due to an issue called the vanishing gradient problem. That‚Äôs where we turn to other variants of RNNs such as LSTMs and GRUs. But, more on that later.","markups":[]},{"name":"c00c","type":1,"text":"To address the third point, one more constraint with ANNs is that they have a fixed number of computation/processing steps (because, once again, the number of hidden layers is a hyperparameter). With RNNs, we can have much more dynamic processing since we operate over vectors. Each neuron in an RNN is almost like an entire layer in an ANN; this will make more sense as we bring up an illustration for you. Exciting stuff.","markups":[{"type":1,"start":408,"end":423}]},{"name":"4abf","type":3,"text":"Show me.","markups":[]},{"name":"0160","type":1,"text":"OK, that‚Äôs enough teasing. Three sections into the article, and you‚Äôre yet to see what an RNN looks like, or appreciate how it really works. Everything comes in due time, though!","markups":[]},{"name":"d391","type":1,"text":"The first thing I‚Äôm going to do is show you what a normal ANN diagram looks like:","markups":[]},{"name":"ce30","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*GapzcZDrwnVbflhlRoWZ9g.png","originalWidth":310,"originalHeight":220}},{"name":"deda","type":1,"text":"Each neuron stores a single scalar value. Thus, each layer can be considered a vector.","markups":[]},{"name":"f077","type":1,"text":"Now I‚Äôm going to show you what this ANN looks like in our RNN visual notation:","markups":[]},{"name":"c5e9","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*ntKLnv52DCUnkseNcm91iQ.png","originalWidth":300,"originalHeight":65}},{"name":"f1fc","type":1,"text":"The two diagrams above represent the same thing. The latter, obviously, looks more succinct than the former. That‚Äôs because, with our RNN visual notation, each neuron (inputs, hidden(s), and outputs) contains a vector of information. The term ‚Äúcell‚Äù is also used, and is interchangeable with neuron. (I‚Äôll use the latter instead of the former.) Red is the input neuron, blue is the hidden neuron, and green is the output neuron. Therefore, an entire ANN layer is encapsulated into one neuron with our RNN illustration. All operations in RNNs, like the mapping from one neuron‚Äôs state to another, are over entire vectors, compared to individual scalars that are summed up with ANNs.","markups":[{"type":1,"start":211,"end":218}]},{"name":"abee","type":1,"text":"Let‚Äôs flip it the other way:","markups":[]},{"name":"535f","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*HewvhrMCcdy-oQcpeeNJ9w.png","originalWidth":50,"originalHeight":231}},{"name":"8a15","type":1,"text":"This is in fact a type of recurrent neural network ‚Äî a one to one recurrent net, because it maps one input to one output. A one to one recurrent net is equivalent to an artificial neural net.","markups":[{"type":1,"start":55,"end":65}]},{"name":"b960","type":1,"text":"We can have a one to many recurrent net, where one input is mapped to multiple outputs. An example of this would be image captioning ‚Äî the input would be the image in some processed form (usually the result of a CNN analyzing the image), and the output would be a sequence of words. Such an RNN may look like this:","markups":[{"type":2,"start":21,"end":25}]},{"name":"7fff","type":4,"text":"Changed the shades of the green nodes‚Ä¶ hope that‚Äôs OK!","markups":[],"layout":1,"metadata":{"id":"1*-Jv3TxauJBwBgWwjoe_UkA.png","originalWidth":190,"originalHeight":230}},{"name":"ea3c","type":1,"text":"This may be confusing at first, so I‚Äôm going to make sure I walk slowly through it. On the x-axis we have time, and on the y-axis we have depth/layers:","markups":[]},{"name":"c4eb","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*OEyIsiEi5SJ9l3GrB5DpuA.png","originalWidth":323,"originalHeight":287}},{"name":"22b2","type":1,"text":"When I refer to ‚Äútime‚Äù on the x-axis, I‚Äôm referring to the order at which these operations occur. Time could also be literal for temporal data, where the input is a sequence. When I say ‚Äúdepth‚Äù on the y-axis, I‚Äôm referring to the mapping from the input layer, to the hidden layer(s), to the output layer, where layer number and thus depth increases.","markups":[]},{"name":"f94b","type":1,"text":"It may look like we have seven neurons now, but we still have three: one input neuron, one hidden neuron, and one output neuron. The difference is that these neurons now experience multiple ‚Äútimesteps‚Äù where they take on different values, which are, again, vectors. The input neuron in our example above doesn‚Äôt, because it‚Äôs not representing sequential data (one to many), but for other architectures it could.","markups":[]},{"name":"e200","type":1,"text":"The hidden neuron will take on the vector value h_1 first, then h_2, and finally h_3. At each timestep, the hidden neuron‚Äôs vector h_t is a function of the vector at the previous timestep h_t-1, except for h_1 which is dependent only on the input x_1. In the diagram above, each hidden vector then gives rise to an output y_t, and this is how we map one input to multiple outputs. You can visualize these functional dependencies with the arrows, which illustrates flow of information in the network.","markups":[{"type":1,"start":48,"end":51},{"type":1,"start":64,"end":67},{"type":1,"start":81,"end":84},{"type":1,"start":131,"end":135},{"type":1,"start":188,"end":193},{"type":1,"start":206,"end":210},{"type":1,"start":247,"end":250},{"type":1,"start":322,"end":325},{"type":2,"start":229,"end":233}]},{"name":"64eb","type":1,"text":"As we progress on the x-axis, the current timestep increases. As we progress on the y-axis, the neuron in question changes. Each point on this graph thus represents one neuron ‚Äî be it input, hidden, or output ‚Äî at some timestep, being fed information from a neuron (be it itself or another) at the previous timestep.","markups":[]},{"name":"5402","type":1,"text":"The RNN would execute like so:","markups":[]},{"name":"3623","type":10,"text":"Input x_1","markups":[{"type":1,"start":6,"end":9}]},{"name":"1923","type":10,"text":"Compute h_1 based on x_1 (the arrow implies functional dependency)","markups":[{"type":1,"start":8,"end":11},{"type":1,"start":21,"end":24}]},{"name":"00ff","type":10,"text":"Compute h_2 based on h_1","markups":[{"type":1,"start":8,"end":11},{"type":1,"start":21,"end":24}]},{"name":"ca4c","type":10,"text":"Compute h_3 based on h_2","markups":[{"type":1,"start":8,"end":11},{"type":1,"start":21,"end":24}]},{"name":"9fac","type":10,"text":"Compute y_1 based on h_1","markups":[{"type":1,"start":8,"end":11},{"type":1,"start":21,"end":24}]},{"name":"649b","type":10,"text":"Compute y_2 based on h_2","markups":[{"type":1,"start":8,"end":11},{"type":1,"start":21,"end":24}]},{"name":"ba63","type":10,"text":"Compute y_3 based on h_3","markups":[{"type":1,"start":8,"end":11},{"type":1,"start":21,"end":24}]},{"name":"bba5","type":1,"text":"You could compute y_t either immediately after h_t has been computed, or, like above, compute all outputs once all hidden states have been computed. I‚Äôm not entirely sure which is more common in practice.","markups":[{"type":1,"start":18,"end":22},{"type":1,"start":47,"end":51}]},{"name":"6c68","type":1,"text":"This allows for more complex and interesting networks than ANNs because we can have as many timesteps as we want.","markups":[]},{"name":"e78f","type":1,"text":"The value of the output neuron at each timestep represents a word in the sentence, in the order the sentence will be constructed. The caption this RNN produces is hence 3 words long. (It‚Äôs actually 2, because the RNN would need to output a period or \x3cEND\x3e marker at the final timestep, but we‚Äôll get into that later.)","markups":[]},{"name":"a009","type":1,"text":"In case you don‚Äôt understand yet exactly why RNNs work, I‚Äôll walk through how these functional dependencies come to fruition when you apply it to a one to many scenario such as image captioning.","markups":[{"type":2,"start":41,"end":45}]},{"name":"2856","type":4,"text":"Lenny and I on student scholarship at WWDC 2013. Good times!","markups":[],"layout":1,"metadata":{"id":"1*iBtLegQFwfsqWZpTVAjrEw.jpeg","originalWidth":1298,"originalHeight":782}},{"name":"6b74","type":1,"text":"When you combine an RNN and CNN, you ‚Äî in practice ‚Äî get an ‚ÄúLCRN‚Äù. The architecture for LCRNs are more complex than what I‚Äôm going to present in the next paragraph; rather, I‚Äôm going to simplify it to convey my point. We‚Äôll actually get fully into how they work later.","markups":[]},{"name":"e029","type":1,"text":"Imagine an RNN tries to caption this image. An accurate result might be:","markups":[]},{"name":"f14e","type":7,"text":"Two people happily posing for a photo inside a building.","markups":[]},{"name":"482a","type":1,"text":"The input to the RNN would be the output of a CNN that processes this image. (However, to be pedantic, it would be the output of the CNN without a classification/softmax layer ‚Äî that is, pulled from the final fully connected layer.) The CNN might pick up on the fact that there are two primary human face-like objects present in the image, which, paired with what the RNN has learned via training, may induce the first hidden state¬π of the recurrent neural network to be one where the most likely candidate word is ‚Äútwo‚Äù.","markups":[{"type":1,"start":431,"end":432}]},{"name":"f5ec","type":1,"text":"Pro-tip¬π: The term ‚Äúhidden state‚Äù refers to the vector of a hidden neuron at a given timestep. ‚ÄúFirst hidden state‚Äù refers to the hidden state at timestep 1.","markups":[{"type":1,"start":7,"end":8}]},{"name":"10db","type":1,"text":"The first output, which represents the word ‚Äútwo‚Äù, was functionally dependent on the first hidden state, which in itself was a function of the input to the RNN. Thus, ‚Äútwo‚Äù was ultimately determined from the information that the CNN gave us and the experience/weights of the RNN. Now, the second word, ‚Äúpeople‚Äù, is functionally dependent on the second hidden state. However, note that the second hidden state is just a function of the first hidden state. This means that the word ‚Äúpeople‚Äù was the most likely candidate given the hidden state where ‚Äútwo‚Äù was likely. In other words, the RNN recognized that, given the word ‚Äútwo‚Äù, the word ‚Äúpeople‚Äù should be next, based on the RNN‚Äôs experience from training and the initial image [analysis] we inputted.","markups":[{"type":2,"start":435,"end":441}]},{"name":"b004","type":1,"text":"The same will occur for every following word; the nth word will be based on the nth hidden state, which, ultimately, is a function of every hidden state before it, and thus could be interpreted purely as an extremely complex and layered function of the input. The weights do the heavy lifting by making sense of all this information and deducing an output from it.","markups":[]},{"name":"d51d","type":1,"text":"To put it bluntly, you can boil down what the RNN is ‚Äúthinking‚Äù to this:","markups":[]},{"name":"4a94","type":6,"text":"Based on what I‚Äôve seen from the input, based on the current timestep I‚Äôm at, and based on what I know from all my training, I need to output: ‚Äúx‚Äù.","markups":[{"type":1,"start":143,"end":147}]},{"name":"a918","type":1,"text":"Thus, each outputted word is dependent on the words before it, all the way back to the input image data. However, this relationship is indirect. It‚Äôs indirect because the outputs are only dependent on the hidden states, not on each other (ie. the RNN doesn‚Äôt deduce ‚Äúpeople‚Äù from ‚Äútwo‚Äù, it deduces ‚Äúpeople‚Äù, partly, from the information ‚Äî the hidden state ‚Äî that gave rise to ‚Äútwo‚Äù). In LCRNs, though, this is explicit instead of implicit; we ‚Äúsample‚Äù the output of one timestep by taking it and literally feeding it back as input into the next timestep. In a sense, LCRNs can hence be interpreted as having many to many architecture.","markups":[{"type":2,"start":220,"end":224},{"type":2,"start":363,"end":372}]},{"name":"cdc3","type":1,"text":"The exact quantitative relationships depend on the RNN‚Äôs weights. But, generally, this is the concept of memory in play. Creating a coherent sentence as we go along is only really possible if we can recall what we said before. And RNNs are able to do exactly that; they remember what they said before and figure out, based on their image captioning expertise, what from this is useful to continue accurately speaking.","markups":[]},{"name":"0d89","type":6,"text":"Yep, I went to France for a holiday. And I actually learned to speak some \x3cwait, shit, what was the language again? oh yea, ‚ÄúFrance‚Äù‚Ä¶\x3e French!","markups":[]},{"name":"5857","type":1,"text":"Obviously, an RNN needs to be trained and have proper weights for this to all function properly. RNNs aren‚Äôt magic; they only work because trained networks identified and learned patterns in data during training time that they now look for during prediction.","markups":[]},{"name":"34be","type":1,"text":"Perhaps this was a bit over-explaining on my part, but hopefully I nailed down some important and core ideas about how RNNs function.","markups":[]},{"name":"05f6","type":1,"text":"So far we‚Äôve looked at one to one and one to many recurrent networks. We can also have many to one:","markups":[{"type":2,"start":87,"end":98}]},{"name":"6c8d","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*KjYQyc-JD_zs5ERQypM9EA.png","originalWidth":190,"originalHeight":228}},{"name":"da0a","type":1,"text":"With many to one (and many to many), the input is in the form of a sequence, and so the hidden states are functionally dependent on both the input at that timestep and the previous hidden state. This is different to one to many, where the hidden state after h_1 is only dependent on the previous hidden state. That‚Äôs why, in the image above, the second hidden state has two arrows directed at it.","markups":[{"type":1,"start":258,"end":262},{"type":2,"start":132,"end":137}]},{"name":"4715","type":1,"text":"Only one output exists in many to one architecture. An example application is sentiment classification, where the input is a sentence (sequence of words) and the output is a probability indicating that the inputted sentence was positive.","markups":[]},{"name":"a55f","type":1,"text":"The final type of recurrent net is many to many, where both the input and output are sequential:","markups":[]},{"name":"86b5","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*MPpLCBI1J6r6VmsDm7G4_g.png","originalWidth":310,"originalHeight":214}},{"name":"7038","type":1,"text":"A use case would be machine translation where a sequence of words in one language needs to be translated to a sequence of words in another.","markups":[]},{"name":"2538","type":1,"text":"We can also go deeper and have multiple hidden layers, and/or a greater number of timesteps:","markups":[]},{"name":"7896","type":4,"text":"We‚Äôre getting deeper and deeper!","markups":[],"layout":1,"metadata":{"id":"1*vfUWsAgW-c5hUntaPhb1aQ.png","originalWidth":313,"originalHeight":305}},{"name":"2703","type":1,"text":"Really, this could be considered as multiple RNNs. Technically, you can consider each ‚Äúhidden layer‚Äù as an RNN itself, given each neuron operates on vectors and updates through time; in ANN context, that volume of operations would be considered an entire network. So this is like stacking RNNs on top of each other. However, in this article I‚Äôll refer to it as multiple hidden layers; different papers and lecturers may take different approaches.","markups":[{"type":2,"start":36,"end":49}]},{"name":"a0dc","type":1,"text":"When we have many timesteps (usually hundreds) and multiple hidden layers, the architecture of the network becomes much more complex and interesting. One feature of this RNN, in particular, is that all the outputs, including the first, depend on not just the input up to that timestep, but all of the inputs. (You can see this because the green neuron is only introduced after the final input timestep.) If this RNN was to translate English to Chinese, the first word of translated Chinese isn‚Äôt just dependent on the first word of the inputted English; it‚Äôs dependent on the entire sentence.","markups":[{"type":2,"start":371,"end":376},{"type":2,"start":576,"end":583}]},{"name":"98d9","type":1,"text":"One way to demonstrate why this matters is to use Google Translate:","markups":[]},{"name":"af3c","type":4,"text":"One of my favorite Green Day lyrics, from the song ‚ÄúFashion Victim‚Äù on WARNING:. Side-note: Based on my experience with Google Translate in Chinese class over the last 8 years, this translation is probably off.","markups":[{"type":1,"start":19,"end":28},{"type":2,"start":197,"end":206}],"layout":1,"metadata":{"id":"1*mptNrzbgaDT3YuQL-tpEOw.png","originalWidth":680,"originalHeight":145}},{"name":"3229","type":1,"text":"Now I‚Äôll input ‚ÄúHe‚Äôs a victim‚Äù and ‚Äúof his own time‚Äù separately. You‚Äôll notice that when you join the two translated outputs, this won‚Äôt be equal to the corresponding phrase in the first translation:","markups":[]},{"name":"aef0","type":4,"text":"What happens if we break up the English into different parts, translate, and join together the translated Chinese parts?","markups":[],"layout":1,"metadata":{"id":"1*lO5oCsSXTy4Loic3SASMlw.png","originalWidth":276,"originalHeight":245}},{"name":"44aa","type":4,"text":"They‚Äôre not equal.","markups":[{"type":1,"start":0,"end":18}],"layout":1,"metadata":{"id":"1*oJRFlstyAI-MkBguW6otfA.png","originalWidth":551,"originalHeight":110}},{"name":"a661","type":1,"text":"What gives? Well, the way sentences are constructed in languages can differ in varying scenarios. Some words in English may also map to multiple different words in Chinese, depending on how it‚Äôs used. It all depends on the context and the entire sentence as a whole ‚Äî the meaning you‚Äôre trying to convey. This is the exact approach a human translator would take.","markups":[{"type":1,"start":223,"end":231}]},{"name":"fad3","type":1,"text":"Another type of many to many architecture exists where each neuron has a state at every timestep, in a ‚Äúsynchronized‚Äù fashion. Here, each output is only dependent on the inputs that were fed in during or before it. Because of this, synchronized many to many probably wouldn‚Äôt be suitable for translation.","markups":[]},{"name":"5a39","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*84IkP_dqLUfImZ5SyZLwjA.png","originalWidth":190,"originalHeight":231}},{"name":"cef1","type":1,"text":"An application for this could be video classification where each frame needs to be mapped to some sort of class or label. Interesting note ‚Äî an RNN is better at this task than CNNs are because what‚Äôs going on in a scene is much easier to understand if you‚Äôve watched the video up to that point and thus can contextualize it. That‚Äôs what humans do!","markups":[{"type":1,"start":325,"end":347}]},{"name":"f0a6","type":1,"text":"Quick note: we can ‚Äúwrap‚Äù the RNN into a much more succinct form, where we collapse the depth and time properties, like so:","markups":[]},{"name":"e5dd","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*7POP9GXAsRlbRRrsrhr-jA.png","originalWidth":190,"originalHeight":250}},{"name":"fc78","type":1,"text":"This notation demonstrates that RNNs take input, process that input through multiple timesteps and hidden layers, and produce output. The arrow both leaving and entering the RNN conveys that an RNN hidden state is functionally dependent on the hidden state at the preceding timestep; it‚Äôs sort of like a loop that feeds itself.","markups":[]},{"name":"3ba7","type":1,"text":"When you ever read about ‚Äúunrolling‚Äù an RNN into a feedforward network that looks like it‚Äôs in the same collapsed format as the diagram above, this means we expand it to show all timesteps and hidden layers like we did before.","markups":[]},{"name":"0d76","type":1,"text":"Another quick note: when somebody or a research paper mentions that they are using ‚Äú512 RNN units‚Äù, this translates to: ‚Äú1 RNN neuron that outputs a 512-wide vector‚Äù; that is, a vector with 512 values. At first, I thought this meant that maybe at each timestep there were 512 separate neurons somehow working in conjunction, but nope, it‚Äôs luckily much simpler than that‚Ä¶ albeit strangely worded.","markups":[]},{"name":"2ae1","type":1,"text":"Furthermore, one ‚ÄúRNN unit‚Äù usually refers to an RNN with one hidden layer; thus, instead of defining RNN as something that is multilayer inherently, we often see people use the phrase like: ‚Äústacking RNNs on top of each other‚Äù. Each RNN will have its on weights, but connecting them gives rise to an overarching multilayer RNN. In this article, we treat recurrent neural networks as a model that can have variable timesteps t and fixed layers ‚Ñì, just make sure you understand that this is not always the case. Our formalism, especially for weights, will slightly differ.","markups":[{"type":1,"start":425,"end":427},{"type":1,"start":444,"end":445}]},{"name":"e5b5","type":3,"text":"Formalism","markups":[]},{"name":"a93d","type":1,"text":"So, now, let‚Äôs walk through the formal mathematical notation involved in RNNs.","markups":[]},{"name":"ad6a","type":1,"text":"If an input or output neuron has a value at timestep t, we denote the vector as:","markups":[{"type":1,"start":53,"end":54}]},{"name":"a4f8","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*_D9bOLepOSSbC2zgK7wreQ.png","originalWidth":141,"originalHeight":77}},{"name":"a6fd","type":1,"text":"For the hidden neurons it‚Äôs a bit different; since we can have multiple hidden layers, we denote the hidden state vector at timestep t and hidden layer ‚Ñì as:","markups":[{"type":1,"start":133,"end":135},{"type":1,"start":152,"end":154}]},{"name":"7987","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*QJFWCdOVxGAge0ZT17hw1g.png","originalWidth":157,"originalHeight":41}},{"name":"7ba4","type":1,"text":"The input is obviously some preset values that we know. The outputs and hidden states are not; they are calculated.","markups":[{"type":2,"start":89,"end":93}]},{"name":"982f","type":1,"text":"Let‚Äôs start with hidden states. First, we‚Äôll revisit the most complex recurrent net we came across earlier ‚Äî the many to many architecture:","markups":[]},{"name":"44bd","type":4,"text":"Many to many, non-synchronized.","markups":[],"layout":1,"metadata":{"id":"1*TJxcM6GI8dMHEq6sK3Ky8Q.png","originalWidth":250,"originalHeight":206}},{"name":"5f3c","type":1,"text":"This RNN has: sequential input, sequential output, multiple timesteps, and multiple hidden layers. The formula we derive for this RNN should generalize for all others.","markups":[]},{"name":"af44","type":1,"text":"First, let‚Äôs list out the possible functional dependencies for a given hidden state, based on the arrows and flow of information in the diagram:","markups":[]},{"name":"5021","type":9,"text":"An input","markups":[]},{"name":"8970","type":9,"text":"Hidden state at the previous timestep, same layer","markups":[]},{"name":"429d","type":9,"text":"Hidden state at the current timestep, previous layer","markups":[]},{"name":"dc21","type":1,"text":"A hidden state can have two functional dependencies at max. Just by looking at the diagram, the only impossible combination is to be dependent on both the input and a hidden state at the current timestep but previous layer. This is because the only hidden states that are dependent on input exist in the first hidden layer, where no such previous layer exists.","markups":[]},{"name":"b721","type":1,"text":"If this is all difficult to follow, make sure once again to look at and trace back the arrows in the RNN that illustrate flow of information throughout the network.","markups":[]},{"name":"5a0a","type":1,"text":"Because of the impossible combination, we define two separate equations: an equation for the hidden state at hidden layer 1, and for layers after 1.","markups":[]},{"name":"8095","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*n5QR9Q9ZGnWFRf7pROtliA.png","originalWidth":400,"originalHeight":107}},{"name":"7d12","type":1,"text":"This probably looks a bit confusing; let me break it down for you. The function ∆íw computes the numeric hidden state vector for timestep t and layer ‚Ñì; it contains the ‚Äúactivation function‚Äù you‚Äôre used to hearing about with ANNs. W are the weights of the recurrent net, and thus ∆í is conditioned on W. We haven‚Äôt exactly defined ∆í just yet, but what‚Äôs important to note is the two parameters it takes. Once you do, this notation simply states what we have stated before in plain English:","markups":[{"type":1,"start":80,"end":83},{"type":1,"start":137,"end":139},{"type":1,"start":149,"end":150},{"type":1,"start":230,"end":232},{"type":1,"start":279,"end":281},{"type":1,"start":299,"end":300},{"type":1,"start":329,"end":330}]},{"name":"a42e","type":6,"text":"Where ‚Ñì = 1, the hidden state at time t and layer ‚Ñì is a function of the hidden state vector at time t-1 and layer ‚Ñì as well as the input vector at time t. Where ‚Ñì \x3e 1, this hidden state is a function of the hidden state vector at time t-1 and layer ‚Ñì as well as the hidden state vector at time t, layer ‚Ñì-1.","markups":[{"type":1,"start":6,"end":8},{"type":1,"start":38,"end":40},{"type":1,"start":50,"end":52},{"type":1,"start":101,"end":105},{"type":1,"start":115,"end":117},{"type":1,"start":153,"end":154},{"type":1,"start":162,"end":167},{"type":1,"start":236,"end":240},{"type":1,"start":250,"end":252},{"type":1,"start":295,"end":296},{"type":1,"start":304,"end":307}]},{"name":"7713","type":1,"text":"You might notice that we have a couple issues:","markups":[]},{"name":"69f7","type":9,"text":"When t = 1 ‚Äî that is, when each neuron is at the initial timestep ‚Äî then no previous timestep exists. However, we still attempt to pass h_0 as a parameter to ∆íw.","markups":[{"type":1,"start":5,"end":10},{"type":1,"start":136,"end":140},{"type":1,"start":158,"end":160}]},{"name":"7b09","type":9,"text":"If no input exists at time t ‚Äî thus, x_t does not exist ‚Äî then we still attempt to pass x_t as a parameter.","markups":[{"type":1,"start":27,"end":28},{"type":1,"start":37,"end":41},{"type":1,"start":88,"end":92}]},{"name":"2062","type":1,"text":"Our respective solutions follow:","markups":[]},{"name":"c5c1","type":9,"text":"Define h_0 for any layer as 0","markups":[{"type":1,"start":7,"end":11}]},{"name":"68f6","type":9,"text":"Consider x_t where no input exists at timestep t as 0","markups":[{"type":1,"start":9,"end":13},{"type":1,"start":47,"end":48}]},{"name":"0ed3","type":1,"text":"If these are 0, then the invalid functional dependency stops existing, and our formal notation still holds up.","markups":[]},{"name":"cf77","type":1,"text":"We actually have five different types of weight matrices:","markups":[]},{"name":"61fa","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*r09EQtFlEA1kIiOJD2aZ6g.png","originalWidth":390,"originalHeight":233}},{"name":"b66b","type":1,"text":"Pro-tip: The indices for each weight matrix tell you what they are used for in the recurrent net. W_xh maps an input vector x to a hidden state vector h. W_hht maps a hidden state vector h to another hidden state vector h along the time axis, ie. from h_t-1 to h_t. On the other hand, W_hhd maps a hidden state vector h to another hidden state vector h along the depth axis, ie. from h^(‚Ñì-1)_t to h^‚Ñì_t. W_hy maps a hidden state vector h to an output vector y.","markups":[{"type":1,"start":98,"end":103},{"type":1,"start":124,"end":126},{"type":1,"start":151,"end":152},{"type":1,"start":154,"end":160},{"type":1,"start":187,"end":188},{"type":1,"start":220,"end":222},{"type":1,"start":252,"end":258},{"type":1,"start":261,"end":264},{"type":1,"start":285,"end":291},{"type":1,"start":318,"end":320},{"type":1,"start":351,"end":353},{"type":1,"start":384,"end":394},{"type":1,"start":397,"end":402},{"type":1,"start":404,"end":409},{"type":1,"start":436,"end":438},{"type":1,"start":458,"end":459}]},{"name":"283a","type":1,"text":"Like with ANNs, we also learn and add a constant bias vector, denoted b_h, that can vertically shift what we pass to the activation function. We can also shift our outputs with b_y. More about bias units here.","markups":[{"type":3,"start":204,"end":208,"href":"https://ayearofai.com/rohan-5-what-are-bias-units-828d942b4f52","title":"","rel":"noopener","anchorType":0},{"type":1,"start":70,"end":73},{"type":1,"start":177,"end":180}]},{"name":"246f","type":1,"text":"For both b_h and W_hht/W_hhd, we actually have multiple weight matrices depending on the value of ‚Ñì, as indicated by the superscript. This is because each hidden layer can have a different set of weights (the network would be extremely uninteresting if this wasn‚Äôt the case), including the bias vector. However, inside a single hidden layer, all timesteps share the same weight matrix. This is important because the number of timesteps is a variable; we may train on sequences with up to 20 values, but in practice output sequences with up to 30 values ‚Äî 10 extra timesteps. If each timestep had an independent weight to learn, those last 10 timesteps wouldn‚Äôt have anything to use.","markups":[{"type":1,"start":9,"end":13},{"type":1,"start":17,"end":28},{"type":1,"start":98,"end":99}]},{"name":"6da9","type":1,"text":"W_hy is just one matrix because only the final layer gives rise to the outputs denoted y. At the final hidden layer ‚Ñì, we could suggest that W_hhd will not exist because W_hy will be in its place.","markups":[{"type":1,"start":0,"end":5},{"type":1,"start":87,"end":88},{"type":1,"start":116,"end":117},{"type":1,"start":141,"end":147},{"type":1,"start":170,"end":175}]},{"name":"e3ed","type":1,"text":"Now we‚Äôll define the function ∆íw:","markups":[{"type":1,"start":30,"end":32}]},{"name":"d692","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*9YGuqXdNiknmZR2HScMDKw.png","originalWidth":404,"originalHeight":226}},{"name":"e06a","type":1,"text":"The function is very similar to the ANN hidden function you‚Äôve seen before; it applies the correct weights to the corresponding parameters, adds the bias, and passes this weighted sum through an activation or ‚Äúsquashing‚Äù function to introduce non-linearities. The key difference, though, is that this is not a weighted sum but rather a weighted sum vector; any W ‚ãÖ h, along with the bias, will have the dimensions of a vector. The tanh function will thus simply output a vector where each value is the tanh of what it was in the inputted vector (sort of like an element-wise tanh). Remember, this contrasts ANNs because RNNs operate over vectors versus scalars.","markups":[{"type":1,"start":361,"end":366},{"type":1,"start":388,"end":389},{"type":1,"start":431,"end":436},{"type":1,"start":506,"end":507},{"type":2,"start":322,"end":323},{"type":2,"start":349,"end":355}]},{"name":"d904","type":1,"text":"If you‚Äôve followed our blog so far, you most likely know about two activation functions: sigmoid and ReLU. tanh is another such function. We mostly use the tanh function with RNNs. This is, I think, mostly because of their role in LSTMs (a variant of RNNs that are used more than RNNs ‚Äî more on that later), the fact that they produce gradients with a greater range, and that their second derivative don‚Äôt die off as quickly.","markups":[]},{"name":"25ba","type":1,"text":"Similar to sigmoid, the tanh function has two horizontal asymptotes and a smooth S-shape. The main difference is that the tanh function asymptotes at y = -1 instead of y = 0, intercepting the y-axis at y = 0 instead of y = 0.5. Thus, the tanh function has a greater range than the sigmoid.","markups":[]},{"name":"0f69","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*NPI9iLLVlYLQ2gu9A9xp0A.png","originalWidth":324,"originalHeight":166}},{"name":"9d21","type":1,"text":"If interested, the tanh equation follows (though I won‚Äôt walk you through it):","markups":[]},{"name":"a172","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*w7LV9vY1hCAXcLk2K_peEg.png","originalWidth":241,"originalHeight":84}},{"name":"85eb","type":1,"text":"The final equation is mapping a hidden state to an output.","markups":[]},{"name":"f531","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*n7simJp73WxCRx_Bz4dXwg.png","originalWidth":177,"originalHeight":44}},{"name":"93b1","type":1,"text":"This is one such possible equation. Depending on the context, we might also remove the bias vector, apply a non-linearity like sigmoid/softmax (for example if the output needs to be a probability distribution), etc.","markups":[{"type":2,"start":8,"end":11}]},{"name":"3584","type":1,"text":"And that‚Äôs how we express recurrent nets, mathematically!","markups":[]},{"name":"2650","type":1,"text":"Quick note: Notation may and will differ between various lectures, research paper, articles, etc. For example ‚Äî some research papers may start indexing at 0 instead of 1. More drastically, most RNN notation is much more general than mine to promote simplicity, ie. doesn‚Äôt cover edge cases like I did or obfuscates certain indices like ‚Ñì with hidden to hidden weight matrices. So, just keep note that specifics don‚Äôt always transfer over and avoid being confused by this. The reason I was meticulous about notation in this article is that I wanted to ensure you understood exactly how RNNs work, fueled by my frustration with the very same problem ~1.5 years ago.","markups":[{"type":1,"start":336,"end":338},{"type":2,"start":210,"end":215}]},{"name":"ed09","type":3,"text":"An example? Okay!","markups":[]},{"name":"e53b","type":1,"text":"Let‚Äôs take a look at a quick example of an RNN in action. I‚Äôm going to adapt a super dumbed down one from Andrej Karpathy‚Äôs Stanford CS231n RNN lecture, where a one to many ‚Äúcharacter level language model‚Äù single layer recurrent neural network needs to output ‚Äúhello‚Äù. We‚Äôll kick it of by giving the RNN the letter ‚Äúh‚Äù , such that it needs to complete the word by outputting the other four letters.","markups":[{"type":3,"start":140,"end":151,"href":"https://www.youtube.com/watch?v=cO0a0QYmFm8&index=10&list=PLlJy-eBtNFt6EuMxFYRiNRS07MCWN5UIA","title":"","rel":"noopener","anchorType":0}]},{"name":"1901","type":1,"text":"Sidenote: this model nicknamed ‚Äúchar-rnn‚Äù ‚Äî remember it for later, where we get to code our own!","markups":[]},{"name":"fc9c","type":1,"text":"The neural network has the vocabulary: h, e, l , o. That is, it only knows these four characters; exactly enough to produce the word ‚Äúhello‚Äù. We will input the first character, ‚Äúh‚Äù, and from there expect the output at the following timesteps to be: ‚Äúe‚Äù, ‚Äúl‚Äù, ‚Äúl‚Äù, and ‚Äúo‚Äù respectively, to form:","markups":[]},{"name":"56af","type":7,"text":"hello","markups":[]},{"name":"9f5f","type":1,"text":"We can represent input and output via one hot encoding, where each character is a vector with a 1 at the corresponding character position and otherwise all 0s. For example, since our vocabulary is [h, e, l, o], we can represent characters using a vector with four values, where a 1 in the first, second, third, and fourth position would represent ‚Äúh‚Äù, ‚Äúe‚Äù, ‚Äúl‚Äù, and ‚Äúo‚Äù respectively.","markups":[]},{"name":"28cd","type":4,"text":"This is called ‚Äúone-hot encoding‚Äù, because only one of the values in the vector is equal to 1 and thus on (or ‚Äúhot‚Äù).","markups":[],"layout":1,"metadata":{"id":"1*pgWSPyximAFHqZtUkiLeKg.png","originalWidth":440,"originalHeight":192}},{"name":"c682","type":1,"text":"This is what we‚Äôd expect with a trained RNN:","markups":[]},{"name":"0f80","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*mmuQb8msqqQLtz580_lpvw.png","originalWidth":250,"originalHeight":227}},{"name":"17a3","type":1,"text":"As you can see, we input the first letter and the word is completed. We don‚Äôt know exactly what the hidden states will be ‚Äî that‚Äôs why they‚Äôre hidden!","markups":[]},{"name":"04ad","type":1,"text":"One interesting technique would be to sample the output at each timestep and feed it into the next as input:","markups":[]},{"name":"9eb9","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*KyVSttLGcexQWDLvSWD0Lg.png","originalWidth":250,"originalHeight":226}},{"name":"08da","type":1,"text":"When we ‚Äúsample‚Äù from a distribution, we select a random character probabilistically following the distribution. For example, in the diagram above, the character with the highest likeliness is ‚Äúe‚Äù at the first timestep‚Äôs output. Let‚Äôs say this likeliness is, concretely, 0.9. Now, when we sample into the next timestep‚Äôs input, there‚Äôs a 90% chance we select ‚Äúe‚Äù; most of the time we will pick the most likely character, but not every time. This adds a level of randomness so you don‚Äôt end up in a loop where you keep sampling the same letter or sequence of letters over and over again.","markups":[{"type":2,"start":364,"end":368}]},{"name":"0880","type":1,"text":"As mentioned earlier, this is used pretty heavily with LCRNs. It‚Äôs even more effective than only relying on the memory of the RNN to output the correct letter at the future timesteps. In a sense, this makes the recurrent net many to many. (Though, not really, because we still only have one preset input.)","markups":[]},{"name":"9cac","type":1,"text":"However, to be clear, this does not mean that the RNN can only rely on these sampled inputs. For example, at timestep 3 the input is ‚Äúl‚Äù and the expected output is also ‚Äúl‚Äù. However, at timestep 4, the input is again ‚Äúl‚Äù but the output is now ‚Äúo‚Äù, to complete the word. Memory is still needed to make a distinction like this.","markups":[{"type":2,"start":58,"end":63}]},{"name":"db16","type":1,"text":"In numerical form, it would look something like this:","markups":[]},{"name":"5acf","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*AguGRuRZg6e6RZ7Ctvn2Ww.png","originalWidth":280,"originalHeight":371}},{"name":"c815","type":1,"text":"Of course, we won‚Äôt get a one-hot vector output during prediction mode; rather, we will get a probability distribution over each letter (so we‚Äôd apply softmax to the output), and will sample from this distribution to get a single character output.","markups":[]},{"name":"afba","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*6067M6Oqz2zNoyyyQC1Suw.png","originalWidth":300,"originalHeight":574}},{"name":"9419","type":1,"text":"Each hidden state would contain a similar sort of vector, though not necessarily something we could interpret like we can for the output.","markups":[]},{"name":"58bf","type":1,"text":"The RNN is saying: given ‚Äúh‚Äù, ‚Äúe‚Äù is most likely to be the next character. Given ‚Äúhe‚Äù, ‚Äúl‚Äù is the next likely character. With ‚Äúhel‚Äù, ‚Äúl‚Äù should be next, and with ‚Äúhell‚Äù, the final character should be ‚Äúo‚Äù.","markups":[]},{"name":"b7ec","type":1,"text":"But, if the neural network wasn‚Äôt trained on the word ‚Äúhello‚Äù, and thus didn‚Äôt have optimal weights (ie. just randomly initialized weights), then we‚Äôd have garble like ‚Äúhleol‚Äù coming out.","markups":[]},{"name":"399a","type":1,"text":"One more important thing to note: start and end tokens. They signify when input begins and when output ends. For example, when the final character is outputted (‚Äúo‚Äù), we can sample this back as input and expect that the ‚Äú\x3cEND\x3e‚Äù token (however we choose to represent it ‚Äî could also use a period) will be outputted at the next timestep; this is the RNN telling us that it has completed the word and its processing as a whole. The use case isn‚Äôt as obvious in this fabricated example, because we know when ‚Äúhello‚Äù has been completed, but consider a real-life scenario where we don‚Äôt: image captioning. In image captioning, the caption could be 1, 2, 3, or n words long, given a reasonable upper limit of n. The end token tells us when the caption has been completed, so we can halt the RNN and complete the prediction loop (which would keep going forever if we were using while or stop after the upper limit/max possible preset constant value of n is reached).","markups":[{"type":10,"start":870,"end":875},{"type":1,"start":34,"end":54}]},{"name":"a127","type":1,"text":"Start tokens are more used for generating content from complete scratch. For example, imagine an RNN read and learned from a bunch of Shakespeare. (This is an actual funny application of character level language models that Karpathy implemented, and we‚Äôll see it in action on a later section.) Now, based on what the RNN learned, we want it to create a brand new Shakespearean sonnet! Feeding in a ‚Äú\x3cSTART\x3e‚Äù token enables it to kick this process off and begin writing without us giving the network some arbitrary pre-determined initial word or character.","markups":[]},{"name":"85be","type":1,"text":"I‚Äôve also noticed that another potential use case of start tokens is when we have some other sort of initial input, like CNN produced image data with image captioning, that doesn‚Äôt ‚Äúfit‚Äù what we‚Äôll normally use for input at timesteps after t=1 (the word outputted at the previous timestep via sampling). As a result, we feed this data directly to the first hidden state and set the input as ‚Äú\x3cSTART\x3e‚Äù instead.","markups":[{"type":1,"start":240,"end":244},{"type":2,"start":101,"end":109}]},{"name":"2aca","type":1,"text":"Now, just to be clear, the RNN doesn‚Äôt magically output these end tokens and recognize the start tokens. We have to add them, along with start tokens, to the training data and vocabulary such that they can be outputted by the recurrent net during prediction time.","markups":[]},{"name":"1c00","type":1,"text":"This is how we can get RNNs to ‚Äúwrite‚Äù! More on some examples of text RNNs have actually generated, Shakespeare most certainly included, in a later section.","markups":[]},{"name":"6b00","type":3,"text":"Training (or, why vanilla RNNs suck.)","markups":[]},{"name":"58b5","type":1,"text":"For a recurrent net to be useful, it needs to learn proper weights via training. That‚Äôs no surprise.","markups":[]},{"name":"8a33","type":1,"text":"Recall this snippet from earlier:","markups":[]},{"name":"f72f","type":6,"text":"But, if the neural network wasn‚Äôt trained on the word ‚Äúhello‚Äù, and thus didn‚Äôt have optimal weights (ie. just randomly initialized weights), then we‚Äôd have garble like ‚Äúhleol‚Äù coming out.","markups":[]},{"name":"f9d1","type":1,"text":"This is, of course, because we initialize the W weights randomly at first, so random stuff will come out.","markups":[{"type":1,"start":46,"end":48}]},{"name":"c7a6","type":1,"text":"But, through multiple iterations of training with a first-order optimization algorithm like gradient descent, we perturb the weights such that the probability of each correct character being outputted at their respective timestep increases. The actual output would be ‚Äúhello‚Äù in one-hot encoding form, and we‚Äôd compute the discrepancy between this output and what the recurrent net predicts (we‚Äôd get the error at each timestep and then add this up) as the total error to then calculate the gradient/update value.","markups":[]},{"name":"2ded","type":1,"text":"So, each output contributes to the error somehow. If the error is an addition of the outputs, then, if we had something like Y outputs, we‚Äôd need to backpropagate them individually and add these up. This is because derivatives are distributed evenly when we‚Äôre differentiating a sum:","markups":[{"type":1,"start":125,"end":126}]},{"name":"6145","type":4,"text":"For any arbitrary weight W.","markups":[{"type":1,"start":25,"end":26}],"layout":1,"metadata":{"id":"1*d5mzuu-EmcZz0IFukW6XsQ.png","originalWidth":651,"originalHeight":79}},{"name":"bb6e","type":1,"text":"But, you should know that, with artificial neural networks, calculating these gradients isn‚Äôt that easy. We have so many weights contributing to the output, and thus need to figure out exactly how much these weights contribute, and by how much we modify them to decrease overall error. To do this, we use the backpropagation algorithm; this algorithm propagates the error between the predicted output of a recurrent net and the actual output in the dataset all the way back to the beginning of the network. Using the chain rule from differential calculus, backprop helps us calculate the gradients of the output error w.r.t. each individual weight (sort of like the error of each individual weight).","markups":[]},{"name":"c4f4","type":1,"text":"Once we have those gradients, we have to use an optimization algorithm to calculate the update values and make the updates. We can use the vanilla gradient descent algorithm to do this, but there are many other possible, better variants as well; learn about them by reading this article, if you want. (I think we‚Äôre long overdue for our own mega-post on optimization!)","markups":[{"type":3,"start":274,"end":286,"href":"http://sebastianruder.com/optimizing-gradient-descent/","title":"","rel":"noopener","anchorType":0}]},{"name":"0be4","type":1,"text":"Backpropagation with RNNs is called ‚ÄúBackpropagation Through Time‚Äù (short for BPTT), since it operates on sequences in time. But don‚Äôt be fooled ‚Äî there‚Äôs not much difference between normal backprop and BPTT; when it comes down to it, BPTT is just backprop, but on RNNs! Remember that when you ‚Äúunroll‚Äù an RNN, it essentially becomes a feedforward network; not an ANN, but a feedforward network in the sense that we can visualize where all the information is flowing and observe the activations at each neuron and timestep, all the way from the input to the final output. Like ANNs, RNNs have functional dependencies that link the entire network together; it‚Äôs just that RNNs operate over vectors instead (yay for matrix calculus?) and extend in depth as well as time. There‚Äôs more work to do to compute the gradients, but it‚Äôs no surprise that backprop works pretty much the same way for recurrent nets that it would for normal ones. Because of this, I‚Äôm not going to walk through all the math and show the derivatives etc. Read our backprop mega-post for all that jazz.","markups":[{"type":2,"start":752,"end":763}]},{"name":"224b","type":1,"text":"One thing to note is that, since we have multiple timesteps in our RNN, each timestep in a single layer will want to change the weight in a different way and have different gradients. However, remember that each hidden layer uses only one weight matrix because the number of timesteps is a variable. Thus, we just average or sum the weight updates between these timesteps and apply this as an update to the W_hh for that entire layer. Also, a general practice is to train on shorter sequences first and then gradually increase sequence size as we train on more and more data.","markups":[{"type":1,"start":407,"end":412}]},{"name":"1fee","type":1,"text":"Now, if you haven‚Äôt already, make sure to read this article that I wrote on vanishing and exploding gradients before proceeding:","markups":[]},{"name":"c672","type":14,"text":"Rohan #4: The vanishing gradient problem\nOh no ‚Äî an obstacle to deep learning!ayearofai.com","markups":[{"type":3,"start":0,"end":91,"href":"https://ayearofai.com/rohan-4-the-vanishing-gradient-problem-ec68f76ffb9b","title":"https://ayearofai.com/rohan-4-the-vanishing-gradient-problem-ec68f76ffb9b","rel":"","anchorType":0},{"type":1,"start":0,"end":40},{"type":2,"start":41,"end":78}],"mixtapeMetadata":{"mediaResourceId":"bb894bd0a8e1cfee65aea593ec3751b3","thumbnailImageId":"1*m2gDBT_nc-iE7R4AM3sHBQ.jpeg","href":"https://ayearofai.com/rohan-4-the-vanishing-gradient-problem-ec68f76ffb9b"}},{"name":"be11","type":1,"text":"You may be thinking: how does this issue apply to RNNs? Well, RNNs are very deep models; on top of often having multiple hidden layers, each hidden layer in practice can have hundreds of timesteps. That‚Äôs like an ANN with hundreds of entire hidden layers! That‚Äôs deep. (Well, it‚Äôs more long because we‚Äôre dealing with the time axis here, but you know what I mean.) tanh derivatives are very similar to sigmoid derivatives in range, so the problem of vanishing gradients is thus even more drastic with RNNs than with ANNs, and training them becomes almost impossible.","markups":[{"type":2,"start":175,"end":183},{"type":2,"start":263,"end":267},{"type":2,"start":268,"end":269},{"type":2,"start":286,"end":291}]},{"name":"4878","type":1,"text":"Imagine trying to propagate the error to the 1st timestep in an RNN with k timesteps. The derivative would look something like this:","markups":[{"type":1,"start":73,"end":74}]},{"name":"44b3","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*gKbRtQfPwGK2d7jnKZdv5w.png","originalWidth":346,"originalHeight":79}},{"name":"0e72","type":1,"text":"With a tanh activation function, that‚Äôs freaking crazy. Then, for getting the derivative of the error with respect to a weight matrix W_hh, we‚Äôd add ‚Äî or, as mentioned before, we could average as well ‚Äî each of these hidden state error gradients, then multiplied by the derivative of the hidden state with respect to the weight, such that we can backprop from the error to the weight:","markups":[{"type":1,"start":134,"end":138}]},{"name":"0639","type":4,"text":"Assuming our sequence is of length k.","markups":[{"type":1,"start":35,"end":36}],"layout":1,"metadata":{"id":"1*jf52uXcsAX6Nn8ghLYoJWQ.png","originalWidth":773,"originalHeight":80}},{"name":"9baa","type":1,"text":"So we‚Äôd be effectively adding together a bunch of terms that have vanished ‚Äî the exception being very late gradients with a small number of terms ‚Äî and so dJ/dWhh would only capture gradient signals from the last few timesteps. (Or, for exploding gradients, it would become infinity).","markups":[{"type":1,"start":155,"end":163}]},{"name":"fd7e","type":1,"text":"But, you might be asking, instead of tanh ‚Äî which is bounded between -1 and 1, and has a similar problem to sigmoid where the peak of the derivative is smaller than 1 ‚Äî why don‚Äôt we just use ReLUs? Don‚Äôt ReLUs, or perhaps leaky ReLUs, solve the vanishing gradient problem?","markups":[]},{"name":"6a76","type":1,"text":"Well, not entirely; it‚Äôs not enough to solve the problem. With RNNs, the problem really lies in the architecture. Even though we could use ReLU to ensure many of the values in the gradient computation are not between -1, 0, and 1 such that they vanish ‚Äî or vice-versa, explode ‚Äî we do still indeed have a lot of other variables other than the activation function derivative in the gradient computation such as the weights; you can revisit the mega-post on backprop we wrote to confirm this. Since weights are also normally randomly initialized in the range -1 to 1, and RNNs are like super deep ANNs, these weights keep multiplying on top of each other and potentially cause the gradients to vanish.","markups":[]},{"name":"4faf","type":1,"text":"This is more my suspicion though ‚Äî I‚Äôm yet to confirm this is the case by testing. I was curious so I asked this exact question on Quora:","markups":[]},{"name":"6eaa","type":11,"text":"","markups":[],"layout":1,"iframe":{"mediaResourceId":"aa3fb6f891aba6d55742cf7dabf3f7f7","iframeWidth":560,"iframeHeight":560,"thumbnailUrl":"https://i.embed.ly/1/image?url=https%3A%2F%2Fwww.quora.com%2Fstatic%2Fimages%2Flogo%2Fwordmark_default.png&key=4fce0568f2ce49e8b54624ef71a8a5bd"}},{"name":"e537","type":1,"text":"From this, something interesting I learned is that: since ReLUs are unbounded (it‚Äôs not restricted to be between -1 and 1 or 0 and 1) unlike sigmoid/tanh, and RNNs are very deep, the activations, especially later ones, can become too big. This is because hidden states have a multiplicative relationship; one hidden state is a multiple of the previous ones, where that multiple specifically is a weight. If we use ReLU, then the hidden state isn‚Äôt limited by any range, and we could have a bunch of numbers bigger than 1 multiplying by each other.","markups":[{"type":2,"start":68,"end":70}]},{"name":"b4fc","type":1,"text":"It ends up being sort of like the exploding gradient problem, but with the values inside the neurons, not gradients. This is also what then causes the gradients to explode: large activations ‚Üí large gradients ‚Üí large change in weights ‚Üí even bigger activations, because updating the weights in the wrong direction ever so slightly can cause the entire network to explode. This makes learning unstable:","markups":[{"type":2,"start":135,"end":139}]},{"name":"2c04","type":6,"text":"This means that the computation within the RNN can potentially blow up to infinity without sensible weights. This makes learning VERY unstable because a slight shift in the weights in the wrong direction during backprop can blow up the activations during the forward pass. So that‚Äôs why you see most people using sigmoid/tanh units, despite the vanishing gradient descent problem.","markups":[]},{"name":"4f2a","type":1,"text":"Also well said:","markups":[]},{"name":"e86a","type":6,"text":"With RNN‚Äôs, the problem is that you are repeatedly applying your RNN to itself, which tends to [mostly] cause exponential blowup or [rarely, but sometimes] shrinkage.","markups":[]},{"name":"4cb6","type":1,"text":"Other issues with ReLU functions are discussed in the article I wrote, and they similarly apply to RNNs. Generally speaking, though, they just don‚Äôt work that well, especially compared to other options we have. Making RNNs perform well with ReLUs is actually a pretty hot topic of research right now, but until someone figures out something genius, RNNs are a lost cause.","markups":[]},{"name":"4a29","type":1,"text":"And that‚Äôs why vanilla RNNs suck. Seriously. In practice, nobody uses them. Even if you didn‚Äôt fully grasp this section on how the vanishing and exploding gradient/activation problem is applicable to them, it doesn‚Äôt matter anyways. Because, everything you‚Äôve read up to this point so far‚Ä¶ throw it all away. Forget about it.","markups":[{"type":1,"start":15,"end":32}]},{"name":"044a","type":1,"text":"Just kidding. Don‚Äôt do that.","markups":[]},{"name":"1d2b","type":3,"text":"Fixing the problem with LSTMs (Part I)","markups":[]},{"name":"daad","type":1,"text":"You shouldn‚Äôt do that because RNNs actually aren‚Äôt a lost cause. They‚Äôre far from it. We just need to make a few‚Ä¶ modifications.","markups":[{"type":2,"start":44,"end":51}]},{"name":"9491","type":1,"text":"Enter the LSTM.","markups":[]},{"name":"32fa","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*JuC5afKk7QIntsvyEn-IFA.png","originalWidth":298,"originalHeight":170}},{"name":"731a","type":1,"text":"Makes sense, no?","markups":[]},{"name":"ea5b","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*tB6QdzunJV08wyep0ZhVMA.png","originalWidth":350,"originalHeight":357}},{"name":"03ff","type":1,"text":"How about this?","markups":[]},{"name":"0c36","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*Oin8uuuQzp_wqtHAX1yyjQ.png","originalWidth":400,"originalHeight":314}},{"name":"0d8c","type":1,"text":"OK. Clearly something‚Äôs not registering here. But that‚Äôs fine; LSTM diagrams are frikin‚Äô difficult for beginners to grasp. I too remember when I first searched up ‚ÄúLSTM‚Äù on Google to encounter something similar to the works of art above. I reacted like this:","markups":[]},{"name":"16e7","type":4,"text":"MRW first Google Image-ing LSTMs.","markups":[],"layout":1,"metadata":{"id":"1*S7ABwK33X7no_MP3epry6A.gif","originalWidth":195,"originalHeight":229}},{"name":"7cd4","type":1,"text":"In this section, I‚Äôm going to embark on a mission to design the first simple, comprehensible, and beautiful LSTM diagram. Wish me luck, because I‚Äôll probably fail.","markups":[]},{"name":"b288","type":1,"text":"With that being said, let‚Äôs dive into Long Short-Term Memory networks. (Yes, that‚Äôs what LSTM stands for.)","markups":[{"type":1,"start":38,"end":69}]},{"name":"bb27","type":1,"text":"With RNNs, the real ‚Äúsubstance‚Äù of the model were the hidden neurons; these were the units that did processing on the input, through time, to produce the outputs. Specifically, at each timestep, a hidden neuron embodies a hidden state that is computed by feeding the weighted sum vector of the input and/or previous hidden states with an added bias vector through a tanh squashing function. We can have multiple hidden neurons, each of which when unrolled forms an entire hidden ‚Äúlayer‚Äù.","markups":[]},{"name":"66c2","type":1,"text":"If you need a refresher on this, look through the ‚ÄúFormalism‚Äù section once again.","markups":[]},{"name":"cd18","type":1,"text":"With LSTMs, we still have hidden states, but they‚Äôre computed through a much more complex mechanism: LSTM cells. Instead of computing each hidden state as a direct function of inputs and other hidden states, we compute it as a function of the LSTM cell‚Äôs value (the ‚Äúcell state‚Äù) at that timestep. Each cell state is in turn functionally dependent on the previous cell state and any available input or previous hidden states. That‚Äôs right ‚Äî hidden states are computed from cell states, and cell states are (in part) computed from older and/or shallower hidden states.","markups":[{"type":2,"start":363,"end":364}]},{"name":"095f","type":1,"text":"The cell state at a specific timestep t is denoted c_t. Like a hidden state, a cell state is just a vector.","markups":[{"type":1,"start":38,"end":40},{"type":1,"start":51,"end":54}]},{"name":"e92a","type":4,"text":"For simplicity‚Äôs sake, I‚Äôve obfuscated layer index ‚Ñì.","markups":[{"type":1,"start":51,"end":52}],"layout":1,"metadata":{"id":"1*sr8XQzvr-WTNSbgWwI9qbQ.png","originalWidth":500,"originalHeight":245}},{"name":"68f5","type":1,"text":"If the diagram above seems a bit trippy, let me break it down for you.","markups":[]},{"name":"8bd2","type":1,"text":"c_t, as highlighted by the three arrows pointing towards it, has multiple potential functional dependencies. Four to be exact, though only a maximum of three can exist at once. These are:","markups":[{"type":1,"start":0,"end":3}]},{"name":"4f8f","type":9,"text":"The previous hidden state in time: h_t-1. Again, if t = 1, then this won‚Äôt exist. If it does, this would be the first arrow pointing into the left side of c_t.","markups":[{"type":1,"start":35,"end":40},{"type":1,"start":52,"end":57},{"type":1,"start":155,"end":158}]},{"name":"f283","type":9,"text":"The previous cell state: c_t-1. If t = 1, the dependency obviously won‚Äôt exist. This refers to the second arrow pointing into the left side of c_t.","markups":[{"type":1,"start":25,"end":30},{"type":1,"start":35,"end":40},{"type":1,"start":143,"end":146}]},{"name":"7237","type":9,"text":"Input at the current timestep: x_t. There may very well be no input available, for example if we are at a hidden layer ‚Ñì \x3e 1. So this dependency doesn‚Äôt always exist. When it does, it‚Äôs the arrow pointing into the bottom of c_t.","markups":[{"type":1,"start":31,"end":34},{"type":1,"start":119,"end":124},{"type":1,"start":224,"end":227}]},{"name":"83f3","type":9,"text":"The previous hidden state in depth: h^(‚Ñì-1)_t. This applies for any hidden layer ‚Ñì \x3e 1. In such case, it would ‚Äî like the input x_t ‚Äî be the arrow pointing into the bottom.","markups":[{"type":1,"start":36,"end":45},{"type":1,"start":81,"end":86},{"type":1,"start":128,"end":131}]},{"name":"2a71","type":1,"text":"Only three can exist at once because the last two are mutually exclusive.","markups":[]},{"name":"5160","type":1,"text":"From there, we pass information to the next cell state c_t+1 and compute h_t. As you can hopefully see, h_t then goes on to also influence c_t+1 (as indicated by the horizontal arrow), along with higher level cell states or final outputs (the vertical arrow).","markups":[{"type":1,"start":55,"end":60},{"type":1,"start":73,"end":76},{"type":1,"start":104,"end":107},{"type":1,"start":139,"end":145}]},{"name":"0f0f","type":1,"text":"Right now the cells are a black box‚Ä¶ literally; we know what is inputted to them and what they output, but we don‚Äôt know their internal process. So‚Ä¶ what‚Äôs inside these cells? What do they do? What are the exact computations involved? How have the equations changed?","markups":[]},{"name":"ee47","type":1,"text":"To help answer the question, I want you to imagine something: a factory. Inside this factory we have workers who perform their own tasks. Those tasks are, specifically, operating on some sort of product that runs down a conveyer belt. Think of, hell, I don‚Äôt know ‚Äî chicken nuggets! The first worker cuts an appropriately sized piece, the second worker applies egg wash, the third worker adds breadcrumbs, the fourth worker chucks it in the fryer, etc.","markups":[{"type":1,"start":220,"end":233}]},{"name":"5a97","type":4,"text":"I‚Äôm not sure what product this conveyer belt carries, but it certainly doesn‚Äôt look appetizing (or like chicken nuggets).","markups":[],"layout":1,"metadata":{"id":"1*4avrG18SFOMJGI4CpDIsoA.png","originalWidth":490,"originalHeight":600}},{"name":"02ea","type":1,"text":"You‚Äôre thinking: ‚ÄúOK Rohan, but how does this relate to LSTMs?‚Äù. Good question.","markups":[]},{"name":"9004","type":1,"text":"Basically, think of the conveyer belt as the cell state, the chicken-nugget-in-progress as information flowing through the cell, and the workers as operations we apply to this information. The final product is the finished chicken nugget ‚Äî or, the cell state value.","markups":[]},{"name":"b54e","type":4,"text":"Chicken. Nugget.","markups":[],"layout":1,"metadata":{"id":"1*qNUGFMhlnl0-mNLIVvyGAg.png","originalWidth":459,"originalHeight":95}},{"name":"bca7","type":1,"text":"The reason we use the analogy of a conveyer belt is because information can flow through a cell super super easily. It‚Äôs theoretically possible for information to speed past a single cell state without being modified at all. In fact, I think the term ‚Äòmodified‚Äô is a really strong one here. With vanilla RNNs, each hidden state takes all the information it has from before and fully transforms it by applying a function over it. LSTM cells instead take information and make minor modifications (like additions or multiplications) to it while it flows through.","markups":[{"type":1,"start":383,"end":393},{"type":1,"start":480,"end":493},{"type":2,"start":383,"end":393},{"type":2,"start":480,"end":493}]},{"name":"eca6","type":4,"text":"Ew. Vanilla RNNs.","markups":[],"layout":1,"metadata":{"id":"1*I_nQdhxdoDa7KrZBTFeHSQ.png","originalWidth":330,"originalHeight":241}},{"name":"72ea","type":1,"text":"Vanilla RNNs look something like this. And it‚Äôs why the vanishing gradient problem exists; during backprop, gradients cannot flow back past these transformations easily, because the tanh derivatives and weights chain up and multiply together and tend to zero. We then add up or average all these gradients that are basically zero, and we get zero.","markups":[]},{"name":"9b8e","type":4,"text":"LSTMs üí¶ üí¶ üí¶","markups":[],"layout":1,"metadata":{"id":"1*360GYNV8kyF5ATWefrSasA.png","originalWidth":450,"originalHeight":276}},{"name":"0a23","type":1,"text":"This is an extreme a simplification ‚Äî and I‚Äôll go on to fill in the blanks later ‚Äî but it‚Äôs sort of what an LSTM looks like. The previous timestep‚Äôs cell state value flows through and instead of transforming the information, we tweak it by adding (another vector) to it. The added term is some function ∆íw of previous information, but this is not the same function as with vanilla RNNs ‚Äî it‚Äôs heavily changed to make sense in this context (more on that soon), do more interesting things, and also reduce the vanishing gradient problem.","markups":[{"type":1,"start":303,"end":306},{"type":1,"start":343,"end":346},{"type":2,"start":240,"end":247},{"type":2,"start":343,"end":346}]},{"name":"ba34","type":1,"text":"Another neat way to think of it is like a live circuit: there are two paths where information, like current, can flow through. After the information passes through ∆íw, it‚Äôs added to the information flowing towards c_t. Thus, in equation form it could look something like this:","markups":[{"type":1,"start":163,"end":166},{"type":1,"start":214,"end":217}]},{"name":"ea17","type":4,"text":"Again‚Ä¶ sort of. We‚Äôll get into the actual equations soon. This is a good proxy to convey my point.","markups":[{"type":1,"start":35,"end":41},{"type":2,"start":41,"end":98}],"layout":1,"metadata":{"id":"1*qGqSrpJmO5h6ZGeIT7RK3w.png","originalWidth":208,"originalHeight":39}},{"name":"c99b","type":1,"text":"With a bit of substitution, we can expand this to:","markups":[]},{"name":"2268","type":4,"text":"Technically, this could expand even more, if you did some sort of recursive substitution to fully simplify the unrolled recurrence formula. You could express c_t for some large value of t as a really really really really long function of, ultimately, c_1.","markups":[{"type":1,"start":158,"end":162},{"type":1,"start":186,"end":188},{"type":1,"start":251,"end":254}],"layout":1,"metadata":{"id":"1*OZs7rDSty0VhDhzTLH4Dgg.png","originalWidth":444,"originalHeight":39}},{"name":"48be","type":1,"text":"Why is this better? Well, if you have basic differentiation knowledge, you‚Äôll know that addition distributes gradients equally. When we take the derivative of this whole expression, it‚Äôll become a long addition of the derivatives of individual terms. As Andrej Karpathy puts it, this additive interaction creates ‚Äúgradient super-highways‚Äù, where gradients can flow back super easily.","markups":[]},{"name":"740d","type":4,"text":"Look ‚Äî it‚Äôs a long conveyer belt! (In a sense, we can use this conveyor belt analogy for the whole unrolled LSTM as well. Each cell state is a subsection of the conveyer belt.)","markups":[{"type":1,"start":93,"end":112},{"type":2,"start":93,"end":112}],"layout":1,"metadata":{"id":"1*n26drGfEkc-Xnmqc2Lw7cw.png","originalWidth":2161,"originalHeight":690}},{"name":"2d87","type":4,"text":"Look ‚Äî it‚Äôs an outdated machine learning algorithm!","markups":[],"layout":1,"metadata":{"id":"1*szBIWNdr0O0doBI8rfGjzw.png","originalWidth":610,"originalHeight":193}},{"name":"68da","type":1,"text":"In the former, gradients are always added together, never multiplied. In the latter, gradients are always multiplied. Thus, in the former, when we inject a gradient at the end, it‚Äôll easily flow back all the way to the beginning. Contributions by the ∆íw function will be made to this gradient flowing on the bottom conveyer belt as well.","markups":[{"type":1,"start":251,"end":254}]},{"name":"779d","type":1,"text":"This is what gradient flow would look like:","markups":[]},{"name":"8074","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*dqOCXyepO590ORWV3VgBKw.png","originalWidth":740,"originalHeight":262}},{"name":"7ea0","type":1,"text":"Before, we discussed that when multiplicative interaction exists between gradients, the gradients either vanish (if they are mostly \x3c 1, as is usually the case for us) or explode (if they are mostly \x3e 1). Here‚Äôs some real calculus to demonstrate this:","markups":[]},{"name":"6f86","type":4,"text":"Former is akin to RNNs. Latter is akin to LSTMs.","markups":[],"layout":1,"metadata":{"id":"1*09oGK1btsVezIoMyBAwqbw.png","originalWidth":287,"originalHeight":288}},{"name":"0175","type":1,"text":"Imagine f being any sort of function, like our ∆íw. When we apply a function to itself repeatedly, the chain rule shows that the overall derivative is the multiplication of multiple different derivative terms. But, when we add functions together, the derivative is simply the addition of the individual derivatives. This won‚Äôt vanish or explode quickly, so our LSTMs won‚Äôt vanish or explode quickly. Yay!","markups":[{"type":1,"start":8,"end":10},{"type":1,"start":47,"end":49}]},{"name":"4e50","type":1,"text":"Furthermore, if some of our gradients vanish ‚Äî for whatever reason ‚Äî then it should still be OK. It won‚Äôt be optimal, but since our gradient terms add together, if some of them vanish it doesn‚Äôt mean the whole thing will vanish (versus if they were multiplied together). Look: 2 + 0 = 2 but 2 √ó 0 = 0.","markups":[]},{"name":"bbb5","type":4,"text":"A gradient super highway? Sounds good to me! http://www.dyoung.com/assets/images/Articles%20images/article4_PPH.jpg","markups":[{"type":3,"start":45,"end":115,"href":"http://www.dyoung.com/assets/images/Articles%20images/article4_PPH.jpg","title":"","rel":"nofollow noopener","anchorType":0}],"layout":1,"metadata":{"id":"1*K7rYONPTfcpCb0xTvO3ydw.jpeg","originalWidth":466,"originalHeight":240}},{"name":"a58a","type":1,"text":"So far, we haven‚Äôt really explored LSTMs. We‚Äôve more setup a foundation for them. And there‚Äôs one glaring issue with our foundation: if we just keep adding information to cell state, it could just grow and grow and grow, and essentially act as a counter that only increments. This is not very useful, and could regularly lead to explosion. We want more fine and rich control over memory. Well, worry not, because this is exactly what LSTMs are capable of doing.","markups":[{"type":2,"start":19,"end":26}]},{"name":"7ba6","type":1,"text":"LSTM cells handle memory in a very intelligent way, enabling them to learn long-term dependencies and perform well. How, exactly? Well, the cell is sort of like an internal memory state that allows for context; it ‚Äúforgets‚Äù, a.k.a. resets, information it doesn‚Äôt find useful from the previous cell state, ‚Äúwrites‚Äù in new information it does find useful from the current input and/or previous hidden state, and similarly only ‚Äúreads‚Äù out part of its information ‚Äî the good stuff ‚Äî in the computation of h_t. This respectively corresponds to the concepts of: resetting memory, writing to memory, and reading from memory. Very similar to how a modern computer system works, and we often describe an LSTM cell as a ‚Äúmemory cell‚Äù.","markups":[{"type":1,"start":502,"end":505},{"type":2,"start":336,"end":340}]},{"name":"6706","type":1,"text":"The ‚Äúwriting to memory‚Äù part is additive ‚Äî it‚Äôs what I showed you in the initial diagrams. Information flows through and we add stuff we think is relevant to it. The ‚Äúresetting memory‚Äù part is multiplicative, and occurs before writing to memory; when information from the previous cell state initially flows in, we multiply it by a vector with values between 0 and 1 to reset or retain parts of it we find useless and useful respectively. The ‚Äúreading from memory‚Äù part is also multiplicative with a similar 0‚Äì1 range vector, but it doesn‚Äôt modify the information flowing through the cell states. Rather, it modifies the information flowing into the hidden states and thus decides what the hidden state is influenced by.","markups":[]},{"name":"6813","type":1,"text":"Both of these multiplications are element wise, like so:","markups":[{"type":2,"start":34,"end":46}]},{"name":"2255","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*YuIuYxt0oYEvGMoTz_J59g.png","originalWidth":167,"originalHeight":82}},{"name":"c869","type":1,"text":"In this equation, when a = 0 the information of c is lost. This is what resetting does, and retaining is the vice versa. I also imagine that values such as 0.5 could be used to diminish the importance of certain information, but not completely wipe it out.","markups":[{"type":1,"start":23,"end":29},{"type":1,"start":48,"end":50}]},{"name":"c920","type":1,"text":"Our (unfinished) cell state computational graph now looks like this:","markups":[]},{"name":"f4a0","type":4,"text":"This is closer to what an LSTM looks like, though we‚Äôre not exactly there yet.","markups":[],"layout":1,"metadata":{"id":"1*_mbUA8vdaTbYreXpdPJccA.png","originalWidth":419,"originalHeight":158}},{"name":"ba90","type":1,"text":"Sidenote: don‚Äôt be scared whenever you see the word ‚Äúmultiplicative‚Äù and don‚Äôt immediately think of ‚Äúvanishing‚Äù or ‚Äúexploding‚Äù. It depends on the context. Here, as I‚Äôll show mathematically in a bit, it‚Äôs fine.","markups":[]},{"name":"9705","type":1,"text":"This concept in general is known as gating, because we ‚Äúgate‚Äù what can flow in and out of the LSTM cell. What we actually multiply and add by to reset, write, and read are known as the ‚Äúgates‚Äù. There are four such gates:","markups":[{"type":1,"start":36,"end":42}]},{"name":"dafe","type":9,"text":"f: forget gate. This is the ‚Äúreset‚Äù tool that wipes out, diminishes, or retains information from the previous cell state. It‚Äôs the first interaction we make, and it‚Äôs multiplicative. That is, we multiply it with the cell state. The sigmoid function is used to compute the forget gate such that its values can be in the range 0 to 1. When a value is 1, we ‚Äúremember‚Äù something, and when it is 0 we ‚Äúforget‚Äù. We might choose to forget, for example, when see a period or some sort of end of sentence marker. This is counterintuitive‚Ä¶ I guess it should really be called the ‚Äúremember gate‚Äù!","markups":[{"type":1,"start":0,"end":1},{"type":2,"start":3,"end":16},{"type":2,"start":571,"end":584}]},{"name":"aeb2","type":9,"text":"g: ?. This gate doesn‚Äôt really have a name, but it‚Äôs partly responsible for the ‚Äúwrite‚Äù process. It stores a value between -1 and 1 that represents how much we want to add to the cell state by, and represents the input to the cell state. It‚Äôs computed with the tanh function. We apply a bounded function to it such that the cell state acts as a stable counter, and it also introduces more complexity. (And it works well.)","markups":[{"type":1,"start":0,"end":1},{"type":1,"start":2,"end":3},{"type":1,"start":5,"end":6},{"type":2,"start":2,"end":6}]},{"name":"0612","type":9,"text":"i: input gate. This is the other gate responsible for the ‚Äúwrite‚Äù process. It controls how much of g we ‚Äúlet in‚Äù, and is thus between 0 and 1, computed with sigmoid. It‚Äôs similar to the forget gate in this sense, in that it blocks input like the forget gate blocks the incoming cell state. We multiply i by g and add this to the cell state. Since i is in the range 0 to 1, and g is in the range -1 to 1, we add a value between -1 and 1 to the cell state. Intuitively, this sort of acts as decrementing or incrementing the counter.","markups":[{"type":1,"start":0,"end":1},{"type":1,"start":2,"end":3},{"type":1,"start":99,"end":100},{"type":1,"start":302,"end":304},{"type":1,"start":307,"end":309},{"type":1,"start":340,"end":341},{"type":1,"start":347,"end":349},{"type":1,"start":377,"end":378},{"type":2,"start":2,"end":14},{"type":2,"start":99,"end":100},{"type":2,"start":302,"end":303},{"type":2,"start":307,"end":309},{"type":2,"start":347,"end":349},{"type":2,"start":377,"end":378}]},{"name":"57eb","type":9,"text":"o: output gate. This is also passed through sigmoid, and is a number between 0 and 1 that modulates which aspects the hidden state can draw from the cell state. It enables the ‚Äúread from memory‚Äù operation. It multiplies with the tanh of the cell state to compute the hidden state. So, I didn‚Äôt bring this up before, but the cell state leaks into a tanh before h_t is computed.","markups":[{"type":1,"start":0,"end":1},{"type":1,"start":15,"end":16},{"type":1,"start":233,"end":234},{"type":1,"start":360,"end":363},{"type":2,"start":2,"end":14},{"type":2,"start":15,"end":16},{"type":2,"start":233,"end":234}]},{"name":"5cb0","type":1,"text":"Here‚Äôs our updated computational graph for the cell state:","markups":[]},{"name":"d768","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*3xq3p-nVgxQXXPSXueVWdw.png","originalWidth":533,"originalHeight":251}},{"name":"01f6","type":1,"text":"Looks like I‚Äôm starting to create a complex diagram of my own. Damn. üòû I guess LSTMs and immediately interpretable diagrams just weren‚Äôt meant to be!","markups":[]},{"name":"7eb5","type":1,"text":"Basically, f interacts with the cell state through a multiplication. i interacts with g through a multiplication as well, the result of which interacts with the cell state through an addition. Finally, the cell state leaks into a tanh (that‚Äôs the shape of the tanh function in the circle), the result of which then interacts with o through multiplication to compute h_t. This does not disrupt the cell state, which flows to the next timestep. h_t then flows forward (and it could flow upward as well).","markups":[{"type":1,"start":11,"end":13},{"type":1,"start":69,"end":71},{"type":1,"start":86,"end":88},{"type":1,"start":330,"end":332},{"type":1,"start":366,"end":369},{"type":1,"start":443,"end":447}]},{"name":"e614","type":1,"text":"Here‚Äôs the equation form:","markups":[]},{"name":"94fe","type":4,"text":"Each gate should actually be indexed by timestep t ‚Äî we‚Äôll see why soon.","markups":[{"type":1,"start":49,"end":51}],"layout":1,"metadata":{"id":"1*B9Qd1pW1kYM_zcg0IhPfUA.png","originalWidth":222,"originalHeight":84}},{"name":"2518","type":1,"text":"As you can see, our cell state has no activation function; the activation function is simply the identity function! Yet, the cell state usually doesn‚Äôt explode ‚Äî it stays stable by ‚Äúforgetting‚Äù and ‚Äúwriting‚Äù, and does interesting things with this gating to promote context, fine control over memory, and long-term dependency learning.","markups":[]},{"name":"b30a","type":1,"text":"So, how are the gates calculated? Well, all of these gates have their own learnable weights and are functions of the last timestep‚Äôs hidden state flowing in and any current timestep inputs, not the cell state (contrary to what I may have implied earlier with the gradient flow diagrams). This should make sense when you think about it; I mean, firstly, the g and i gates literally represent input, so they better be functionally dependent on hidden states and input data! On an intuitive level, the gates help us modify the cell state, and we modify the cell state based on our current context. External stimulus that provide context should be used to compute these gates, and since context = input + hidden states our gates are functionally dependent on input and hidden states.","markups":[{"type":1,"start":357,"end":359},{"type":1,"start":363,"end":365},{"type":2,"start":133,"end":146}]},{"name":"27fb","type":1,"text":"Since every gate has a different value at each timestep, we index by timestep t just like for hidden states, cell states, or something similar.","markups":[{"type":1,"start":78,"end":80}]},{"name":"6fb5","type":1,"text":"We could generalize for multiple hidden layers as well:","markups":[]},{"name":"53d0","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*BWz6E_IFi6UTLkSNSoq1Yg.png","originalWidth":290,"originalHeight":41}},{"name":"1ae0","type":1,"text":"But, for simplicity‚Äôs sake, let‚Äôs assume we are at the first hidden layer, or that there is only one hidden layer in the LSTM. This way, we can obfuscate the ‚Ñì term and ignore influence from hidden states in the previous depth. We‚Äôll also forget about edge cases and assume input exists at the current timestep. In practice, we obviously can‚Äôt make said assumptions, but for the sake of demonstrating the equations it becomes too tedious otherwise.","markups":[{"type":1,"start":158,"end":160}]},{"name":"c042","type":1,"text":"Sidenote: we make this assumption for the rest of the discussion on LSTMs in this article.","markups":[{"type":1,"start":0,"end":90}]},{"name":"5a86","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*hP6I692iv7oc6AkcWINDaw.png","originalWidth":384,"originalHeight":180}},{"name":"033c","type":1,"text":"Like with the RNN hidden state, the index of each weight matrix is descriptive; for example, W_xf are the weights that map input x to the forget gate f. Each gate has weight matrices that map input and hidden states to itself, including biases.","markups":[{"type":1,"start":93,"end":98},{"type":1,"start":129,"end":131},{"type":1,"start":150,"end":151}]},{"name":"6989","type":1,"text":"And this is the beauty of LSTMs; the whole thing is end-to-end differentiable. These gates can learn when to allow data to flow and what data should flow depending on the context it sees (the input and the hidden states). It learns this based on patterns it sees while training. In this sense, it‚Äôs sort of like how a CNN learns feature detectors for images, but the patterns are way more complex and less human interpretable with LSTMs. This is why they perform so well.","markups":[{"type":2,"start":95,"end":101}]},{"name":"1be4","type":4,"text":"üò® üò± üò∞ : perhaps your immediate reaction.","markups":[{"type":1,"start":0,"end":3}],"layout":1,"metadata":{"id":"1*VJL6ONtLK77GpO2XmFCH7g.png","originalWidth":551,"originalHeight":321}},{"name":"b263","type":1,"text":"Okay, this looks scarier, but it‚Äôs actually not much different to what we had before, especially once you look past the intimidating web of arrows. One notable change is that we‚Äôre showing the previous hidden state in time and the current input flowing in. This diagram makes the assumption that we‚Äôre in the first layer and at some timestep \x3e 1 where input exists. We then show how the f, i, g, and o gates are computed from this information ‚Äî the hidden state and inputs are fed into an activation function like sigmoid (or, for g, a tanh; you can tell because it‚Äôs double the height of the others) ‚Äî and it‚Äôs expressed through the web of arrows. It‚Äôs implied that we weight the two terms entering our activation functions, adding them up with a bias vector, but it‚Äôs not necessarily explicit in the diagram.","markups":[{"type":1,"start":387,"end":388},{"type":1,"start":390,"end":391},{"type":1,"start":393,"end":396},{"type":1,"start":400,"end":401},{"type":1,"start":531,"end":532}]},{"name":"b56c","type":1,"text":"Let‚Äôs embed this into our overall LSTM diagram for a single timestep:","markups":[]},{"name":"689a","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*0h88NXeFxkb-xD1rBq4lgA.png","originalWidth":770,"originalHeight":349}},{"name":"147c","type":1,"text":"Now let‚Äôs zoom out and view our entire unrolled single layer, three timestep LSTM:","markups":[]},{"name":"4127","type":4,"text":"","markups":[],"layout":5,"metadata":{"id":"1*-lhIk-yAsXk88gcPvEeIRQ.png","originalWidth":4225,"originalHeight":1204}},{"name":"c9d0","type":1,"text":"It‚Äôs beautiful, isn‚Äôt it? The full screen width size just adds to the effect! Here‚Äôs a link to the full res version.","markups":[{"type":3,"start":78,"end":84,"href":"https://drive.google.com/file/d/0BwbWRPtraa2zQUsydXRKSkd3YUU/view?usp=sharing","title":"","rel":"noopener","anchorType":0}]},{"name":"c678","type":1,"text":"The only thing that would look more beautiful would be multiple LSTM cells that stack on top of each other (multiple hidden layers)! üòç","markups":[]},{"name":"e117","type":3,"text":"Fixing the problem with LSTMs (Part II)","markups":[]},{"name":"f89a","type":1,"text":"You‚Äôve come a long way, young padawan. But there‚Äôs still a bit left to go. Part I focused on the motivation for LSTMs, how they work, and a bit on why they reduce the vanishing gradient problem. Now, having a full understanding of LSTMs, Part II will hone in on the latter part‚Äîanalyzing on a more close, technical level why our gradients stop vanishing as quickly. You won‚Äôt find a lot of this information online easily; I had to search and ask left and right to find an explanation better and more comprehensive than what you‚Äôll find in other current tutorials.","markups":[]},{"name":"a182","type":1,"text":"Firstly, truncated BPTT is often used with LSTMs; it‚Äôs a method to speed up training. In particular, note that if we input a sequence of length 1000 into an LSTM, and want to train it, it‚Äôs equivalent to training a 1000 layer neural network. Doing forward and backwards passes into this is very memory and time consuming, especially while backpropagating the error when we need to compute a derivative like this:","markups":[]},{"name":"7285","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*UqC4IRIfcDfoiwD8zvqW2A.png","originalWidth":87,"originalHeight":79}},{"name":"06fe","type":1,"text":"‚Ä¶which would include a lot of terms.","markups":[{"type":2,"start":23,"end":26}]},{"name":"792f","type":1,"text":"When we backprop the error, and add all the gradients up, this is what we get:","markups":[]},{"name":"b325","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*ucOvP6wOs9MHzH8WKiykbQ.png","originalWidth":675,"originalHeight":79}},{"name":"9c5f","type":1,"text":"Truncated BPTT does two things:","markups":[]},{"name":"5bfb","type":9,"text":"Instead of doing a forward pass on the whole sequence and then doing a backwards pass, we process the sequence timestep by timestep and do a backwards pass ‚Äúevery so often‚Äù. That is ‚Äî we compute h_1 and c_1, then h_2 and c_2, then h_3 and c_3, and then at some point in time, quantified by k1, we do a backwards pass. Every k1 timesteps, we perform BPTT; if k1 = 10, for example, then once we compute h_10 and c_10 we perform BPTT. Same for h_20 and c_20, and so on so forth. When we perform the backwards pass, our error J won‚Äôt be the same as if we did a full forwards pass and full backwards pass, since we haven‚Äôt observed all the outputs yet‚Äîwe wouldn‚Äôt have even computed all the potential outputs yet! Instead, the error describes what we‚Äôve observed and computed so far, because we process the sequence timestep by timestep. Intuitively, it‚Äôs like we train on a small subset of the training sequence, and this subset increases in length each time, which enables us to continue learning long-term dependencies. We could denote the error at timestep t ‚Äîwhere t is a multiple of k1 ‚Äî with truncated backprop as J^t. So:","markups":[{"type":1,"start":195,"end":199},{"type":1,"start":203,"end":206},{"type":1,"start":213,"end":217},{"type":1,"start":221,"end":224},{"type":1,"start":231,"end":234},{"type":1,"start":239,"end":242},{"type":1,"start":290,"end":294},{"type":1,"start":324,"end":327},{"type":1,"start":358,"end":365},{"type":1,"start":401,"end":406},{"type":1,"start":410,"end":415},{"type":1,"start":441,"end":446},{"type":1,"start":450,"end":454},{"type":1,"start":522,"end":524},{"type":1,"start":1056,"end":1057},{"type":1,"start":1065,"end":1067},{"type":1,"start":1084,"end":1086},{"type":1,"start":1115,"end":1119}]},{"name":"dd7e","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*Br6EoWvUmGTNoX3NqkZVpA.png","originalWidth":445,"originalHeight":80}},{"name":"eb5a","type":1,"text":"For example, if t = 20 and k1 = 10, our second (because 20 √∑ 10 = 2) round of BPTT would be:","markups":[{"type":1,"start":16,"end":22},{"type":1,"start":27,"end":34},{"type":2,"start":40,"end":46}]},{"name":"6f7d","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*pg91-TmNosH9B7Py0wjCdQ.png","originalWidth":667,"originalHeight":80}},{"name":"f441","type":9,"text":"On top of this, instead of backpropagating from J^t all the way to the first timestep c_1, we set a cut-off point. This cut-off point, quantified by k2, is the timestep at which our cell states stop contributing to the overall gradient. For example, if k2 = 10, and we‚Äôre backpropagating at t = 20, then c_10 is the final cell state to contribute to the overall gradient. Everything before c_10 will have no say. This is designed such that we avoid computing derivatives between cell states far apart in time, which would include a huge number of terms (as mentioned earlier). The equation is now:","markups":[{"type":1,"start":48,"end":52},{"type":1,"start":86,"end":89},{"type":1,"start":149,"end":151},{"type":1,"start":253,"end":260},{"type":1,"start":291,"end":297},{"type":1,"start":304,"end":309},{"type":1,"start":390,"end":395}]},{"name":"73ca","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*GmP4nvsdBTyyRwo7ffRW2g.png","originalWidth":515,"originalHeight":80}},{"name":"956c","type":1,"text":"So, with t = 20, k2 = 10, and k1 = 10, our second round of BPTT would follow:","markups":[{"type":1,"start":9,"end":15},{"type":1,"start":17,"end":24},{"type":1,"start":30,"end":37}]},{"name":"12a1","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*WyHlRZljjmHEaFKsGg0JQg.png","originalWidth":674,"originalHeight":80}},{"name":"a297","type":1,"text":"Both k1 and k2 are hyperparameters. k1 does not have to equal k2.","markups":[{"type":1,"start":5,"end":7},{"type":1,"start":12,"end":14},{"type":1,"start":36,"end":39},{"type":1,"start":62,"end":64}]},{"name":"93ed","type":1,"text":"These two techniques combined enables truncated BPTT to not lose the ability to learn long term dependencies. Here‚Äôs a formal definition:","markups":[]},{"name":"72ee","type":6,"text":"[Truncated BPTT] processes the sequence one timestep at a time, and every k1 timesteps, it runs BPTT for k2 timesteps, so a parameter update can be cheap if k2 is small. Consequently, its hidden states have been exposed to many timesteps and so may contain useful information about the far past, which would be opportunistically exploited.","markups":[{"type":1,"start":74,"end":76},{"type":1,"start":105,"end":107},{"type":1,"start":157,"end":159}]},{"name":"ae91","type":6,"text":"‚Äî ‚ÄúTraining Recurrent Neural Networks‚Äù, 2.8.6, Page 23","markups":[{"type":3,"start":2,"end":38,"href":"http://www.cs.utoronto.ca/~ilya/pubs/ilya_sutskever_phd_thesis.pdf","title":"","rel":"","anchorType":0}]},{"name":"28a4","type":1,"text":"The same paper gives nice pseudocode for truncated BPTT:","markups":[]},{"name":"f5d9","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*0SnUb2iYt1RNa7JsGG-7gQ.png","originalWidth":413,"originalHeight":113}},{"name":"399f","type":1,"text":"The rest of the math in this section will not be in the context of using truncated backprop, because it‚Äôs a technique vs. something rooted in the mathematical foundation of LSTMs.","markups":[]},{"name":"d546","type":1,"text":"Moving on ‚Äî before, we saw this diagram:","markups":[]},{"name":"dc69","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*n26drGfEkc-Xnmqc2Lw7cw.png","originalWidth":2161,"originalHeight":690}},{"name":"48f4","type":1,"text":"In this context, ∆íw = i ‚äô g, because it‚Äôs the value we‚Äôre adding to the cell state.","markups":[{"type":1,"start":17,"end":27}]},{"name":"db0e","type":1,"text":"But this diagram is a bit of a lie. Why? It ignores forget gates. So, does the presence of forget gates affect the vanishing gradient problem? Quite significantly, actually. How? Let‚Äôs bring up our cell state equation to see:","markups":[]},{"name":"4484","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*i6rbrX0k9mKLXewD4korCw.png","originalWidth":236,"originalHeight":39}},{"name":"09ae","type":1,"text":"With the forget gate, we now include a multiplicative interaction. Our new diagram will look like this:","markups":[]},{"name":"931c","type":4,"text":"Do not confuse forget gate ∆í with function ∆íw in this diagram. I know, it‚Äôs confusing‚Ä¶ üò¢","markups":[{"type":1,"start":27,"end":28},{"type":1,"start":43,"end":45}],"layout":1,"metadata":{"id":"1*UVx1vL6ADQGTBeKSaWX7bw.png","originalWidth":2436,"originalHeight":986}},{"name":"bf2c","type":1,"text":"When our gradients flow back, they will be affected by this multiplicative interaction. So, let‚Äôs compute the new derivative:","markups":[]},{"name":"0504","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*60XFfJvc0t9a0ekdMTAp_Q.png","originalWidth":113,"originalHeight":79}},{"name":"f174","type":1,"text":"This seems super neat, actually. Obviously the gradient will be f, because f acts as a blocker and controls how much c_t-1 influences c_t; it‚Äôs the gate that you can fully or partially open and close that lets information from c_t-1 flow through! It‚Äôs just intuitive that it would propagate back perfectly.","markups":[{"type":1,"start":64,"end":65},{"type":1,"start":75,"end":77},{"type":1,"start":117,"end":123},{"type":1,"start":134,"end":137},{"type":1,"start":227,"end":233},{"type":2,"start":33,"end":42}]},{"name":"d859","type":1,"text":"But, if you‚Äôve payed close attention so far, you might be asking: ‚Äúwait, what happened to ∆íw‚Äôs contribution to the gradient?‚Äù If you‚Äôre a hardcore mathematician, you might also be worried that we‚Äôre content with leaving the gradient as just f. This is because the gates f, i, and g are all functions of c_t-1; they are functions of h_t-1, which is, in turn, a function of c_t-1! The diagram shows this visually, as well. It seems we‚Äôre failing to apply calculus properly. We‚Äôd need to backprop through f and through i ‚äô g to complete the derivative.","markups":[{"type":1,"start":90,"end":92},{"type":1,"start":125,"end":126},{"type":1,"start":241,"end":242},{"type":1,"start":270,"end":271},{"type":1,"start":272,"end":274},{"type":1,"start":280,"end":282},{"type":1,"start":303,"end":308},{"type":1,"start":332,"end":337},{"type":1,"start":372,"end":377},{"type":1,"start":502,"end":503},{"type":1,"start":516,"end":522},{"type":2,"start":67,"end":90},{"type":2,"start":92,"end":126}]},{"name":"3f0c","type":1,"text":"Let‚Äôs walk through the differentiation to show why you‚Äôre actually not wrong, but neither am I:","markups":[{"type":1,"start":76,"end":78},{"type":2,"start":77,"end":78}]},{"name":"f0c6","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*x1mvDnbZOmZ1CnHJo23tZg.png","originalWidth":380,"originalHeight":162}},{"name":"80e0","type":1,"text":"Now, with the first derivative, we need to apply product rule. Why? Because we‚Äôre differentiating the product of two functions of c_t-1. The former being the forget gate, and the latter being just c_t-1. Let‚Äôs do it:","markups":[{"type":1,"start":130,"end":135},{"type":1,"start":197,"end":202}]},{"name":"b39b","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*TwjnkG6vtuIke1pkAzTzzA.png","originalWidth":271,"originalHeight":124}},{"name":"00fd","type":1,"text":"Then, from product rule:","markups":[]},{"name":"416d","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*cgq4UnWxun6rQ6H00gDzWg.png","originalWidth":381,"originalHeight":164}},{"name":"7dd1","type":1,"text":"That‚Äôs the first derivative done. We purposely choose not to compute the derivative of the forget gate with respect to the previous cell state on previous. You‚Äôll see why in a bit.","markups":[]},{"name":"e8f2","type":1,"text":"Now let‚Äôs tackle the second one:","markups":[]},{"name":"be20","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*1T0iaNg6vY4pEOz-ybkjuw.png","originalWidth":150,"originalHeight":77}},{"name":"10b0","type":1,"text":"You‚Äôll notice that it‚Äôs also two functions of c_t-1 multiplied together, so we use the product rule again:","markups":[{"type":1,"start":46,"end":52}]},{"name":"f85e","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*tVfrKCc1T7eRgypliDe19A.png","originalWidth":320,"originalHeight":124}},{"name":"618f","type":1,"text":"So:","markups":[]},{"name":"c8bf","type":4,"text":"Once again, we purposely do not simplify the gate derivative terms.","markups":[],"layout":1,"metadata":{"id":"1*s2nnva6Yhb2AuZDYYALbEA.png","originalWidth":361,"originalHeight":164}},{"name":"41e0","type":1,"text":"Thus, our overall derivative becomes:","markups":[]},{"name":"7b20","type":4,"text":"Notice that the first term in this derivative is our forget gate.","markups":[],"layout":1,"metadata":{"id":"1*o0dzU_s9WxoTYfOkQo1a0A.png","originalWidth":461,"originalHeight":79}},{"name":"9307","type":1,"text":"Pay attention to the caption of the diagram.","markups":[]},{"name":"4712","type":1,"text":"This is actually our real derivative. Modern LSTM implementations just use an auto differentiation library to compute derivatives, so they‚Äôll probably come up with this. However, effectively, our gradient is just the forget gate, because the other three terms tend towards zero. Yup ‚Äî they vanish. Why?","markups":[{"type":2,"start":21,"end":26},{"type":2,"start":179,"end":190}]},{"name":"b5a4","type":1,"text":"When we backprop error in LSTMs, we backprop through cell states to propagate the error from the outputs to the cell state we want. For example, if we want to backprop the error from the output at time t down k timesteps, then we need to compute the derivative of the cell state at time t to the cell state at time t-k. Look what happens when we do that:","markups":[{"type":1,"start":202,"end":203},{"type":1,"start":209,"end":211},{"type":1,"start":287,"end":289},{"type":1,"start":315,"end":318}]},{"name":"db33","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*dBFbl6NCqp94Lnyb0taFWg.png","originalWidth":642,"originalHeight":79}},{"name":"57e3","type":1,"text":"We didn‚Äôt simplify the gate w.r.t. cell state derivatives for a reason; as we backpropagate through time, they begin to vanish. Thus, whatever they multiplied with is killed off from making contributions to the gradient, too. So, effectively:","markups":[]},{"name":"08ea","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*Q9BJ7JxQ08YvfBW_OsJabw.png","originalWidth":458,"originalHeight":71}},{"name":"e607","type":1,"text":"The rationale behind this is pretty simple, and we don‚Äôt need math for it; these gates are the outputs of non-linearities eg. sigmoid and tanh. If we were to get the derivative of them in getting our cell state derivative, then this derivative would contain the derivatives of sigmoid/tanh in them. But, just because we don‚Äôt need to use math to show this, doesn‚Äôt mean we don‚Äôt want to üòè :","markups":[{"type":2,"start":326,"end":330},{"type":2,"start":379,"end":384}]},{"name":"0dbb","type":4,"text":"I obfuscated the input to the sigmoid function for the input gate, just for simplicity.","markups":[],"layout":1,"metadata":{"id":"1*-mUDovQ8ovejmWPNoFSI1g.png","originalWidth":541,"originalHeight":79}},{"name":"f6be","type":1,"text":"Recall from our vanishing gradient article that the max output of sigmoid‚Äôs first order derivative is 0.25, and it‚Äôs something similar for tanh. This becomes textbook vanishing gradient problem. As we backprop through more and more cell states, the gradient terms become longer and longer, and this will definitely vanish. When they don‚Äôt vanish, they‚Äôll be super minor contributions, so we can just leave them out for brevity.","markups":[]},{"name":"95ec","type":6,"text":"Sidenote: one person reached out to me unsure of why gradients with long terms ‚Äî aka, that are equal to the product of a lot of terms ‚Äî usually vanishes/explodes. Here‚Äôs what I said in response:","markups":[]},{"name":"eabd","type":6,"text":"‚ÄúIf you have long gradient terms, you probably have the vanishing gradient problem, unless you can guarantee those terms are around 1 each. If they‚Äôre not, it‚Äôll explode or vanish. And, given the nature of the problems where this is an issue, it‚Äôs very unlikely they‚Äôll be around 1 each. Especially if they are the output some non-linear function like sigmoid/tanh or their derivatives.","markups":[]},{"name":"944e","type":6,"text":"For example, let‚Äôs say the gradient term = k_1 √ó k_2 √ó k_3 √ó ‚Ä¶ √ó k_100. 100 terms in this product.","markups":[]},{"name":"1583","type":6,"text":"If each of these terms is, let‚Äôs say, around 0.5, then you have 0.5¬π‚Å∞‚Å∞ = some absurdly low number. If you have each term be arond 1.5, then you have 1.5¬π‚Å∞‚Å∞ which is some absurdly high number.\n\nWhen we introduce tanh/sigmoid and/or their derivatives in these huge products, you can guarantee that they‚Äôll saturate and die off. As mentioned, the max for sigmoid‚Äôs first order derivative is 0.25, so just imagine something like 0.25¬π‚Å∞‚Å∞.","markups":[]},{"name":"59c9","type":1,"text":"Ultimately, the reason I obfuscate these terms that vanish in the derivative is because I would like to show the effect of the forget gate on gradient flow now. If I included the other terms, the same implications would be present, but the math would just take longer to type out and render.","markups":[]},{"name":"e361","type":1,"text":"Because ∆íw = i ‚äô g, we can redraw our diagram showing that ∆íw won‚Äôt make any contributions to the gradient flow back. Again ‚Äî ∆íw does, but it‚Äôs effectively negligible, so we can just exclude it from our updated gradient flow diagram, which follows:","markups":[{"type":1,"start":8,"end":18},{"type":1,"start":59,"end":62},{"type":1,"start":126,"end":128}]},{"name":"3217","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*RsHULZCgY6p-5bKkE99Q-Q.png","originalWidth":1095,"originalHeight":453}},{"name":"8811","type":1,"text":"But wait! This doesn‚Äôt look good; the gradients have to multiply by this f_t gate at each timestep. Before, they didn‚Äôt have to multiply by anything (or, in other words, they multiplied by 1) and flowed past super easily.","markups":[{"type":1,"start":73,"end":76}]},{"name":"9feb","type":1,"text":"Machine learning researchers coined a name for the type of function we had before we introduced the forget gate where the derivative of one cell state w.r.t. the previous is 1.0: ‚ÄúConstant Error Carousel‚Äù (CEC). With our new function, the derivative is equal to f. You‚Äôll see this referred to as a ‚Äúlinear carousel‚Äù in papers.","markups":[{"type":1,"start":174,"end":177},{"type":1,"start":262,"end":263}]},{"name":"fdbb","type":1,"text":"Before we introduced a forget gate ‚Äî where all we had was the additive interaction from ∆íw ‚Äî our cell state function was a CEC:","markups":[{"type":1,"start":88,"end":91}]},{"name":"2bf9","type":4,"text":"A CEC ‚Äî same as before, but no forget gate.","markups":[],"layout":1,"metadata":{"id":"1*9O__qOVOK1wxJDFy6m4YAQ.png","originalWidth":208,"originalHeight":84}},{"name":"61b2","type":1,"text":"The derivative of this cell state w.r.t. the previous one, again as long as we don‚Äôt backprop through the i and g gates, is just 1. That‚Äôs why gradients flow back super comfortably, without vanishing at all. Basically, for a CEC to exist in this context, the coefficient of c_t-1 needs to be 1.","markups":[{"type":1,"start":106,"end":108},{"type":1,"start":112,"end":113},{"type":1,"start":274,"end":280}]},{"name":"7ab2","type":1,"text":"Once we introduced this multiplicative interaction (for good reason), we got a linear carousel; the coefficient of c_t-1 is f. So, in our case, when f = 1 (when we‚Äôre not going to forget) our function becomes a CEC, and our gradients will pretty much never vanish. If it‚Äôs close to 0, though, the gradient term will immediately die. Gradients will stay on the carousel for a while until the forget gate is triggered; the effect on the gradient is like a step function, in that it‚Äôs constant with a value of 1 and then drops off to zero/dies when we have f ‚âà 0.","markups":[{"type":1,"start":115,"end":121},{"type":1,"start":124,"end":125},{"type":1,"start":149,"end":154},{"type":1,"start":554,"end":559}]},{"name":"3337","type":1,"text":"Intuitively, this seems problematic. Let‚Äôs do some math to investigate:","markups":[]},{"name":"0955","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*UbqEhAyW7bMv_tDf-cvuWg.png","originalWidth":372,"originalHeight":164}},{"name":"2abc","type":1,"text":"The derivative of a cell state to the previous is f_t. The derivative of a cell state to two prior cell states is f_t ‚äô f_t-1. Thus:","markups":[{"type":1,"start":50,"end":53},{"type":1,"start":114,"end":125}]},{"name":"1596","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*p9OndETS7tR-zUU-1TuaTw.png","originalWidth":603,"originalHeight":83}},{"name":"8238","type":1,"text":"As we backpropagate through time, these forget gates keep chaining up and multiplying together to form the overall gradient term.","markups":[]},{"name":"6f12","type":1,"text":"Now, imagine an LSTM with 100 timesteps. If we wanted to get the derivative of the error w.r.t. a weight like W_xi, to optimize it, remember that with BPTT we add up or average all the gradients from the different timesteps:","markups":[{"type":1,"start":110,"end":114}]},{"name":"d867","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*rhb_2DO5MulJvzV9QxasMg.png","originalWidth":692,"originalHeight":101}},{"name":"a73a","type":1,"text":"OK. Now let‚Äôs look at an early (in time) term, like the gradient propagated from the error to the third cell:","markups":[]},{"name":"9f34","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*fAaYlJPgsPjRGJGoyc8XCw.png","originalWidth":106,"originalHeight":79}},{"name":"9972","type":1,"text":"Remember that J is an addition of errors from Y individual outputs, so we backpropagate through each of the outputs first:","markups":[{"type":1,"start":14,"end":16},{"type":1,"start":45,"end":48}]},{"name":"7c21","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*vsybuvtlGl-cQqUI1Pqbag.png","originalWidth":609,"originalHeight":164}},{"name":"4355","type":1,"text":"The first few terms, where we backprop y_k to c_3 where k \x3c 3, would just be equal to zero because c_3 only exists after these outputs have been computed.","markups":[{"type":1,"start":39,"end":43},{"type":1,"start":46,"end":50},{"type":1,"start":56,"end":61},{"type":1,"start":99,"end":103}]},{"name":"e686","type":1,"text":"Let‚Äôs assume that Y = 100 and continue with our assumption that t = 100 (so each timestep gives rise to an output), for simplicity. With this, let‚Äôs now look at the last term in this sum.","markups":[{"type":1,"start":18,"end":26},{"type":1,"start":64,"end":72}]},{"name":"53d1","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*JJIQxpb1mHjDn5KaoOKQkA.png","originalWidth":501,"originalHeight":79}},{"name":"6208","type":1,"text":"That‚Äôs a lot of forget gates chained together. If one of these forget gates is [approximately] zero, the whole gradient dies. If these also tend to be a small number between 0 and 1, the whole thing will vanish, and c_3 won‚Äôt make any contributions to the gradient here.","markups":[{"type":1,"start":47,"end":124},{"type":1,"start":216,"end":220}]},{"name":"fb13","type":1,"text":"This isn‚Äôt intrinsically an issue though! Because, when a forget gate is zero, it means that cell is no longer making any contributions past that point. If f_4 is zero, then any y outputs at/past timestep 4 won‚Äôt be influenced by c_3 (as well as c_2 and c_1) because we ‚Äúerased‚Äù it from memory. Therefore that particular gradient should be zero. If y_80 is zero, then any outputs at/past timestep 80 won‚Äôt be influenced by c_1, c_2, ‚Ä¶ , c_79. Same story here. If these forget gates are between 0 and 1, then the influence of our cell decays over time anyways, and our gradients will be very small, so they‚Äôll reflect that. Gers 1999 calls this ‚Äúreleasing resources‚Äù.","markups":[{"type":3,"start":623,"end":632,"href":"https://pdfs.semanticscholar.org/1154/0131eae85b2e11d53df7f1360eeb6476e7f4.pdf","title":"","rel":"nofollow","anchorType":0},{"type":1,"start":156,"end":160},{"type":1,"start":178,"end":179},{"type":1,"start":230,"end":234},{"type":1,"start":246,"end":250},{"type":1,"start":254,"end":257},{"type":1,"start":349,"end":353},{"type":1,"start":423,"end":441},{"type":2,"start":11,"end":25}]},{"name":"b6f8","type":1,"text":"Cell c_3 will still contribute to the overall gradient, though. For example, take this term:","markups":[{"type":1,"start":5,"end":9}]},{"name":"0851","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*KZjK3wcZpYG_qnjkMB1HkA.png","originalWidth":474,"originalHeight":79}},{"name":"8736","type":1,"text":"Here, we‚Äôre looking at y_12 instead of y_100. Chances are that, if you have a sequence of length 100, your 100th cell state isn‚Äôt drawing from your 3rd; the forget gate would have been triggered at some point by then. However, the 12th cell state probably will still be drawing from the ones before it.","markups":[{"type":1,"start":23,"end":28},{"type":1,"start":39,"end":44}]},{"name":"5cd8","type":1,"text":"If we decide not to forget in the first 12 timesteps, ie. f_1 ‚Ä¶ f_12 are each not far from 1, then c_3 would have more influence over y_12 and the error that stems from y_12. Thus, the gradient would not vanish and c_3 still contributes to update W_xi, it just doesn‚Äôt contribute a gradient where it‚Äôs not warranted to (that is, where it doesn‚Äôt actually contribute to any activation, because it‚Äôs been forgotten). To summarize: one activated forget gate will indeed kill off gradient flow to cell(s), but that is a good thing because the network is learning that that gradient from the future has no benefit and is completely irrelevant to those particular cell(s), since those cells have been forgotten by then. In practice, different cells learn different ranges of context, some short, some long. This is a benefit for LSTMs.","markups":[{"type":1,"start":58,"end":69},{"type":1,"start":99,"end":103},{"type":1,"start":134,"end":139},{"type":1,"start":169,"end":173},{"type":1,"start":215,"end":218},{"type":1,"start":247,"end":251},{"type":2,"start":811,"end":819}]},{"name":"7b24","type":1,"text":"So, given a gradient between two cell states in time, when all of these forget gates are [approximately] equal to 1, the gradient signal will remain stable, because we‚Äôre multiplying by 1 at each timestep ‚Äî effectively, not multiplying by anything at all. In such a case, our gradient flow diagram would look like this:","markups":[]},{"name":"9b4b","type":4,"text":"It‚Äôs‚Ä¶ it‚Äôs beautiful!","markups":[],"layout":1,"metadata":{"id":"1*rBJm9F6zz8drQnDlWd7wvQ.png","originalWidth":2190,"originalHeight":694}},{"name":"f12e","type":1,"text":"The gradient will have literally zero interactions or disturbances, and will just flow through like it‚Äôs driving 150 mph on an empty countryside America highway. The beauty of CECs is that they‚Äôre always like this.","markups":[{"type":2,"start":197,"end":204}]},{"name":"87cf","type":1,"text":"But, let‚Äôs get back to reality. LSTMs aren‚Äôt CECs. One disadvantage of these forget gates chaining together is that it could block learning. That is, when we set out to train our LSTM, the forget gates have not been learned; we have to learn them while we learn everything else. So, if they all start around 0, no gradients will flow through our cell states when we perform BPTT, and learning won‚Äôt happen at all.","markups":[]},{"name":"3db3","type":1,"text":"The obvious solution is to set the forget gate bias to a very large value when training, so it starts at 1 instead of 0 (because y = 1 is to the far right of the sigmoid function, so adding to the input will ensure ~1 will be the output). In early stages of training, the forget gates equalling/approximating 1 will result in learning not being blocked. So many papers do this and mention it explicitly such that this forget gate bias could even be considered a hyperparameter.","markups":[{"type":1,"start":129,"end":134}]},{"name":"d763","type":1,"text":"By introducing forget gates, we stray from CECs and thus the guarantee that our gradients will never ever vanish. But, again, we do it for good reason. And when gradients vanish it‚Äôs because we chose to forget that cell ‚Äî so it‚Äôs not necessarily a bad thing. We just need to make sure the forget gates don‚Äôt block learning in initial stages of training.","markups":[]},{"name":"e1c7","type":1,"text":"We can try computing some more derivatives, just for fun! Let‚Äôs sub in real values for the timesteps, backprop across more than one timestep, and do it for a gate this time.","markups":[]},{"name":"62f7","type":1,"text":"We‚Äôll expand c_4 and express it in terms of our gates only. In the process, each c_t, except c_1, will collapse into a few interactions between the f, i, and g gate:","markups":[{"type":1,"start":13,"end":17},{"type":1,"start":81,"end":86},{"type":1,"start":93,"end":96},{"type":1,"start":148,"end":149},{"type":1,"start":151,"end":152},{"type":1,"start":153,"end":154},{"type":1,"start":157,"end":160}]},{"name":"26eb","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*2LZxa4YAMGCJqOrirI1-_w.png","originalWidth":669,"originalHeight":166}},{"name":"5092","type":1,"text":"Now, let‚Äôs get the derivative of c_4 with respect to one of the earliest possible gates, like g_2. In the expression above, this turns out to just be the coefficient of g_2:","markups":[{"type":1,"start":33,"end":36},{"type":1,"start":94,"end":97},{"type":1,"start":169,"end":172}]},{"name":"c936","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*rHDurdaN9SKnChWfyVk38w.png","originalWidth":205,"originalHeight":79}},{"name":"708a","type":1,"text":"We experience the same neatness here as with the cell state backprop! It makes complete sense that the gradient would be i_2 ‚äô f_3 ‚äô f_4, since i_2 controls what influence g_2 has over c_2, f_3 controls what influence c_2 has on c_3, and f_4 controls what influence c_3 has over c_4. Notice the chaining up of the forget gates üëª; everything about the carousels I just talked about ‚Äî and what they imply about vanishing gradients ‚Äî applies here.","markups":[{"type":1,"start":121,"end":126},{"type":1,"start":127,"end":136},{"type":1,"start":144,"end":148},{"type":1,"start":172,"end":176},{"type":1,"start":185,"end":188},{"type":1,"start":190,"end":194},{"type":1,"start":218,"end":222},{"type":1,"start":229,"end":232},{"type":1,"start":238,"end":242},{"type":1,"start":266,"end":270},{"type":1,"start":279,"end":282}]},{"name":"6229","type":1,"text":"I‚Äôll leave it up to you to derive something similar for the other gates.","markups":[]},{"name":"f15d","type":1,"text":"And that‚Äôs it! That‚Äôs why LSTMs rock their socks off when it comes to keeping their gradients in check,.","markups":[]},{"name":"627a","type":1,"text":"Here‚Äôs a neat GIF showing a visual representation of the gradients that exist at each timestep, starting from timestep 128 and going all the way to the first, during backprop. More noise represents greater values:","markups":[]},{"name":"2414","type":11,"text":"Super highway indeed. imgur.com/gallery/vaNahKE.","markups":[{"type":3,"start":22,"end":47,"href":"http://imgur.com/gallery/vaNahKE","title":"","rel":"noopener","anchorType":0}],"layout":3,"iframe":{"mediaResourceId":"c239248e2e0b9a4aadc7b43d8c08ca12","iframeWidth":854,"iframeHeight":480,"thumbnailUrl":"https://i.embed.ly/1/image?url=http%3A%2F%2Fi.imgur.com%2FvaNahKEh.jpg&key=4fce0568f2ce49e8b54624ef71a8a5bd"}},{"name":"868a","type":1,"text":"As you can see, the vanilla RNN‚Äôs gradients die off way quicker than the LSTM‚Äôs. The RNN is almost immediate in comparison. LSTMs seem like a super highway indeed, although it does seem that they do vanish. In this diagram, it can be suggested that the gradients perhaps die for the LSTM eventually because we chose to forget early cell states; again, this depends on the application at hand, and is learnable. (I‚Äôm not sure if this GIF uses truncated backprop, so that could be another thing. In general, I don‚Äôt know the context of this GIF.) Also, part of the gradient signal definitely vanishes‚Äîit‚Äôs the signals that pass through the f/i/g gates that we looked at earlier and obfuscated from the cell state‚Üícell state derivative. We showed they would vanish because of tanh/sigmoid derivatives; initially, these signals will make a fairly significant contribution, but over time they‚Äôll get smaller and smaller. That‚Äôs the explanation for this GIF.","markups":[{"type":1,"start":638,"end":644},{"type":2,"start":400,"end":409}]},{"name":"2b66","type":1,"text":"Exploding gradients is still an issue, though. Recall that when we have a bunch of gradient terms added together, if some of them vanish it doesn‚Äôt mean the whole thing will vanish (versus if they were multiplied together). However, if some of the gradients explode, the whole thing explodes; x + 0 = x, but x + ‚àû = ‚àû. If cell states become unstable and grow too much in some rare scenario, then our gradients could explode. In such a case we‚Äôd need to implement gradient clipping, which is where we choose some arbitrary threshold that gradients cannot be larger than; so, grad = min(grad, clip_threshold). This would enable the LSTM to deal with such cases without essentially collapsing. Many successful LSTM applications use gradient clipping.","markups":[{"type":10,"start":574,"end":606},{"type":1,"start":312,"end":313},{"type":1,"start":316,"end":317}]},{"name":"69b6","type":1,"text":"Usually, though, exploding gradients are avoided because sooner or later the forget gate in the carousel is triggered and we reset the memory.","markups":[]},{"name":"3679","type":1,"text":"There are variants of LSTMs. People have tried modifying the model, like computing the hidden state without using tanh activation (so h_t = o ‚äô c_t) or ditching the i input gate and only using g, since that would still satisfy the -1 to 1 range. The results didn‚Äôt change by much.","markups":[{"type":1,"start":134,"end":147},{"type":1,"start":165,"end":167},{"type":1,"start":193,"end":194}]},{"name":"dc3e","type":1,"text":"In fact, some researchers even applied evolutionary algorithms to spawn and test a bunch of variants on the standard LSTM equations. Most of the good ones just worked roughly the same.","markups":[]},{"name":"c7f8","type":1,"text":"This highlights an issue with LSTMs ‚Äî they are definitely fairly handwavy. We use them because their architecture allows us to prevent gradients from vanishing such that we can learn long-term dependencies, but there‚Äôs not much theoretical or empirical backing for them. ANNs and RNNs make sense in that they‚Äôre biologically inspired and that they‚Äôre essentially just deep composite functions that have parameters we can optimize. LSTMs stray so far from statistical methods and introduce complex concepts/architectures that work but aren‚Äôt necessarily justified from the get-go. Fully understanding why LSTMs work so well and coming up with better/simpler architectures is a hot topic of research right now.","markups":[]},{"name":"4bca","type":1,"text":"There are also other variants of RNNs, similar to LSTMs, like GRUs (Gated Recurrent Units). LSTM is still the king, but GRUs in particular have grown in popularity and are seen in many recent, well-respected research papers. It‚Äôs a must learn next to LSTMs, but this article would get too bloated with it. TL;DR: GRUs have a less complex architecture than LSTMs but achieves similar results, and they can control the flow of information without requiring a memory cell. And, they‚Äôre fairly new. (See, told you ‚Äúcoming up with better/simpler architectures is a hot topic of research right now‚Äù is true!)","markups":[{"type":3,"start":62,"end":66,"href":"https://en.wikipedia.org/wiki/Gated_recurrent_unit","title":"","rel":"noopener","anchorType":0}]},{"name":"c57a","type":3,"text":"Yay RNNs!","markups":[]},{"name":"8a09","type":1,"text":"Wanna see a couple cool things? Practical applications were talked about in the first section, and the next section will walk through more technical applications of RNNs ie. in recent research papers. This section, instead, contains some fun things RNNs have done or produced that‚Äôll ease yourself from the hell that was actually understanding them. As simply as we try to convey things on this blog, LSTMs be LSTMs.","markups":[]},{"name":"7337","type":1,"text":"Sidenote: now, don‚Äôt be frightened by ‚ÄúRNNs‚Äù. Do be frightened by ‚Äúvanilla RNNs‚Äù, in most contexts, but we use RNN as an umbrella term, and it most often refers to an LSTM or a similar variant like GRU.","markups":[{"type":1,"start":46,"end":49},{"type":2,"start":46,"end":49}]},{"name":"f30f","type":1,"text":"Many if not all of these are taken from Andrej Karpathy‚Äôs CS231n lecture, or his blog post on the same subject:","markups":[{"type":3,"start":58,"end":72,"href":"https://www.youtube.com/watch?v=cO0a0QYmFm8&index=10&list=PLlJy-eBtNFt6EuMxFYRiNRS07MCWN5UIA","title":"","rel":"noopener","anchorType":0}]},{"name":"d19b","type":14,"text":"The Unreasonable Effectiveness of Recurrent Neural Networks\nMusings of a Computer Scientist.karpathy.github.io","markups":[{"type":3,"start":0,"end":110,"href":"http://karpathy.github.io/2015/05/21/rnn-effectiveness/","title":"http://karpathy.github.io/2015/05/21/rnn-effectiveness/","rel":"","anchorType":0},{"type":1,"start":0,"end":59},{"type":2,"start":60,"end":92}],"mixtapeMetadata":{"mediaResourceId":"3019eda2afc61d3398ef9b0a1762edf9","thumbnailImageId":"0*7sIxt7RqO7deGldw.","href":"http://karpathy.github.io/2015/05/21/rnn-effectiveness/"}},{"name":"83d4","type":1,"text":"You should most certainly visit either his blog post or lecture for info on exactly how these experiments were conducted, and for more interpretation on their results. Taking a look at the ‚ÄòVisualizing the predictions and the ‚Äúneuron‚Äù firings in the RNN‚Äô section would also be helpful to gain more insight and intuition on how RNNs work and learn over time.","markups":[{"type":1,"start":11,"end":25}]},{"name":"c051","type":1,"text":"A recurrent neural network generated this body of text, after it ‚Äúread‚Äù a bunch of Shakespeare:","markups":[]},{"name":"802f","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*BkvFHx8nYL1-NHmzZ_BbCQ.png","originalWidth":574,"originalHeight":619}},{"name":"62d6","type":1,"text":"Similarly, Karpathy gave an LSTM a lot of Paul Graham‚Äôs startup advice and life wisdom to read, and it produced this:","markups":[]},{"name":"3654","type":7,"text":"‚ÄúThe surprised in investors weren‚Äôt going to raise money. I‚Äôm not the company with the time there are all interesting quickly, don‚Äôt have to get off the same programmers. There‚Äôs a super-angel round fundraising, why do you can do. If you have a different physical investment are become in people who reduced in a startup with the way to argument the acquirer could see them just that you‚Äôre also the founders will part of users‚Äô affords that and an alternation to the idea. [2] Don‚Äôt work at first member to see the way kids will seem in advance of a bad successful startup. And if you have to act the big company too.‚Äù","markups":[]},{"name":"ee9f","type":1,"text":"A lot of relevant terminology, but it doesn‚Äôt really‚Ä¶ come together üòñ.","markups":[]},{"name":"1fc3","type":1,"text":"An LSTM can even generate valid XML, after reading Wikipedia!:","markups":[]},{"name":"89df","type":8,"text":"\x3cpage\x3e\n  \x3ctitle\x3eAntichrist\x3c/title\x3e\n  \x3cid\x3e865\x3c/id\x3e\n  \x3crevision\x3e\n    \x3cid\x3e15900676\x3c/id\x3e\n    \x3ctimestamp\x3e2002-08-03T18:14:12Z\x3c/timestamp\x3e\n    \x3ccontributor\x3e\n      \x3cusername\x3eParis\x3c/username\x3e\n      \x3cid\x3e23\x3c/id\x3e\n    \x3c/contributor\x3e\n    \x3cminor /\x3e\n    \x3ccomment\x3eAutomated conversion\x3c/comment\x3e\n    \x3ctext xml:space=\"preserve\"\x3e#REDIRECT [[Christianity]]\x3c/text\x3e\n  \x3c/revision\x3e\n\x3c/page\x3e","markups":[{"type":10,"start":0,"end":365}]},{"name":"7056","type":1,"text":"After a similar LSTM read the raw Latex version of an advanced algebra textbook, it came up with this ‚Äî put frankly ‚Äî fancy looking bogus. Let‚Äôs be real, you could definitely believe this was actual math üòú:","markups":[]},{"name":"221d","type":11,"text":"","markups":[],"layout":1,"iframe":{"mediaResourceId":"32e75fb2f7f388775689a155c5c27d86","iframeWidth":600,"iframeHeight":780}},{"name":"e129","type":1,"text":"An LSTM also read the Linux source code, and tried to write some code of its own:","markups":[]},{"name":"ce46","type":8,"text":"/*\n * Increment the size file of the new incorrect UI_FILTER group information\n * of the size generatively.\n */\nstatic int indicate_policy(void)\n{\n  int error;\n  if (fd == MARN_EPT) {\n    /*\n     * The kernel blank will coeld it to userspace.\n     */\n    if (ss-\x3esegment \x3c mem_total)\n      unblock_graph_and_set_blocked();\n    else\n      ret = 1;\n    goto bail;\n  }\n  segaddr = in_SB(in.addr);\n  selector = seg / 16;\n  setup_works = true;\n  for (i = 0; i \x3c blocks; i++) {\n    seq = buf[i++];\n    bpf = bd-\x3ebd.next + i * search;\n    if (fd) {\n      current = blocked;\n    }\n  }\n  rw-\x3ename = \"Getjbbregs\";\n  bprm_self_clearl(&iv-\x3eversion);\n  regs-\x3enew = blocks[(BPF_STATS \x3c\x3c info-\x3ehistoridac)] | PFMR_CLOBATHINC_SECONDS \x3c\x3c 12;\n  return segtable;\n}","markups":[{"type":10,"start":0,"end":745},{"type":1,"start":112,"end":118},{"type":1,"start":119,"end":122},{"type":1,"start":123,"end":138},{"type":1,"start":139,"end":143},{"type":1,"start":149,"end":152},{"type":1,"start":162,"end":164},{"type":1,"start":169,"end":171},{"type":1,"start":255,"end":257},{"type":1,"start":261,"end":263},{"type":1,"start":271,"end":272},{"type":1,"start":327,"end":331},{"type":1,"start":342,"end":343},{"type":1,"start":351,"end":355},{"type":1,"start":376,"end":377},{"type":1,"start":405,"end":406},{"type":1,"start":411,"end":412},{"type":1,"start":431,"end":432},{"type":1,"start":441,"end":444},{"type":1,"start":448,"end":449},{"type":1,"start":455,"end":456},{"type":1,"start":466,"end":468},{"type":1,"start":480,"end":481},{"type":1,"start":487,"end":489},{"type":1,"start":500,"end":501},{"type":1,"start":504,"end":506},{"type":1,"start":514,"end":515},{"type":1,"start":518,"end":519},{"type":1,"start":532,"end":534},{"type":1,"start":556,"end":557},{"type":1,"start":581,"end":583},{"type":1,"start":588,"end":589},{"type":1,"start":623,"end":624},{"type":1,"start":626,"end":628},{"type":1,"start":644,"end":646},{"type":1,"start":650,"end":651},{"type":1,"start":670,"end":672},{"type":1,"start":677,"end":679},{"type":1,"start":692,"end":693},{"type":1,"start":718,"end":720},{"type":1,"start":727,"end":733},{"type":2,"start":0,"end":111},{"type":2,"start":188,"end":250}]},{"name":"da41","type":1,"text":"SUPERINTELLIGENCE MUCH‚ÄΩ SELF-RECURSIVE IMPROVEMENT MUCH‚ÄΩ THE END OF THE UNIVERSE MUCH‚ÄΩ","markups":[{"type":1,"start":0,"end":86},{"type":2,"start":0,"end":86}]},{"name":"7094","type":1,"text":"Nope. Just some code doesn‚Äôt compile or make any sense. It even has its own bogus comments!","markups":[]},{"name":"b3fc","type":1,"text":"Generating music? Easy! A fun watch:","markups":[]},{"name":"eae8","type":11,"text":"","markups":[],"layout":1,"iframe":{"mediaResourceId":"8de70b7fa3e5cb979099278112052953","iframeWidth":854,"iframeHeight":480,"thumbnailUrl":"https://i.embed.ly/1/image?url=https%3A%2F%2Fi.ytimg.com%2Fvi%2FA2gyidoFsoI%2Fhqdefault.jpg&key=4fce0568f2ce49e8b54624ef71a8a5bd"}},{"name":"c80a","type":1,"text":"A more informative watch:","markups":[]},{"name":"565f","type":11,"text":"","markups":[],"layout":1,"iframe":{"mediaResourceId":"3f992ef4ac506dafa8d2d8badfc31dc2","iframeWidth":854,"iframeHeight":480,"thumbnailUrl":"https://i.embed.ly/1/image?url=https%3A%2F%2Fi.ytimg.com%2Fvi%2FaSr8_QQYpYM%2Fhqdefault.jpg&key=4fce0568f2ce49e8b54624ef71a8a5bd"}},{"name":"6518","type":1,"text":"Something even cooler and‚Ä¶ creepier (seriously, the results after the first couple iterations of training are so unsettling):","markups":[]},{"name":"7c5c","type":11,"text":"","markups":[],"layout":1,"iframe":{"mediaResourceId":"b5f70a5d514e61ad4646217c70974843","iframeWidth":854,"iframeHeight":480,"thumbnailUrl":"https://i.embed.ly/1/image?url=https%3A%2F%2Fi.ytimg.com%2Fvi%2FNG-LATBZNBs%2Fhqdefault.jpg&key=4fce0568f2ce49e8b54624ef71a8a5bd"}},{"name":"7eb1","type":3,"text":"In Practice","markups":[]},{"name":"a21c","type":1,"text":"So we‚Äôve seen how RNNs work in theory; now where do they fit in in practice?","markups":[]},{"name":"e928","type":1,"text":"As it turns out, recurrent neural networks can do a whole lot. I‚Äôll try to cover a few of the important, significant, and interesting uses that have cropped up over the last few years.","markups":[]},{"name":"e30a","type":13,"text":"Bidirectional Recurrent Neural Networks","markups":[]},{"name":"5985","type":1,"text":"The Problem: Giving the network access to a sequence of vectors is fine and dandy, but what if we want our output at time t to be conditioned on an input vector that comes at a later timestep? Take the example of speech recognition, where our input vectors are some kind of audio features at time t and the output is the predicted phoneme at that time. In our traditional RNN architecture, the output at time t is conditioned only on input vectors 1..t, but as it turns out future information might be useful too. The sounds at time step t+1 (and maybe t+2, t+3, ‚Ä¶) are likely part of the same phoneme, and therefore could help us make more accurate predictions. But our network won‚Äôt have access to them until we already output a prediction at time t. That‚Äôs bad.","markups":[{"type":3,"start":331,"end":338,"href":"https://en.wikipedia.org/wiki/Phoneme","title":"","rel":"noopener","anchorType":0},{"type":1,"start":0,"end":11},{"type":1,"start":122,"end":123},{"type":1,"start":297,"end":298},{"type":1,"start":409,"end":410},{"type":1,"start":448,"end":452},{"type":1,"start":538,"end":542},{"type":1,"start":553,"end":556},{"type":1,"start":558,"end":561},{"type":1,"start":750,"end":751},{"type":2,"start":338,"end":339},{"type":2,"start":426,"end":430}]},{"name":"60ef","type":1,"text":"The Solution: We essentially ‚Äúdouble up‚Äù each RNN neuron into two independent neurons ‚Äî a ‚Äúforward‚Äù neuron and a ‚Äúbackward‚Äù neuron. The forward neuron is the same as a regular RNN neuron, which gets inputs 0..T sequentially, updating its internal state and outputting some value at each time step along the way. The backward neuron follows the same general principle, but it sees the input vectors in reverse order.","markups":[{"type":1,"start":0,"end":12},{"type":1,"start":206,"end":210}]},{"name":"e2ef","type":1,"text":"We‚Äôll look at an example to make sense of all this.","markups":[]},{"name":"2749","type":4,"text":"This is a typical recurrent neural network: at each timestep, the hidden state is updated based on the latest input.","markups":[],"layout":4,"metadata":{"id":"1*Vsvw39SW0xEwRLijLRb3qg.png","originalWidth":404,"originalHeight":504}},{"name":"89e5","type":4,"text":"This is a bidirectional recurrent neural network. There are two neurons: one that takes inputs like normal, and one that takes them in reverse. Their output is combined to produce one output.","markups":[],"layout":4,"metadata":{"id":"1*JZNjmHjYFVcHrPKTDmvoXQ.png","originalWidth":606,"originalHeight":504}},{"name":"ea03","type":1,"text":"Let‚Äôs walk through this timestep-by-timestep. At t=0, our vanilla RNN cell takes the input, updates its hidden state, and outputs a value. Now let‚Äôs look at the BiRNN: the ‚Äúforward‚Äù half of our BiRNN neuron does exactly the same thing, but the ‚Äúbackward‚Äù half looks through all of our inputs ‚Äî in reverse order, t=T..0 ‚Äî and updates its hidden state with each one. Then when we get to the t=0 input vector, it updates its hidden state one last time and outputs a final value. We then take this final output value and combine it with the ‚Äúforward‚Äù half (‚Äúcombine‚Äù is pretty loosely-defined, usually just by concatenation or addition). Moving on to t=1, our ‚Äúforward‚Äù part reads in the next input, updates state, and outputs another value. Combined with the second-to-last output of our ‚Äúbackward‚Äù counterpart, and we have the second output of our BiRNN neuron. Rinse and repeat.","markups":[{"type":1,"start":49,"end":52},{"type":1,"start":312,"end":318},{"type":1,"start":389,"end":392},{"type":1,"start":647,"end":650}]},{"name":"1bf8","type":1,"text":"And that‚Äôs the general idea. Neat, right? BiRNNs (and their more adept cousin, BiLSTMs) are used all over the place. Maybe we‚Äôll see them popping up in some of the other case studies that we‚Äôll be looking at.","markups":[]},{"name":"7549","type":13,"text":"Autoencoders","markups":[]},{"name":"cdb3","type":1,"text":"Remember when we talked about autoencoders? Turns out we can use RNNs there too!","markups":[{"type":3,"start":30,"end":42,"href":"https://ayearofai.com/lenny-2-autoencoders-and-word-embeddings-oh-my-576403b0113a#.oevzdnnnp","title":"","rel":"noopener","anchorType":0}]},{"name":"107c","type":1,"text":"Let‚Äôs refresh: what is an autoencoder? Put simply, it‚Äôs a clever way of tricking a neural network to learn a useful representation of some data. Let‚Äôs say we have a dataset of images of faces, and we want to compress the thousands of numbers representing RGB values of pixels down into a 500-dimensional latent vector. We construct a network as such, where the middle layer has 500 neurons:","markups":[]},{"name":"08e3","type":4,"text":"https://en.wikipedia.org/wiki/Autoencoder#/media/File:Autoencoder_structure.png","markups":[{"type":3,"start":0,"end":79,"href":"https://en.wikipedia.org/wiki/Autoencoder#/media/File:Autoencoder_structure.png","title":"","rel":"nofollow noopener noopener","anchorType":0}],"layout":1,"metadata":{"id":"0*M1bVZtZ6UPTyXoiy.","originalWidth":677,"originalHeight":506}},{"name":"fe44","type":1,"text":"‚Ä¶and train it to reproduce the input in the output.","markups":[]},{"name":"46d5","type":1,"text":"Let‚Äôs explore this idea a little further. Imagine that data is flowing through our network, starting with the input layer and through each subsequent layer. We can view each layer as performing a transformation, converting our input to another vector, and then that vector into another, until we get our output. If we train our network to reproduce the input, that means that each intermediate vector must still represent the same information as the input, in some form or another. Essentially, the activations of each layer are a new representation of our input vector. If our network trains well, we can convert a 10,000-dimensional vector of pixel values into a 500-dimensional vector of image features which can be converted back into a 10,000-dimensional vector of pixel values that approximates what the input would have been.","markups":[]},{"name":"f1d3","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"0*RP5VZyqDJ9JI5wBk.","originalWidth":201,"originalHeight":34}},{"name":"1008","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"0*Tc8mc_NWMmQZ15ND.","originalWidth":200,"originalHeight":34}},{"name":"c1de","type":1,"text":"Let‚Äôs make this a tad more concrete. We have two functions, f and g. f is our encoder, mapping from an n-long vector to an m-long vector. (n is the size of our input, m is the size of our latent representation.) g is our decoder, which maps back from an m-long vector to an n-long vector. In the normal autoencoder setting, both f and g are neural networks trained jointly (or different parts of a single network, same thing really) to reconstruct x.","markups":[{"type":1,"start":60,"end":61},{"type":1,"start":66,"end":67},{"type":1,"start":69,"end":70},{"type":1,"start":103,"end":104},{"type":1,"start":123,"end":124},{"type":1,"start":139,"end":140},{"type":1,"start":167,"end":168},{"type":1,"start":212,"end":213},{"type":1,"start":254,"end":255},{"type":1,"start":274,"end":275},{"type":1,"start":329,"end":330},{"type":1,"start":335,"end":336}]},{"name":"f029","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"0*1R_heM-ujpUvlbGM.","originalWidth":182,"originalHeight":37}},{"name":"1680","type":1,"text":"So, where do RNNs fit in? Let‚Äôs say our inputs are now sequences of vectors instead of a single vector. We can use a similar concept, with both the encoder and decoder represented using an RNN. Here‚Äôs how it works: we feed our input sequence into the encoder RNN. With each input vector of the sequence, this encoder updates its internal state. Eventually, once it has seen the entire input, we have some final network internal state which represents our entire input sequence. Neat! Now, we make the hidden state of our decoder RNN the initial hidden state of our encoder, and ask it to spit out a sequence. Ideally, it spits out something close to what the initial sequence was.","markups":[{"type":2,"start":251,"end":262},{"type":2,"start":309,"end":316},{"type":2,"start":521,"end":532},{"type":2,"start":565,"end":572}]},{"name":"b8c2","type":1,"text":"Going back to our math-y definitions, we see that it basically fits in to the same framework, except we have q n-long vectors going into f and coming out of g. So q n-long vectors go in to f, and a single m-long vector comes out. We then give this m-long vector back to g, which spits out q n-long vectors.","markups":[{"type":1,"start":109,"end":110},{"type":1,"start":137,"end":138},{"type":1,"start":157,"end":158},{"type":1,"start":163,"end":164},{"type":1,"start":165,"end":166},{"type":1,"start":189,"end":190},{"type":1,"start":205,"end":206},{"type":1,"start":248,"end":249},{"type":1,"start":270,"end":271},{"type":1,"start":289,"end":290},{"type":1,"start":291,"end":292}]},{"name":"5a64","type":1,"text":"That was a lot of letters, but you get the idea (I hope).","markups":[]},{"name":"b829","type":1,"text":"Like much of deep learning, the concept itself is pretty simple, but the implications are pretty cool. We can take any sequence ‚Äî a variable-length sequence, mind you ‚Äî and convert it into a fixed-size vector. And then convert that back to a variable-length sequence.","markups":[]},{"name":"5546","type":1,"text":"It turns out this model is actually incredibly powerful, so let‚Äôs take a look at one particularly useful (and successful) application: machine translation.","markups":[]},{"name":"4be2","type":13,"text":"Neural Machine Translation","markups":[]},{"name":"9d22","type":1,"text":"Let‚Äôs take these ideas we just learned about sequence-to-sequence (or seq2seq, for short) RNNs and apply them to machine translation. We throw in a sequence of words in one language, and it outputs a sequence of words in another. Simple enough, right?","markups":[]},{"name":"cd0e","type":1,"text":"The model we‚Äôre going to look at specifically is Google‚Äôs implementation of NMT. You can read all the gory details in their paper, but for now why don‚Äôt I give you the watered-down version.","markups":[{"type":3,"start":115,"end":129,"href":"https://arxiv.org/pdf/1609.08144.pdf","title":"","rel":"noopener","anchorType":0}]},{"name":"3f1b","type":1,"text":"At it‚Äôs core, the GNMT architecture is just another seq2seq model. We have an encoder, consisting of 8 LSTM layers with skip connections (the first layer is bidirectional). We also have a decoder, once again containing 8 LSTM layers with skip connections. (A skip connection in a neural network is a connection which skips a layer and connects to the next available layer.) The decoder network outputs a probability distribution of words (well, sort of ‚Äî we‚Äôll talk more about that later), which we sample from to get our [translated] sentence. üéâ","markups":[]},{"name":"afc6","type":1,"text":"Here‚Äôs a scary diagram from the paper:","markups":[]},{"name":"6a00","type":4,"text":"https://arxiv.org/abs/1609.08144","markups":[{"type":3,"start":0,"end":32,"href":"https://arxiv.org/abs/1609.08144","title":"","rel":"nofollow noopener noopener","anchorType":0}],"layout":1,"metadata":{"id":"0*mk1BeF8ANMbAVOzD.","originalWidth":656,"originalHeight":363}},{"name":"a450","type":1,"text":"But there are a few other aspects to the GNMT that are important to note (there‚Äôs actually lots of interesting stuff going on in this architecture, so I really recommend you do read the paper).","markups":[{"type":3,"start":177,"end":191,"href":"https://arxiv.org/pdf/1609.08144.pdf","title":"","rel":"noopener","anchorType":0}]},{"name":"1fbc","type":1,"text":"Let‚Äôs turn our attention to the center of the above diagram. This is a critical part of the GNMT architecture (and GNMT is certainly not the first to use attention) which allows the decoder to focus on certain parts of the encoder‚Äôs output as it produces output. Specifically, the GNMT architecture differs from the traditional seq2seq model in that our encoder does not produce a single fixed-width vector (the final hidden state) representing the entire output. Instead, we actually look at the output from each time step, and each time step gives us some latent representation. While decoding, we combine all of these hidden vectors into one context vector using something called soft attention.","markups":[{"type":1,"start":370,"end":371},{"type":2,"start":15,"end":24},{"type":2,"start":367,"end":371},{"type":2,"start":645,"end":652},{"type":2,"start":683,"end":697}]},{"name":"369f","type":4,"text":"https://arxiv.org/abs/1609.08144","markups":[{"type":3,"start":0,"end":32,"href":"https://arxiv.org/abs/1609.08144","title":"","rel":"nofollow noopener noopener","anchorType":0}],"layout":1,"metadata":{"id":"0*ua03RdgdNWPw1_Jd.","originalWidth":704,"originalHeight":244}},{"name":"ddb5","type":1,"text":"More concretely, that works like this (at every decoder time step). We first look at the output of the first decoder layer from the last time step. Following the notation from the paper, we‚Äôll call that yi-1. We also have a series of encoder outputs, x1‚Ä¶xM, one for each encoder timestep. For each encoder timestep, we give our special attention function yi-1 and xt and get back a single fixed-size vector st, which we then run through a softmax. So, we‚Äôve converted our encoder information from that timestep (and some decoder information) into a single attention vector ‚Äî this attention vector tells us which parts of the encoder output we should look at more closely. We multiply this attention vector by our encoder output xt, which has the effect of ‚Äúfocusing‚Äù more on certain values and less on others. Finally, we take the sum of those ‚Äúfocused‚Äù vectors over each encoder timestep to produce our attention context for this timestep ai, which is fed to every decoder layer.","markups":[{"type":1,"start":203,"end":207},{"type":1,"start":251,"end":256},{"type":1,"start":355,"end":360},{"type":1,"start":364,"end":366},{"type":1,"start":407,"end":409},{"type":1,"start":728,"end":730},{"type":1,"start":940,"end":942},{"type":2,"start":132,"end":136},{"type":2,"start":298,"end":305},{"type":2,"start":382,"end":406}]},{"name":"8681","type":1,"text":"Oh yeah, that attention function? That‚Äôs just yet another neural network.","markups":[{"type":2,"start":50,"end":57}]},{"name":"145a","type":1,"text":"Attention mechanisms like this one are pretty common in many deep learning architectures. This is an example of soft attention: we learn a distribution over our inputs and compute a weighted sum. This process is fully-differentiated, so we can use standard backpropogation to figure out how to train our attention model. Another possible mechanism is called hard attention, in which we select just one of the possible inputs and ‚Äúfocus‚Äù solely on that input. This process is not differentiable, so we need to use some other algorithm (usually some kind of reinforcement learning) to train a hard attention algorithm.","markups":[{"type":2,"start":358,"end":372}]},{"name":"10b2","type":1,"text":"GNMT combines all kinds of other cool ideas to achieve state-of-the-art results, including a wordpiece model which segments words into smaller ‚Äúwordpieces‚Äù to help translate rarer words and neat parallelization techniques that let them train this monstrosity of an architecture in reasonable time.","markups":[]},{"name":"818e","type":1,"text":"A few months ago, Google put their GNMT model into production. Cutting-edge research is being implemented in the real world at an incredible rapid pace within the field of machine learning, and this is just one of countless examples.","markups":[{"type":3,"start":18,"end":61,"href":"https://blog.google/products/translate/found-translation-more-accurate-fluent-sentences-google-translate/","title":"","rel":"noopener","anchorType":0}]},{"name":"0c13","type":13,"text":"Long-Term Recurrent Convolutional Networks","markups":[]},{"name":"c3b8","type":1,"text":"(Not to be confused with LCRNs.)","markups":[]},{"name":"e750","type":1,"text":"The Problem: We have a sequence of images that we need to make predictions for. CNNs are good at processing images, RNNs are good at processing sequences‚Ä¶how do we put the two together?","markups":[{"type":1,"start":0,"end":11}]},{"name":"606b","type":1,"text":"The Solution: The solution proposed in this paper is as straightforward as you would expect: take your image, extract features using a CNN, and feed this feature vector to your LSTM.","markups":[{"type":3,"start":39,"end":49,"href":"https://arxiv.org/pdf/1411.4389.pdf","title":"","rel":"noopener","anchorType":0},{"type":1,"start":0,"end":12}]},{"name":"fdf8","type":4,"text":"https://arxiv.org/abs/1411.4389","markups":[{"type":3,"start":0,"end":31,"href":"https://arxiv.org/abs/1411.4389","title":"","rel":"nofollow noopener noopener","anchorType":0}],"layout":1,"metadata":{"id":"0*qiQ7DvCkHydXAFZ1.","originalWidth":355,"originalHeight":319}},{"name":"1c9f","type":1,"text":"That‚Äôs really all there is to it, and the reason it works is because (as we‚Äôve seen before) CNNs are incredibly adept at converting raw pixel data to a more meaningful representation. This saves the LSTMs the problem of parsing through the pixels to figure out what‚Äôs going on in the image and allows the LSTM weights to focus on converting a vector of image features into some meaningful sequence (say, a caption). It‚Äôs the same reason that using a word embedding is often preferred to a one-hot vector when feeding in words to an NLP model: the more meaningful your representation is, the easier it is to make further predictions with it.","markups":[{"type":3,"start":70,"end":90,"href":"https://ayearofai.com/rohan-lenny-2-convolutional-neural-networks-5f4cd480a60b?gi=25a4c415cc58","title":"","rel":"noopener","anchorType":0}]},{"name":"b189","type":13,"text":"Image Captioning","markups":[]},{"name":"3e15","type":1,"text":"(To be confused with LCRNs!)","markups":[]},{"name":"7c86","type":1,"text":"So there has been a lot of really impressive work on image captioning lately, but I wanna give a special shout-out to this 2015 paper from Karpathy et al. It was one of the first ML papers I had ever read, and really got me excited about the field. And, it uses RNNs, so that‚Äôs cool too.","markups":[{"type":3,"start":118,"end":133,"href":"http://cs.stanford.edu/people/karpathy/cvpr2015.pdf","title":"","rel":"noopener","anchorType":0},{"type":2,"start":148,"end":153}]},{"name":"ca5d","type":1,"text":"The idea behind image captioning is kind of self-explanatory, but I‚Äôll explain it anyway. You give the model an image, it gives you a caption. Which is kind of insane if you think about it ‚Äî a computer can go from pixels to interpreting what it‚Äôs seeing, and from that generate real and grammatical sentences to explain what it sees. I still can‚Äôt really believe stuff like this actually works, but somehow it does.","markups":[]},{"name":"3370","type":1,"text":"The model described in this specific paper combines our old pal convolutional neural networks with our newly-discovered RNNs. Step 1 is to pass our image through a convolutional neural network and extract some features from the last fully-connected layer. This lets us convert our pixel representation of the image into something that‚Äôs hopefully a bit more meaningful. We take this image feature vector and use it to initialize the hidden state of our RNN.","markups":[]},{"name":"de23","type":1,"text":"This is where it gets cool. We feed our network a start token, and it gives us a word (more accurately, a distribution of words, which we sample to get the first word of our caption). We feed this word back as the next input, and sample another word from the output. And again, and again, and again, until we finally sample an end token and have a complete caption.","markups":[]},{"name":"0330","type":1,"text":"It‚Äôs not strictly necessary to feed the word that we sampled back to the network, but that‚Äôs pretty common practice to help the network condition its output on the previous word (the hidden state is critical for this too, of course). The results from this particular paper were pretty cool, you can see some of the results here.","markups":[{"type":3,"start":323,"end":327,"href":"http://cs.stanford.edu/people/karpathy/deepimagesent/","title":"","rel":"noopener","anchorType":0}]},{"name":"6ec4","type":13,"text":"Neural Machine Translation, Again","markups":[]},{"name":"aba2","type":1,"text":"Yes, NMTs are just that cool that I need to talk about them again.","markups":[]},{"name":"7a0e","type":1,"text":"The Problem: With our good ol‚Äô GNMT architecture, we can train a massive model to convert from language A to language B. That‚Äôs great ‚Äî except, if we support more than a hundred languages, we need to train more than 10,000 different language-pair models, each of which can take months to converge. That‚Äôs no good, and it‚Äôs the reason that when Google put GNMT in production, they only did so for eight language-pairs (still a monumental achievement). But‚Ä¶what if we didn‚Äôt need to train a separate model for each language pair? What if we could train one model for all the language pairs ‚Äî impossible, right?","markups":[{"type":1,"start":0,"end":11},{"type":1,"start":216,"end":222}]},{"name":"1717","type":1,"text":"The Solution: Apparently it‚Äôs not impossible, and to make things even crazier, we can use the original GNMT architecture without modification. The only real change is that we prepend a special language token to the beginning of each sequence telling it what language to translate to. (We also use one shared wordpiece model for all language, instead of one per language pair.)","markups":[{"type":1,"start":0,"end":12},{"type":1,"start":79,"end":141}]},{"name":"90b7","type":1,"text":"So we‚Äôve condensed tens of thousands of NMT models into a single model that is able to translate to and from any language it was trained on. The paper elaborates on the implications and benefits of this more than I will, but to summarize:","markups":[{"type":3,"start":141,"end":150,"href":"https://arxiv.org/pdf/1611.04558.pdf","title":"","rel":"noopener","anchorType":0}]},{"name":"2db1","type":9,"text":"One model instead of tens of thousands. Months of training time saved, simpler production deployment, fewer parameters ‚Äî simplicity wins out over complexity.","markups":[]},{"name":"66a7","type":9,"text":"We might have more training data for some language pairs than others. When we have separate models for each language pair, this means that the pairs with less data will have significantly poorer performance. If we put them all into one model, the language pairs with less data can still benefit from all of the data in the other language pairs, because all of the language pairs share weights (since they all use the same model).","markups":[]},{"name":"2a4f","type":9,"text":"This one is absolutely nuts. If we train our network to translate English ‚Üí Spanish and Spanish ‚Üí French, our network automatically knows how to translate English ‚Üí French (reasonably well).","markups":[{"type":1,"start":106,"end":171}]},{"name":"0314","type":1,"text":"Expanding on that last point some more: the authors of the paper even found evidence of an interlingua, or an intermediate representation that is shared by multiple languages. Being able to learn an interlingua is the ideal end goal to create a fully generalized multilingual NMT: we learn an encoder/decoder to convert to/from the interlingua for each language, and we immediately know how to translate to and from that language. We aren‚Äôt quite there yet, but this is a major step in that direction. Creating a larger multilingual NMT model and giving it even more data could be all it takes to achieve new state-of-the-art translation results.","markups":[{"type":2,"start":91,"end":102},{"type":2,"start":441,"end":446}]},{"name":"e74d","type":13,"text":"So, yeah","markups":[]},{"name":"1de9","type":1,"text":"RNNs are pretty awesome. There are new RNN papers published literally every day and it‚Äôs impossible to cover everything ‚Äî if you think I missed something important, definitely let me know. (From Rohan: Except Neural Turing Machines and Learning to Learn. Those are dope, we know it, and we‚Äôre going to be covering them soon!)","markups":[{"type":3,"start":176,"end":187,"href":"https://twitter.com/LennyKhazan","title":"","rel":"noopener","anchorType":0}]},{"name":"449c","type":3,"text":"Building a Vanilla Recurrent Neural Network","markups":[]},{"name":"55e8","type":1,"text":"Let‚Äôs get practical for a minute and see how we can build one of these things in practice. We‚Äôll stick with char-rnn (the single layer character level language model we talked about much earlier) with raw numpy so we can see the nitty-gritty details, but if you‚Äôre using one of these in practice there are much better solutions! For out-of-the-box functional deep learning models Keras is the de facto framework that people seem to use. For more creative models and all kinds of other fancy stuff I‚Äôm a fan of the newly-released PyTorch, or the ‚Äúolder‚Äù TensorFlow.","markups":[{"type":3,"start":380,"end":385,"href":"https://keras.io/","title":"","rel":"noopener","anchorType":0},{"type":3,"start":529,"end":536,"href":"http://pytorch.org/","title":"","rel":"noopener","anchorType":0},{"type":3,"start":553,"end":563,"href":"https://www.tensorflow.org/","title":"","rel":"noopener","anchorType":0},{"type":2,"start":296,"end":328}]},{"name":"5997","type":1,"text":"I‚Äôm going to walk us through this implementation line by line so we can see exactly what‚Äôs going on. It‚Äôs really well-commented, so feel free to peruse it on your own too.","markups":[{"type":3,"start":29,"end":48,"href":"https://gist.github.com/karpathy/d4dee566867f8291f086","title":"","rel":"noopener","anchorType":0}]},{"name":"8745","type":1,"text":"Afterwards, I challenge you to code an LSTM!","markups":[]},{"name":"4989","type":11,"text":"","markups":[],"layout":1,"iframe":{"mediaResourceId":"515164c3f20643d9534d745371d34b9f","thumbnailUrl":"https://i.embed.ly/1/image?url=https%3A%2F%2Favatars0.githubusercontent.com%2Fu%2F241138%3Fv%3D3%26s%3D400&key=4fce0568f2ce49e8b54624ef71a8a5bd"}},{"name":"8a37","type":1,"text":"import numpy as np","markups":[{"type":10,"start":0,"end":18}]},{"name":"54ad","type":1,"text":"Well, duh.","markups":[]},{"name":"a14a","type":1,"text":"data = open(‚Äòinput.txt‚Äô, ‚Äòr‚Äô).read()\nchars = list(set(data))\ndata_size, vocab_size = len(data), len(chars)\nprint ‚Äòdata has %d characters, %d unique.‚Äô % (data_size, vocab_size)\nchar_to_ix = { ch:i for i,ch in enumerate(chars) }\nix_to_char = { i:ch for i,ch in enumerate(chars) }","markups":[{"type":10,"start":0,"end":277}]},{"name":"3f08","type":1,"text":"We load in our data and get a list of all of the characters that appear in it. We set up two dictionaries: one mapping characters to an index, and one for the reverse. We‚Äôll use this when converting characters to/from a one-hot encoding later on.","markups":[]},{"name":"f0a4","type":1,"text":"hidden_size = 100\nseq_length = 25\nlearning_rate = 1e-1","markups":[{"type":10,"start":0,"end":54}]},{"name":"b15a","type":1,"text":"Typical hyperparam stuff. Our RNN layer will have a hidden size of 100, and we‚Äôll train our network on batches of 25 characters at a time. Since we‚Äôll be training our network with BPTT, we need to make sure the sequences are sufficiently short that we can unroll the network all the way and keep everything in memory. Finally, set the learning rate to .1.","markups":[]},{"name":"a836","type":1,"text":"Wxh = np.random.randn(hidden_size, vocab_size)*0.01 # input to hidden\nWhh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden\nWhy = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output\nbh = np.zeros((hidden_size, 1)) # hidden bias\nby = np.zeros((vocab_size, 1)) # output bias","markups":[{"type":10,"start":0,"end":69},{"type":10,"start":70,"end":303}]},{"name":"7dce","type":1,"text":"We set up our parameters ‚Äî note that this is just a typical RNN, no fancy LSTM cells. We have weight matrices for updating our hidden state with each input, updating our hidden state with each timestep, and producing an output (and biases for our hidden state + output). We could be doing some fancy weight initialization here, but some normally-distributed randomness is sufficient for breaking symmetry.","markups":[]},{"name":"c19b","type":1,"text":"Now let‚Äôs talk loss function. We start by computing the forward pass, then computing the backward pass, just like with any neural network.","markups":[]},{"name":"08d8","type":1,"text":"xs, hs, ys, ps = {}, {}, {}, {}\nhs[-1] = np.copy(hprev)\nloss = 0","markups":[{"type":10,"start":0,"end":64}]},{"name":"7017","type":1,"text":"We start off by just setting up some variables to store our one-hot inputs, hidden states, outputs, and softmax probabilities.","markups":[]},{"name":"03ad","type":1,"text":"for t in xrange(len(inputs)):","markups":[{"type":10,"start":0,"end":29}]},{"name":"7bca","type":1,"text":"Go through each timestep, and for each timestep‚Ä¶","markups":[]},{"name":"d87e","type":1,"text":"xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation\nxs[t][inputs[t]] = 1","markups":[{"type":10,"start":0,"end":87}]},{"name":"0962","type":1,"text":"Convert our input character at this timestep to a one-hot vector.","markups":[]},{"name":"ea9f","type":1,"text":"hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state","markups":[{"type":10,"start":0,"end":78}]},{"name":"de7c","type":1,"text":"Update our hidden state. We saw this formula already ‚Äî use our Wxh and Whh matrices to update our hidden state based on the last state and our input, and add a bias.","markups":[{"type":1,"start":63,"end":66},{"type":1,"start":71,"end":74}]},{"name":"15c1","type":1,"text":"ys[t] = np.dot(Why, hs[t]) + by","markups":[{"type":10,"start":0,"end":31}]},{"name":"26df","type":1,"text":"Compute our output‚Ä¶","markups":[]},{"name":"4308","type":1,"text":"ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars","markups":[{"type":10,"start":0,"end":76}]},{"name":"46cd","type":1,"text":"‚Ä¶and convert it to a probability distribution with a softmax.","markups":[]},{"name":"b3bb","type":1,"text":"loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)","markups":[{"type":10,"start":0,"end":67}]},{"name":"011a","type":1,"text":"Accumulate the loss for this time step as the negative log of the predicted probability. Ideally, we would have a probability of 1 for the actual next character. If it is 1, the loss is 0, log(1) = 0. As the predicted probability approaches 0, the loss approaches inf, because log(0) = -inf.","markups":[{"type":2,"start":139,"end":145}]},{"name":"d366","type":1,"text":"That‚Äôs it for the forward pass (not bad, right? Boiled down, it‚Äôs like six lines of code. Piece of cake).","markups":[]},{"name":"b1db","type":1,"text":"dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n dhnext = np.zeros_like(hs[0])","markups":[{"type":10,"start":0,"end":77},{"type":10,"start":78,"end":126},{"type":10,"start":127,"end":157}]},{"name":"cdb0","type":1,"text":"Setting up some variables for our backward pass ‚Äî the gradients of our weight matrices, the gradients for our biases, and the gradients from the next timestep (we‚Äôll see how that works in a bit).","markups":[]},{"name":"2fd8","type":1,"text":"for t in reversed(xrange(len(inputs))):","markups":[{"type":10,"start":0,"end":39}]},{"name":"14bf","type":1,"text":"Go through our sequence in reverse as we back up the gradients.","markups":[]},{"name":"9148","type":1,"text":"dy = np.copy(ps[t])\n dy[targets[t]] -= 1 # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here","markups":[{"type":10,"start":0,"end":137},{"type":3,"start":64,"end":120,"href":"http://cs231n.github.io/neural-networks-case-study/#grad","title":"","rel":"noopener","anchorType":0}]},{"name":"3fb3","type":1,"text":"First, get the gradient of the output, dy. As it turns out, the gradient of the cross-entropy loss is really as copying over the distribution and subtracting 1 from the correct class.","markups":[{"type":3,"start":43,"end":58,"href":"http://cs231n.github.io/neural-networks-case-study/#grad","title":"","rel":"noopener","anchorType":0}]},{"name":"b8b8","type":1,"text":"Remember backpropogation? When we have a weighted sum, the gradient of the weights is just the corresponding value that it is being multiplied by, because the other terms drop out and that one weight is treated as a constant. So, computing the gradient of our Why matrix is super simple: just multiply the gradient of loss w.r.t. the output (dy) by the derivative of the output w.r.t. Why (which is just the hidden state at our given timestep), and we get the derivative of the loss w.r.t. Why.","markups":[{"type":3,"start":0,"end":24,"href":"https://ayearofai.com/rohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d","title":"","rel":"noopener","anchorType":0}]},{"name":"9790","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"0*TVvKSJJqaM9CDjlk.","originalWidth":426,"originalHeight":86}},{"name":"247c","type":1,"text":"dWhy += np.dot(dy, hs[t].T)","markups":[{"type":10,"start":0,"end":27}]},{"name":"2ff0","type":1,"text":"Like the other gradients (except dy, of course) we accumulate these gradients over all timesteps and apply them at the end.","markups":[]},{"name":"c397","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"0*QKDwSXVEJ9fHQ4hT.","originalWidth":321,"originalHeight":86}},{"name":"1c4f","type":1,"text":"dby += dy","markups":[{"type":10,"start":0,"end":9}]},{"name":"a781","type":1,"text":"The derivative of loss w.r.t. output (dy) multiplied by the derivative of our output w.r.t. the bias (which is 1) gives us the derivative of our output w.r.t. the bias. So far so good.","markups":[]},{"name":"6890","type":1,"text":"dh = np.dot(Why.T, dy) + dhnext # backprop into h","markups":[{"type":10,"start":0,"end":49}]},{"name":"9602","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*cVr1t2s7gsC4Sd6vowSkHA.png","originalWidth":265,"originalHeight":74}},{"name":"6dbe","type":1,"text":"We compute dL/dh using the chain rule, and accumulate it over all timesteps (hence + dhnext). We‚Äôll need this for the next step.","markups":[{"type":10,"start":83,"end":91}]},{"name":"6d9e","type":1,"text":"dhraw = (1 ‚Äî hs[t] * hs[t]) * dh # backprop through tanh nonlinearity","markups":[{"type":10,"start":0,"end":69}]},{"name":"796c","type":1,"text":"This computes the derivative of the np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) line from earlier.","markups":[{"type":10,"start":36,"end":91}]},{"name":"7fa1","type":1,"text":"dbh += dhraw","markups":[{"type":10,"start":0,"end":12}]},{"name":"f74d","type":1,"text":"Which is also our bh derivative, for the same reason that the by derivative was just dy.","markups":[]},{"name":"a0f2","type":1,"text":"dWxh += np.dot(dhraw, xs[t].T)\ndWhh += np.dot(dhraw, hs[t-1].T)","markups":[{"type":10,"start":0,"end":63}]},{"name":"cea8","type":1,"text":"We accumulate our weight gradients.","markups":[]},{"name":"1bb0","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*uf-YEbf0258UhbDc5QZLRw.png","originalWidth":286,"originalHeight":77}},{"name":"d79b","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*Vl1LVzPJSZKDplA9J5cElg.png","originalWidth":287,"originalHeight":77}},{"name":"012f","type":1,"text":"dhnext = np.dot(Whh.T, dhraw)","markups":[{"type":10,"start":0,"end":29}]},{"name":"bb22","type":1,"text":"And finally, store dh for this timestep so we can use it for the previous one.","markups":[]},{"name":"1ea0","type":1,"text":"for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\nnp.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients","markups":[{"type":10,"start":0,"end":117}]},{"name":"a901","type":1,"text":"Last but not least, a little gradient clipping so we don‚Äôt get no exploding gradients.","markups":[]},{"name":"8392","type":1,"text":"return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]","markups":[{"type":10,"start":0,"end":58}]},{"name":"4dc5","type":1,"text":"And then return all the gradients so we can apply an optimizer step. And that‚Äôs it for the backprop code; not too bad, right?","markups":[{"type":2,"start":110,"end":113}]},{"name":"4eaf","type":1,"text":"def sample(h, seed_ix, n):","markups":[{"type":10,"start":0,"end":26}]},{"name":"ed15","type":1,"text":"This method is used for sampling a generated sequence from the network, starting with state h, first letter seed_ix, with length n.","markups":[{"type":10,"start":92,"end":93},{"type":10,"start":108,"end":115},{"type":10,"start":129,"end":130}]},{"name":"d445","type":1,"text":"x = np.zeros((vocab_size, 1)) \nx[seed_ix] = 1","markups":[{"type":10,"start":0,"end":45}]},{"name":"a8f6","type":1,"text":"Set up our one-hot encoded input vector based on the seed character.","markups":[]},{"name":"8a82","type":1,"text":"ixes = []","markups":[{"type":10,"start":0,"end":9}]},{"name":"7e2b","type":1,"text":"And an array to keep track of our sequence.","markups":[]},{"name":"3cae","type":1,"text":"for t in xrange(n):","markups":[{"type":10,"start":0,"end":19}]},{"name":"c8e6","type":1,"text":"To generate each character in our sequence‚Ä¶","markups":[]},{"name":"18cf","type":1,"text":"h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)","markups":[{"type":10,"start":0,"end":49}]},{"name":"6d86","type":1,"text":"Update our hidden state! We saw this formula in the last function, too.","markups":[]},{"name":"ce6f","type":1,"text":"y = np.dot(Why, h) + by\np = np.exp(y) / np.sum(np.exp(y))","markups":[{"type":10,"start":0,"end":57}]},{"name":"d108","type":1,"text":"Generate our output and run it through a softmax. Again, straight from the last function.","markups":[]},{"name":"62ae","type":1,"text":"ix = np.random.choice(range(vocab_size), p=p.ravel())","markups":[{"type":10,"start":0,"end":53}]},{"name":"fd39","type":1,"text":"Sample from our output distribution using some numpy magic.","markups":[]},{"name":"cec1","type":1,"text":"x = np.zeros((vocab_size, 1))\nx[ix] = 1\nixes.append(ix)","markups":[{"type":10,"start":0,"end":55}]},{"name":"e8b3","type":1,"text":"Convert the sampled value into a one-hot encoding and append it to the array.","markups":[]},{"name":"4928","type":1,"text":"return ixes","markups":[{"type":10,"start":0,"end":11}]},{"name":"e074","type":1,"text":"‚Ä¶and of course, return the final sequence when we‚Äôre done.","markups":[]},{"name":"6968","type":1,"text":"n, p = 0, 0","markups":[{"type":10,"start":0,"end":11}]},{"name":"2eb1","type":1,"text":"n is the number of training iterations we‚Äôve done. p is the index into our training data for where we are now.","markups":[{"type":1,"start":0,"end":1},{"type":1,"start":51,"end":52}]},{"name":"ee12","type":1,"text":"mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)","markups":[{"type":10,"start":0,"end":77}]},{"name":"9951","type":1,"text":"Set up memory variables for the Adagrad algorithm (out of scope of this post, maybe next time ‚Äî it‚Äôs just a variant on gradient descent).","markups":[]},{"name":"ffc4","type":1,"text":"while True:","markups":[{"type":10,"start":0,"end":11}]},{"name":"bcde","type":1,"text":"Training loop.","markups":[]},{"name":"a04e","type":1,"text":"if p+seq_length+1 \x3e= len(data) or n == 0:","markups":[{"type":10,"start":0,"end":41}]},{"name":"3444","type":1,"text":"This is a little check to see if we need to reset our memory because we‚Äôre starting back at the beginning of our data.","markups":[]},{"name":"a0be","type":1,"text":"hprev = np.zeros((hidden_size,1)) # reset RNN memory","markups":[{"type":10,"start":0,"end":52}]},{"name":"038a","type":1,"text":"‚Ä¶and if we are, reset the memory.","markups":[]},{"name":"9c3d","type":1,"text":"p = 0","markups":[{"type":10,"start":0,"end":5}]},{"name":"bd23","type":1,"text":"And reset the data pointer.","markups":[]},{"name":"0161","type":1,"text":"inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\ntargets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]","markups":[{"type":10,"start":0,"end":118}]},{"name":"70c2","type":1,"text":"We grab a seq_length-long piece of the data as our input to the network. At each timestep, we want to predict the next character; this means that our ‚Äútargets‚Äù will be the next character for each input. We get a snippet of the input the same length as the input, but offset by 1, for the target.","markups":[{"type":10,"start":10,"end":20}]},{"name":"3f42","type":1,"text":"if n % 100 == 0:\nsample_ix = sample(hprev, inputs[0], 200)\ntxt = ‚Äò‚Äô.join(ix_to_char[ix] for ix in sample_ix)\nprint ‚Äò ‚Äî ‚Äî \\n %s \\n ‚Äî ‚Äî ‚Äò % (txt, )","markups":[{"type":10,"start":0,"end":145}]},{"name":"a7eb","type":1,"text":"Here we just print to the terminal a sample every 100 training steps so we can see how its doing. Ideally, this will print out gibberish the first few times, before gradually printing out more and more reasonable language.","markups":[]},{"name":"e412","type":1,"text":"loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)","markups":[{"type":10,"start":0,"end":73}]},{"name":"538d","type":1,"text":"Do a forward pass, backward pass, and get the gradients.","markups":[]},{"name":"9d99","type":1,"text":"smooth_loss = smooth_loss * 0.999 + loss * 0.001","markups":[{"type":10,"start":0,"end":48}]},{"name":"4ab1","type":1,"text":"Adagrad stuff.","markups":[]},{"name":"3823","type":1,"text":"if n % 100 == 0: print ‚Äòiter %d, loss: %f‚Äô % (n, smooth_loss) # print progress","markups":[{"type":10,"start":0,"end":78}]},{"name":"b38c","type":1,"text":"Keep up with progress.","markups":[]},{"name":"995f","type":1,"text":"for param, dparam, mem in zip([Wxh, Whh, Why, bh, by], [dWxh, dWhh, dWhy, dbh, dby], [mWxh, mWhh, mWhy, mbh, mby]):\nmem += dparam * dparam\nparam += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update","markups":[{"type":10,"start":0,"end":210}]},{"name":"9509","type":1,"text":"More Adagrad. We should really do an article on optimization algorithms.","markups":[]},{"name":"a2fc","type":1,"text":"p += seq_length # move data pointer\nn += 1 # iteration counter","markups":[{"type":10,"start":0,"end":62}]},{"name":"a889","type":1,"text":"Annnddd finally, we update our data pointer and iteration counter.","markups":[]},{"name":"7850","type":1,"text":"And that‚Äôs it. We have an RNN. Neat-o. Reminder: your challenge is to code an LSTM‚Ä¶ and TensorFlow doesn‚Äôt count!","markups":[]},{"name":"5a53","type":3,"text":"Conclusion","markups":[]},{"name":"8a00","type":1,"text":"Wow. That was a lot. If you came in knowing nothing or very little about recurrent neural nets, you sure as hell know a lot now. And you don‚Äôt just know about something cool; you know about something very important ‚Äî something that can equip you to read and understand some of the most prominent and hottest recent research papers in machine learning.","markups":[{"type":1,"start":16,"end":19},{"type":2,"start":16,"end":19},{"type":2,"start":200,"end":214}]},{"name":"eec5","type":1,"text":"Something this article didn‚Äôt do so good at was making sure the calculus and derivatives were in the context of operating on vectors (because, remember, RNNs/LSTMs operate over vectors). In many cases the derivatives were in the 1-D context. It‚Äôs not something you need to worry about, but you might want to look into.","markups":[]},{"name":"f6a4","type":1,"text":"We‚Äôre finally at the point where we can focus our energies on this blog towards cooler stuff including hot research papers like Neural Turing Machines or Learning to Learn, case studies eg. AlphaGo, other parts of machine learning and artificial intelligence (I‚Äôm, Rohan, personally looking forward to optimization!), or different algorithms like GANs. There‚Äôs very little compulsory content or ‚Äúgroundwork‚Äù we need to cover anymore. So, now, we‚Äôre officially onto the cool stuff.","markups":[]},{"name":"4f33","type":1,"text":"That‚Äôs right. A Year Of AI is officially‚Ä¶ cool.","markups":[{"type":1,"start":0,"end":13},{"type":2,"start":42,"end":46}]},{"name":"f41b","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*necEBipfgD-Z3_9R9usFgg.png","originalWidth":80,"originalHeight":80}}],"sections":[{"name":"20d1","startIndex":0},{"name":"4110","startIndex":3},{"name":"6590","startIndex":4},{"name":"da40","startIndex":258},{"name":"5df4","startIndex":297},{"name":"ccfc","startIndex":432},{"name":"70a1","startIndex":445}]},"postDisplay":{"coverless":true}},"virtuals":{"statusForCollection":"APPROVED","allowNotes":true,"previewImage":{"imageId":"1*khIKl9t4XmZGSsKhW_Yg2w.png","filter":"","backgroundSize":"","originalWidth":2000,"originalHeight":907,"strategy":"resample","height":0,"width":0},"wordCount":20536,"imageCount":115,"readingTime":83.9943396226415,"subtitle":"The ultimate guide to machine learning‚Äôs favorite child.","publishedInCount":1,"usersBySocialRecommends":[],"recommends":88,"socialRecommends":[],"isBookmarked":false,"tags":[{"slug":"machine-learning","name":"Machine Learning","postCount":12356,"virtuals":{"isFollowing":false},"metadata":{"followerCount":16196,"postCount":12356,"coverImage":{"id":"1*7MK-WgDf8HA8isSGDjPhyw.png","originalWidth":1800,"originalHeight":764}},"type":"Tag"},{"slug":"artificial-intelligence","name":"Artificial Intelligence","postCount":19403,"virtuals":{"isFollowing":false},"metadata":{"followerCount":584449,"postCount":19403,"coverImage":{"id":"1*gAn_BSffVBcwCIR6bDgK1g.jpeg"}},"type":"Tag"},{"slug":"data-science","name":"Data Science","postCount":9039,"virtuals":{"isFollowing":false},"metadata":{"followerCount":9762,"postCount":9039,"coverImage":{"id":"1*W2vzGrXR1ua5KN-X0m9oMw.jpeg","originalWidth":1920,"originalHeight":1431,"isFeatured":true}},"type":"Tag"},{"slug":"deep-learning","name":"Deep Learning","postCount":2237,"virtuals":{"isFollowing":false},"metadata":{"followerCount":5889,"postCount":2237,"coverImage":{"id":"1*khIKl9t4XmZGSsKhW_Yg2w.png","originalWidth":2000,"originalHeight":907}},"type":"Tag"},{"slug":"algorithms","name":"Algorithms","postCount":2129,"virtuals":{"isFollowing":false},"metadata":{"followerCount":1643,"postCount":2129,"coverImage":{"id":"1*khIKl9t4XmZGSsKhW_Yg2w.png","originalWidth":2000,"originalHeight":907}},"type":"Tag"}],"socialRecommendsCount":0,"responsesCreatedCount":4,"links":{"entries":[{"url":"http://imgur.com/gallery/vaNahKE","alts":[{"type":3,"url":"imgur://imgur.com/gallery/vaNahKE?from=fbreferral"},{"type":2,"url":"imgur://imgur.com/gallery/vaNahKE?from=fbreferral"}]},{"url":"https://blog.google/products/translate/found-translation-more-accurate-fluent-sentences-google-translate/","alts":[{"type":1,"url":"https://cdn.ampproject.org/c/blog.google:443/products/translate/found-translation-more-accurate-fluent-sentences-google-translate/amp/"}]},{"url":"https://www.youtube.com/watch?v=cO0a0QYmFm8&index=10&list=PLlJy-eBtNFt6EuMxFYRiNRS07MCWN5UIA","alts":[{"type":2,"url":"vnd.youtube://www.youtube.com/watch?v=cO0a0QYmFm8&list=PLlJy-eBtNFt6EuMxFYRiNRS07MCWN5UIA&index=10&feature=applinks"},{"type":3,"url":"vnd.youtube://www.youtube.com/watch?v=cO0a0QYmFm8&list=PLlJy-eBtNFt6EuMxFYRiNRS07MCWN5UIA&index=10&feature=applinks"}]},{"url":"https://ayearofai.com/the-goal-of-our-blog-c104d7b6377a#.75o5qyayi","alts":[{"type":2,"url":"medium://p/c104d7b6377a"},{"type":3,"url":"medium://p/c104d7b6377a"}]},{"url":"https://ayearofai.com/lenny-2-autoencoders-and-word-embeddings-oh-my-576403b0113a#.oevzdnnnp","alts":[{"type":2,"url":"medium://p/576403b0113a"},{"type":3,"url":"medium://p/576403b0113a"}]},{"url":"https://ayearofai.com/rohan-5-what-are-bias-units-828d942b4f52","alts":[{"type":2,"url":"medium://p/828d942b4f52"},{"type":3,"url":"medium://p/828d942b4f52"}]},{"url":"https://ayearofai.com/rohan-4-the-vanishing-gradient-problem-ec68f76ffb9b","alts":[{"type":3,"url":"medium://p/ec68f76ffb9b"},{"type":2,"url":"medium://p/ec68f76ffb9b"}]},{"url":"https://medium.com/a-year-of-artificial-intelligence/0-2016-is-the-year-i-venture-into-artificial-intelligence-d702d65eb919#.bfjoaqxu5","alts":[{"type":2,"url":"medium://p/d702d65eb919"},{"type":3,"url":"medium://p/d702d65eb919"}]},{"url":"https://ayearofai.com/rohan-4-the-vanishing-gradient-problem-ec68f76ffb9b#.58nevwsoa","alts":[{"type":2,"url":"medium://p/ec68f76ffb9b"},{"type":3,"url":"medium://p/ec68f76ffb9b"}]},{"url":"https://ayearofai.com/rohan-lenny-2-convolutional-neural-networks-5f4cd480a60b#.ebwc0kf4z","alts":[{"type":2,"url":"medium://p/5f4cd480a60b"},{"type":3,"url":"medium://p/5f4cd480a60b"}]},{"url":"https://ayearofai.com/rohan-lenny-2-convolutional-neural-networks-5f4cd480a60b?gi=25a4c415cc58","alts":[{"type":2,"url":"medium://p/5f4cd480a60b"},{"type":3,"url":"medium://p/5f4cd480a60b"}]},{"url":"https://ayearofai.com/rohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d#.quwnoqtot","alts":[{"type":2,"url":"medium://p/abf4609d4f9d"},{"type":3,"url":"medium://p/abf4609d4f9d"}]},{"url":"https://medium.com/a-year-of-artificial-intelligence","alts":[{"type":2,"url":"medium://a-year-of-artificial-intelligence"},{"type":3,"url":"medium://a-year-of-artificial-intelligence"}]},{"url":"https://ayearofai.com/rohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d","alts":[{"type":2,"url":"medium://p/abf4609d4f9d"},{"type":3,"url":"medium://p/abf4609d4f9d"}]}],"version":"0.3","generatedAt":1492389086840},"isLockedPreviewOnly":false,"takeoverId":"","metaDescription":"","totalClapCount":0},"coverless":true,"slug":"rohan-lenny-3-recurrent-neural-networks","translationSourcePostId":"","translationSourceCreatorId":"","isApprovedTranslation":false,"inResponseToPostId":"","inResponseToRemovedAt":0,"isTitleSynthesized":true,"allowResponses":true,"importedUrl":"","importedPublishedAt":0,"visibility":0,"uniqueSlug":"rohan-lenny-3-recurrent-neural-networks-10300100899b","previewContent":{"bodyModel":{"paragraphs":[{"name":"be8e","type":4,"text":"","markups":[],"layout":10,"metadata":{"id":"1*khIKl9t4XmZGSsKhW_Yg2w.png","originalWidth":2000,"originalHeight":907}},{"name":"84b9","type":3,"text":"Rohan & Lenny #3: Recurrent Neural Networks & LSTMs","markups":[],"alignment":1},{"name":"9485","type":13,"text":"The ultimate guide to machine learning‚Äôs favorite‚Ä¶","markups":[],"alignment":1}],"sections":[{"startIndex":0}]},"isFullContent":false},"license":0,"inResponseToMediaResourceId":"","canonicalUrl":"https://ayearofai.com/rohan-lenny-3-recurrent-neural-networks-10300100899b","approvedHomeCollectionId":"bb87da25612c","approvedHomeCollection":{"id":"bb87da25612c","name":"A Year of Artificial Intelligence","slug":"a-year-of-artificial-intelligence","tags":["DATA SCIENCE","ARTIFICIAL INTELLIGENCE","MACHINE LEARNING","TECHNOLOGY","COMPUTER SCIENCE"],"creatorId":"cb55958ea3bb","description":"Our ongoing effort to make the mathematics, science, linguistics, and philosophy of artificial intelligence fun and simple.","shortDescription":"Our ongoing effort to make the mathematics, science‚Ä¶","image":{"imageId":"1*NZsNSuNxe_O2YW1ybboOvA.jpeg","filter":"","backgroundSize":"","originalWidth":0,"originalHeight":0,"strategy":"resample","height":0,"width":0},"metadata":{"followerCount":521,"postCount":6,"activeAt":1492072051871},"virtuals":{"permissions":{"canPublish":false,"canPublishAll":false,"canRepublish":false,"canRemove":false,"canManageAll":false,"canSubmit":false,"canEditPosts":false,"canAddWriters":false,"canViewStats":false,"canSendNewsletter":false,"canViewLockedPosts":false,"canViewCloaked":false,"canEditOwnPosts":false,"canBeAssignedAuthor":false},"isSubscribed":false,"isNewsletterSubscribed":false,"memberOfMembershipPlanId":""},"logo":{"imageId":"1*NZsNSuNxe_O2YW1ybboOvA.jpeg","filter":"","backgroundSize":"","originalWidth":1500,"originalHeight":1000,"strategy":"resample","height":0,"width":0},"twitterUsername":"mckapur","facebookPageName":"mckapur","publicEmail":"me@rohankapur.com","domain":"ayearofai.com","sections":[{"type":2,"collectionHeaderMetadata":{"title":"A Year of Artificial Intelligence","description":"Where we make the mathematics, science, linguistics, and philosophy of artificial intelligence fun and simple.","backgroundImage":{"id":"1*UlHnUWTtTHWOrOuJdHjVww.png","originalWidth":2000,"originalHeight":1333,"focusPercentX":51.785714285714285,"focusPercentY":43.31550802139037},"logoImage":{"id":"1*NZsNSuNxe_O2YW1ybboOvA.jpeg","originalWidth":1500,"originalHeight":1000,"alt":"A Year of Artificial Intelligence"},"alignment":1,"layout":6}},{"type":1,"postListMetadata":{"source":1,"layout":2,"number":1,"postIds":[],"sectionHeader":"New"}},{"type":1,"postListMetadata":{"source":1,"layout":4,"number":14,"postIds":[]}}],"tintColor":"#FF000000","lightText":true,"favicon":{"imageId":"","filter":"","backgroundSize":"","originalWidth":0,"originalHeight":0,"strategy":"resample","height":0,"width":0},"colorPalette":{"defaultBackgroundSpectrum":{"colorPoints":[{"color":"#FF868484","point":0},{"color":"#FF7C7B7A","point":0.1},{"color":"#FF737171","point":0.2},{"color":"#FF696867","point":0.3},{"color":"#FF5F5E5E","point":0.4},{"color":"#FF555454","point":0.5},{"color":"#FF4A4949","point":0.6},{"color":"#FF3F3E3E","point":0.7},{"color":"#FF343333","point":0.8},{"color":"#FF272727","point":0.9},{"color":"#FF1A1A1A","point":1}],"backgroundColor":"#FFFFFFFF"},"tintBackgroundSpectrum":{"colorPoints":[{"color":"#FF000000","point":0},{"color":"#FF1E1D1D","point":0.1},{"color":"#FF3C3B3B","point":0.2},{"color":"#FF565555","point":0.3},{"color":"#FF6F6D6D","point":0.4},{"color":"#FF868484","point":0.5},{"color":"#FF9C9A99","point":0.6},{"color":"#FFB1AEAE","point":0.7},{"color":"#FFC5C3C2","point":0.8},{"color":"#FFD9D6D6","point":0.9},{"color":"#FFECE9E9","point":1}],"backgroundColor":"#FF000000"},"highlightSpectrum":{"colorPoints":[{"color":"#FFF5F2F1","point":0},{"color":"#FFF3F0EF","point":0.1},{"color":"#FFF1EEED","point":0.2},{"color":"#FFEFECEC","point":0.3},{"color":"#FFEDEAEA","point":0.4},{"color":"#FFEBE8E8","point":0.5},{"color":"#FFE9E6E6","point":0.6},{"color":"#FFE7E5E4","point":0.7},{"color":"#FFE5E3E2","point":0.8},{"color":"#FFE4E1E0","point":0.9},{"color":"#FFE2DFDE","point":1}],"backgroundColor":"#FFFFFFFF"}},"navItems":[{"type":1,"title":"Algorithms","tagSlug":"algorithms","url":"https://ayearofai.com/tagged/algorithms","source":"tagSlug"},{"type":1,"title":"Today I Learned","tagSlug":"today-i-learned","url":"https://ayearofai.com/tagged/today-i-learned","source":"tagSlug"},{"type":1,"title":"Case Studies","tagSlug":"case-studies","url":"https://ayearofai.com/tagged/case-studies","source":"tagSlug"},{"type":1,"title":"Philosophical","tagSlug":"philosophical","url":"https://ayearofai.com/tagged/philosophical","source":"tagSlug"},{"type":1,"title":"Meta","tagSlug":"meta","url":"https://ayearofai.com/tagged/meta","source":"tagSlug"}],"colorBehavior":2,"instantArticlesState":0,"acceleratedMobilePagesState":0,"ampLogo":{"imageId":"","filter":"","backgroundSize":"","originalWidth":0,"originalHeight":0,"strategy":"resample","height":0,"width":0},"header":{"title":"A Year of Artificial Intelligence","description":"Where we make the mathematics, science, linguistics, and philosophy of artificial intelligence fun and simple.","backgroundImage":{"id":"1*UlHnUWTtTHWOrOuJdHjVww.png","originalWidth":2000,"originalHeight":1333,"focusPercentX":51.785714285714285,"focusPercentY":43.31550802139037},"logoImage":{"id":"1*NZsNSuNxe_O2YW1ybboOvA.jpeg","originalWidth":1500,"originalHeight":1000,"alt":"A Year of Artificial Intelligence"},"alignment":1,"layout":6},"type":"Collection"},"newsletterId":"","webCanonicalUrl":"https://ayearofai.com/rohan-lenny-3-recurrent-neural-networks-10300100899b","mediumUrl":"https://ayearofai.com/rohan-lenny-3-recurrent-neural-networks-10300100899b","migrationId":"","notifyFollowers":true,"notifyTwitter":true,"isSponsored":false,"isRequestToPubDisabled":false,"notifyFacebook":false,"responseHiddenOnParentPostAt":0,"isSeries":false,"isSubscriptionLocked":false,"seriesLastAppendedAt":0,"type":"Post"},"mentionedUsers":[{"userId":"de8e2540b759","name":"Lenny Khazan","username":"lennykhazan","createdAt":1368901081367,"lastPostCreatedAt":1491975881207,"imageId":"1*vltCFMOhqgoMSe7Wa5xJaA.png","backgroundImageId":"","bio":"Tinkering with machine learning. http://getcontra.com/","twitterScreenName":"LennyKhazan","facebookAccountId":"","allowNotes":1,"mediumMemberAt":0,"mediumMemberWaitlistedAt":0,"type":"User"},{"userId":"cb55958ea3bb","name":"Rohan Kapur","username":"mckapur","createdAt":1383813905817,"lastPostCreatedAt":1492253036060,"imageId":"1*pGDfwp8yLzgCeG1RkPUxgQ.jpeg","backgroundImageId":"","bio":"rohankapur.com","twitterScreenName":"mckapur","socialStats":{"userId":"cb55958ea3bb","usersFollowedCount":204,"usersFollowedByCount":592,"type":"SocialStats"},"social":{"userId":"lo_6d640aa2d2ff","targetUserId":"cb55958ea3bb","type":"Social"},"facebookAccountId":"1004843339565980","allowNotes":1,"mediumMemberAt":0,"mediumMemberWaitlistedAt":0,"type":"User"}],"collaborators":[],"membershipPlans":[],"collectionUserRelations":[],"mode":null,"references":{"User":{"cb55958ea3bb":{"userId":"cb55958ea3bb","name":"Rohan Kapur","username":"mckapur","createdAt":1383813905817,"lastPostCreatedAt":1492253036060,"imageId":"1*pGDfwp8yLzgCeG1RkPUxgQ.jpeg","backgroundImageId":"","bio":"rohankapur.com","twitterScreenName":"mckapur","socialStats":{"userId":"cb55958ea3bb","usersFollowedCount":204,"usersFollowedByCount":592,"type":"SocialStats"},"social":{"userId":"lo_6d640aa2d2ff","targetUserId":"cb55958ea3bb","type":"Social"},"facebookAccountId":"1004843339565980","allowNotes":1,"type":"User"}},"Collection":{"bb87da25612c":{"id":"bb87da25612c","name":"A Year of Artificial Intelligence","slug":"a-year-of-artificial-intelligence","tags":["DATA SCIENCE","ARTIFICIAL INTELLIGENCE","MACHINE LEARNING","TECHNOLOGY","COMPUTER SCIENCE"],"creatorId":"cb55958ea3bb","description":"Our ongoing effort to make the mathematics, science, linguistics, and philosophy of artificial intelligence fun and simple.","shortDescription":"Our ongoing effort to make the mathematics, science‚Ä¶","image":{"imageId":"1*NZsNSuNxe_O2YW1ybboOvA.jpeg","filter":"","backgroundSize":"","originalWidth":0,"originalHeight":0,"strategy":"resample","height":0,"width":0},"metadata":{"followerCount":521,"postCount":6,"activeAt":1492072051871},"virtuals":{"permissions":{"canPublish":false,"canPublishAll":false,"canRepublish":false,"canRemove":false,"canManageAll":false,"canSubmit":false,"canEditPosts":false,"canAddWriters":false,"canViewStats":false,"canSendNewsletter":false,"canViewLockedPosts":false,"canViewCloaked":false,"canEditOwnPosts":false,"canBeAssignedAuthor":false},"isSubscribed":false,"isNewsletterSubscribed":false,"memberOfMembershipPlanId":""},"logo":{"imageId":"1*NZsNSuNxe_O2YW1ybboOvA.jpeg","filter":"","backgroundSize":"","originalWidth":1500,"originalHeight":1000,"strategy":"resample","height":0,"width":0},"twitterUsername":"mckapur","facebookPageName":"mckapur","publicEmail":"me@rohankapur.com","domain":"ayearofai.com","sections":[{"type":2,"collectionHeaderMetadata":{"title":"A Year of Artificial Intelligence","description":"Where we make the mathematics, science, linguistics, and philosophy of artificial intelligence fun and simple.","backgroundImage":{"id":"1*UlHnUWTtTHWOrOuJdHjVww.png","originalWidth":2000,"originalHeight":1333,"focusPercentX":51.785714285714285,"focusPercentY":43.31550802139037},"logoImage":{"id":"1*NZsNSuNxe_O2YW1ybboOvA.jpeg","originalWidth":1500,"originalHeight":1000,"alt":"A Year of Artificial Intelligence"},"alignment":1,"layout":6}},{"type":1,"postListMetadata":{"source":1,"layout":2,"number":1,"postIds":[],"sectionHeader":"New"}},{"type":1,"postListMetadata":{"source":1,"layout":4,"number":14,"postIds":[]}}],"tintColor":"#FF000000","lightText":true,"favicon":{"imageId":"","filter":"","backgroundSize":"","originalWidth":0,"originalHeight":0,"strategy":"resample","height":0,"width":0},"colorPalette":{"defaultBackgroundSpectrum":{"colorPoints":[{"color":"#FF868484","point":0},{"color":"#FF7C7B7A","point":0.1},{"color":"#FF737171","point":0.2},{"color":"#FF696867","point":0.3},{"color":"#FF5F5E5E","point":0.4},{"color":"#FF555454","point":0.5},{"color":"#FF4A4949","point":0.6},{"color":"#FF3F3E3E","point":0.7},{"color":"#FF343333","point":0.8},{"color":"#FF272727","point":0.9},{"color":"#FF1A1A1A","point":1}],"backgroundColor":"#FFFFFFFF"},"tintBackgroundSpectrum":{"colorPoints":[{"color":"#FF000000","point":0},{"color":"#FF1E1D1D","point":0.1},{"color":"#FF3C3B3B","point":0.2},{"color":"#FF565555","point":0.3},{"color":"#FF6F6D6D","point":0.4},{"color":"#FF868484","point":0.5},{"color":"#FF9C9A99","point":0.6},{"color":"#FFB1AEAE","point":0.7},{"color":"#FFC5C3C2","point":0.8},{"color":"#FFD9D6D6","point":0.9},{"color":"#FFECE9E9","point":1}],"backgroundColor":"#FF000000"},"highlightSpectrum":{"colorPoints":[{"color":"#FFF5F2F1","point":0},{"color":"#FFF3F0EF","point":0.1},{"color":"#FFF1EEED","point":0.2},{"color":"#FFEFECEC","point":0.3},{"color":"#FFEDEAEA","point":0.4},{"color":"#FFEBE8E8","point":0.5},{"color":"#FFE9E6E6","point":0.6},{"color":"#FFE7E5E4","point":0.7},{"color":"#FFE5E3E2","point":0.8},{"color":"#FFE4E1E0","point":0.9},{"color":"#FFE2DFDE","point":1}],"backgroundColor":"#FFFFFFFF"}},"navItems":[{"type":1,"title":"Algorithms","tagSlug":"algorithms","url":"https://ayearofai.com/tagged/algorithms","source":"tagSlug"},{"type":1,"title":"Today I Learned","tagSlug":"today-i-learned","url":"https://ayearofai.com/tagged/today-i-learned","source":"tagSlug"},{"type":1,"title":"Case Studies","tagSlug":"case-studies","url":"https://ayearofai.com/tagged/case-studies","source":"tagSlug"},{"type":1,"title":"Philosophical","tagSlug":"philosophical","url":"https://ayearofai.com/tagged/philosophical","source":"tagSlug"},{"type":1,"title":"Meta","tagSlug":"meta","url":"https://ayearofai.com/tagged/meta","source":"tagSlug"}],"colorBehavior":2,"instantArticlesState":0,"acceleratedMobilePagesState":0,"ampLogo":{"imageId":"","filter":"","backgroundSize":"","originalWidth":0,"originalHeight":0,"strategy":"resample","height":0,"width":0},"header":{"title":"A Year of Artificial Intelligence","description":"Where we make the mathematics, science, linguistics, and philosophy of artificial intelligence fun and simple.","backgroundImage":{"id":"1*UlHnUWTtTHWOrOuJdHjVww.png","originalWidth":2000,"originalHeight":1333,"focusPercentX":51.785714285714285,"focusPercentY":43.31550802139037},"logoImage":{"id":"1*NZsNSuNxe_O2YW1ybboOvA.jpeg","originalWidth":1500,"originalHeight":1000,"alt":"A Year of Artificial Intelligence"},"alignment":1,"layout":6},"type":"Collection"}},"Social":{"cb55958ea3bb":{"userId":"lo_6d640aa2d2ff","targetUserId":"cb55958ea3bb","type":"Social"}},"SocialStats":{"cb55958ea3bb":{"userId":"cb55958ea3bb","usersFollowedCount":204,"usersFollowedByCount":592,"type":"SocialStats"}}}})
// ]]></script></body></html>