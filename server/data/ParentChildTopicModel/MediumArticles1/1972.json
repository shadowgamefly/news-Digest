{"name": "1972", "parent": "", "title": "Rohan & Lenny #3: Recurrent Neural Networks &\u00a0LSTMs", "sentences": [{"84b9": "Rohan & Lenny #3: Recurrent Neural Networks &\u00a0LSTMs"}, {"9485": "The ultimate guide to machine learning\u2019s favorite\u00a0child."}, {"4569": "This is the third group (Lenny and Rohan) entry in our journey to extend our knowledge of artificial intelligence and convey that knowledge in a simple, fun, and accessible manner. Learn more about our motives in this introduction post."}, {"a16a": "It seems like most of our posts on this blog start with \u201cWe\u2019re back!\u201d, so\u2026 you know the drill. It\u2019s been a while since our last post\u200a\u2014\u200ajust over 5 months\u200a\u2014\u200abut it certainly doesn\u2019t feel that way. Whether our articles are more spaced out than we\u2019d like them to be, well, we haven\u2019t actually discussed that yet. But I, Rohan, would definitely like to get into a more frequent routine. Since November, we\u2019ve been grinding on school (basically, getting it over and done with), banging out Contra v2, and lazing around more than we should. End of senior year is a fun time."}, {"93ae": "It\u2019s 2017. We started A Year Of AI in 2016. Last year. Don\u2019t panic, though. If you\u2019ve read our letter, you\u2019ll know that, despite our name and inception date, we\u2019re not going anywhere anytime soon. There\u2019s a good chance we\u2019ll move off Medium, but we\u2019re still both obsessed with AI and writing these posts to hopefully make other people obsessed, as well."}, {"f6d1": "I wrote the first article on this blog just over a year ago, and mentioned that my goal for the year was to be accepted into Stanford University as an undergrad student. A few months ago, I achieved this goal. At Stanford, I\u2019ll probably be studying Symbolic Systems, which is a program that explores both the humanities and STEM to inform an understanding of artificial intelligence and the nature of minds. Needless to say, A Year of AI will continue to document the new things I learn \ud83d\ude00."}, {"b842": "Anyways, you can find plenty of articles on recurrent neural networks (RNNs) online. My favorite one, personally, is from Andrej Karpathy\u2019s blog. I read it about 1.5 years ago when I was learning about RNNs. We definitely think there\u2019s space to simplify the topic even more, though. As usual, that\u2019s our aim for the article\u200a\u2014\u200ato teach you RNNs in a fun, simple manner. We\u2019re also importantly doing this for completion purposes; we want people to hop onto A Year of AI and be able to work their way up all the way from logistic regression to neural machine translation (don\u2019t worry, you\u2019ll find out what means soon enough), and thus recurrent neural networks is a vital addition. After this, we want to look at and summarize/simplify a bunch of new super interesting research papers, and for most of them RNNs are a key ingredient. Finally, we think this article contains so much meat and ties together content unlike any other RNN tutorial on the interwebs."}, {"60e2": "Before we get started, you should try to familiarize yourself with \u201cvanilla\u201d neural networks. If you need a refresher, check out our neural networks and backpropogation mega-post from earlier this year. This is so you know the basics of machine learning, linear algebra, neural network architecture, cost functions, optimization methods, training/test sets, activation functions/what they do, softmax, etc. Reading our article on convolutional neural networks may also make you more comfortable entering this post, especially because we often reference CNNs. Checking out this article I wrote on vanishing gradients will help later on, as well."}, {"81ec": "Rule of thumb: the more you know, the better!"}, {"3288": "Table of\u00a0Contents"}, {"003f": "I can\u2019t link to each section, but here\u2019s what we cover in this article (save the intro and conclusion):"}, {"02a8": "What can RNNs\u00a0do?"}, {"1c1d": "There are a number of very important tasks that ANNs and CNNs cannot solve, that RNNs are used for instead. Tasks like: image captioning, language translation, sentiment classification, predictive typing, video classification, natural language processing, speech recognition, and a lot more interesting things that have been presented in recent research papers (for example\u2026 learning to learn by gradient descent by gradient descent!)."}, {"1d0f": "Image captioning, taken from CS231n slides: http://cs231n.stanford.edu/slides/2016/winter1516_lecture10.pdf"}, {"02ef": "RNNs are very powerful. Y\u2019know how regular neural networks have been proved to be \u201cuniversal function approximators\u201d\u00a0? If you didn\u2019t:"}, {"8206": "In the mathematical theory of artificial neural networks, the universal approximation theorem states that a feed-forward network with a single hidden layer containing a finite number of neurons can approximate continuous functions on compact subsets of R^n, under mild assumptions on the activation function."}, {"314e": "That\u2019s pretty confusing. Basically, what this states is that an artificial neural network can compute any function. Even if someone gives you an extremely wiggly, complex looking function, it\u2019s guaranteed that there exists a neural network that can produce (or at least extremely closely approximate) it. The proof itself is very complex, but this is a brilliant article offering a visual approach as to why it\u2019s true."}, {"a351": "So, that\u2019s great. ANNs are universal function approximators. RNNs take it a step further, though; they can compute/describe programs. In fact, some RNNs with proper weights and architecture qualify as Turing Complete:"}, {"8f10": "A Turing Complete system means a system in which a program can be written that will find an answer (although with no guarantees regarding runtime or memory)."}, {"c6a2": "So, if somebody says \u201cmy new thing is Turing Complete\u201d that means in principle (although often not in practice) it could be used to solve any computation problem."}, {"1429": "\u2014 http://stackoverflow.com/a/7320/1260708"}, {"2ba5": "That\u2019s cool, isn\u2019t it? Now, this is all theoretical, and in practice means less than you think, so don\u2019t get too hyped. Hopefully, though, this gives some more insight into why RNNs are super important for future developments in machine learning\u200a\u2014\u200aand why you should read on."}, {"662c": "At this point, if you weren\u2019t previously hooked on learning what the heck these things are, you should be now. (If you still aren\u2019t, just bare with me. Things will get spicy soon.) So, let\u2019s dive in."}, {"72f6": "Why?"}, {"54dc": "We took a bit of a detour to talk about how great RNNs are, but haven\u2019t focused on why ANNs can\u2019t perform well in the tasks that RNNs can."}, {"b88b": "Why do we need another neural network model? Why do we need recurrent neural networks when we already have the beloved ANNs (and CNNs) in all their glory?"}, {"95c9": "It boils down to a few things:"}, {"341a": "Let\u2019s address the first three points individually. The first issue refers to the fact that ANNs have a fixed input size and a fixed output size. ANNs have an elaborate list of hyperparameters, and this notably includes the number of neurons in the input layer and output layer. But what if we wanted input data and/or output data of variable size, instead of something that needs to have its size as a preset constant? RNNs allow us to do that. In this aspect, they offer more flexibility than ANNs."}, {"605a": "We might choose this architecture for our ANN, with 4 inputs and 1 output. But that\u2019s it\u200a\u2014\u200awe can\u2019t input a vector with 5 values, for example. https://qph.ec.quoracdn.net/main-qimg-050d12c5da82f4f97fdd942d7777b8e4."}, {"1c8e": "I\u2019ll give you a couple examples of why this matters."}, {"e228": "It\u2019s unclear how we could use an ANN by itself to perform a task like image captioning, because the network would need to output a sentence\u200a\u2014\u200aa list of words in a specific order\u200a\u2014\u200awhich is a sequence. It would be a sequence of vectors, because each word would need to be represented numerically. In machine learning and data science, we represent words numerically as vectors; these are called word embeddings. An ANN can only output a single word/label, like in image classification where we treat the output as the label with the highest value in the final vector that is a softmax probability distribution over all classes. The only way to make sentences work with ANNs would be to have billions of output neurons that each map to a single possible sentence in the permutation of all [sensible] sentences that can be formed by the vocabulary we have. And that doesn\u2019t sound like a good idea."}, {"8f28": "A reminder of what the output of an ANN looks like\u200a\u2014\u200aa probability distribution over classes\u200a\u2014\u200aand how we convert that into a single final result (one-hot encoding): by taking the label with the greatest probability and making it 1, with the rest\u00a00."}, {"f8ec": "Wow, that was a lot of words. Nevertheless, I hope it\u2019s clear that, with ANNs, there\u2019s no feasible way to output a sequence."}, {"f308": "Now, what about inputting a sequence into an ANN? In other words, \u201ctemporal\u201d data: data that varies over time, and is thus a sequence. Take the example of sentiment classification where we input a sentence (sequence of words = sequence of vectors = sequence of set of values where each value goes into an individual neuron) and want to output its sentiment: positive or negative. The output part seems easy, because it\u2019s just one neuron that\u2019s either rounded to 1 (positive) or 0 (negative). And, for the input, you might be thinking: couldn\u2019t we input each \u201cset of values\u201d separately? Input the first word, wait for the neural net to fully feed forward and produce an output, then input the next word, etc. etc."}, {"8029": "Let\u2019s take the case of this utterly false, and most certainly negative sentence, to evaluate:"}, {"6e25": "This is just an alternative fact, believe me! Lenny is actually a great coder. The best I know of. The\u00a0best."}, {"322d": "We\u2019d input \u201cLenny\u201d first, then \u201cKhazan\u201d, then \u201cis\u201d, etc. But, at each feedforward iteration, the output would be completely useless. Why? Because the output would be dependent on only that word. We\u2019d be finding the sentiment of a single word, which is useless, because we want the sentiment of the entire sentence. Sentiment analysis only makes sense when all the words come together, dependent on each other, to form a sentence."}, {"13ff": "Think of it this way\u200a\u2014\u200athis means you\u2019re essentially running a neural network a bunch of times, just with new data at each separate iteration. Those run-throughs aren\u2019t linked in any way; they\u2019re independent. Once you feedforward and fully run the neural network, it forgets everything it just did. This sentence only makes sense and can only be interpretable because it\u2019s a collection of words put together in a specific order to form meaning. The relevance of each word is dependent on the words that precede it: the context. This is why RNNs are being used heavily in NLP; they retain context by having memory. ANNs have no memory."}, {"e71f": "I like this quote from another article on RNNs:"}, {"9cd3": "Humans don\u2019t start their thinking from scratch every second. As you read this essay, you understand each word based on your understanding of previous words. You don\u2019t throw everything away and start thinking from scratch again. Your thoughts have persistence."}, {"b760": "\u2014 http://colah.github.io/posts/2015\u201308-Understanding-LSTMs/"}, {"e0e4": "(Furthermore, take the case where we had sequential data in both the input and the output. Translating one language to another is a good example of this. Clearly, ANNs aren\u2019t the answer.)"}, {"fa2f": "RNNs don\u2019t just need memory; they need long term memory. Let\u2019s take the example of predictive typing. Let\u2019s say we typed the following sentence in an SMS message to 911, and the operating system needs to fill in the blank:"}, {"3077": "The face of a criminal?"}, {"325c": "Here, if the RNN wasn\u2019t able to look back much (ie. before \u201cshould\u201d), then many different options could arise:"}, {"da28": "Lenny in the military? Make it into a TV show! I\u2019d watch\u00a0it."}, {"930e": "The word \u201csent\u201d would indicate to the RNN that a location needs to be outputted. However, if the RNN was able to retain information from all the way back, such as the word \u201ccriminal\u201d, then it would be much more confident that:"}, {"c9f1": "The probability of outputting \u201cjail\u201d drastically increases when it sees the word \u201ccriminal\u201d is present. That\u2019s why context matters, be it predictive typing, image captioning, machine translation, etc. The output or outputs of a recurrent neural network will always be functionally dependent on (meaning, a function of) information from the very beginning, but how much it chooses to \u201cforget\u201d or \u201cretain\u201d (that is, varying degrees of influence from earlier information) depends on the weights that it learns from the training data."}, {"eb76": "As it turns out, RNNs\u200a\u2014\u200aespecially deep ones\u200a\u2014\u200aare rarely good at retaining much information, due to an issue called the vanishing gradient problem. That\u2019s where we turn to other variants of RNNs such as LSTMs and GRUs. But, more on that later."}, {"c00c": "To address the third point, one more constraint with ANNs is that they have a fixed number of computation/processing steps (because, once again, the number of hidden layers is a hyperparameter). With RNNs, we can have much more dynamic processing since we operate over vectors. Each neuron in an RNN is almost like an entire layer in an ANN; this will make more sense as we bring up an illustration for you. Exciting stuff."}, {"4abf": "Show me."}, {"0160": "OK, that\u2019s enough teasing. Three sections into the article, and you\u2019re yet to see what an RNN looks like, or appreciate how it really works. Everything comes in due time, though!"}, {"d391": "The first thing I\u2019m going to do is show you what a normal ANN diagram looks like:"}, {"deda": "Each neuron stores a single scalar value. Thus, each layer can be considered a vector."}, {"f077": "Now I\u2019m going to show you what this ANN looks like in our RNN visual notation:"}, {"f1fc": "The two diagrams above represent the same thing. The latter, obviously, looks more succinct than the former. That\u2019s because, with our RNN visual notation, each neuron (inputs, hidden(s), and outputs) contains a vector of information. The term \u201ccell\u201d is also used, and is interchangeable with neuron. (I\u2019ll use the latter instead of the former.) Red is the input neuron, blue is the hidden neuron, and green is the output neuron. Therefore, an entire ANN layer is encapsulated into one neuron with our RNN illustration. All operations in RNNs, like the mapping from one neuron\u2019s state to another, are over entire vectors, compared to individual scalars that are summed up with ANNs."}, {"abee": "Let\u2019s flip it the other way:"}, {"8a15": "This is in fact a type of recurrent neural network\u200a\u2014\u200aa one to one recurrent net, because it maps one input to one output. A one to one recurrent net is equivalent to an artificial neural net."}, {"b960": "We can have a one to many recurrent net, where one input is mapped to multiple outputs. An example of this would be image captioning\u200a\u2014\u200athe input would be the image in some processed form (usually the result of a CNN analyzing the image), and the output would be a sequence of words. Such an RNN may look like this:"}, {"7fff": "Changed the shades of the green nodes\u2026 hope that\u2019s\u00a0OK!"}, {"ea3c": "This may be confusing at first, so I\u2019m going to make sure I walk slowly through it. On the x-axis we have time, and on the y-axis we have depth/layers:"}, {"22b2": "When I refer to \u201ctime\u201d on the x-axis, I\u2019m referring to the order at which these operations occur. Time could also be literal for temporal data, where the input is a sequence. When I say \u201cdepth\u201d on the y-axis, I\u2019m referring to the mapping from the input layer, to the hidden layer(s), to the output layer, where layer number and thus depth increases."}, {"f94b": "It may look like we have seven neurons now, but we still have three: one input neuron, one hidden neuron, and one output neuron. The difference is that these neurons now experience multiple \u201ctimesteps\u201d where they take on different values, which are, again, vectors. The input neuron in our example above doesn\u2019t, because it\u2019s not representing sequential data (one to many), but for other architectures it could."}, {"e200": "The hidden neuron will take on the vector value h_1 first, then h_2, and finally h_3. At each timestep, the hidden neuron\u2019s vector h_t is a function of the vector at the previous timestep h_t-1, except for h_1 which is dependent only on the input x_1. In the diagram above, each hidden vector then gives rise to an output y_t, and this is how we map one input to multiple outputs. You can visualize these functional dependencies with the arrows, which illustrates flow of information in the network."}, {"64eb": "As we progress on the x-axis, the current timestep increases. As we progress on the y-axis, the neuron in question changes. Each point on this graph thus represents one neuron\u200a\u2014\u200abe it input, hidden, or output\u200a\u2014\u200aat some timestep, being fed information from a neuron (be it itself or another) at the previous timestep."}, {"5402": "The RNN would execute like so:"}, {"bba5": "You could compute y_t either immediately after h_t has been computed, or, like above, compute all outputs once all hidden states have been computed. I\u2019m not entirely sure which is more common in practice."}, {"6c68": "This allows for more complex and interesting networks than ANNs because we can have as many timesteps as we want."}, {"e78f": "The value of the output neuron at each timestep represents a word in the sentence, in the order the sentence will be constructed. The caption this RNN produces is hence 3 words long. (It\u2019s actually 2, because the RNN would need to output a period or <END> marker at the final timestep, but we\u2019ll get into that later.)"}, {"a009": "In case you don\u2019t understand yet exactly why RNNs work, I\u2019ll walk through how these functional dependencies come to fruition when you apply it to a one to many scenario such as image captioning."}, {"2856": "Lenny and I on student scholarship at WWDC 2013. Good\u00a0times!"}, {"6b74": "When you combine an RNN and CNN, you\u200a\u2014\u200ain practice\u200a\u2014\u200aget an \u201cLCRN\u201d. The architecture for LCRNs are more complex than what I\u2019m going to present in the next paragraph; rather, I\u2019m going to simplify it to convey my point. We\u2019ll actually get fully into how they work later."}, {"e029": "Imagine an RNN tries to caption this image. An accurate result might be:"}, {"f14e": "Two people happily posing for a photo inside a building."}, {"482a": "The input to the RNN would be the output of a CNN that processes this image. (However, to be pedantic, it would be the output of the CNN without a classification/softmax layer\u200a\u2014\u200athat is, pulled from the final fully connected layer.) The CNN might pick up on the fact that there are two primary human face-like objects present in the image, which, paired with what the RNN has learned via training, may induce the first hidden state\u00b9 of the recurrent neural network to be one where the most likely candidate word is \u201ctwo\u201d."}, {"f5ec": "Pro-tip\u00b9: The term \u201chidden state\u201d refers to the vector of a hidden neuron at a given timestep. \u201cFirst hidden state\u201d refers to the hidden state at timestep 1."}, {"10db": "The first output, which represents the word \u201ctwo\u201d, was functionally dependent on the first hidden state, which in itself was a function of the input to the RNN. Thus, \u201ctwo\u201d was ultimately determined from the information that the CNN gave us and the experience/weights of the RNN. Now, the second word, \u201cpeople\u201d, is functionally dependent on the second hidden state. However, note that the second hidden state is just a function of the first hidden state. This means that the word \u201cpeople\u201d was the most likely candidate given the hidden state where \u201ctwo\u201d was likely. In other words, the RNN recognized that, given the word \u201ctwo\u201d, the word \u201cpeople\u201d should be next, based on the RNN\u2019s experience from training and the initial image [analysis] we inputted."}, {"b004": "The same will occur for every following word; the nth word will be based on the nth hidden state, which, ultimately, is a function of every hidden state before it, and thus could be interpreted purely as an extremely complex and layered function of the input. The weights do the heavy lifting by making sense of all this information and deducing an output from it."}, {"d51d": "To put it bluntly, you can boil down what the RNN is \u201cthinking\u201d to this:"}, {"4a94": "Based on what I\u2019ve seen from the input, based on the current timestep I\u2019m at, and based on what I know from all my training, I need to output: \u201cx\u201d."}, {"a918": "Thus, each outputted word is dependent on the words before it, all the way back to the input image data. However, this relationship is indirect. It\u2019s indirect because the outputs are only dependent on the hidden states, not on each other (ie. the RNN doesn\u2019t deduce \u201cpeople\u201d from \u201ctwo\u201d, it deduces \u201cpeople\u201d, partly, from the information\u200a\u2014\u200athe hidden state\u200a\u2014\u200athat gave rise to \u201ctwo\u201d). In LCRNs, though, this is explicit instead of implicit; we \u201csample\u201d the output of one timestep by taking it and literally feeding it back as input into the next timestep. In a sense, LCRNs can hence be interpreted as having many to many architecture."}, {"cdc3": "The exact quantitative relationships depend on the RNN\u2019s weights. But, generally, this is the concept of memory in play. Creating a coherent sentence as we go along is only really possible if we can recall what we said before. And RNNs are able to do exactly that; they remember what they said before and figure out, based on their image captioning expertise, what from this is useful to continue accurately speaking."}, {"0d89": "Yep, I went to France for a holiday. And I actually learned to speak some <wait, shit, what was the language again? oh yea, \u201cFrance\u201d\u2026> French!"}, {"5857": "Obviously, an RNN needs to be trained and have proper weights for this to all function properly. RNNs aren\u2019t magic; they only work because trained networks identified and learned patterns in data during training time that they now look for during prediction."}, {"34be": "Perhaps this was a bit over-explaining on my part, but hopefully I nailed down some important and core ideas about how RNNs function."}, {"05f6": "So far we\u2019ve looked at one to one and one to many recurrent networks. We can also have many to one:"}, {"da0a": "With many to one (and many to many), the input is in the form of a sequence, and so the hidden states are functionally dependent on both the input at that timestep and the previous hidden state. This is different to one to many, where the hidden state after h_1 is only dependent on the previous hidden state. That\u2019s why, in the image above, the second hidden state has two arrows directed at it."}, {"4715": "Only one output exists in many to one architecture. An example application is sentiment classification, where the input is a sentence (sequence of words) and the output is a probability indicating that the inputted sentence was positive."}, {"a55f": "The final type of recurrent net is many to many, where both the input and output are sequential:"}, {"7038": "A use case would be machine translation where a sequence of words in one language needs to be translated to a sequence of words in another."}, {"2538": "We can also go deeper and have multiple hidden layers, and/or a greater number of timesteps:"}, {"7896": "We\u2019re getting deeper and\u00a0deeper!"}, {"2703": "Really, this could be considered as multiple RNNs. Technically, you can consider each \u201chidden layer\u201d as an RNN itself, given each neuron operates on vectors and updates through time; in ANN context, that volume of operations would be considered an entire network. So this is like stacking RNNs on top of each other. However, in this article I\u2019ll refer to it as multiple hidden layers; different papers and lecturers may take different approaches."}, {"a0dc": "When we have many timesteps (usually hundreds) and multiple hidden layers, the architecture of the network becomes much more complex and interesting. One feature of this RNN, in particular, is that all the outputs, including the first, depend on not just the input up to that timestep, but all of the inputs. (You can see this because the green neuron is only introduced after the final input timestep.) If this RNN was to translate English to Chinese, the first word of translated Chinese isn\u2019t just dependent on the first word of the inputted English; it\u2019s dependent on the entire sentence."}, {"98d9": "One way to demonstrate why this matters is to use Google Translate:"}, {"af3c": "One of my favorite Green Day lyrics, from the song \u201cFashion Victim\u201d on WARNING:. Side-note: Based on my experience with Google Translate in Chinese class over the last 8 years, this translation is probably\u00a0off."}, {"3229": "Now I\u2019ll input \u201cHe\u2019s a victim\u201d and \u201cof his own time\u201d separately. You\u2019ll notice that when you join the two translated outputs, this won\u2019t be equal to the corresponding phrase in the first translation:"}, {"aef0": "What happens if we break up the English into different parts, translate, and join together the translated Chinese\u00a0parts?"}, {"44aa": "They\u2019re not\u00a0equal."}, {"a661": "What gives? Well, the way sentences are constructed in languages can differ in varying scenarios. Some words in English may also map to multiple different words in Chinese, depending on how it\u2019s used. It all depends on the context and the entire sentence as a whole\u200a\u2014\u200athe meaning you\u2019re trying to convey. This is the exact approach a human translator would take."}, {"fad3": "Another type of many to many architecture exists where each neuron has a state at every timestep, in a \u201csynchronized\u201d fashion. Here, each output is only dependent on the inputs that were fed in during or before it. Because of this, synchronized many to many probably wouldn\u2019t be suitable for translation."}, {"cef1": "An application for this could be video classification where each frame needs to be mapped to some sort of class or label. Interesting note\u200a\u2014\u200aan RNN is better at this task than CNNs are because what\u2019s going on in a scene is much easier to understand if you\u2019ve watched the video up to that point and thus can contextualize it. That\u2019s what humans do!"}, {"f0a6": "Quick note: we can \u201cwrap\u201d the RNN into a much more succinct form, where we collapse the depth and time properties, like so:"}, {"fc78": "This notation demonstrates that RNNs take input, process that input through multiple timesteps and hidden layers, and produce output. The arrow both leaving and entering the RNN conveys that an RNN hidden state is functionally dependent on the hidden state at the preceding timestep; it\u2019s sort of like a loop that feeds itself."}, {"3ba7": "When you ever read about \u201cunrolling\u201d an RNN into a feedforward network that looks like it\u2019s in the same collapsed format as the diagram above, this means we expand it to show all timesteps and hidden layers like we did before."}, {"0d76": "Another quick note: when somebody or a research paper mentions that they are using \u201c512 RNN units\u201d, this translates to: \u201c1 RNN neuron that outputs a 512-wide vector\u201d; that is, a vector with 512 values. At first, I thought this meant that maybe at each timestep there were 512 separate neurons somehow working in conjunction, but nope, it\u2019s luckily much simpler than that\u2026 albeit strangely worded."}, {"2ae1": "Furthermore, one \u201cRNN unit\u201d usually refers to an RNN with one hidden layer; thus, instead of defining RNN as something that is multilayer inherently, we often see people use the phrase like: \u201cstacking RNNs on top of each other\u201d. Each RNN will have its on weights, but connecting them gives rise to an overarching multilayer RNN. In this article, we treat recurrent neural networks as a model that can have variable timesteps t and fixed layers \u2113, just make sure you understand that this is not always the case. Our formalism, especially for weights, will slightly differ."}, {"e5b5": "Formalism"}, {"a93d": "So, now, let\u2019s walk through the formal mathematical notation involved in RNNs."}, {"ad6a": "If an input or output neuron has a value at timestep t, we denote the vector as:"}, {"a6fd": "For the hidden neurons it\u2019s a bit different; since we can have multiple hidden layers, we denote the hidden state vector at timestep t and hidden layer \u2113 as:"}, {"7ba4": "The input is obviously some preset values that we know. The outputs and hidden states are not; they are calculated."}, {"982f": "Let\u2019s start with hidden states. First, we\u2019ll revisit the most complex recurrent net we came across earlier\u200a\u2014\u200athe many to many architecture:"}, {"44bd": "Many to many, non-synchronized."}, {"5f3c": "This RNN has: sequential input, sequential output, multiple timesteps, and multiple hidden layers. The formula we derive for this RNN should generalize for all others."}, {"af44": "First, let\u2019s list out the possible functional dependencies for a given hidden state, based on the arrows and flow of information in the diagram:"}, {"dc21": "A hidden state can have two functional dependencies at max. Just by looking at the diagram, the only impossible combination is to be dependent on both the input and a hidden state at the current timestep but previous layer. This is because the only hidden states that are dependent on input exist in the first hidden layer, where no such previous layer exists."}, {"b721": "If this is all difficult to follow, make sure once again to look at and trace back the arrows in the RNN that illustrate flow of information throughout the network."}, {"5a0a": "Because of the impossible combination, we define two separate equations: an equation for the hidden state at hidden layer 1, and for layers after 1."}, {"7d12": "This probably looks a bit confusing; let me break it down for you. The function \u0192w computes the numeric hidden state vector for timestep t and layer \u2113; it contains the \u201cactivation function\u201d you\u2019re used to hearing about with ANNs. W are the weights of the recurrent net, and thus \u0192 is conditioned on W. We haven\u2019t exactly defined \u0192 just yet, but what\u2019s important to note is the two parameters it takes. Once you do, this notation simply states what we have stated before in plain English:"}, {"a42e": "Where \u2113 = 1, the hidden state at time t and layer \u2113 is a function of the hidden state vector at time t-1 and layer \u2113 as well as the input vector at time t. Where \u2113 > 1, this hidden state is a function of the hidden state vector at time t-1 and layer \u2113 as well as the hidden state vector at time t, layer \u2113-1."}, {"7713": "You might notice that we have a couple issues:"}, {"2062": "Our respective solutions follow:"}, {"0ed3": "If these are 0, then the invalid functional dependency stops existing, and our formal notation still holds up."}, {"cf77": "We actually have five different types of weight matrices:"}, {"b66b": "Pro-tip: The indices for each weight matrix tell you what they are used for in the recurrent net. W_xh maps an input vector x to a hidden state vector h. W_hht maps a hidden state vector h to another hidden state vector h along the time axis, ie. from h_t-1 to h_t. On the other hand, W_hhd maps a hidden state vector h to another hidden state vector h along the depth axis, ie. from h^(\u2113-1)_t to h^\u2113_t. W_hy maps a hidden state vector h to an output vector y."}, {"283a": "Like with ANNs, we also learn and add a constant bias vector, denoted b_h, that can vertically shift what we pass to the activation function. We can also shift our outputs with b_y. More about bias units here."}, {"246f": "For both b_h and W_hht/W_hhd, we actually have multiple weight matrices depending on the value of \u2113, as indicated by the superscript. This is because each hidden layer can have a different set of weights (the network would be extremely uninteresting if this wasn\u2019t the case), including the bias vector. However, inside a single hidden layer, all timesteps share the same weight matrix. This is important because the number of timesteps is a variable; we may train on sequences with up to 20 values, but in practice output sequences with up to 30 values\u200a\u2014\u200a10 extra timesteps. If each timestep had an independent weight to learn, those last 10 timesteps wouldn\u2019t have anything to use."}, {"6da9": "W_hy is just one matrix because only the final layer gives rise to the outputs denoted y. At the final hidden layer \u2113, we could suggest that W_hhd will not exist because W_hy will be in its place."}, {"e3ed": "Now we\u2019ll define the function \u0192w:"}, {"e06a": "The function is very similar to the ANN hidden function you\u2019ve seen before; it applies the correct weights to the corresponding parameters, adds the bias, and passes this weighted sum through an activation or \u201csquashing\u201d function to introduce non-linearities. The key difference, though, is that this is not a weighted sum but rather a weighted sum vector; any W \u22c5 h, along with the bias, will have the dimensions of a vector. The tanh function will thus simply output a vector where each value is the tanh of what it was in the inputted vector (sort of like an element-wise tanh). Remember, this contrasts ANNs because RNNs operate over vectors versus scalars."}, {"d904": "If you\u2019ve followed our blog so far, you most likely know about two activation functions: sigmoid and ReLU. tanh is another such function. We mostly use the tanh function with RNNs. This is, I think, mostly because of their role in LSTMs (a variant of RNNs that are used more than RNNs\u200a\u2014\u200amore on that later), the fact that they produce gradients with a greater range, and that their second derivative don\u2019t die off as quickly."}, {"25ba": "Similar to sigmoid, the tanh function has two horizontal asymptotes and a smooth S-shape. The main difference is that the tanh function asymptotes at y = -1 instead of y = 0, intercepting the y-axis at y = 0 instead of y = 0.5. Thus, the tanh function has a greater range than the sigmoid."}, {"9d21": "If interested, the tanh equation follows (though I won\u2019t walk you through it):"}, {"85eb": "The final equation is mapping a hidden state to an output."}, {"93b1": "This is one such possible equation. Depending on the context, we might also remove the bias vector, apply a non-linearity like sigmoid/softmax (for example if the output needs to be a probability distribution), etc."}, {"3584": "And that\u2019s how we express recurrent nets, mathematically!"}, {"2650": "Quick note: Notation may and will differ between various lectures, research paper, articles, etc. For example\u200a\u2014\u200asome research papers may start indexing at 0 instead of 1. More drastically, most RNN notation is much more general than mine to promote simplicity, ie. doesn\u2019t cover edge cases like I did or obfuscates certain indices like \u2113 with hidden to hidden weight matrices. So, just keep note that specifics don\u2019t always transfer over and avoid being confused by this. The reason I was meticulous about notation in this article is that I wanted to ensure you understood exactly how RNNs work, fueled by my frustration with the very same problem ~1.5 years ago."}, {"ed09": "An example?\u00a0Okay!"}, {"e53b": "Let\u2019s take a look at a quick example of an RNN in action. I\u2019m going to adapt a super dumbed down one from Andrej Karpathy\u2019s Stanford CS231n RNN lecture, where a one to many \u201ccharacter level language model\u201d single layer recurrent neural network needs to output \u201chello\u201d. We\u2019ll kick it of by giving the RNN the letter \u201ch\u201d\u00a0, such that it needs to complete the word by outputting the other four letters."}, {"1901": "Sidenote: this model nicknamed \u201cchar-rnn\u201d\u200a\u2014\u200aremember it for later, where we get to code our own!"}, {"fc9c": "The neural network has the vocabulary: h, e, l\u00a0, o. That is, it only knows these four characters; exactly enough to produce the word \u201chello\u201d. We will input the first character, \u201ch\u201d, and from there expect the output at the following timesteps to be: \u201ce\u201d, \u201cl\u201d, \u201cl\u201d, and \u201co\u201d respectively, to form:"}, {"56af": "hello"}, {"9f5f": "We can represent input and output via one hot encoding, where each character is a vector with a 1 at the corresponding character position and otherwise all 0s. For example, since our vocabulary is [h, e, l, o], we can represent characters using a vector with four values, where a 1 in the first, second, third, and fourth position would represent \u201ch\u201d, \u201ce\u201d, \u201cl\u201d, and \u201co\u201d respectively."}, {"28cd": "This is called \u201cone-hot encoding\u201d, because only one of the values in the vector is equal to 1 and thus on (or\u00a0\u201chot\u201d)."}, {"c682": "This is what we\u2019d expect with a trained RNN:"}, {"17a3": "As you can see, we input the first letter and the word is completed. We don\u2019t know exactly what the hidden states will be\u200a\u2014\u200athat\u2019s why they\u2019re hidden!"}, {"04ad": "One interesting technique would be to sample the output at each timestep and feed it into the next as input:"}, {"08da": "When we \u201csample\u201d from a distribution, we select a random character probabilistically following the distribution. For example, in the diagram above, the character with the highest likeliness is \u201ce\u201d at the first timestep\u2019s output. Let\u2019s say this likeliness is, concretely, 0.9. Now, when we sample into the next timestep\u2019s input, there\u2019s a 90% chance we select \u201ce\u201d; most of the time we will pick the most likely character, but not every time. This adds a level of randomness so you don\u2019t end up in a loop where you keep sampling the same letter or sequence of letters over and over again."}, {"0880": "As mentioned earlier, this is used pretty heavily with LCRNs. It\u2019s even more effective than only relying on the memory of the RNN to output the correct letter at the future timesteps. In a sense, this makes the recurrent net many to many. (Though, not really, because we still only have one preset input.)"}, {"9cac": "However, to be clear, this does not mean that the RNN can only rely on these sampled inputs. For example, at timestep 3 the input is \u201cl\u201d and the expected output is also \u201cl\u201d. However, at timestep 4, the input is again \u201cl\u201d but the output is now \u201co\u201d, to complete the word. Memory is still needed to make a distinction like this."}, {"db16": "In numerical form, it would look something like this:"}, {"c815": "Of course, we won\u2019t get a one-hot vector output during prediction mode; rather, we will get a probability distribution over each letter (so we\u2019d apply softmax to the output), and will sample from this distribution to get a single character output."}, {"9419": "Each hidden state would contain a similar sort of vector, though not necessarily something we could interpret like we can for the output."}, {"58bf": "The RNN is saying: given \u201ch\u201d, \u201ce\u201d is most likely to be the next character. Given \u201che\u201d, \u201cl\u201d is the next likely character. With \u201chel\u201d, \u201cl\u201d should be next, and with \u201chell\u201d, the final character should be \u201co\u201d."}, {"b7ec": "But, if the neural network wasn\u2019t trained on the word \u201chello\u201d, and thus didn\u2019t have optimal weights (ie. just randomly initialized weights), then we\u2019d have garble like \u201chleol\u201d coming out."}, {"399a": "One more important thing to note: start and end tokens. They signify when input begins and when output ends. For example, when the final character is outputted (\u201co\u201d), we can sample this back as input and expect that the \u201c<END>\u201d token (however we choose to represent it\u200a\u2014\u200acould also use a period) will be outputted at the next timestep; this is the RNN telling us that it has completed the word and its processing as a whole. The use case isn\u2019t as obvious in this fabricated example, because we know when \u201chello\u201d has been completed, but consider a real-life scenario where we don\u2019t: image captioning. In image captioning, the caption could be 1, 2, 3, or n words long, given a reasonable upper limit of n. The end token tells us when the caption has been completed, so we can halt the RNN and complete the prediction loop (which would keep going forever if we were using while or stop after the upper limit/max possible preset constant value of n is reached)."}, {"a127": "Start tokens are more used for generating content from complete scratch. For example, imagine an RNN read and learned from a bunch of Shakespeare. (This is an actual funny application of character level language models that Karpathy implemented, and we\u2019ll see it in action on a later section.) Now, based on what the RNN learned, we want it to create a brand new Shakespearean sonnet! Feeding in a \u201c<START>\u201d token enables it to kick this process off and begin writing without us giving the network some arbitrary pre-determined initial word or character."}, {"85be": "I\u2019ve also noticed that another potential use case of start tokens is when we have some other sort of initial input, like CNN produced image data with image captioning, that doesn\u2019t \u201cfit\u201d what we\u2019ll normally use for input at timesteps after t=1 (the word outputted at the previous timestep via sampling). As a result, we feed this data directly to the first hidden state and set the input as \u201c<START>\u201d instead."}, {"2aca": "Now, just to be clear, the RNN doesn\u2019t magically output these end tokens and recognize the start tokens. We have to add them, along with start tokens, to the training data and vocabulary such that they can be outputted by the recurrent net during prediction time."}, {"1c00": "This is how we can get RNNs to \u201cwrite\u201d! More on some examples of text RNNs have actually generated, Shakespeare most certainly included, in a later section."}, {"6b00": "Training (or, why vanilla RNNs\u00a0suck.)"}, {"58b5": "For a recurrent net to be useful, it needs to learn proper weights via training. That\u2019s no surprise."}, {"8a33": "Recall this snippet from earlier:"}, {"f72f": "But, if the neural network wasn\u2019t trained on the word \u201chello\u201d, and thus didn\u2019t have optimal weights (ie. just randomly initialized weights), then we\u2019d have garble like \u201chleol\u201d coming out."}, {"f9d1": "This is, of course, because we initialize the W weights randomly at first, so random stuff will come out."}, {"c7a6": "But, through multiple iterations of training with a first-order optimization algorithm like gradient descent, we perturb the weights such that the probability of each correct character being outputted at their respective timestep increases. The actual output would be \u201chello\u201d in one-hot encoding form, and we\u2019d compute the discrepancy between this output and what the recurrent net predicts (we\u2019d get the error at each timestep and then add this up) as the total error to then calculate the gradient/update value."}, {"2ded": "So, each output contributes to the error somehow. If the error is an addition of the outputs, then, if we had something like Y outputs, we\u2019d need to backpropagate them individually and add these up. This is because derivatives are distributed evenly when we\u2019re differentiating a sum:"}, {"6145": "For any arbitrary weight\u00a0W."}, {"bb6e": "But, you should know that, with artificial neural networks, calculating these gradients isn\u2019t that easy. We have so many weights contributing to the output, and thus need to figure out exactly how much these weights contribute, and by how much we modify them to decrease overall error. To do this, we use the backpropagation algorithm; this algorithm propagates the error between the predicted output of a recurrent net and the actual output in the dataset all the way back to the beginning of the network. Using the chain rule from differential calculus, backprop helps us calculate the gradients of the output error w.r.t. each individual weight (sort of like the error of each individual weight)."}, {"c4f4": "Once we have those gradients, we have to use an optimization algorithm to calculate the update values and make the updates. We can use the vanilla gradient descent algorithm to do this, but there are many other possible, better variants as well; learn about them by reading this article, if you want. (I think we\u2019re long overdue for our own mega-post on optimization!)"}, {"0be4": "Backpropagation with RNNs is called \u201cBackpropagation Through Time\u201d (short for BPTT), since it operates on sequences in time. But don\u2019t be fooled\u200a\u2014\u200athere\u2019s not much difference between normal backprop and BPTT; when it comes down to it, BPTT is just backprop, but on RNNs! Remember that when you \u201cunroll\u201d an RNN, it essentially becomes a feedforward network; not an ANN, but a feedforward network in the sense that we can visualize where all the information is flowing and observe the activations at each neuron and timestep, all the way from the input to the final output. Like ANNs, RNNs have functional dependencies that link the entire network together; it\u2019s just that RNNs operate over vectors instead (yay for matrix calculus?) and extend in depth as well as time. There\u2019s more work to do to compute the gradients, but it\u2019s no surprise that backprop works pretty much the same way for recurrent nets that it would for normal ones. Because of this, I\u2019m not going to walk through all the math and show the derivatives etc. Read our backprop mega-post for all that jazz."}, {"224b": "One thing to note is that, since we have multiple timesteps in our RNN, each timestep in a single layer will want to change the weight in a different way and have different gradients. However, remember that each hidden layer uses only one weight matrix because the number of timesteps is a variable. Thus, we just average or sum the weight updates between these timesteps and apply this as an update to the W_hh for that entire layer. Also, a general practice is to train on shorter sequences first and then gradually increase sequence size as we train on more and more data."}, {"1fee": "Now, if you haven\u2019t already, make sure to read this article that I wrote on vanishing and exploding gradients before proceeding:"}, {"c672": "Rohan #4: The vanishing gradient problemOh no\u200a\u2014\u200aan obstacle to deep learning!ayearofai.com"}, {"be11": "You may be thinking: how does this issue apply to RNNs? Well, RNNs are very deep models; on top of often having multiple hidden layers, each hidden layer in practice can have hundreds of timesteps. That\u2019s like an ANN with hundreds of entire hidden layers! That\u2019s deep. (Well, it\u2019s more long because we\u2019re dealing with the time axis here, but you know what I mean.) tanh derivatives are very similar to sigmoid derivatives in range, so the problem of vanishing gradients is thus even more drastic with RNNs than with ANNs, and training them becomes almost impossible."}, {"4878": "Imagine trying to propagate the error to the 1st timestep in an RNN with k timesteps. The derivative would look something like this:"}, {"0e72": "With a tanh activation function, that\u2019s freaking crazy. Then, for getting the derivative of the error with respect to a weight matrix W_hh, we\u2019d add\u200a\u2014\u200aor, as mentioned before, we could average as well\u200a\u2014\u200aeach of these hidden state error gradients, then multiplied by the derivative of the hidden state with respect to the weight, such that we can backprop from the error to the weight:"}, {"0639": "Assuming our sequence is of length\u00a0k."}, {"9baa": "So we\u2019d be effectively adding together a bunch of terms that have vanished\u200a\u2014\u200athe exception being very late gradients with a small number of terms\u200a\u2014\u200aand so dJ/dWhh would only capture gradient signals from the last few timesteps. (Or, for exploding gradients, it would become infinity)."}, {"fd7e": "But, you might be asking, instead of tanh\u200a\u2014\u200awhich is bounded between -1 and 1, and has a similar problem to sigmoid where the peak of the derivative is smaller than 1\u200a\u2014\u200awhy don\u2019t we just use ReLUs? Don\u2019t ReLUs, or perhaps leaky ReLUs, solve the vanishing gradient problem?"}, {"6a76": "Well, not entirely; it\u2019s not enough to solve the problem. With RNNs, the problem really lies in the architecture. Even though we could use ReLU to ensure many of the values in the gradient computation are not between -1, 0, and 1 such that they vanish\u200a\u2014\u200aor vice-versa, explode\u200a\u2014\u200awe do still indeed have a lot of other variables other than the activation function derivative in the gradient computation such as the weights; you can revisit the mega-post on backprop we wrote to confirm this. Since weights are also normally randomly initialized in the range -1 to 1, and RNNs are like super deep ANNs, these weights keep multiplying on top of each other and potentially cause the gradients to vanish."}, {"4faf": "This is more my suspicion though\u200a\u2014\u200aI\u2019m yet to confirm this is the case by testing. I was curious so I asked this exact question on Quora:"}, {"e537": "From this, something interesting I learned is that: since ReLUs are unbounded (it\u2019s not restricted to be between -1 and 1 or 0 and 1) unlike sigmoid/tanh, and RNNs are very deep, the activations, especially later ones, can become too big. This is because hidden states have a multiplicative relationship; one hidden state is a multiple of the previous ones, where that multiple specifically is a weight. If we use ReLU, then the hidden state isn\u2019t limited by any range, and we could have a bunch of numbers bigger than 1 multiplying by each other."}, {"b4fc": "It ends up being sort of like the exploding gradient problem, but with the values inside the neurons, not gradients. This is also what then causes the gradients to explode: large activations \u2192 large gradients \u2192 large change in weights \u2192 even bigger activations, because updating the weights in the wrong direction ever so slightly can cause the entire network to explode. This makes learning unstable:"}, {"2c04": "This means that the computation within the RNN can potentially blow up to infinity without sensible weights. This makes learning VERY unstable because a slight shift in the weights in the wrong direction during backprop can blow up the activations during the forward pass. So that\u2019s why you see most people using sigmoid/tanh units, despite the vanishing gradient descent problem."}, {"4f2a": "Also well said:"}, {"e86a": "With RNN\u2019s, the problem is that you are repeatedly applying your RNN to itself, which tends to [mostly] cause exponential blowup or [rarely, but sometimes] shrinkage."}, {"4cb6": "Other issues with ReLU functions are discussed in the article I wrote, and they similarly apply to RNNs. Generally speaking, though, they just don\u2019t work that well, especially compared to other options we have. Making RNNs perform well with ReLUs is actually a pretty hot topic of research right now, but until someone figures out something genius, RNNs are a lost cause."}, {"4a29": "And that\u2019s why vanilla RNNs suck. Seriously. In practice, nobody uses them. Even if you didn\u2019t fully grasp this section on how the vanishing and exploding gradient/activation problem is applicable to them, it doesn\u2019t matter anyways. Because, everything you\u2019ve read up to this point so far\u2026 throw it all away. Forget about it."}, {"044a": "Just kidding. Don\u2019t do that."}, {"1d2b": "Fixing the problem with LSTMs (Part\u00a0I)"}, {"daad": "You shouldn\u2019t do that because RNNs actually aren\u2019t a lost cause. They\u2019re far from it. We just need to make a few\u2026 modifications."}, {"9491": "Enter the LSTM."}, {"731a": "Makes sense, no?"}, {"03ff": "How about this?"}, {"0d8c": "OK. Clearly something\u2019s not registering here. But that\u2019s fine; LSTM diagrams are frikin\u2019 difficult for beginners to grasp. I too remember when I first searched up \u201cLSTM\u201d on Google to encounter something similar to the works of art above. I reacted like this:"}, {"16e7": "MRW first Google Image-ing LSTMs."}, {"7cd4": "In this section, I\u2019m going to embark on a mission to design the first simple, comprehensible, and beautiful LSTM diagram. Wish me luck, because I\u2019ll probably fail."}, {"b288": "With that being said, let\u2019s dive into Long Short-Term Memory networks. (Yes, that\u2019s what LSTM stands for.)"}, {"bb27": "With RNNs, the real \u201csubstance\u201d of the model were the hidden neurons; these were the units that did processing on the input, through time, to produce the outputs. Specifically, at each timestep, a hidden neuron embodies a hidden state that is computed by feeding the weighted sum vector of the input and/or previous hidden states with an added bias vector through a tanh squashing function. We can have multiple hidden neurons, each of which when unrolled forms an entire hidden \u201clayer\u201d."}, {"66c2": "If you need a refresher on this, look through the \u201cFormalism\u201d section once again."}, {"cd18": "With LSTMs, we still have hidden states, but they\u2019re computed through a much more complex mechanism: LSTM cells. Instead of computing each hidden state as a direct function of inputs and other hidden states, we compute it as a function of the LSTM cell\u2019s value (the \u201ccell state\u201d) at that timestep. Each cell state is in turn functionally dependent on the previous cell state and any available input or previous hidden states. That\u2019s right\u200a\u2014\u200ahidden states are computed from cell states, and cell states are (in part) computed from older and/or shallower hidden states."}, {"095f": "The cell state at a specific timestep t is denoted c_t. Like a hidden state, a cell state is just a vector."}, {"e92a": "For simplicity\u2019s sake, I\u2019ve obfuscated layer index\u00a0\u2113."}, {"68f5": "If the diagram above seems a bit trippy, let me break it down for you."}, {"8bd2": "c_t, as highlighted by the three arrows pointing towards it, has multiple potential functional dependencies. Four to be exact, though only a maximum of three can exist at once. These are:"}, {"2a71": "Only three can exist at once because the last two are mutually exclusive."}, {"5160": "From there, we pass information to the next cell state c_t+1 and compute h_t. As you can hopefully see, h_t then goes on to also influence c_t+1 (as indicated by the horizontal arrow), along with higher level cell states or final outputs (the vertical arrow)."}, {"0f0f": "Right now the cells are a black box\u2026 literally; we know what is inputted to them and what they output, but we don\u2019t know their internal process. So\u2026 what\u2019s inside these cells? What do they do? What are the exact computations involved? How have the equations changed?"}, {"ee47": "To help answer the question, I want you to imagine something: a factory. Inside this factory we have workers who perform their own tasks. Those tasks are, specifically, operating on some sort of product that runs down a conveyer belt. Think of, hell, I don\u2019t know\u200a\u2014\u200achicken nuggets! The first worker cuts an appropriately sized piece, the second worker applies egg wash, the third worker adds breadcrumbs, the fourth worker chucks it in the fryer, etc."}, {"5a97": "I\u2019m not sure what product this conveyer belt carries, but it certainly doesn\u2019t look appetizing (or like chicken nuggets)."}, {"02ea": "You\u2019re thinking: \u201cOK Rohan, but how does this relate to LSTMs?\u201d. Good question."}, {"9004": "Basically, think of the conveyer belt as the cell state, the chicken-nugget-in-progress as information flowing through the cell, and the workers as operations we apply to this information. The final product is the finished chicken nugget\u200a\u2014\u200aor, the cell state value."}, {"b54e": "Chicken. Nugget."}, {"bca7": "The reason we use the analogy of a conveyer belt is because information can flow through a cell super super easily. It\u2019s theoretically possible for information to speed past a single cell state without being modified at all. In fact, I think the term \u2018modified\u2019 is a really strong one here. With vanilla RNNs, each hidden state takes all the information it has from before and fully transforms it by applying a function over it. LSTM cells instead take information and make minor modifications (like additions or multiplications) to it while it flows through."}, {"eca6": "Ew. Vanilla\u00a0RNNs."}, {"72ea": "Vanilla RNNs look something like this. And it\u2019s why the vanishing gradient problem exists; during backprop, gradients cannot flow back past these transformations easily, because the tanh derivatives and weights chain up and multiply together and tend to zero. We then add up or average all these gradients that are basically zero, and we get zero."}, {"9b8e": "LSTMs \ud83d\udca6 \ud83d\udca6\u00a0\ud83d\udca6"}, {"0a23": "This is an extreme a simplification\u200a\u2014\u200aand I\u2019ll go on to fill in the blanks later\u200a\u2014\u200abut it\u2019s sort of what an LSTM looks like. The previous timestep\u2019s cell state value flows through and instead of transforming the information, we tweak it by adding (another vector) to it. The added term is some function \u0192w of previous information, but this is not the same function as with vanilla RNNs\u200a\u2014\u200ait\u2019s heavily changed to make sense in this context (more on that soon), do more interesting things, and also reduce the vanishing gradient problem."}, {"ba34": "Another neat way to think of it is like a live circuit: there are two paths where information, like current, can flow through. After the information passes through \u0192w, it\u2019s added to the information flowing towards c_t. Thus, in equation form it could look something like this:"}, {"ea17": "Again\u2026 sort of. We\u2019ll get into the actual equations soon. This is a good proxy to convey my\u00a0point."}, {"c99b": "With a bit of substitution, we can expand this to:"}, {"2268": "Technically, this could expand even more, if you did some sort of recursive substitution to fully simplify the unrolled recurrence formula. You could express c_t for some large value of t as a really really really really long function of, ultimately, c_1."}, {"48be": "Why is this better? Well, if you have basic differentiation knowledge, you\u2019ll know that addition distributes gradients equally. When we take the derivative of this whole expression, it\u2019ll become a long addition of the derivatives of individual terms. As Andrej Karpathy puts it, this additive interaction creates \u201cgradient super-highways\u201d, where gradients can flow back super easily."}, {"740d": "Look\u200a\u2014\u200ait\u2019s a long conveyer belt! (In a sense, we can use this conveyor belt analogy for the whole unrolled LSTM as well. Each cell state is a subsection of the conveyer\u00a0belt.)"}, {"2d87": "Look\u200a\u2014\u200ait\u2019s an outdated machine learning algorithm!"}, {"68da": "In the former, gradients are always added together, never multiplied. In the latter, gradients are always multiplied. Thus, in the former, when we inject a gradient at the end, it\u2019ll easily flow back all the way to the beginning. Contributions by the \u0192w function will be made to this gradient flowing on the bottom conveyer belt as well."}, {"779d": "This is what gradient flow would look like:"}, {"7ea0": "Before, we discussed that when multiplicative interaction exists between gradients, the gradients either vanish (if they are mostly < 1, as is usually the case for us) or explode (if they are mostly > 1). Here\u2019s some real calculus to demonstrate this:"}, {"6f86": "Former is akin to RNNs. Latter is akin to\u00a0LSTMs."}, {"0175": "Imagine f being any sort of function, like our \u0192w. When we apply a function to itself repeatedly, the chain rule shows that the overall derivative is the multiplication of multiple different derivative terms. But, when we add functions together, the derivative is simply the addition of the individual derivatives. This won\u2019t vanish or explode quickly, so our LSTMs won\u2019t vanish or explode quickly. Yay!"}, {"4e50": "Furthermore, if some of our gradients vanish\u200a\u2014\u200afor whatever reason\u200a\u2014\u200athen it should still be OK. It won\u2019t be optimal, but since our gradient terms add together, if some of them vanish it doesn\u2019t mean the whole thing will vanish (versus if they were multiplied together). Look: 2 + 0 = 2 but 2 \u00d7 0 = 0."}, {"bbb5": "A gradient super highway? Sounds good to me! http://www.dyoung.com/assets/images/Articles%20images/article4_PPH.jpg"}, {"a58a": "So far, we haven\u2019t really explored LSTMs. We\u2019ve more setup a foundation for them. And there\u2019s one glaring issue with our foundation: if we just keep adding information to cell state, it could just grow and grow and grow, and essentially act as a counter that only increments. This is not very useful, and could regularly lead to explosion. We want more fine and rich control over memory. Well, worry not, because this is exactly what LSTMs are capable of doing."}, {"7ba6": "LSTM cells handle memory in a very intelligent way, enabling them to learn long-term dependencies and perform well. How, exactly? Well, the cell is sort of like an internal memory state that allows for context; it \u201cforgets\u201d, a.k.a. resets, information it doesn\u2019t find useful from the previous cell state, \u201cwrites\u201d in new information it does find useful from the current input and/or previous hidden state, and similarly only \u201creads\u201d out part of its information\u200a\u2014\u200athe good stuff\u200a\u2014\u200ain the computation of h_t. This respectively corresponds to the concepts of: resetting memory, writing to memory, and reading from memory. Very similar to how a modern computer system works, and we often describe an LSTM cell as a \u201cmemory cell\u201d."}, {"6706": "The \u201cwriting to memory\u201d part is additive\u200a\u2014\u200ait\u2019s what I showed you in the initial diagrams. Information flows through and we add stuff we think is relevant to it. The \u201cresetting memory\u201d part is multiplicative, and occurs before writing to memory; when information from the previous cell state initially flows in, we multiply it by a vector with values between 0 and 1 to reset or retain parts of it we find useless and useful respectively. The \u201creading from memory\u201d part is also multiplicative with a similar 0\u20131 range vector, but it doesn\u2019t modify the information flowing through the cell states. Rather, it modifies the information flowing into the hidden states and thus decides what the hidden state is influenced by."}, {"6813": "Both of these multiplications are element wise, like so:"}, {"c869": "In this equation, when a = 0 the information of c is lost. This is what resetting does, and retaining is the vice versa. I also imagine that values such as 0.5 could be used to diminish the importance of certain information, but not completely wipe it out."}, {"c920": "Our (unfinished) cell state computational graph now looks like this:"}, {"f4a0": "This is closer to what an LSTM looks like, though we\u2019re not exactly there\u00a0yet."}, {"ba90": "Sidenote: don\u2019t be scared whenever you see the word \u201cmultiplicative\u201d and don\u2019t immediately think of \u201cvanishing\u201d or \u201cexploding\u201d. It depends on the context. Here, as I\u2019ll show mathematically in a bit, it\u2019s fine."}, {"9705": "This concept in general is known as gating, because we \u201cgate\u201d what can flow in and out of the LSTM cell. What we actually multiply and add by to reset, write, and read are known as the \u201cgates\u201d. There are four such gates:"}, {"5cb0": "Here\u2019s our updated computational graph for the cell state:"}, {"01f6": "Looks like I\u2019m starting to create a complex diagram of my own. Damn. \ud83d\ude1e I guess LSTMs and immediately interpretable diagrams just weren\u2019t meant to be!"}, {"7eb5": "Basically, f interacts with the cell state through a multiplication. i interacts with g through a multiplication as well, the result of which interacts with the cell state through an addition. Finally, the cell state leaks into a tanh (that\u2019s the shape of the tanh function in the circle), the result of which then interacts with o through multiplication to compute h_t. This does not disrupt the cell state, which flows to the next timestep. h_t then flows forward (and it could flow upward as well)."}, {"e614": "Here\u2019s the equation form:"}, {"94fe": "Each gate should actually be indexed by timestep t\u200a\u2014\u200awe\u2019ll see why\u00a0soon."}, {"2518": "As you can see, our cell state has no activation function; the activation function is simply the identity function! Yet, the cell state usually doesn\u2019t explode\u200a\u2014\u200ait stays stable by \u201cforgetting\u201d and \u201cwriting\u201d, and does interesting things with this gating to promote context, fine control over memory, and long-term dependency learning."}, {"b30a": "So, how are the gates calculated? Well, all of these gates have their own learnable weights and are functions of the last timestep\u2019s hidden state flowing in and any current timestep inputs, not the cell state (contrary to what I may have implied earlier with the gradient flow diagrams). This should make sense when you think about it; I mean, firstly, the g and i gates literally represent input, so they better be functionally dependent on hidden states and input data! On an intuitive level, the gates help us modify the cell state, and we modify the cell state based on our current context. External stimulus that provide context should be used to compute these gates, and since context = input + hidden states our gates are functionally dependent on input and hidden states."}, {"27fb": "Since every gate has a different value at each timestep, we index by timestep t just like for hidden states, cell states, or something similar."}, {"6fb5": "We could generalize for multiple hidden layers as well:"}, {"1ae0": "But, for simplicity\u2019s sake, let\u2019s assume we are at the first hidden layer, or that there is only one hidden layer in the LSTM. This way, we can obfuscate the \u2113 term and ignore influence from hidden states in the previous depth. We\u2019ll also forget about edge cases and assume input exists at the current timestep. In practice, we obviously can\u2019t make said assumptions, but for the sake of demonstrating the equations it becomes too tedious otherwise."}, {"c042": "Sidenote: we make this assumption for the rest of the discussion on LSTMs in this article."}, {"033c": "Like with the RNN hidden state, the index of each weight matrix is descriptive; for example, W_xf are the weights that map input x to the forget gate f. Each gate has weight matrices that map input and hidden states to itself, including biases."}, {"6989": "And this is the beauty of LSTMs; the whole thing is end-to-end differentiable. These gates can learn when to allow data to flow and what data should flow depending on the context it sees (the input and the hidden states). It learns this based on patterns it sees while training. In this sense, it\u2019s sort of like how a CNN learns feature detectors for images, but the patterns are way more complex and less human interpretable with LSTMs. This is why they perform so well."}, {"1be4": "\ud83d\ude28 \ud83d\ude31 \ud83d\ude30\u00a0: perhaps your immediate reaction."}, {"b263": "Okay, this looks scarier, but it\u2019s actually not much different to what we had before, especially once you look past the intimidating web of arrows. One notable change is that we\u2019re showing the previous hidden state in time and the current input flowing in. This diagram makes the assumption that we\u2019re in the first layer and at some timestep > 1 where input exists. We then show how the f, i, g, and o gates are computed from this information\u200a\u2014\u200athe hidden state and inputs are fed into an activation function like sigmoid (or, for g, a tanh; you can tell because it\u2019s double the height of the others)\u200a\u2014\u200aand it\u2019s expressed through the web of arrows. It\u2019s implied that we weight the two terms entering our activation functions, adding them up with a bias vector, but it\u2019s not necessarily explicit in the diagram."}, {"b56c": "Let\u2019s embed this into our overall LSTM diagram for a single timestep:"}, {"147c": "Now let\u2019s zoom out and view our entire unrolled single layer, three timestep LSTM:"}, {"c9d0": "It\u2019s beautiful, isn\u2019t it? The full screen width size just adds to the effect! Here\u2019s a link to the full res version."}, {"c678": "The only thing that would look more beautiful would be multiple LSTM cells that stack on top of each other (multiple hidden layers)! \ud83d\ude0d"}, {"e117": "Fixing the problem with LSTMs (Part\u00a0II)"}, {"f89a": "You\u2019ve come a long way, young padawan. But there\u2019s still a bit left to go. Part I focused on the motivation for LSTMs, how they work, and a bit on why they reduce the vanishing gradient problem. Now, having a full understanding of LSTMs, Part II will hone in on the latter part\u2014analyzing on a more close, technical level why our gradients stop vanishing as quickly. You won\u2019t find a lot of this information online easily; I had to search and ask left and right to find an explanation better and more comprehensive than what you\u2019ll find in other current tutorials."}, {"a182": "Firstly, truncated BPTT is often used with LSTMs; it\u2019s a method to speed up training. In particular, note that if we input a sequence of length 1000 into an LSTM, and want to train it, it\u2019s equivalent to training a 1000 layer neural network. Doing forward and backwards passes into this is very memory and time consuming, especially while backpropagating the error when we need to compute a derivative like this:"}, {"06fe": "\u2026which would include a lot of terms."}, {"792f": "When we backprop the error, and add all the gradients up, this is what we get:"}, {"9c5f": "Truncated BPTT does two things:"}, {"eb5a": "For example, if t = 20 and k1 = 10, our second (because 20 \u00f7 10 = 2) round of BPTT would be:"}, {"956c": "So, with t = 20, k2 = 10, and k1 = 10, our second round of BPTT would follow:"}, {"a297": "Both k1 and k2 are hyperparameters. k1 does not have to equal k2."}, {"93ed": "These two techniques combined enables truncated BPTT to not lose the ability to learn long term dependencies. Here\u2019s a formal definition:"}, {"72ee": "[Truncated BPTT] processes the sequence one timestep at a time, and every k1 timesteps, it runs BPTT for k2 timesteps, so a parameter update can be cheap if k2 is small. Consequently, its hidden states have been exposed to many timesteps and so may contain useful information about the far past, which would be opportunistically exploited."}, {"ae91": "\u2014 \u201cTraining Recurrent Neural Networks\u201d, 2.8.6, Page 23"}, {"28a4": "The same paper gives nice pseudocode for truncated BPTT:"}, {"399f": "The rest of the math in this section will not be in the context of using truncated backprop, because it\u2019s a technique vs. something rooted in the mathematical foundation of LSTMs."}, {"d546": "Moving on\u200a\u2014\u200abefore, we saw this diagram:"}, {"48f4": "In this context, \u0192w = i \u2299 g, because it\u2019s the value we\u2019re adding to the cell state."}, {"db0e": "But this diagram is a bit of a lie. Why? It ignores forget gates. So, does the presence of forget gates affect the vanishing gradient problem? Quite significantly, actually. How? Let\u2019s bring up our cell state equation to see:"}, {"09ae": "With the forget gate, we now include a multiplicative interaction. Our new diagram will look like this:"}, {"931c": "Do not confuse forget gate \u0192 with function \u0192w in this diagram. I know, it\u2019s confusing\u2026 \ud83d\ude22"}, {"bf2c": "When our gradients flow back, they will be affected by this multiplicative interaction. So, let\u2019s compute the new derivative:"}, {"f174": "This seems super neat, actually. Obviously the gradient will be f, because f acts as a blocker and controls how much c_t-1 influences c_t; it\u2019s the gate that you can fully or partially open and close that lets information from c_t-1 flow through! It\u2019s just intuitive that it would propagate back perfectly."}, {"d859": "But, if you\u2019ve payed close attention so far, you might be asking: \u201cwait, what happened to \u0192w\u2019s contribution to the gradient?\u201d If you\u2019re a hardcore mathematician, you might also be worried that we\u2019re content with leaving the gradient as just f. This is because the gates f, i, and g are all functions of c_t-1; they are functions of h_t-1, which is, in turn, a function of c_t-1! The diagram shows this visually, as well. It seems we\u2019re failing to apply calculus properly. We\u2019d need to backprop through f and through i \u2299 g to complete the derivative."}, {"3f0c": "Let\u2019s walk through the differentiation to show why you\u2019re actually not wrong, but neither am I:"}, {"80e0": "Now, with the first derivative, we need to apply product rule. Why? Because we\u2019re differentiating the product of two functions of c_t-1. The former being the forget gate, and the latter being just c_t-1. Let\u2019s do it:"}, {"00fd": "Then, from product rule:"}, {"7dd1": "That\u2019s the first derivative done. We purposely choose not to compute the derivative of the forget gate with respect to the previous cell state on previous. You\u2019ll see why in a bit."}, {"e8f2": "Now let\u2019s tackle the second one:"}, {"10b0": "You\u2019ll notice that it\u2019s also two functions of c_t-1 multiplied together, so we use the product rule again:"}, {"618f": "So:"}, {"c8bf": "Once again, we purposely do not simplify the gate derivative terms."}, {"41e0": "Thus, our overall derivative becomes:"}, {"7b20": "Notice that the first term in this derivative is our forget\u00a0gate."}, {"9307": "Pay attention to the caption of the diagram."}, {"4712": "This is actually our real derivative. Modern LSTM implementations just use an auto differentiation library to compute derivatives, so they\u2019ll probably come up with this. However, effectively, our gradient is just the forget gate, because the other three terms tend towards zero. Yup\u200a\u2014\u200athey vanish. Why?"}, {"b5a4": "When we backprop error in LSTMs, we backprop through cell states to propagate the error from the outputs to the cell state we want. For example, if we want to backprop the error from the output at time t down k timesteps, then we need to compute the derivative of the cell state at time t to the cell state at time t-k. Look what happens when we do that:"}, {"57e3": "We didn\u2019t simplify the gate w.r.t. cell state derivatives for a reason; as we backpropagate through time, they begin to vanish. Thus, whatever they multiplied with is killed off from making contributions to the gradient, too. So, effectively:"}, {"e607": "The rationale behind this is pretty simple, and we don\u2019t need math for it; these gates are the outputs of non-linearities eg. sigmoid and tanh. If we were to get the derivative of them in getting our cell state derivative, then this derivative would contain the derivatives of sigmoid/tanh in them. But, just because we don\u2019t need to use math to show this, doesn\u2019t mean we don\u2019t want to \ud83d\ude0f\u00a0:"}, {"0dbb": "I obfuscated the input to the sigmoid function for the input gate, just for simplicity."}, {"f6be": "Recall from our vanishing gradient article that the max output of sigmoid\u2019s first order derivative is 0.25, and it\u2019s something similar for tanh. This becomes textbook vanishing gradient problem. As we backprop through more and more cell states, the gradient terms become longer and longer, and this will definitely vanish. When they don\u2019t vanish, they\u2019ll be super minor contributions, so we can just leave them out for brevity."}, {"95ec": "Sidenote: one person reached out to me unsure of why gradients with long terms\u200a\u2014\u200aaka, that are equal to the product of a lot of terms\u200a\u2014\u200ausually vanishes/explodes. Here\u2019s what I said in response:"}, {"eabd": "\u201cIf you have long gradient terms, you probably have the vanishing gradient problem, unless you can guarantee those terms are around 1 each. If they\u2019re not, it\u2019ll explode or vanish. And, given the nature of the problems where this is an issue, it\u2019s very unlikely they\u2019ll be around 1 each. Especially if they are the output some non-linear function like sigmoid/tanh or their derivatives."}, {"944e": "For example, let\u2019s say the gradient term = k_1 \u00d7 k_2 \u00d7 k_3 \u00d7\u00a0\u2026 \u00d7 k_100. 100 terms in this product."}, {"1583": "If each of these terms is, let\u2019s say, around 0.5, then you have 0.5\u00b9\u2070\u2070 = some absurdly low number. If you have each term be arond 1.5, then you have 1.5\u00b9\u2070\u2070 which is some absurdly high number.When we introduce tanh/sigmoid and/or their derivatives in these huge products, you can guarantee that they\u2019ll saturate and die off. As mentioned, the max for sigmoid\u2019s first order derivative is 0.25, so just imagine something like 0.25\u00b9\u2070\u2070."}, {"59c9": "Ultimately, the reason I obfuscate these terms that vanish in the derivative is because I would like to show the effect of the forget gate on gradient flow now. If I included the other terms, the same implications would be present, but the math would just take longer to type out and render."}, {"e361": "Because \u0192w = i \u2299 g, we can redraw our diagram showing that \u0192w won\u2019t make any contributions to the gradient flow back. Again\u200a\u2014\u200a\u0192w does, but it\u2019s effectively negligible, so we can just exclude it from our updated gradient flow diagram, which follows:"}, {"8811": "But wait! This doesn\u2019t look good; the gradients have to multiply by this f_t gate at each timestep. Before, they didn\u2019t have to multiply by anything (or, in other words, they multiplied by 1) and flowed past super easily."}, {"9feb": "Machine learning researchers coined a name for the type of function we had before we introduced the forget gate where the derivative of one cell state w.r.t. the previous is 1.0: \u201cConstant Error Carousel\u201d (CEC). With our new function, the derivative is equal to f. You\u2019ll see this referred to as a \u201clinear carousel\u201d in papers."}, {"fdbb": "Before we introduced a forget gate\u200a\u2014\u200awhere all we had was the additive interaction from \u0192w\u200a\u2014\u200aour cell state function was a CEC:"}, {"2bf9": "A CEC\u200a\u2014\u200asame as before, but no forget\u00a0gate."}, {"61b2": "The derivative of this cell state w.r.t. the previous one, again as long as we don\u2019t backprop through the i and g gates, is just 1. That\u2019s why gradients flow back super comfortably, without vanishing at all. Basically, for a CEC to exist in this context, the coefficient of c_t-1 needs to be 1."}, {"7ab2": "Once we introduced this multiplicative interaction (for good reason), we got a linear carousel; the coefficient of c_t-1 is f. So, in our case, when f = 1 (when we\u2019re not going to forget) our function becomes a CEC, and our gradients will pretty much never vanish. If it\u2019s close to 0, though, the gradient term will immediately die. Gradients will stay on the carousel for a while until the forget gate is triggered; the effect on the gradient is like a step function, in that it\u2019s constant with a value of 1 and then drops off to zero/dies when we have f \u2248 0."}, {"3337": "Intuitively, this seems problematic. Let\u2019s do some math to investigate:"}, {"2abc": "The derivative of a cell state to the previous is f_t. The derivative of a cell state to two prior cell states is f_t \u2299 f_t-1. Thus:"}, {"8238": "As we backpropagate through time, these forget gates keep chaining up and multiplying together to form the overall gradient term."}, {"6f12": "Now, imagine an LSTM with 100 timesteps. If we wanted to get the derivative of the error w.r.t. a weight like W_xi, to optimize it, remember that with BPTT we add up or average all the gradients from the different timesteps:"}, {"a73a": "OK. Now let\u2019s look at an early (in time) term, like the gradient propagated from the error to the third cell:"}, {"9972": "Remember that J is an addition of errors from Y individual outputs, so we backpropagate through each of the outputs first:"}, {"4355": "The first few terms, where we backprop y_k to c_3 where k < 3, would just be equal to zero because c_3 only exists after these outputs have been computed."}, {"e686": "Let\u2019s assume that Y = 100 and continue with our assumption that t = 100 (so each timestep gives rise to an output), for simplicity. With this, let\u2019s now look at the last term in this sum."}, {"6208": "That\u2019s a lot of forget gates chained together. If one of these forget gates is [approximately] zero, the whole gradient dies. If these also tend to be a small number between 0 and 1, the whole thing will vanish, and c_3 won\u2019t make any contributions to the gradient here."}, {"fb13": "This isn\u2019t intrinsically an issue though! Because, when a forget gate is zero, it means that cell is no longer making any contributions past that point. If f_4 is zero, then any y outputs at/past timestep 4 won\u2019t be influenced by c_3 (as well as c_2 and c_1) because we \u201cerased\u201d it from memory. Therefore that particular gradient should be zero. If y_80 is zero, then any outputs at/past timestep 80 won\u2019t be influenced by c_1, c_2,\u00a0\u2026\u00a0, c_79. Same story here. If these forget gates are between 0 and 1, then the influence of our cell decays over time anyways, and our gradients will be very small, so they\u2019ll reflect that. Gers 1999 calls this \u201creleasing resources\u201d."}, {"b6f8": "Cell c_3 will still contribute to the overall gradient, though. For example, take this term:"}, {"8736": "Here, we\u2019re looking at y_12 instead of y_100. Chances are that, if you have a sequence of length 100, your 100th cell state isn\u2019t drawing from your 3rd; the forget gate would have been triggered at some point by then. However, the 12th cell state probably will still be drawing from the ones before it."}, {"5cd8": "If we decide not to forget in the first 12 timesteps, ie. f_1\u00a0\u2026 f_12 are each not far from 1, then c_3 would have more influence over y_12 and the error that stems from y_12. Thus, the gradient would not vanish and c_3 still contributes to update W_xi, it just doesn\u2019t contribute a gradient where it\u2019s not warranted to (that is, where it doesn\u2019t actually contribute to any activation, because it\u2019s been forgotten). To summarize: one activated forget gate will indeed kill off gradient flow to cell(s), but that is a good thing because the network is learning that that gradient from the future has no benefit and is completely irrelevant to those particular cell(s), since those cells have been forgotten by then. In practice, different cells learn different ranges of context, some short, some long. This is a benefit for LSTMs."}, {"7b24": "So, given a gradient between two cell states in time, when all of these forget gates are [approximately] equal to 1, the gradient signal will remain stable, because we\u2019re multiplying by 1 at each timestep\u200a\u2014\u200aeffectively, not multiplying by anything at all. In such a case, our gradient flow diagram would look like this:"}, {"9b4b": "It\u2019s\u2026 it\u2019s beautiful!"}, {"f12e": "The gradient will have literally zero interactions or disturbances, and will just flow through like it\u2019s driving 150 mph on an empty countryside America highway. The beauty of CECs is that they\u2019re always like this."}, {"87cf": "But, let\u2019s get back to reality. LSTMs aren\u2019t CECs. One disadvantage of these forget gates chaining together is that it could block learning. That is, when we set out to train our LSTM, the forget gates have not been learned; we have to learn them while we learn everything else. So, if they all start around 0, no gradients will flow through our cell states when we perform BPTT, and learning won\u2019t happen at all."}, {"3db3": "The obvious solution is to set the forget gate bias to a very large value when training, so it starts at 1 instead of 0 (because y = 1 is to the far right of the sigmoid function, so adding to the input will ensure ~1 will be the output). In early stages of training, the forget gates equalling/approximating 1 will result in learning not being blocked. So many papers do this and mention it explicitly such that this forget gate bias could even be considered a hyperparameter."}, {"d763": "By introducing forget gates, we stray from CECs and thus the guarantee that our gradients will never ever vanish. But, again, we do it for good reason. And when gradients vanish it\u2019s because we chose to forget that cell\u200a\u2014\u200aso it\u2019s not necessarily a bad thing. We just need to make sure the forget gates don\u2019t block learning in initial stages of training."}, {"e1c7": "We can try computing some more derivatives, just for fun! Let\u2019s sub in real values for the timesteps, backprop across more than one timestep, and do it for a gate this time."}, {"62f7": "We\u2019ll expand c_4 and express it in terms of our gates only. In the process, each c_t, except c_1, will collapse into a few interactions between the f, i, and g gate:"}, {"5092": "Now, let\u2019s get the derivative of c_4 with respect to one of the earliest possible gates, like g_2. In the expression above, this turns out to just be the coefficient of g_2:"}, {"708a": "We experience the same neatness here as with the cell state backprop! It makes complete sense that the gradient would be i_2 \u2299 f_3 \u2299 f_4, since i_2 controls what influence g_2 has over c_2, f_3 controls what influence c_2 has on c_3, and f_4 controls what influence c_3 has over c_4. Notice the chaining up of the forget gates \ud83d\udc7b; everything about the carousels I just talked about\u200a\u2014\u200aand what they imply about vanishing gradients\u200a\u2014\u200aapplies here."}, {"6229": "I\u2019ll leave it up to you to derive something similar for the other gates."}, {"f15d": "And that\u2019s it! That\u2019s why LSTMs rock their socks off when it comes to keeping their gradients in check,."}, {"627a": "Here\u2019s a neat GIF showing a visual representation of the gradients that exist at each timestep, starting from timestep 128 and going all the way to the first, during backprop. More noise represents greater values:"}, {"868a": "As you can see, the vanilla RNN\u2019s gradients die off way quicker than the LSTM\u2019s. The RNN is almost immediate in comparison. LSTMs seem like a super highway indeed, although it does seem that they do vanish. In this diagram, it can be suggested that the gradients perhaps die for the LSTM eventually because we chose to forget early cell states; again, this depends on the application at hand, and is learnable. (I\u2019m not sure if this GIF uses truncated backprop, so that could be another thing. In general, I don\u2019t know the context of this GIF.) Also, part of the gradient signal definitely vanishes\u2014it\u2019s the signals that pass through the f/i/g gates that we looked at earlier and obfuscated from the cell state\u2192cell state derivative. We showed they would vanish because of tanh/sigmoid derivatives; initially, these signals will make a fairly significant contribution, but over time they\u2019ll get smaller and smaller. That\u2019s the explanation for this GIF."}, {"2b66": "Exploding gradients is still an issue, though. Recall that when we have a bunch of gradient terms added together, if some of them vanish it doesn\u2019t mean the whole thing will vanish (versus if they were multiplied together). However, if some of the gradients explode, the whole thing explodes; x + 0 = x, but x + \u221e = \u221e. If cell states become unstable and grow too much in some rare scenario, then our gradients could explode. In such a case we\u2019d need to implement gradient clipping, which is where we choose some arbitrary threshold that gradients cannot be larger than; so, grad = min(grad, clip_threshold). This would enable the LSTM to deal with such cases without essentially collapsing. Many successful LSTM applications use gradient clipping."}, {"69b6": "Usually, though, exploding gradients are avoided because sooner or later the forget gate in the carousel is triggered and we reset the memory."}, {"3679": "There are variants of LSTMs. People have tried modifying the model, like computing the hidden state without using tanh activation (so h_t = o \u2299 c_t) or ditching the i input gate and only using g, since that would still satisfy the -1 to 1 range. The results didn\u2019t change by much."}, {"dc3e": "In fact, some researchers even applied evolutionary algorithms to spawn and test a bunch of variants on the standard LSTM equations. Most of the good ones just worked roughly the same."}, {"c7f8": "This highlights an issue with LSTMs\u200a\u2014\u200athey are definitely fairly handwavy. We use them because their architecture allows us to prevent gradients from vanishing such that we can learn long-term dependencies, but there\u2019s not much theoretical or empirical backing for them. ANNs and RNNs make sense in that they\u2019re biologically inspired and that they\u2019re essentially just deep composite functions that have parameters we can optimize. LSTMs stray so far from statistical methods and introduce complex concepts/architectures that work but aren\u2019t necessarily justified from the get-go. Fully understanding why LSTMs work so well and coming up with better/simpler architectures is a hot topic of research right now."}, {"4bca": "There are also other variants of RNNs, similar to LSTMs, like GRUs (Gated Recurrent Units). LSTM is still the king, but GRUs in particular have grown in popularity and are seen in many recent, well-respected research papers. It\u2019s a must learn next to LSTMs, but this article would get too bloated with it. TL;DR: GRUs have a less complex architecture than LSTMs but achieves similar results, and they can control the flow of information without requiring a memory cell. And, they\u2019re fairly new. (See, told you \u201ccoming up with better/simpler architectures is a hot topic of research right now\u201d is true!)"}, {"c57a": "Yay RNNs!"}, {"8a09": "Wanna see a couple cool things? Practical applications were talked about in the first section, and the next section will walk through more technical applications of RNNs ie. in recent research papers. This section, instead, contains some fun things RNNs have done or produced that\u2019ll ease yourself from the hell that was actually understanding them. As simply as we try to convey things on this blog, LSTMs be LSTMs."}, {"7337": "Sidenote: now, don\u2019t be frightened by \u201cRNNs\u201d. Do be frightened by \u201cvanilla RNNs\u201d, in most contexts, but we use RNN as an umbrella term, and it most often refers to an LSTM or a similar variant like GRU."}, {"f30f": "Many if not all of these are taken from Andrej Karpathy\u2019s CS231n lecture, or his blog post on the same subject:"}, {"d19b": "The Unreasonable Effectiveness of Recurrent Neural NetworksMusings of a Computer Scientist.karpathy.github.io"}, {"83d4": "You should most certainly visit either his blog post or lecture for info on exactly how these experiments were conducted, and for more interpretation on their results. Taking a look at the \u2018Visualizing the predictions and the \u201cneuron\u201d firings in the RNN\u2019 section would also be helpful to gain more insight and intuition on how RNNs work and learn over time."}, {"c051": "A recurrent neural network generated this body of text, after it \u201cread\u201d a bunch of Shakespeare:"}, {"62d6": "Similarly, Karpathy gave an LSTM a lot of Paul Graham\u2019s startup advice and life wisdom to read, and it produced this:"}, {"3654": "\u201cThe surprised in investors weren\u2019t going to raise money. I\u2019m not the company with the time there are all interesting quickly, don\u2019t have to get off the same programmers. There\u2019s a super-angel round fundraising, why do you can do. If you have a different physical investment are become in people who reduced in a startup with the way to argument the acquirer could see them just that you\u2019re also the founders will part of users\u2019 affords that and an alternation to the idea. [2] Don\u2019t work at first member to see the way kids will seem in advance of a bad successful startup. And if you have to act the big company\u00a0too.\u201d"}, {"ee9f": "A lot of relevant terminology, but it doesn\u2019t really\u2026 come together \ud83d\ude16."}, {"1fc3": "An LSTM can even generate valid XML, after reading Wikipedia!:"}, {"89df": "<page>  <title>Antichrist</title>  <id>865</id>  <revision>    <id>15900676</id>    <timestamp>2002-08-03T18:14:12Z</timestamp>    <contributor>      <username>Paris</username>      <id>23</id>    </contributor>    <minor />    <comment>Automated conversion</comment>    <text xml:space=\"preserve\">#REDIRECT [[Christianity]]</text>  </revision></page>"}, {"7056": "After a similar LSTM read the raw Latex version of an advanced algebra textbook, it came up with this\u200a\u2014\u200aput frankly\u200a\u2014\u200afancy looking bogus. Let\u2019s be real, you could definitely believe this was actual math \ud83d\ude1c:"}, {"e129": "An LSTM also read the Linux source code, and tried to write some code of its own:"}, {"ce46": "/* * Increment the size file of the new incorrect UI_FILTER group information * of the size generatively. */static int indicate_policy(void){  int error;  if (fd == MARN_EPT) {    /*     * The kernel blank will coeld it to userspace.     */    if (ss->segment < mem_total)      unblock_graph_and_set_blocked();    else      ret = 1;    goto bail;  }  segaddr = in_SB(in.addr);  selector = seg / 16;  setup_works = true;  for (i = 0; i < blocks; i++) {    seq = buf[i++];    bpf = bd->bd.next + i * search;    if (fd) {      current = blocked;    }  }  rw->name = \"Getjbbregs\";  bprm_self_clearl(&iv->version);  regs->new = blocks[(BPF_STATS << info->historidac)] | PFMR_CLOBATHINC_SECONDS << 12;  return segtable;}"}, {"da41": "SUPERINTELLIGENCE MUCH\u203d SELF-RECURSIVE IMPROVEMENT MUCH\u203d THE END OF THE UNIVERSE MUCH\u203d"}, {"7094": "Nope. Just some code doesn\u2019t compile or make any sense. It even has its own bogus comments!"}, {"b3fc": "Generating music? Easy! A fun watch:"}, {"c80a": "A more informative watch:"}, {"6518": "Something even cooler and\u2026 creepier (seriously, the results after the first couple iterations of training are so unsettling):"}, {"7eb1": "In Practice"}, {"a21c": "So we\u2019ve seen how RNNs work in theory; now where do they fit in in practice?"}, {"e928": "As it turns out, recurrent neural networks can do a whole lot. I\u2019ll try to cover a few of the important, significant, and interesting uses that have cropped up over the last few years."}, {"e30a": "Bidirectional Recurrent Neural\u00a0Networks"}, {"5985": "The Problem: Giving the network access to a sequence of vectors is fine and dandy, but what if we want our output at time t to be conditioned on an input vector that comes at a later timestep? Take the example of speech recognition, where our input vectors are some kind of audio features at time t and the output is the predicted phoneme at that time. In our traditional RNN architecture, the output at time t is conditioned only on input vectors 1..t, but as it turns out future information might be useful too. The sounds at time step t+1 (and maybe t+2, t+3,\u00a0\u2026) are likely part of the same phoneme, and therefore could help us make more accurate predictions. But our network won\u2019t have access to them until we already output a prediction at time t. That\u2019s bad."}, {"60ef": "The Solution: We essentially \u201cdouble up\u201d each RNN neuron into two independent neurons\u200a\u2014\u200aa \u201cforward\u201d neuron and a \u201cbackward\u201d neuron. The forward neuron is the same as a regular RNN neuron, which gets inputs 0..T sequentially, updating its internal state and outputting some value at each time step along the way. The backward neuron follows the same general principle, but it sees the input vectors in reverse order."}, {"e2ef": "We\u2019ll look at an example to make sense of all this."}, {"2749": "This is a typical recurrent neural network: at each timestep, the hidden state is updated based on the latest\u00a0input."}, {"89e5": "This is a bidirectional recurrent neural network. There are two neurons: one that takes inputs like normal, and one that takes them in reverse. Their output is combined to produce one\u00a0output."}, {"ea03": "Let\u2019s walk through this timestep-by-timestep. At t=0, our vanilla RNN cell takes the input, updates its hidden state, and outputs a value. Now let\u2019s look at the BiRNN: the \u201cforward\u201d half of our BiRNN neuron does exactly the same thing, but the \u201cbackward\u201d half looks through all of our inputs\u200a\u2014\u200ain reverse order, t=T..0\u200a\u2014\u200aand updates its hidden state with each one. Then when we get to the t=0 input vector, it updates its hidden state one last time and outputs a final value. We then take this final output value and combine it with the \u201cforward\u201d half (\u201ccombine\u201d is pretty loosely-defined, usually just by concatenation or addition). Moving on to t=1, our \u201cforward\u201d part reads in the next input, updates state, and outputs another value. Combined with the second-to-last output of our \u201cbackward\u201d counterpart, and we have the second output of our BiRNN neuron. Rinse and repeat."}, {"1bf8": "And that\u2019s the general idea. Neat, right? BiRNNs (and their more adept cousin, BiLSTMs) are used all over the place. Maybe we\u2019ll see them popping up in some of the other case studies that we\u2019ll be looking at."}, {"7549": "Autoencoders"}, {"cdb3": "Remember when we talked about autoencoders? Turns out we can use RNNs there too!"}, {"107c": "Let\u2019s refresh: what is an autoencoder? Put simply, it\u2019s a clever way of tricking a neural network to learn a useful representation of some data. Let\u2019s say we have a dataset of images of faces, and we want to compress the thousands of numbers representing RGB values of pixels down into a 500-dimensional latent vector. We construct a network as such, where the middle layer has 500 neurons:"}, {"08e3": "https://en.wikipedia.org/wiki/Autoencoder#/media/File:Autoencoder_structure.png"}, {"fe44": "\u2026and train it to reproduce the input in the output."}, {"46d5": "Let\u2019s explore this idea a little further. Imagine that data is flowing through our network, starting with the input layer and through each subsequent layer. We can view each layer as performing a transformation, converting our input to another vector, and then that vector into another, until we get our output. If we train our network to reproduce the input, that means that each intermediate vector must still represent the same information as the input, in some form or another. Essentially, the activations of each layer are a new representation of our input vector. If our network trains well, we can convert a 10,000-dimensional vector of pixel values into a 500-dimensional vector of image features which can be converted back into a 10,000-dimensional vector of pixel values that approximates what the input would have been."}, {"c1de": "Let\u2019s make this a tad more concrete. We have two functions, f and g. f is our encoder, mapping from an n-long vector to an m-long vector. (n is the size of our input, m is the size of our latent representation.) g is our decoder, which maps back from an m-long vector to an n-long vector. In the normal autoencoder setting, both f and g are neural networks trained jointly (or different parts of a single network, same thing really) to reconstruct x."}, {"1680": "So, where do RNNs fit in? Let\u2019s say our inputs are now sequences of vectors instead of a single vector. We can use a similar concept, with both the encoder and decoder represented using an RNN. Here\u2019s how it works: we feed our input sequence into the encoder RNN. With each input vector of the sequence, this encoder updates its internal state. Eventually, once it has seen the entire input, we have some final network internal state which represents our entire input sequence. Neat! Now, we make the hidden state of our decoder RNN the initial hidden state of our encoder, and ask it to spit out a sequence. Ideally, it spits out something close to what the initial sequence was."}, {"b8c2": "Going back to our math-y definitions, we see that it basically fits in to the same framework, except we have q n-long vectors going into f and coming out of g. So q n-long vectors go in to f, and a single m-long vector comes out. We then give this m-long vector back to g, which spits out q n-long vectors."}, {"5a64": "That was a lot of letters, but you get the idea (I hope)."}, {"b829": "Like much of deep learning, the concept itself is pretty simple, but the implications are pretty cool. We can take any sequence\u200a\u2014\u200aa variable-length sequence, mind you\u200a\u2014\u200aand convert it into a fixed-size vector. And then convert that back to a variable-length sequence."}, {"5546": "It turns out this model is actually incredibly powerful, so let\u2019s take a look at one particularly useful (and successful) application: machine translation."}, {"4be2": "Neural Machine Translation"}, {"9d22": "Let\u2019s take these ideas we just learned about sequence-to-sequence (or seq2seq, for short) RNNs and apply them to machine translation. We throw in a sequence of words in one language, and it outputs a sequence of words in another. Simple enough, right?"}, {"cd0e": "The model we\u2019re going to look at specifically is Google\u2019s implementation of NMT. You can read all the gory details in their paper, but for now why don\u2019t I give you the watered-down version."}, {"3f1b": "At it\u2019s core, the GNMT architecture is just another seq2seq model. We have an encoder, consisting of 8 LSTM layers with skip connections (the first layer is bidirectional). We also have a decoder, once again containing 8 LSTM layers with skip connections. (A skip connection in a neural network is a connection which skips a layer and connects to the next available layer.) The decoder network outputs a probability distribution of words (well, sort of\u200a\u2014\u200awe\u2019ll talk more about that later), which we sample from to get our [translated] sentence. \ud83c\udf89"}, {"afc6": "Here\u2019s a scary diagram from the paper:"}, {"6a00": "https://arxiv.org/abs/1609.08144"}, {"a450": "But there are a few other aspects to the GNMT that are important to note (there\u2019s actually lots of interesting stuff going on in this architecture, so I really recommend you do read the paper)."}, {"1fbc": "Let\u2019s turn our attention to the center of the above diagram. This is a critical part of the GNMT architecture (and GNMT is certainly not the first to use attention) which allows the decoder to focus on certain parts of the encoder\u2019s output as it produces output. Specifically, the GNMT architecture differs from the traditional seq2seq model in that our encoder does not produce a single fixed-width vector (the final hidden state) representing the entire output. Instead, we actually look at the output from each time step, and each time step gives us some latent representation. While decoding, we combine all of these hidden vectors into one context vector using something called soft attention."}, {"369f": "https://arxiv.org/abs/1609.08144"}, {"ddb5": "More concretely, that works like this (at every decoder time step). We first look at the output of the first decoder layer from the last time step. Following the notation from the paper, we\u2019ll call that yi-1. We also have a series of encoder outputs, x1\u2026xM, one for each encoder timestep. For each encoder timestep, we give our special attention function yi-1 and xt and get back a single fixed-size vector st, which we then run through a softmax. So, we\u2019ve converted our encoder information from that timestep (and some decoder information) into a single attention vector\u200a\u2014\u200athis attention vector tells us which parts of the encoder output we should look at more closely. We multiply this attention vector by our encoder output xt, which has the effect of \u201cfocusing\u201d more on certain values and less on others. Finally, we take the sum of those \u201cfocused\u201d vectors over each encoder timestep to produce our attention context for this timestep ai, which is fed to every decoder layer."}, {"8681": "Oh yeah, that attention function? That\u2019s just yet another neural network."}, {"145a": "Attention mechanisms like this one are pretty common in many deep learning architectures. This is an example of soft attention: we learn a distribution over our inputs and compute a weighted sum. This process is fully-differentiated, so we can use standard backpropogation to figure out how to train our attention model. Another possible mechanism is called hard attention, in which we select just one of the possible inputs and \u201cfocus\u201d solely on that input. This process is not differentiable, so we need to use some other algorithm (usually some kind of reinforcement learning) to train a hard attention algorithm."}, {"10b2": "GNMT combines all kinds of other cool ideas to achieve state-of-the-art results, including a wordpiece model which segments words into smaller \u201cwordpieces\u201d to help translate rarer words and neat parallelization techniques that let them train this monstrosity of an architecture in reasonable time."}, {"818e": "A few months ago, Google put their GNMT model into production. Cutting-edge research is being implemented in the real world at an incredible rapid pace within the field of machine learning, and this is just one of countless examples."}, {"0c13": "Long-Term Recurrent Convolutional Networks"}, {"c3b8": "(Not to be confused with LCRNs.)"}, {"e750": "The Problem: We have a sequence of images that we need to make predictions for. CNNs are good at processing images, RNNs are good at processing sequences\u2026how do we put the two together?"}, {"606b": "The Solution: The solution proposed in this paper is as straightforward as you would expect: take your image, extract features using a CNN, and feed this feature vector to your LSTM."}, {"fdf8": "https://arxiv.org/abs/1411.4389"}, {"1c9f": "That\u2019s really all there is to it, and the reason it works is because (as we\u2019ve seen before) CNNs are incredibly adept at converting raw pixel data to a more meaningful representation. This saves the LSTMs the problem of parsing through the pixels to figure out what\u2019s going on in the image and allows the LSTM weights to focus on converting a vector of image features into some meaningful sequence (say, a caption). It\u2019s the same reason that using a word embedding is often preferred to a one-hot vector when feeding in words to an NLP model: the more meaningful your representation is, the easier it is to make further predictions with it."}, {"b189": "Image Captioning"}, {"3e15": "(To be confused with LCRNs!)"}, {"7c86": "So there has been a lot of really impressive work on image captioning lately, but I wanna give a special shout-out to this 2015 paper from Karpathy et al. It was one of the first ML papers I had ever read, and really got me excited about the field. And, it uses RNNs, so that\u2019s cool too."}, {"ca5d": "The idea behind image captioning is kind of self-explanatory, but I\u2019ll explain it anyway. You give the model an image, it gives you a caption. Which is kind of insane if you think about it\u200a\u2014\u200aa computer can go from pixels to interpreting what it\u2019s seeing, and from that generate real and grammatical sentences to explain what it sees. I still can\u2019t really believe stuff like this actually works, but somehow it does."}, {"3370": "The model described in this specific paper combines our old pal convolutional neural networks with our newly-discovered RNNs. Step 1 is to pass our image through a convolutional neural network and extract some features from the last fully-connected layer. This lets us convert our pixel representation of the image into something that\u2019s hopefully a bit more meaningful. We take this image feature vector and use it to initialize the hidden state of our RNN."}, {"de23": "This is where it gets cool. We feed our network a start token, and it gives us a word (more accurately, a distribution of words, which we sample to get the first word of our caption). We feed this word back as the next input, and sample another word from the output. And again, and again, and again, until we finally sample an end token and have a complete caption."}, {"0330": "It\u2019s not strictly necessary to feed the word that we sampled back to the network, but that\u2019s pretty common practice to help the network condition its output on the previous word (the hidden state is critical for this too, of course). The results from this particular paper were pretty cool, you can see some of the results here."}, {"6ec4": "Neural Machine Translation, Again"}, {"aba2": "Yes, NMTs are just that cool that I need to talk about them again."}, {"7a0e": "The Problem: With our good ol\u2019 GNMT architecture, we can train a massive model to convert from language A to language B. That\u2019s great\u200a\u2014\u200aexcept, if we support more than a hundred languages, we need to train more than 10,000 different language-pair models, each of which can take months to converge. That\u2019s no good, and it\u2019s the reason that when Google put GNMT in production, they only did so for eight language-pairs (still a monumental achievement). But\u2026what if we didn\u2019t need to train a separate model for each language pair? What if we could train one model for all the language pairs\u200a\u2014\u200aimpossible, right?"}, {"1717": "The Solution: Apparently it\u2019s not impossible, and to make things even crazier, we can use the original GNMT architecture without modification. The only real change is that we prepend a special language token to the beginning of each sequence telling it what language to translate to. (We also use one shared wordpiece model for all language, instead of one per language pair.)"}, {"90b7": "So we\u2019ve condensed tens of thousands of NMT models into a single model that is able to translate to and from any language it was trained on. The paper elaborates on the implications and benefits of this more than I will, but to summarize:"}, {"0314": "Expanding on that last point some more: the authors of the paper even found evidence of an interlingua, or an intermediate representation that is shared by multiple languages. Being able to learn an interlingua is the ideal end goal to create a fully generalized multilingual NMT: we learn an encoder/decoder to convert to/from the interlingua for each language, and we immediately know how to translate to and from that language. We aren\u2019t quite there yet, but this is a major step in that direction. Creating a larger multilingual NMT model and giving it even more data could be all it takes to achieve new state-of-the-art translation results."}, {"e74d": "So, yeah"}, {"1de9": "RNNs are pretty awesome. There are new RNN papers published literally every day and it\u2019s impossible to cover everything\u200a\u2014\u200aif you think I missed something important, definitely let me know. (From Rohan: Except Neural Turing Machines and Learning to Learn. Those are dope, we know it, and we\u2019re going to be covering them soon!)"}, {"449c": "Building a Vanilla Recurrent Neural\u00a0Network"}, {"55e8": "Let\u2019s get practical for a minute and see how we can build one of these things in practice. We\u2019ll stick with char-rnn (the single layer character level language model we talked about much earlier) with raw numpy so we can see the nitty-gritty details, but if you\u2019re using one of these in practice there are much better solutions! For out-of-the-box functional deep learning models Keras is the de facto framework that people seem to use. For more creative models and all kinds of other fancy stuff I\u2019m a fan of the newly-released PyTorch, or the \u201colder\u201d TensorFlow."}, {"5997": "I\u2019m going to walk us through this implementation line by line so we can see exactly what\u2019s going on. It\u2019s really well-commented, so feel free to peruse it on your own too."}, {"8745": "Afterwards, I challenge you to code an LSTM!"}, {"8a37": "import numpy as np"}, {"54ad": "Well, duh."}, {"a14a": "data = open(\u2018input.txt\u2019, \u2018r\u2019).read()chars = list(set(data))data_size, vocab_size = len(data), len(chars)print \u2018data has %d characters, %d unique.\u2019 % (data_size, vocab_size)char_to_ix = { ch:i for i,ch in enumerate(chars) }ix_to_char = { i:ch for i,ch in enumerate(chars) }"}, {"3f08": "We load in our data and get a list of all of the characters that appear in it. We set up two dictionaries: one mapping characters to an index, and one for the reverse. We\u2019ll use this when converting characters to/from a one-hot encoding later on."}, {"f0a4": "hidden_size = 100seq_length = 25learning_rate = 1e-1"}, {"b15a": "Typical hyperparam stuff. Our RNN layer will have a hidden size of 100, and we\u2019ll train our network on batches of 25 characters at a time. Since we\u2019ll be training our network with BPTT, we need to make sure the sequences are sufficiently short that we can unroll the network all the way and keep everything in memory. Finally, set the learning rate to\u00a0.1."}, {"a836": "Wxh = np.random.randn(hidden_size, vocab_size)*0.01 # input to hiddenWhh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hiddenWhy = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to outputbh = np.zeros((hidden_size, 1)) # hidden biasby = np.zeros((vocab_size, 1)) # output bias"}, {"7dce": "We set up our parameters\u200a\u2014\u200anote that this is just a typical RNN, no fancy LSTM cells. We have weight matrices for updating our hidden state with each input, updating our hidden state with each timestep, and producing an output (and biases for our hidden state + output). We could be doing some fancy weight initialization here, but some normally-distributed randomness is sufficient for breaking symmetry."}, {"c19b": "Now let\u2019s talk loss function. We start by computing the forward pass, then computing the backward pass, just like with any neural network."}, {"08d8": "xs, hs, ys, ps = {}, {}, {}, {}hs[-1] = np.copy(hprev)loss = 0"}, {"7017": "We start off by just setting up some variables to store our one-hot inputs, hidden states, outputs, and softmax probabilities."}, {"03ad": "for t in xrange(len(inputs)):"}, {"7bca": "Go through each timestep, and for each timestep\u2026"}, {"d87e": "xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representationxs[t][inputs[t]] = 1"}, {"0962": "Convert our input character at this timestep to a one-hot vector."}, {"ea9f": "hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state"}, {"de7c": "Update our hidden state. We saw this formula already\u200a\u2014\u200ause our Wxh and Whh matrices to update our hidden state based on the last state and our input, and add a bias."}, {"15c1": "ys[t] = np.dot(Why, hs[t]) + by"}, {"26df": "Compute our output\u2026"}, {"4308": "ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars"}, {"46cd": "\u2026and convert it to a probability distribution with a softmax."}, {"b3bb": "loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)"}, {"011a": "Accumulate the loss for this time step as the negative log of the predicted probability. Ideally, we would have a probability of 1 for the actual next character. If it is 1, the loss is 0, log(1) = 0. As the predicted probability approaches 0, the loss approaches inf, because log(0) = -inf."}, {"d366": "That\u2019s it for the forward pass (not bad, right? Boiled down, it\u2019s like six lines of code. Piece of cake)."}, {"b1db": "dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why) dbh, dby = np.zeros_like(bh), np.zeros_like(by) dhnext = np.zeros_like(hs[0])"}, {"cdb0": "Setting up some variables for our backward pass\u200a\u2014\u200athe gradients of our weight matrices, the gradients for our biases, and the gradients from the next timestep (we\u2019ll see how that works in a bit)."}, {"2fd8": "for t in reversed(xrange(len(inputs))):"}, {"14bf": "Go through our sequence in reverse as we back up the gradients."}, {"9148": "dy = np.copy(ps[t])\u00a0dy[targets[t]] -= 1 # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here"}, {"3fb3": "First, get the gradient of the output, dy. As it turns out, the gradient of the cross-entropy loss is really as copying over the distribution and subtracting 1 from the correct class."}, {"b8b8": "Remember backpropogation? When we have a weighted sum, the gradient of the weights is just the corresponding value that it is being multiplied by, because the other terms drop out and that one weight is treated as a constant. So, computing the gradient of our Why matrix is super simple: just multiply the gradient of loss w.r.t. the output (dy) by the derivative of the output w.r.t. Why (which is just the hidden state at our given timestep), and we get the derivative of the loss w.r.t. Why."}, {"247c": "dWhy += np.dot(dy, hs[t].T)"}, {"2ff0": "Like the other gradients (except dy, of course) we accumulate these gradients over all timesteps and apply them at the end."}, {"1c4f": "dby += dy"}, {"a781": "The derivative of loss w.r.t. output (dy) multiplied by the derivative of our output w.r.t. the bias (which is 1) gives us the derivative of our output w.r.t. the bias. So far so good."}, {"6890": "dh = np.dot(Why.T, dy) + dhnext # backprop into h"}, {"6dbe": "We compute dL/dh using the chain rule, and accumulate it over all timesteps (hence + dhnext). We\u2019ll need this for the next step."}, {"6d9e": "dhraw = (1\u200a\u2014\u200ahs[t] * hs[t]) * dh # backprop through tanh nonlinearity"}, {"796c": "This computes the derivative of the np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) line from earlier."}, {"7fa1": "dbh += dhraw"}, {"f74d": "Which is also our bh derivative, for the same reason that the by derivative was just dy."}, {"a0f2": "dWxh += np.dot(dhraw, xs[t].T)dWhh += np.dot(dhraw, hs[t-1].T)"}, {"cea8": "We accumulate our weight gradients."}, {"012f": "dhnext = np.dot(Whh.T, dhraw)"}, {"bb22": "And finally, store dh for this timestep so we can use it for the previous one."}, {"1ea0": "for dparam in [dWxh, dWhh, dWhy, dbh, dby]:np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients"}, {"a901": "Last but not least, a little gradient clipping so we don\u2019t get no exploding gradients."}, {"8392": "return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]"}, {"4dc5": "And then return all the gradients so we can apply an optimizer step. And that\u2019s it for the backprop code; not too bad, right?"}, {"4eaf": "def sample(h, seed_ix, n):"}, {"ed15": "This method is used for sampling a generated sequence from the network, starting with state h, first letter seed_ix, with length n."}, {"d445": "x = np.zeros((vocab_size, 1))\u00a0x[seed_ix] = 1"}, {"a8f6": "Set up our one-hot encoded input vector based on the seed character."}, {"8a82": "ixes = []"}, {"7e2b": "And an array to keep track of our sequence."}, {"3cae": "for t in xrange(n):"}, {"c8e6": "To generate each character in our sequence\u2026"}, {"18cf": "h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)"}, {"6d86": "Update our hidden state! We saw this formula in the last function, too."}, {"ce6f": "y = np.dot(Why, h) + byp = np.exp(y) / np.sum(np.exp(y))"}, {"d108": "Generate our output and run it through a softmax. Again, straight from the last function."}, {"62ae": "ix = np.random.choice(range(vocab_size), p=p.ravel())"}, {"fd39": "Sample from our output distribution using some numpy magic."}, {"cec1": "x = np.zeros((vocab_size, 1))x[ix] = 1ixes.append(ix)"}, {"e8b3": "Convert the sampled value into a one-hot encoding and append it to the array."}, {"4928": "return ixes"}, {"e074": "\u2026and of course, return the final sequence when we\u2019re done."}, {"6968": "n, p = 0, 0"}, {"2eb1": "n is the number of training iterations we\u2019ve done. p is the index into our training data for where we are now."}, {"ee12": "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)"}, {"9951": "Set up memory variables for the Adagrad algorithm (out of scope of this post, maybe next time\u200a\u2014\u200ait\u2019s just a variant on gradient descent)."}, {"ffc4": "while True:"}, {"bcde": "Training loop."}, {"a04e": "if p+seq_length+1 >= len(data) or n == 0:"}, {"3444": "This is a little check to see if we need to reset our memory because we\u2019re starting back at the beginning of our data."}, {"a0be": "hprev = np.zeros((hidden_size,1)) # reset RNN memory"}, {"038a": "\u2026and if we are, reset the memory."}, {"9c3d": "p = 0"}, {"bd23": "And reset the data pointer."}, {"0161": "inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]"}, {"70c2": "We grab a seq_length-long piece of the data as our input to the network. At each timestep, we want to predict the next character; this means that our \u201ctargets\u201d will be the next character for each input. We get a snippet of the input the same length as the input, but offset by 1, for the target."}, {"3f42": "if n % 100 == 0:sample_ix = sample(hprev, inputs[0], 200)txt = \u2018\u2019.join(ix_to_char[ix] for ix in sample_ix)print \u2018\u200a\u2014\u200a\u2014 \\n %s \\n\u200a\u2014\u200a\u2014 \u2018 % (txt, )"}, {"a7eb": "Here we just print to the terminal a sample every 100 training steps so we can see how its doing. Ideally, this will print out gibberish the first few times, before gradually printing out more and more reasonable language."}, {"e412": "loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)"}, {"538d": "Do a forward pass, backward pass, and get the gradients."}, {"9d99": "smooth_loss = smooth_loss * 0.999 + loss * 0.001"}, {"4ab1": "Adagrad stuff."}, {"3823": "if n % 100 == 0: print \u2018iter %d, loss: %f\u2019 % (n, smooth_loss) # print progress"}, {"b38c": "Keep up with progress."}, {"995f": "for param, dparam, mem in zip([Wxh, Whh, Why, bh, by], [dWxh, dWhh, dWhy, dbh, dby], [mWxh, mWhh, mWhy, mbh, mby]):mem += dparam * dparamparam += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update"}, {"9509": "More Adagrad. We should really do an article on optimization algorithms."}, {"a2fc": "p += seq_length # move data pointern += 1 # iteration counter"}, {"a889": "Annnddd finally, we update our data pointer and iteration counter."}, {"7850": "And that\u2019s it. We have an RNN. Neat-o. Reminder: your challenge is to code an LSTM\u2026 and TensorFlow doesn\u2019t count!"}, {"5a53": "Conclusion"}, {"8a00": "Wow. That was a lot. If you came in knowing nothing or very little about recurrent neural nets, you sure as hell know a lot now. And you don\u2019t just know about something cool; you know about something very important\u200a\u2014\u200asomething that can equip you to read and understand some of the most prominent and hottest recent research papers in machine learning."}, {"eec5": "Something this article didn\u2019t do so good at was making sure the calculus and derivatives were in the context of operating on vectors (because, remember, RNNs/LSTMs operate over vectors). In many cases the derivatives were in the 1-D context. It\u2019s not something you need to worry about, but you might want to look into."}, {"f6a4": "We\u2019re finally at the point where we can focus our energies on this blog towards cooler stuff including hot research papers like Neural Turing Machines or Learning to Learn, case studies eg. AlphaGo, other parts of machine learning and artificial intelligence (I\u2019m, Rohan, personally looking forward to optimization!), or different algorithms like GANs. There\u2019s very little compulsory content or \u201cgroundwork\u201d we need to cover anymore. So, now, we\u2019re officially onto the cool stuff."}, {"4f33": "That\u2019s right. A Year Of AI is officially\u2026 cool."}], "content": "Rohan & Lenny #3: Recurrent Neural Networks &\u00a0LSTMs The ultimate guide to machine learning\u2019s favorite\u00a0child. This is the third group (Lenny and Rohan) entry in our journey to extend our knowledge of artificial intelligence and convey that knowledge in a simple, fun, and accessible manner. Learn more about our motives in this introduction post. It seems like most of our posts on this blog start with \u201cWe\u2019re back!\u201d, so\u2026 you know the drill. It\u2019s been a while since our last post\u200a\u2014\u200ajust over 5 months\u200a\u2014\u200abut it certainly doesn\u2019t feel that way. Whether our articles are more spaced out than we\u2019d like them to be, well, we haven\u2019t actually discussed that yet. But I, Rohan, would definitely like to get into a more frequent routine. Since November, we\u2019ve been grinding on school (basically, getting it over and done with), banging out Contra v2, and lazing around more than we should. End of senior year is a fun time. It\u2019s 2017. We started A Year Of AI in 2016. Last year. Don\u2019t panic, though. If you\u2019ve read our letter, you\u2019ll know that, despite our name and inception date, we\u2019re not going anywhere anytime soon. There\u2019s a good chance we\u2019ll move off Medium, but we\u2019re still both obsessed with AI and writing these posts to hopefully make other people obsessed, as well. I wrote the first article on this blog just over a year ago, and mentioned that my goal for the year was to be accepted into Stanford University as an undergrad student. A few months ago, I achieved this goal. At Stanford, I\u2019ll probably be studying Symbolic Systems, which is a program that explores both the humanities and STEM to inform an understanding of artificial intelligence and the nature of minds. Needless to say, A Year of AI will continue to document the new things I learn \ud83d\ude00. Anyways, you can find plenty of articles on recurrent neural networks (RNNs) online. My favorite one, personally, is from Andrej Karpathy\u2019s blog. I read it about 1.5 years ago when I was learning about RNNs. We definitely think there\u2019s space to simplify the topic even more, though. As usual, that\u2019s our aim for the article\u200a\u2014\u200ato teach you RNNs in a fun, simple manner. We\u2019re also importantly doing this for completion purposes; we want people to hop onto A Year of AI and be able to work their way up all the way from logistic regression to neural machine translation (don\u2019t worry, you\u2019ll find out what means soon enough), and thus recurrent neural networks is a vital addition. After this, we want to look at and summarize/simplify a bunch of new super interesting research papers, and for most of them RNNs are a key ingredient. Finally, we think this article contains so much meat and ties together content unlike any other RNN tutorial on the interwebs. Before we get started, you should try to familiarize yourself with \u201cvanilla\u201d neural networks. If you need a refresher, check out our neural networks and backpropogation mega-post from earlier this year. This is so you know the basics of machine learning, linear algebra, neural network architecture, cost functions, optimization methods, training/test sets, activation functions/what they do, softmax, etc. Reading our article on convolutional neural networks may also make you more comfortable entering this post, especially because we often reference CNNs. Checking out this article I wrote on vanishing gradients will help later on, as well. Rule of thumb: the more you know, the better! Table of\u00a0Contents I can\u2019t link to each section, but here\u2019s what we cover in this article (save the intro and conclusion): What can RNNs\u00a0do? There are a number of very important tasks that ANNs and CNNs cannot solve, that RNNs are used for instead. Tasks like: image captioning, language translation, sentiment classification, predictive typing, video classification, natural language processing, speech recognition, and a lot more interesting things that have been presented in recent research papers (for example\u2026 learning to learn by gradient descent by gradient descent!). Image captioning, taken from CS231n slides: http://cs231n.stanford.edu/slides/2016/winter1516_lecture10.pdf RNNs are very powerful. Y\u2019know how regular neural networks have been proved to be \u201cuniversal function approximators\u201d\u00a0? If you didn\u2019t: In the mathematical theory of artificial neural networks, the universal approximation theorem states that a feed-forward network with a single hidden layer containing a finite number of neurons can approximate continuous functions on compact subsets of R^n, under mild assumptions on the activation function. That\u2019s pretty confusing. Basically, what this states is that an artificial neural network can compute any function. Even if someone gives you an extremely wiggly, complex looking function, it\u2019s guaranteed that there exists a neural network that can produce (or at least extremely closely approximate) it. The proof itself is very complex, but this is a brilliant article offering a visual approach as to why it\u2019s true. So, that\u2019s great. ANNs are universal function approximators. RNNs take it a step further, though; they can compute/describe programs. In fact, some RNNs with proper weights and architecture qualify as Turing Complete: A Turing Complete system means a system in which a program can be written that will find an answer (although with no guarantees regarding runtime or memory). So, if somebody says \u201cmy new thing is Turing Complete\u201d that means in principle (although often not in practice) it could be used to solve any computation problem. \u2014 http://stackoverflow.com/a/7320/1260708 That\u2019s cool, isn\u2019t it? Now, this is all theoretical, and in practice means less than you think, so don\u2019t get too hyped. Hopefully, though, this gives some more insight into why RNNs are super important for future developments in machine learning\u200a\u2014\u200aand why you should read on. At this point, if you weren\u2019t previously hooked on learning what the heck these things are, you should be now. (If you still aren\u2019t, just bare with me. Things will get spicy soon.) So, let\u2019s dive in. Why? We took a bit of a detour to talk about how great RNNs are, but haven\u2019t focused on why ANNs can\u2019t perform well in the tasks that RNNs can. Why do we need another neural network model? Why do we need recurrent neural networks when we already have the beloved ANNs (and CNNs) in all their glory? It boils down to a few things: Let\u2019s address the first three points individually. The first issue refers to the fact that ANNs have a fixed input size and a fixed output size. ANNs have an elaborate list of hyperparameters, and this notably includes the number of neurons in the input layer and output layer. But what if we wanted input data and/or output data of variable size, instead of something that needs to have its size as a preset constant? RNNs allow us to do that. In this aspect, they offer more flexibility than ANNs. We might choose this architecture for our ANN, with 4 inputs and 1 output. But that\u2019s it\u200a\u2014\u200awe can\u2019t input a vector with 5 values, for example. https://qph.ec.quoracdn.net/main-qimg-050d12c5da82f4f97fdd942d7777b8e4. I\u2019ll give you a couple examples of why this matters. It\u2019s unclear how we could use an ANN by itself to perform a task like image captioning, because the network would need to output a sentence\u200a\u2014\u200aa list of words in a specific order\u200a\u2014\u200awhich is a sequence. It would be a sequence of vectors, because each word would need to be represented numerically. In machine learning and data science, we represent words numerically as vectors; these are called word embeddings. An ANN can only output a single word/label, like in image classification where we treat the output as the label with the highest value in the final vector that is a softmax probability distribution over all classes. The only way to make sentences work with ANNs would be to have billions of output neurons that each map to a single possible sentence in the permutation of all [sensible] sentences that can be formed by the vocabulary we have. And that doesn\u2019t sound like a good idea. A reminder of what the output of an ANN looks like\u200a\u2014\u200aa probability distribution over classes\u200a\u2014\u200aand how we convert that into a single final result (one-hot encoding): by taking the label with the greatest probability and making it 1, with the rest\u00a00. Wow, that was a lot of words. Nevertheless, I hope it\u2019s clear that, with ANNs, there\u2019s no feasible way to output a sequence. Now, what about inputting a sequence into an ANN? In other words, \u201ctemporal\u201d data: data that varies over time, and is thus a sequence. Take the example of sentiment classification where we input a sentence (sequence of words = sequence of vectors = sequence of set of values where each value goes into an individual neuron) and want to output its sentiment: positive or negative. The output part seems easy, because it\u2019s just one neuron that\u2019s either rounded to 1 (positive) or 0 (negative). And, for the input, you might be thinking: couldn\u2019t we input each \u201cset of values\u201d separately? Input the first word, wait for the neural net to fully feed forward and produce an output, then input the next word, etc. etc. Let\u2019s take the case of this utterly false, and most certainly negative sentence, to evaluate: This is just an alternative fact, believe me! Lenny is actually a great coder. The best I know of. The\u00a0best. We\u2019d input \u201cLenny\u201d first, then \u201cKhazan\u201d, then \u201cis\u201d, etc. But, at each feedforward iteration, the output would be completely useless. Why? Because the output would be dependent on only that word. We\u2019d be finding the sentiment of a single word, which is useless, because we want the sentiment of the entire sentence. Sentiment analysis only makes sense when all the words come together, dependent on each other, to form a sentence. Think of it this way\u200a\u2014\u200athis means you\u2019re essentially running a neural network a bunch of times, just with new data at each separate iteration. Those run-throughs aren\u2019t linked in any way; they\u2019re independent. Once you feedforward and fully run the neural network, it forgets everything it just did. This sentence only makes sense and can only be interpretable because it\u2019s a collection of words put together in a specific order to form meaning. The relevance of each word is dependent on the words that precede it: the context. This is why RNNs are being used heavily in NLP; they retain context by having memory. ANNs have no memory. I like this quote from another article on RNNs: Humans don\u2019t start their thinking from scratch every second. As you read this essay, you understand each word based on your understanding of previous words. You don\u2019t throw everything away and start thinking from scratch again. Your thoughts have persistence. \u2014 http://colah.github.io/posts/2015\u201308-Understanding-LSTMs/ (Furthermore, take the case where we had sequential data in both the input and the output. Translating one language to another is a good example of this. Clearly, ANNs aren\u2019t the answer.) RNNs don\u2019t just need memory; they need long term memory. Let\u2019s take the example of predictive typing. Let\u2019s say we typed the following sentence in an SMS message to 911, and the operating system needs to fill in the blank: The face of a criminal? Here, if the RNN wasn\u2019t able to look back much (ie. before \u201cshould\u201d), then many different options could arise: Lenny in the military? Make it into a TV show! I\u2019d watch\u00a0it. The word \u201csent\u201d would indicate to the RNN that a location needs to be outputted. However, if the RNN was able to retain information from all the way back, such as the word \u201ccriminal\u201d, then it would be much more confident that: The probability of outputting \u201cjail\u201d drastically increases when it sees the word \u201ccriminal\u201d is present. That\u2019s why context matters, be it predictive typing, image captioning, machine translation, etc. The output or outputs of a recurrent neural network will always be functionally dependent on (meaning, a function of) information from the very beginning, but how much it chooses to \u201cforget\u201d or \u201cretain\u201d (that is, varying degrees of influence from earlier information) depends on the weights that it learns from the training data. As it turns out, RNNs\u200a\u2014\u200aespecially deep ones\u200a\u2014\u200aare rarely good at retaining much information, due to an issue called the vanishing gradient problem. That\u2019s where we turn to other variants of RNNs such as LSTMs and GRUs. But, more on that later. To address the third point, one more constraint with ANNs is that they have a fixed number of computation/processing steps (because, once again, the number of hidden layers is a hyperparameter). With RNNs, we can have much more dynamic processing since we operate over vectors. Each neuron in an RNN is almost like an entire layer in an ANN; this will make more sense as we bring up an illustration for you. Exciting stuff. Show me. OK, that\u2019s enough teasing. Three sections into the article, and you\u2019re yet to see what an RNN looks like, or appreciate how it really works. Everything comes in due time, though! The first thing I\u2019m going to do is show you what a normal ANN diagram looks like: Each neuron stores a single scalar value. Thus, each layer can be considered a vector. Now I\u2019m going to show you what this ANN looks like in our RNN visual notation: The two diagrams above represent the same thing. The latter, obviously, looks more succinct than the former. That\u2019s because, with our RNN visual notation, each neuron (inputs, hidden(s), and outputs) contains a vector of information. The term \u201ccell\u201d is also used, and is interchangeable with neuron. (I\u2019ll use the latter instead of the former.) Red is the input neuron, blue is the hidden neuron, and green is the output neuron. Therefore, an entire ANN layer is encapsulated into one neuron with our RNN illustration. All operations in RNNs, like the mapping from one neuron\u2019s state to another, are over entire vectors, compared to individual scalars that are summed up with ANNs. Let\u2019s flip it the other way: This is in fact a type of recurrent neural network\u200a\u2014\u200aa one to one recurrent net, because it maps one input to one output. A one to one recurrent net is equivalent to an artificial neural net. We can have a one to many recurrent net, where one input is mapped to multiple outputs. An example of this would be image captioning\u200a\u2014\u200athe input would be the image in some processed form (usually the result of a CNN analyzing the image), and the output would be a sequence of words. Such an RNN may look like this: Changed the shades of the green nodes\u2026 hope that\u2019s\u00a0OK! This may be confusing at first, so I\u2019m going to make sure I walk slowly through it. On the x-axis we have time, and on the y-axis we have depth/layers: When I refer to \u201ctime\u201d on the x-axis, I\u2019m referring to the order at which these operations occur. Time could also be literal for temporal data, where the input is a sequence. When I say \u201cdepth\u201d on the y-axis, I\u2019m referring to the mapping from the input layer, to the hidden layer(s), to the output layer, where layer number and thus depth increases. It may look like we have seven neurons now, but we still have three: one input neuron, one hidden neuron, and one output neuron. The difference is that these neurons now experience multiple \u201ctimesteps\u201d where they take on different values, which are, again, vectors. The input neuron in our example above doesn\u2019t, because it\u2019s not representing sequential data (one to many), but for other architectures it could. The hidden neuron will take on the vector value h_1 first, then h_2, and finally h_3. At each timestep, the hidden neuron\u2019s vector h_t is a function of the vector at the previous timestep h_t-1, except for h_1 which is dependent only on the input x_1. In the diagram above, each hidden vector then gives rise to an output y_t, and this is how we map one input to multiple outputs. You can visualize these functional dependencies with the arrows, which illustrates flow of information in the network. As we progress on the x-axis, the current timestep increases. As we progress on the y-axis, the neuron in question changes. Each point on this graph thus represents one neuron\u200a\u2014\u200abe it input, hidden, or output\u200a\u2014\u200aat some timestep, being fed information from a neuron (be it itself or another) at the previous timestep. The RNN would execute like so: You could compute y_t either immediately after h_t has been computed, or, like above, compute all outputs once all hidden states have been computed. I\u2019m not entirely sure which is more common in practice. This allows for more complex and interesting networks than ANNs because we can have as many timesteps as we want. The value of the output neuron at each timestep represents a word in the sentence, in the order the sentence will be constructed. The caption this RNN produces is hence 3 words long. (It\u2019s actually 2, because the RNN would need to output a period or <END> marker at the final timestep, but we\u2019ll get into that later.) In case you don\u2019t understand yet exactly why RNNs work, I\u2019ll walk through how these functional dependencies come to fruition when you apply it to a one to many scenario such as image captioning. Lenny and I on student scholarship at WWDC 2013. Good\u00a0times! When you combine an RNN and CNN, you\u200a\u2014\u200ain practice\u200a\u2014\u200aget an \u201cLCRN\u201d. The architecture for LCRNs are more complex than what I\u2019m going to present in the next paragraph; rather, I\u2019m going to simplify it to convey my point. We\u2019ll actually get fully into how they work later. Imagine an RNN tries to caption this image. An accurate result might be: Two people happily posing for a photo inside a building. The input to the RNN would be the output of a CNN that processes this image. (However, to be pedantic, it would be the output of the CNN without a classification/softmax layer\u200a\u2014\u200athat is, pulled from the final fully connected layer.) The CNN might pick up on the fact that there are two primary human face-like objects present in the image, which, paired with what the RNN has learned via training, may induce the first hidden state\u00b9 of the recurrent neural network to be one where the most likely candidate word is \u201ctwo\u201d. Pro-tip\u00b9: The term \u201chidden state\u201d refers to the vector of a hidden neuron at a given timestep. \u201cFirst hidden state\u201d refers to the hidden state at timestep 1. The first output, which represents the word \u201ctwo\u201d, was functionally dependent on the first hidden state, which in itself was a function of the input to the RNN. Thus, \u201ctwo\u201d was ultimately determined from the information that the CNN gave us and the experience/weights of the RNN. Now, the second word, \u201cpeople\u201d, is functionally dependent on the second hidden state. However, note that the second hidden state is just a function of the first hidden state. This means that the word \u201cpeople\u201d was the most likely candidate given the hidden state where \u201ctwo\u201d was likely. In other words, the RNN recognized that, given the word \u201ctwo\u201d, the word \u201cpeople\u201d should be next, based on the RNN\u2019s experience from training and the initial image [analysis] we inputted. The same will occur for every following word; the nth word will be based on the nth hidden state, which, ultimately, is a function of every hidden state before it, and thus could be interpreted purely as an extremely complex and layered function of the input. The weights do the heavy lifting by making sense of all this information and deducing an output from it. To put it bluntly, you can boil down what the RNN is \u201cthinking\u201d to this: Based on what I\u2019ve seen from the input, based on the current timestep I\u2019m at, and based on what I know from all my training, I need to output: \u201cx\u201d. Thus, each outputted word is dependent on the words before it, all the way back to the input image data. However, this relationship is indirect. It\u2019s indirect because the outputs are only dependent on the hidden states, not on each other (ie. the RNN doesn\u2019t deduce \u201cpeople\u201d from \u201ctwo\u201d, it deduces \u201cpeople\u201d, partly, from the information\u200a\u2014\u200athe hidden state\u200a\u2014\u200athat gave rise to \u201ctwo\u201d). In LCRNs, though, this is explicit instead of implicit; we \u201csample\u201d the output of one timestep by taking it and literally feeding it back as input into the next timestep. In a sense, LCRNs can hence be interpreted as having many to many architecture. The exact quantitative relationships depend on the RNN\u2019s weights. But, generally, this is the concept of memory in play. Creating a coherent sentence as we go along is only really possible if we can recall what we said before. And RNNs are able to do exactly that; they remember what they said before and figure out, based on their image captioning expertise, what from this is useful to continue accurately speaking. Yep, I went to France for a holiday. And I actually learned to speak some <wait, shit, what was the language again? oh yea, \u201cFrance\u201d\u2026> French! Obviously, an RNN needs to be trained and have proper weights for this to all function properly. RNNs aren\u2019t magic; they only work because trained networks identified and learned patterns in data during training time that they now look for during prediction. Perhaps this was a bit over-explaining on my part, but hopefully I nailed down some important and core ideas about how RNNs function. So far we\u2019ve looked at one to one and one to many recurrent networks. We can also have many to one: With many to one (and many to many), the input is in the form of a sequence, and so the hidden states are functionally dependent on both the input at that timestep and the previous hidden state. This is different to one to many, where the hidden state after h_1 is only dependent on the previous hidden state. That\u2019s why, in the image above, the second hidden state has two arrows directed at it. Only one output exists in many to one architecture. An example application is sentiment classification, where the input is a sentence (sequence of words) and the output is a probability indicating that the inputted sentence was positive. The final type of recurrent net is many to many, where both the input and output are sequential: A use case would be machine translation where a sequence of words in one language needs to be translated to a sequence of words in another. We can also go deeper and have multiple hidden layers, and/or a greater number of timesteps: We\u2019re getting deeper and\u00a0deeper! Really, this could be considered as multiple RNNs. Technically, you can consider each \u201chidden layer\u201d as an RNN itself, given each neuron operates on vectors and updates through time; in ANN context, that volume of operations would be considered an entire network. So this is like stacking RNNs on top of each other. However, in this article I\u2019ll refer to it as multiple hidden layers; different papers and lecturers may take different approaches. When we have many timesteps (usually hundreds) and multiple hidden layers, the architecture of the network becomes much more complex and interesting. One feature of this RNN, in particular, is that all the outputs, including the first, depend on not just the input up to that timestep, but all of the inputs. (You can see this because the green neuron is only introduced after the final input timestep.) If this RNN was to translate English to Chinese, the first word of translated Chinese isn\u2019t just dependent on the first word of the inputted English; it\u2019s dependent on the entire sentence. One way to demonstrate why this matters is to use Google Translate: One of my favorite Green Day lyrics, from the song \u201cFashion Victim\u201d on WARNING:. Side-note: Based on my experience with Google Translate in Chinese class over the last 8 years, this translation is probably\u00a0off. Now I\u2019ll input \u201cHe\u2019s a victim\u201d and \u201cof his own time\u201d separately. You\u2019ll notice that when you join the two translated outputs, this won\u2019t be equal to the corresponding phrase in the first translation: What happens if we break up the English into different parts, translate, and join together the translated Chinese\u00a0parts? They\u2019re not\u00a0equal. What gives? Well, the way sentences are constructed in languages can differ in varying scenarios. Some words in English may also map to multiple different words in Chinese, depending on how it\u2019s used. It all depends on the context and the entire sentence as a whole\u200a\u2014\u200athe meaning you\u2019re trying to convey. This is the exact approach a human translator would take. Another type of many to many architecture exists where each neuron has a state at every timestep, in a \u201csynchronized\u201d fashion. Here, each output is only dependent on the inputs that were fed in during or before it. Because of this, synchronized many to many probably wouldn\u2019t be suitable for translation. An application for this could be video classification where each frame needs to be mapped to some sort of class or label. Interesting note\u200a\u2014\u200aan RNN is better at this task than CNNs are because what\u2019s going on in a scene is much easier to understand if you\u2019ve watched the video up to that point and thus can contextualize it. That\u2019s what humans do! Quick note: we can \u201cwrap\u201d the RNN into a much more succinct form, where we collapse the depth and time properties, like so: This notation demonstrates that RNNs take input, process that input through multiple timesteps and hidden layers, and produce output. The arrow both leaving and entering the RNN conveys that an RNN hidden state is functionally dependent on the hidden state at the preceding timestep; it\u2019s sort of like a loop that feeds itself. When you ever read about \u201cunrolling\u201d an RNN into a feedforward network that looks like it\u2019s in the same collapsed format as the diagram above, this means we expand it to show all timesteps and hidden layers like we did before. Another quick note: when somebody or a research paper mentions that they are using \u201c512 RNN units\u201d, this translates to: \u201c1 RNN neuron that outputs a 512-wide vector\u201d; that is, a vector with 512 values. At first, I thought this meant that maybe at each timestep there were 512 separate neurons somehow working in conjunction, but nope, it\u2019s luckily much simpler than that\u2026 albeit strangely worded. Furthermore, one \u201cRNN unit\u201d usually refers to an RNN with one hidden layer; thus, instead of defining RNN as something that is multilayer inherently, we often see people use the phrase like: \u201cstacking RNNs on top of each other\u201d. Each RNN will have its on weights, but connecting them gives rise to an overarching multilayer RNN. In this article, we treat recurrent neural networks as a model that can have variable timesteps t and fixed layers \u2113, just make sure you understand that this is not always the case. Our formalism, especially for weights, will slightly differ. Formalism So, now, let\u2019s walk through the formal mathematical notation involved in RNNs. If an input or output neuron has a value at timestep t, we denote the vector as: For the hidden neurons it\u2019s a bit different; since we can have multiple hidden layers, we denote the hidden state vector at timestep t and hidden layer \u2113 as: The input is obviously some preset values that we know. The outputs and hidden states are not; they are calculated. Let\u2019s start with hidden states. First, we\u2019ll revisit the most complex recurrent net we came across earlier\u200a\u2014\u200athe many to many architecture: Many to many, non-synchronized. This RNN has: sequential input, sequential output, multiple timesteps, and multiple hidden layers. The formula we derive for this RNN should generalize for all others. First, let\u2019s list out the possible functional dependencies for a given hidden state, based on the arrows and flow of information in the diagram: A hidden state can have two functional dependencies at max. Just by looking at the diagram, the only impossible combination is to be dependent on both the input and a hidden state at the current timestep but previous layer. This is because the only hidden states that are dependent on input exist in the first hidden layer, where no such previous layer exists. If this is all difficult to follow, make sure once again to look at and trace back the arrows in the RNN that illustrate flow of information throughout the network. Because of the impossible combination, we define two separate equations: an equation for the hidden state at hidden layer 1, and for layers after 1. This probably looks a bit confusing; let me break it down for you. The function \u0192w computes the numeric hidden state vector for timestep t and layer \u2113; it contains the \u201cactivation function\u201d you\u2019re used to hearing about with ANNs. W are the weights of the recurrent net, and thus \u0192 is conditioned on W. We haven\u2019t exactly defined \u0192 just yet, but what\u2019s important to note is the two parameters it takes. Once you do, this notation simply states what we have stated before in plain English: Where \u2113 = 1, the hidden state at time t and layer \u2113 is a function of the hidden state vector at time t-1 and layer \u2113 as well as the input vector at time t. Where \u2113 > 1, this hidden state is a function of the hidden state vector at time t-1 and layer \u2113 as well as the hidden state vector at time t, layer \u2113-1. You might notice that we have a couple issues: Our respective solutions follow: If these are 0, then the invalid functional dependency stops existing, and our formal notation still holds up. We actually have five different types of weight matrices: Pro-tip: The indices for each weight matrix tell you what they are used for in the recurrent net. W_xh maps an input vector x to a hidden state vector h. W_hht maps a hidden state vector h to another hidden state vector h along the time axis, ie. from h_t-1 to h_t. On the other hand, W_hhd maps a hidden state vector h to another hidden state vector h along the depth axis, ie. from h^(\u2113-1)_t to h^\u2113_t. W_hy maps a hidden state vector h to an output vector y. Like with ANNs, we also learn and add a constant bias vector, denoted b_h, that can vertically shift what we pass to the activation function. We can also shift our outputs with b_y. More about bias units here. For both b_h and W_hht/W_hhd, we actually have multiple weight matrices depending on the value of \u2113, as indicated by the superscript. This is because each hidden layer can have a different set of weights (the network would be extremely uninteresting if this wasn\u2019t the case), including the bias vector. However, inside a single hidden layer, all timesteps share the same weight matrix. This is important because the number of timesteps is a variable; we may train on sequences with up to 20 values, but in practice output sequences with up to 30 values\u200a\u2014\u200a10 extra timesteps. If each timestep had an independent weight to learn, those last 10 timesteps wouldn\u2019t have anything to use. W_hy is just one matrix because only the final layer gives rise to the outputs denoted y. At the final hidden layer \u2113, we could suggest that W_hhd will not exist because W_hy will be in its place. Now we\u2019ll define the function \u0192w: The function is very similar to the ANN hidden function you\u2019ve seen before; it applies the correct weights to the corresponding parameters, adds the bias, and passes this weighted sum through an activation or \u201csquashing\u201d function to introduce non-linearities. The key difference, though, is that this is not a weighted sum but rather a weighted sum vector; any W \u22c5 h, along with the bias, will have the dimensions of a vector. The tanh function will thus simply output a vector where each value is the tanh of what it was in the inputted vector (sort of like an element-wise tanh). Remember, this contrasts ANNs because RNNs operate over vectors versus scalars. If you\u2019ve followed our blog so far, you most likely know about two activation functions: sigmoid and ReLU. tanh is another such function. We mostly use the tanh function with RNNs. This is, I think, mostly because of their role in LSTMs (a variant of RNNs that are used more than RNNs\u200a\u2014\u200amore on that later), the fact that they produce gradients with a greater range, and that their second derivative don\u2019t die off as quickly. Similar to sigmoid, the tanh function has two horizontal asymptotes and a smooth S-shape. The main difference is that the tanh function asymptotes at y = -1 instead of y = 0, intercepting the y-axis at y = 0 instead of y = 0.5. Thus, the tanh function has a greater range than the sigmoid. If interested, the tanh equation follows (though I won\u2019t walk you through it): The final equation is mapping a hidden state to an output. This is one such possible equation. Depending on the context, we might also remove the bias vector, apply a non-linearity like sigmoid/softmax (for example if the output needs to be a probability distribution), etc. And that\u2019s how we express recurrent nets, mathematically! Quick note: Notation may and will differ between various lectures, research paper, articles, etc. For example\u200a\u2014\u200asome research papers may start indexing at 0 instead of 1. More drastically, most RNN notation is much more general than mine to promote simplicity, ie. doesn\u2019t cover edge cases like I did or obfuscates certain indices like \u2113 with hidden to hidden weight matrices. So, just keep note that specifics don\u2019t always transfer over and avoid being confused by this. The reason I was meticulous about notation in this article is that I wanted to ensure you understood exactly how RNNs work, fueled by my frustration with the very same problem ~1.5 years ago. An example?\u00a0Okay! Let\u2019s take a look at a quick example of an RNN in action. I\u2019m going to adapt a super dumbed down one from Andrej Karpathy\u2019s Stanford CS231n RNN lecture, where a one to many \u201ccharacter level language model\u201d single layer recurrent neural network needs to output \u201chello\u201d. We\u2019ll kick it of by giving the RNN the letter \u201ch\u201d\u00a0, such that it needs to complete the word by outputting the other four letters. Sidenote: this model nicknamed \u201cchar-rnn\u201d\u200a\u2014\u200aremember it for later, where we get to code our own! The neural network has the vocabulary: h, e, l\u00a0, o. That is, it only knows these four characters; exactly enough to produce the word \u201chello\u201d. We will input the first character, \u201ch\u201d, and from there expect the output at the following timesteps to be: \u201ce\u201d, \u201cl\u201d, \u201cl\u201d, and \u201co\u201d respectively, to form: hello We can represent input and output via one hot encoding, where each character is a vector with a 1 at the corresponding character position and otherwise all 0s. For example, since our vocabulary is [h, e, l, o], we can represent characters using a vector with four values, where a 1 in the first, second, third, and fourth position would represent \u201ch\u201d, \u201ce\u201d, \u201cl\u201d, and \u201co\u201d respectively. This is called \u201cone-hot encoding\u201d, because only one of the values in the vector is equal to 1 and thus on (or\u00a0\u201chot\u201d). This is what we\u2019d expect with a trained RNN: As you can see, we input the first letter and the word is completed. We don\u2019t know exactly what the hidden states will be\u200a\u2014\u200athat\u2019s why they\u2019re hidden! One interesting technique would be to sample the output at each timestep and feed it into the next as input: When we \u201csample\u201d from a distribution, we select a random character probabilistically following the distribution. For example, in the diagram above, the character with the highest likeliness is \u201ce\u201d at the first timestep\u2019s output. Let\u2019s say this likeliness is, concretely, 0.9. Now, when we sample into the next timestep\u2019s input, there\u2019s a 90% chance we select \u201ce\u201d; most of the time we will pick the most likely character, but not every time. This adds a level of randomness so you don\u2019t end up in a loop where you keep sampling the same letter or sequence of letters over and over again. As mentioned earlier, this is used pretty heavily with LCRNs. It\u2019s even more effective than only relying on the memory of the RNN to output the correct letter at the future timesteps. In a sense, this makes the recurrent net many to many. (Though, not really, because we still only have one preset input.) However, to be clear, this does not mean that the RNN can only rely on these sampled inputs. For example, at timestep 3 the input is \u201cl\u201d and the expected output is also \u201cl\u201d. However, at timestep 4, the input is again \u201cl\u201d but the output is now \u201co\u201d, to complete the word. Memory is still needed to make a distinction like this. In numerical form, it would look something like this: Of course, we won\u2019t get a one-hot vector output during prediction mode; rather, we will get a probability distribution over each letter (so we\u2019d apply softmax to the output), and will sample from this distribution to get a single character output. Each hidden state would contain a similar sort of vector, though not necessarily something we could interpret like we can for the output. The RNN is saying: given \u201ch\u201d, \u201ce\u201d is most likely to be the next character. Given \u201che\u201d, \u201cl\u201d is the next likely character. With \u201chel\u201d, \u201cl\u201d should be next, and with \u201chell\u201d, the final character should be \u201co\u201d. But, if the neural network wasn\u2019t trained on the word \u201chello\u201d, and thus didn\u2019t have optimal weights (ie. just randomly initialized weights), then we\u2019d have garble like \u201chleol\u201d coming out. One more important thing to note: start and end tokens. They signify when input begins and when output ends. For example, when the final character is outputted (\u201co\u201d), we can sample this back as input and expect that the \u201c<END>\u201d token (however we choose to represent it\u200a\u2014\u200acould also use a period) will be outputted at the next timestep; this is the RNN telling us that it has completed the word and its processing as a whole. The use case isn\u2019t as obvious in this fabricated example, because we know when \u201chello\u201d has been completed, but consider a real-life scenario where we don\u2019t: image captioning. In image captioning, the caption could be 1, 2, 3, or n words long, given a reasonable upper limit of n. The end token tells us when the caption has been completed, so we can halt the RNN and complete the prediction loop (which would keep going forever if we were using while or stop after the upper limit/max possible preset constant value of n is reached). Start tokens are more used for generating content from complete scratch. For example, imagine an RNN read and learned from a bunch of Shakespeare. (This is an actual funny application of character level language models that Karpathy implemented, and we\u2019ll see it in action on a later section.) Now, based on what the RNN learned, we want it to create a brand new Shakespearean sonnet! Feeding in a \u201c<START>\u201d token enables it to kick this process off and begin writing without us giving the network some arbitrary pre-determined initial word or character. I\u2019ve also noticed that another potential use case of start tokens is when we have some other sort of initial input, like CNN produced image data with image captioning, that doesn\u2019t \u201cfit\u201d what we\u2019ll normally use for input at timesteps after t=1 (the word outputted at the previous timestep via sampling). As a result, we feed this data directly to the first hidden state and set the input as \u201c<START>\u201d instead. Now, just to be clear, the RNN doesn\u2019t magically output these end tokens and recognize the start tokens. We have to add them, along with start tokens, to the training data and vocabulary such that they can be outputted by the recurrent net during prediction time. This is how we can get RNNs to \u201cwrite\u201d! More on some examples of text RNNs have actually generated, Shakespeare most certainly included, in a later section. Training (or, why vanilla RNNs\u00a0suck.) For a recurrent net to be useful, it needs to learn proper weights via training. That\u2019s no surprise. Recall this snippet from earlier: But, if the neural network wasn\u2019t trained on the word \u201chello\u201d, and thus didn\u2019t have optimal weights (ie. just randomly initialized weights), then we\u2019d have garble like \u201chleol\u201d coming out. This is, of course, because we initialize the W weights randomly at first, so random stuff will come out. But, through multiple iterations of training with a first-order optimization algorithm like gradient descent, we perturb the weights such that the probability of each correct character being outputted at their respective timestep increases. The actual output would be \u201chello\u201d in one-hot encoding form, and we\u2019d compute the discrepancy between this output and what the recurrent net predicts (we\u2019d get the error at each timestep and then add this up) as the total error to then calculate the gradient/update value. So, each output contributes to the error somehow. If the error is an addition of the outputs, then, if we had something like Y outputs, we\u2019d need to backpropagate them individually and add these up. This is because derivatives are distributed evenly when we\u2019re differentiating a sum: For any arbitrary weight\u00a0W. But, you should know that, with artificial neural networks, calculating these gradients isn\u2019t that easy. We have so many weights contributing to the output, and thus need to figure out exactly how much these weights contribute, and by how much we modify them to decrease overall error. To do this, we use the backpropagation algorithm; this algorithm propagates the error between the predicted output of a recurrent net and the actual output in the dataset all the way back to the beginning of the network. Using the chain rule from differential calculus, backprop helps us calculate the gradients of the output error w.r.t. each individual weight (sort of like the error of each individual weight). Once we have those gradients, we have to use an optimization algorithm to calculate the update values and make the updates. We can use the vanilla gradient descent algorithm to do this, but there are many other possible, better variants as well; learn about them by reading this article, if you want. (I think we\u2019re long overdue for our own mega-post on optimization!) Backpropagation with RNNs is called \u201cBackpropagation Through Time\u201d (short for BPTT), since it operates on sequences in time. But don\u2019t be fooled\u200a\u2014\u200athere\u2019s not much difference between normal backprop and BPTT; when it comes down to it, BPTT is just backprop, but on RNNs! Remember that when you \u201cunroll\u201d an RNN, it essentially becomes a feedforward network; not an ANN, but a feedforward network in the sense that we can visualize where all the information is flowing and observe the activations at each neuron and timestep, all the way from the input to the final output. Like ANNs, RNNs have functional dependencies that link the entire network together; it\u2019s just that RNNs operate over vectors instead (yay for matrix calculus?) and extend in depth as well as time. There\u2019s more work to do to compute the gradients, but it\u2019s no surprise that backprop works pretty much the same way for recurrent nets that it would for normal ones. Because of this, I\u2019m not going to walk through all the math and show the derivatives etc. Read our backprop mega-post for all that jazz. One thing to note is that, since we have multiple timesteps in our RNN, each timestep in a single layer will want to change the weight in a different way and have different gradients. However, remember that each hidden layer uses only one weight matrix because the number of timesteps is a variable. Thus, we just average or sum the weight updates between these timesteps and apply this as an update to the W_hh for that entire layer. Also, a general practice is to train on shorter sequences first and then gradually increase sequence size as we train on more and more data. Now, if you haven\u2019t already, make sure to read this article that I wrote on vanishing and exploding gradients before proceeding: Rohan #4: The vanishing gradient problemOh no\u200a\u2014\u200aan obstacle to deep learning!ayearofai.com You may be thinking: how does this issue apply to RNNs? Well, RNNs are very deep models; on top of often having multiple hidden layers, each hidden layer in practice can have hundreds of timesteps. That\u2019s like an ANN with hundreds of entire hidden layers! That\u2019s deep. (Well, it\u2019s more long because we\u2019re dealing with the time axis here, but you know what I mean.) tanh derivatives are very similar to sigmoid derivatives in range, so the problem of vanishing gradients is thus even more drastic with RNNs than with ANNs, and training them becomes almost impossible. Imagine trying to propagate the error to the 1st timestep in an RNN with k timesteps. The derivative would look something like this: With a tanh activation function, that\u2019s freaking crazy. Then, for getting the derivative of the error with respect to a weight matrix W_hh, we\u2019d add\u200a\u2014\u200aor, as mentioned before, we could average as well\u200a\u2014\u200aeach of these hidden state error gradients, then multiplied by the derivative of the hidden state with respect to the weight, such that we can backprop from the error to the weight: Assuming our sequence is of length\u00a0k. So we\u2019d be effectively adding together a bunch of terms that have vanished\u200a\u2014\u200athe exception being very late gradients with a small number of terms\u200a\u2014\u200aand so dJ/dWhh would only capture gradient signals from the last few timesteps. (Or, for exploding gradients, it would become infinity). But, you might be asking, instead of tanh\u200a\u2014\u200awhich is bounded between -1 and 1, and has a similar problem to sigmoid where the peak of the derivative is smaller than 1\u200a\u2014\u200awhy don\u2019t we just use ReLUs? Don\u2019t ReLUs, or perhaps leaky ReLUs, solve the vanishing gradient problem? Well, not entirely; it\u2019s not enough to solve the problem. With RNNs, the problem really lies in the architecture. Even though we could use ReLU to ensure many of the values in the gradient computation are not between -1, 0, and 1 such that they vanish\u200a\u2014\u200aor vice-versa, explode\u200a\u2014\u200awe do still indeed have a lot of other variables other than the activation function derivative in the gradient computation such as the weights; you can revisit the mega-post on backprop we wrote to confirm this. Since weights are also normally randomly initialized in the range -1 to 1, and RNNs are like super deep ANNs, these weights keep multiplying on top of each other and potentially cause the gradients to vanish. This is more my suspicion though\u200a\u2014\u200aI\u2019m yet to confirm this is the case by testing. I was curious so I asked this exact question on Quora: From this, something interesting I learned is that: since ReLUs are unbounded (it\u2019s not restricted to be between -1 and 1 or 0 and 1) unlike sigmoid/tanh, and RNNs are very deep, the activations, especially later ones, can become too big. This is because hidden states have a multiplicative relationship; one hidden state is a multiple of the previous ones, where that multiple specifically is a weight. If we use ReLU, then the hidden state isn\u2019t limited by any range, and we could have a bunch of numbers bigger than 1 multiplying by each other. It ends up being sort of like the exploding gradient problem, but with the values inside the neurons, not gradients. This is also what then causes the gradients to explode: large activations \u2192 large gradients \u2192 large change in weights \u2192 even bigger activations, because updating the weights in the wrong direction ever so slightly can cause the entire network to explode. This makes learning unstable: This means that the computation within the RNN can potentially blow up to infinity without sensible weights. This makes learning VERY unstable because a slight shift in the weights in the wrong direction during backprop can blow up the activations during the forward pass. So that\u2019s why you see most people using sigmoid/tanh units, despite the vanishing gradient descent problem. Also well said: With RNN\u2019s, the problem is that you are repeatedly applying your RNN to itself, which tends to [mostly] cause exponential blowup or [rarely, but sometimes] shrinkage. Other issues with ReLU functions are discussed in the article I wrote, and they similarly apply to RNNs. Generally speaking, though, they just don\u2019t work that well, especially compared to other options we have. Making RNNs perform well with ReLUs is actually a pretty hot topic of research right now, but until someone figures out something genius, RNNs are a lost cause. And that\u2019s why vanilla RNNs suck. Seriously. In practice, nobody uses them. Even if you didn\u2019t fully grasp this section on how the vanishing and exploding gradient/activation problem is applicable to them, it doesn\u2019t matter anyways. Because, everything you\u2019ve read up to this point so far\u2026 throw it all away. Forget about it. Just kidding. Don\u2019t do that. Fixing the problem with LSTMs (Part\u00a0I) You shouldn\u2019t do that because RNNs actually aren\u2019t a lost cause. They\u2019re far from it. We just need to make a few\u2026 modifications. Enter the LSTM. Makes sense, no? How about this? OK. Clearly something\u2019s not registering here. But that\u2019s fine; LSTM diagrams are frikin\u2019 difficult for beginners to grasp. I too remember when I first searched up \u201cLSTM\u201d on Google to encounter something similar to the works of art above. I reacted like this: MRW first Google Image-ing LSTMs. In this section, I\u2019m going to embark on a mission to design the first simple, comprehensible, and beautiful LSTM diagram. Wish me luck, because I\u2019ll probably fail. With that being said, let\u2019s dive into Long Short-Term Memory networks. (Yes, that\u2019s what LSTM stands for.) With RNNs, the real \u201csubstance\u201d of the model were the hidden neurons; these were the units that did processing on the input, through time, to produce the outputs. Specifically, at each timestep, a hidden neuron embodies a hidden state that is computed by feeding the weighted sum vector of the input and/or previous hidden states with an added bias vector through a tanh squashing function. We can have multiple hidden neurons, each of which when unrolled forms an entire hidden \u201clayer\u201d. If you need a refresher on this, look through the \u201cFormalism\u201d section once again. With LSTMs, we still have hidden states, but they\u2019re computed through a much more complex mechanism: LSTM cells. Instead of computing each hidden state as a direct function of inputs and other hidden states, we compute it as a function of the LSTM cell\u2019s value (the \u201ccell state\u201d) at that timestep. Each cell state is in turn functionally dependent on the previous cell state and any available input or previous hidden states. That\u2019s right\u200a\u2014\u200ahidden states are computed from cell states, and cell states are (in part) computed from older and/or shallower hidden states. The cell state at a specific timestep t is denoted c_t. Like a hidden state, a cell state is just a vector. For simplicity\u2019s sake, I\u2019ve obfuscated layer index\u00a0\u2113. If the diagram above seems a bit trippy, let me break it down for you. c_t, as highlighted by the three arrows pointing towards it, has multiple potential functional dependencies. Four to be exact, though only a maximum of three can exist at once. These are: Only three can exist at once because the last two are mutually exclusive. From there, we pass information to the next cell state c_t+1 and compute h_t. As you can hopefully see, h_t then goes on to also influence c_t+1 (as indicated by the horizontal arrow), along with higher level cell states or final outputs (the vertical arrow). Right now the cells are a black box\u2026 literally; we know what is inputted to them and what they output, but we don\u2019t know their internal process. So\u2026 what\u2019s inside these cells? What do they do? What are the exact computations involved? How have the equations changed? To help answer the question, I want you to imagine something: a factory. Inside this factory we have workers who perform their own tasks. Those tasks are, specifically, operating on some sort of product that runs down a conveyer belt. Think of, hell, I don\u2019t know\u200a\u2014\u200achicken nuggets! The first worker cuts an appropriately sized piece, the second worker applies egg wash, the third worker adds breadcrumbs, the fourth worker chucks it in the fryer, etc. I\u2019m not sure what product this conveyer belt carries, but it certainly doesn\u2019t look appetizing (or like chicken nuggets). You\u2019re thinking: \u201cOK Rohan, but how does this relate to LSTMs?\u201d. Good question. Basically, think of the conveyer belt as the cell state, the chicken-nugget-in-progress as information flowing through the cell, and the workers as operations we apply to this information. The final product is the finished chicken nugget\u200a\u2014\u200aor, the cell state value. Chicken. Nugget. The reason we use the analogy of a conveyer belt is because information can flow through a cell super super easily. It\u2019s theoretically possible for information to speed past a single cell state without being modified at all. In fact, I think the term \u2018modified\u2019 is a really strong one here. With vanilla RNNs, each hidden state takes all the information it has from before and fully transforms it by applying a function over it. LSTM cells instead take information and make minor modifications (like additions or multiplications) to it while it flows through. Ew. Vanilla\u00a0RNNs. Vanilla RNNs look something like this. And it\u2019s why the vanishing gradient problem exists; during backprop, gradients cannot flow back past these transformations easily, because the tanh derivatives and weights chain up and multiply together and tend to zero. We then add up or average all these gradients that are basically zero, and we get zero. LSTMs \ud83d\udca6 \ud83d\udca6\u00a0\ud83d\udca6 This is an extreme a simplification\u200a\u2014\u200aand I\u2019ll go on to fill in the blanks later\u200a\u2014\u200abut it\u2019s sort of what an LSTM looks like. The previous timestep\u2019s cell state value flows through and instead of transforming the information, we tweak it by adding (another vector) to it. The added term is some function \u0192w of previous information, but this is not the same function as with vanilla RNNs\u200a\u2014\u200ait\u2019s heavily changed to make sense in this context (more on that soon), do more interesting things, and also reduce the vanishing gradient problem. Another neat way to think of it is like a live circuit: there are two paths where information, like current, can flow through. After the information passes through \u0192w, it\u2019s added to the information flowing towards c_t. Thus, in equation form it could look something like this: Again\u2026 sort of. We\u2019ll get into the actual equations soon. This is a good proxy to convey my\u00a0point. With a bit of substitution, we can expand this to: Technically, this could expand even more, if you did some sort of recursive substitution to fully simplify the unrolled recurrence formula. You could express c_t for some large value of t as a really really really really long function of, ultimately, c_1. Why is this better? Well, if you have basic differentiation knowledge, you\u2019ll know that addition distributes gradients equally. When we take the derivative of this whole expression, it\u2019ll become a long addition of the derivatives of individual terms. As Andrej Karpathy puts it, this additive interaction creates \u201cgradient super-highways\u201d, where gradients can flow back super easily. Look\u200a\u2014\u200ait\u2019s a long conveyer belt! (In a sense, we can use this conveyor belt analogy for the whole unrolled LSTM as well. Each cell state is a subsection of the conveyer\u00a0belt.) Look\u200a\u2014\u200ait\u2019s an outdated machine learning algorithm! In the former, gradients are always added together, never multiplied. In the latter, gradients are always multiplied. Thus, in the former, when we inject a gradient at the end, it\u2019ll easily flow back all the way to the beginning. Contributions by the \u0192w function will be made to this gradient flowing on the bottom conveyer belt as well. This is what gradient flow would look like: Before, we discussed that when multiplicative interaction exists between gradients, the gradients either vanish (if they are mostly < 1, as is usually the case for us) or explode (if they are mostly > 1). Here\u2019s some real calculus to demonstrate this: Former is akin to RNNs. Latter is akin to\u00a0LSTMs. Imagine f being any sort of function, like our \u0192w. When we apply a function to itself repeatedly, the chain rule shows that the overall derivative is the multiplication of multiple different derivative terms. But, when we add functions together, the derivative is simply the addition of the individual derivatives. This won\u2019t vanish or explode quickly, so our LSTMs won\u2019t vanish or explode quickly. Yay! Furthermore, if some of our gradients vanish\u200a\u2014\u200afor whatever reason\u200a\u2014\u200athen it should still be OK. It won\u2019t be optimal, but since our gradient terms add together, if some of them vanish it doesn\u2019t mean the whole thing will vanish (versus if they were multiplied together). Look: 2 + 0 = 2 but 2 \u00d7 0 = 0. A gradient super highway? Sounds good to me! http://www.dyoung.com/assets/images/Articles%20images/article4_PPH.jpg So far, we haven\u2019t really explored LSTMs. We\u2019ve more setup a foundation for them. And there\u2019s one glaring issue with our foundation: if we just keep adding information to cell state, it could just grow and grow and grow, and essentially act as a counter that only increments. This is not very useful, and could regularly lead to explosion. We want more fine and rich control over memory. Well, worry not, because this is exactly what LSTMs are capable of doing. LSTM cells handle memory in a very intelligent way, enabling them to learn long-term dependencies and perform well. How, exactly? Well, the cell is sort of like an internal memory state that allows for context; it \u201cforgets\u201d, a.k.a. resets, information it doesn\u2019t find useful from the previous cell state, \u201cwrites\u201d in new information it does find useful from the current input and/or previous hidden state, and similarly only \u201creads\u201d out part of its information\u200a\u2014\u200athe good stuff\u200a\u2014\u200ain the computation of h_t. This respectively corresponds to the concepts of: resetting memory, writing to memory, and reading from memory. Very similar to how a modern computer system works, and we often describe an LSTM cell as a \u201cmemory cell\u201d. The \u201cwriting to memory\u201d part is additive\u200a\u2014\u200ait\u2019s what I showed you in the initial diagrams. Information flows through and we add stuff we think is relevant to it. The \u201cresetting memory\u201d part is multiplicative, and occurs before writing to memory; when information from the previous cell state initially flows in, we multiply it by a vector with values between 0 and 1 to reset or retain parts of it we find useless and useful respectively. The \u201creading from memory\u201d part is also multiplicative with a similar 0\u20131 range vector, but it doesn\u2019t modify the information flowing through the cell states. Rather, it modifies the information flowing into the hidden states and thus decides what the hidden state is influenced by. Both of these multiplications are element wise, like so: In this equation, when a = 0 the information of c is lost. This is what resetting does, and retaining is the vice versa. I also imagine that values such as 0.5 could be used to diminish the importance of certain information, but not completely wipe it out. Our (unfinished) cell state computational graph now looks like this: This is closer to what an LSTM looks like, though we\u2019re not exactly there\u00a0yet. Sidenote: don\u2019t be scared whenever you see the word \u201cmultiplicative\u201d and don\u2019t immediately think of \u201cvanishing\u201d or \u201cexploding\u201d. It depends on the context. Here, as I\u2019ll show mathematically in a bit, it\u2019s fine. This concept in general is known as gating, because we \u201cgate\u201d what can flow in and out of the LSTM cell. What we actually multiply and add by to reset, write, and read are known as the \u201cgates\u201d. There are four such gates: Here\u2019s our updated computational graph for the cell state: Looks like I\u2019m starting to create a complex diagram of my own. Damn. \ud83d\ude1e I guess LSTMs and immediately interpretable diagrams just weren\u2019t meant to be! Basically, f interacts with the cell state through a multiplication. i interacts with g through a multiplication as well, the result of which interacts with the cell state through an addition. Finally, the cell state leaks into a tanh (that\u2019s the shape of the tanh function in the circle), the result of which then interacts with o through multiplication to compute h_t. This does not disrupt the cell state, which flows to the next timestep. h_t then flows forward (and it could flow upward as well). Here\u2019s the equation form: Each gate should actually be indexed by timestep t\u200a\u2014\u200awe\u2019ll see why\u00a0soon. As you can see, our cell state has no activation function; the activation function is simply the identity function! Yet, the cell state usually doesn\u2019t explode\u200a\u2014\u200ait stays stable by \u201cforgetting\u201d and \u201cwriting\u201d, and does interesting things with this gating to promote context, fine control over memory, and long-term dependency learning. So, how are the gates calculated? Well, all of these gates have their own learnable weights and are functions of the last timestep\u2019s hidden state flowing in and any current timestep inputs, not the cell state (contrary to what I may have implied earlier with the gradient flow diagrams). This should make sense when you think about it; I mean, firstly, the g and i gates literally represent input, so they better be functionally dependent on hidden states and input data! On an intuitive level, the gates help us modify the cell state, and we modify the cell state based on our current context. External stimulus that provide context should be used to compute these gates, and since context = input + hidden states our gates are functionally dependent on input and hidden states. Since every gate has a different value at each timestep, we index by timestep t just like for hidden states, cell states, or something similar. We could generalize for multiple hidden layers as well: But, for simplicity\u2019s sake, let\u2019s assume we are at the first hidden layer, or that there is only one hidden layer in the LSTM. This way, we can obfuscate the \u2113 term and ignore influence from hidden states in the previous depth. We\u2019ll also forget about edge cases and assume input exists at the current timestep. In practice, we obviously can\u2019t make said assumptions, but for the sake of demonstrating the equations it becomes too tedious otherwise. Sidenote: we make this assumption for the rest of the discussion on LSTMs in this article. Like with the RNN hidden state, the index of each weight matrix is descriptive; for example, W_xf are the weights that map input x to the forget gate f. Each gate has weight matrices that map input and hidden states to itself, including biases. And this is the beauty of LSTMs; the whole thing is end-to-end differentiable. These gates can learn when to allow data to flow and what data should flow depending on the context it sees (the input and the hidden states). It learns this based on patterns it sees while training. In this sense, it\u2019s sort of like how a CNN learns feature detectors for images, but the patterns are way more complex and less human interpretable with LSTMs. This is why they perform so well. \ud83d\ude28 \ud83d\ude31 \ud83d\ude30\u00a0: perhaps your immediate reaction. Okay, this looks scarier, but it\u2019s actually not much different to what we had before, especially once you look past the intimidating web of arrows. One notable change is that we\u2019re showing the previous hidden state in time and the current input flowing in. This diagram makes the assumption that we\u2019re in the first layer and at some timestep > 1 where input exists. We then show how the f, i, g, and o gates are computed from this information\u200a\u2014\u200athe hidden state and inputs are fed into an activation function like sigmoid (or, for g, a tanh; you can tell because it\u2019s double the height of the others)\u200a\u2014\u200aand it\u2019s expressed through the web of arrows. It\u2019s implied that we weight the two terms entering our activation functions, adding them up with a bias vector, but it\u2019s not necessarily explicit in the diagram. Let\u2019s embed this into our overall LSTM diagram for a single timestep: Now let\u2019s zoom out and view our entire unrolled single layer, three timestep LSTM: It\u2019s beautiful, isn\u2019t it? The full screen width size just adds to the effect! Here\u2019s a link to the full res version. The only thing that would look more beautiful would be multiple LSTM cells that stack on top of each other (multiple hidden layers)! \ud83d\ude0d Fixing the problem with LSTMs (Part\u00a0II) You\u2019ve come a long way, young padawan. But there\u2019s still a bit left to go. Part I focused on the motivation for LSTMs, how they work, and a bit on why they reduce the vanishing gradient problem. Now, having a full understanding of LSTMs, Part II will hone in on the latter part\u2014analyzing on a more close, technical level why our gradients stop vanishing as quickly. You won\u2019t find a lot of this information online easily; I had to search and ask left and right to find an explanation better and more comprehensive than what you\u2019ll find in other current tutorials. Firstly, truncated BPTT is often used with LSTMs; it\u2019s a method to speed up training. In particular, note that if we input a sequence of length 1000 into an LSTM, and want to train it, it\u2019s equivalent to training a 1000 layer neural network. Doing forward and backwards passes into this is very memory and time consuming, especially while backpropagating the error when we need to compute a derivative like this: \u2026which would include a lot of terms. When we backprop the error, and add all the gradients up, this is what we get: Truncated BPTT does two things: For example, if t = 20 and k1 = 10, our second (because 20 \u00f7 10 = 2) round of BPTT would be: So, with t = 20, k2 = 10, and k1 = 10, our second round of BPTT would follow: Both k1 and k2 are hyperparameters. k1 does not have to equal k2. These two techniques combined enables truncated BPTT to not lose the ability to learn long term dependencies. Here\u2019s a formal definition: [Truncated BPTT] processes the sequence one timestep at a time, and every k1 timesteps, it runs BPTT for k2 timesteps, so a parameter update can be cheap if k2 is small. Consequently, its hidden states have been exposed to many timesteps and so may contain useful information about the far past, which would be opportunistically exploited. \u2014 \u201cTraining Recurrent Neural Networks\u201d, 2.8.6, Page 23 The same paper gives nice pseudocode for truncated BPTT: The rest of the math in this section will not be in the context of using truncated backprop, because it\u2019s a technique vs. something rooted in the mathematical foundation of LSTMs. Moving on\u200a\u2014\u200abefore, we saw this diagram: In this context, \u0192w = i \u2299 g, because it\u2019s the value we\u2019re adding to the cell state. But this diagram is a bit of a lie. Why? It ignores forget gates. So, does the presence of forget gates affect the vanishing gradient problem? Quite significantly, actually. How? Let\u2019s bring up our cell state equation to see: With the forget gate, we now include a multiplicative interaction. Our new diagram will look like this: Do not confuse forget gate \u0192 with function \u0192w in this diagram. I know, it\u2019s confusing\u2026 \ud83d\ude22 When our gradients flow back, they will be affected by this multiplicative interaction. So, let\u2019s compute the new derivative: This seems super neat, actually. Obviously the gradient will be f, because f acts as a blocker and controls how much c_t-1 influences c_t; it\u2019s the gate that you can fully or partially open and close that lets information from c_t-1 flow through! It\u2019s just intuitive that it would propagate back perfectly. But, if you\u2019ve payed close attention so far, you might be asking: \u201cwait, what happened to \u0192w\u2019s contribution to the gradient?\u201d If you\u2019re a hardcore mathematician, you might also be worried that we\u2019re content with leaving the gradient as just f. This is because the gates f, i, and g are all functions of c_t-1; they are functions of h_t-1, which is, in turn, a function of c_t-1! The diagram shows this visually, as well. It seems we\u2019re failing to apply calculus properly. We\u2019d need to backprop through f and through i \u2299 g to complete the derivative. Let\u2019s walk through the differentiation to show why you\u2019re actually not wrong, but neither am I: Now, with the first derivative, we need to apply product rule. Why? Because we\u2019re differentiating the product of two functions of c_t-1. The former being the forget gate, and the latter being just c_t-1. Let\u2019s do it: Then, from product rule: That\u2019s the first derivative done. We purposely choose not to compute the derivative of the forget gate with respect to the previous cell state on previous. You\u2019ll see why in a bit. Now let\u2019s tackle the second one: You\u2019ll notice that it\u2019s also two functions of c_t-1 multiplied together, so we use the product rule again: So: Once again, we purposely do not simplify the gate derivative terms. Thus, our overall derivative becomes: Notice that the first term in this derivative is our forget\u00a0gate. Pay attention to the caption of the diagram. This is actually our real derivative. Modern LSTM implementations just use an auto differentiation library to compute derivatives, so they\u2019ll probably come up with this. However, effectively, our gradient is just the forget gate, because the other three terms tend towards zero. Yup\u200a\u2014\u200athey vanish. Why? When we backprop error in LSTMs, we backprop through cell states to propagate the error from the outputs to the cell state we want. For example, if we want to backprop the error from the output at time t down k timesteps, then we need to compute the derivative of the cell state at time t to the cell state at time t-k. Look what happens when we do that: We didn\u2019t simplify the gate w.r.t. cell state derivatives for a reason; as we backpropagate through time, they begin to vanish. Thus, whatever they multiplied with is killed off from making contributions to the gradient, too. So, effectively: The rationale behind this is pretty simple, and we don\u2019t need math for it; these gates are the outputs of non-linearities eg. sigmoid and tanh. If we were to get the derivative of them in getting our cell state derivative, then this derivative would contain the derivatives of sigmoid/tanh in them. But, just because we don\u2019t need to use math to show this, doesn\u2019t mean we don\u2019t want to \ud83d\ude0f\u00a0: I obfuscated the input to the sigmoid function for the input gate, just for simplicity. Recall from our vanishing gradient article that the max output of sigmoid\u2019s first order derivative is 0.25, and it\u2019s something similar for tanh. This becomes textbook vanishing gradient problem. As we backprop through more and more cell states, the gradient terms become longer and longer, and this will definitely vanish. When they don\u2019t vanish, they\u2019ll be super minor contributions, so we can just leave them out for brevity. Sidenote: one person reached out to me unsure of why gradients with long terms\u200a\u2014\u200aaka, that are equal to the product of a lot of terms\u200a\u2014\u200ausually vanishes/explodes. Here\u2019s what I said in response: \u201cIf you have long gradient terms, you probably have the vanishing gradient problem, unless you can guarantee those terms are around 1 each. If they\u2019re not, it\u2019ll explode or vanish. And, given the nature of the problems where this is an issue, it\u2019s very unlikely they\u2019ll be around 1 each. Especially if they are the output some non-linear function like sigmoid/tanh or their derivatives. For example, let\u2019s say the gradient term = k_1 \u00d7 k_2 \u00d7 k_3 \u00d7\u00a0\u2026 \u00d7 k_100. 100 terms in this product. If each of these terms is, let\u2019s say, around 0.5, then you have 0.5\u00b9\u2070\u2070 = some absurdly low number. If you have each term be arond 1.5, then you have 1.5\u00b9\u2070\u2070 which is some absurdly high number.When we introduce tanh/sigmoid and/or their derivatives in these huge products, you can guarantee that they\u2019ll saturate and die off. As mentioned, the max for sigmoid\u2019s first order derivative is 0.25, so just imagine something like 0.25\u00b9\u2070\u2070. Ultimately, the reason I obfuscate these terms that vanish in the derivative is because I would like to show the effect of the forget gate on gradient flow now. If I included the other terms, the same implications would be present, but the math would just take longer to type out and render. Because \u0192w = i \u2299 g, we can redraw our diagram showing that \u0192w won\u2019t make any contributions to the gradient flow back. Again\u200a\u2014\u200a\u0192w does, but it\u2019s effectively negligible, so we can just exclude it from our updated gradient flow diagram, which follows: But wait! This doesn\u2019t look good; the gradients have to multiply by this f_t gate at each timestep. Before, they didn\u2019t have to multiply by anything (or, in other words, they multiplied by 1) and flowed past super easily. Machine learning researchers coined a name for the type of function we had before we introduced the forget gate where the derivative of one cell state w.r.t. the previous is 1.0: \u201cConstant Error Carousel\u201d (CEC). With our new function, the derivative is equal to f. You\u2019ll see this referred to as a \u201clinear carousel\u201d in papers. Before we introduced a forget gate\u200a\u2014\u200awhere all we had was the additive interaction from \u0192w\u200a\u2014\u200aour cell state function was a CEC: A CEC\u200a\u2014\u200asame as before, but no forget\u00a0gate. The derivative of this cell state w.r.t. the previous one, again as long as we don\u2019t backprop through the i and g gates, is just 1. That\u2019s why gradients flow back super comfortably, without vanishing at all. Basically, for a CEC to exist in this context, the coefficient of c_t-1 needs to be 1. Once we introduced this multiplicative interaction (for good reason), we got a linear carousel; the coefficient of c_t-1 is f. So, in our case, when f = 1 (when we\u2019re not going to forget) our function becomes a CEC, and our gradients will pretty much never vanish. If it\u2019s close to 0, though, the gradient term will immediately die. Gradients will stay on the carousel for a while until the forget gate is triggered; the effect on the gradient is like a step function, in that it\u2019s constant with a value of 1 and then drops off to zero/dies when we have f \u2248 0. Intuitively, this seems problematic. Let\u2019s do some math to investigate: The derivative of a cell state to the previous is f_t. The derivative of a cell state to two prior cell states is f_t \u2299 f_t-1. Thus: As we backpropagate through time, these forget gates keep chaining up and multiplying together to form the overall gradient term. Now, imagine an LSTM with 100 timesteps. If we wanted to get the derivative of the error w.r.t. a weight like W_xi, to optimize it, remember that with BPTT we add up or average all the gradients from the different timesteps: OK. Now let\u2019s look at an early (in time) term, like the gradient propagated from the error to the third cell: Remember that J is an addition of errors from Y individual outputs, so we backpropagate through each of the outputs first: The first few terms, where we backprop y_k to c_3 where k < 3, would just be equal to zero because c_3 only exists after these outputs have been computed. Let\u2019s assume that Y = 100 and continue with our assumption that t = 100 (so each timestep gives rise to an output), for simplicity. With this, let\u2019s now look at the last term in this sum. That\u2019s a lot of forget gates chained together. If one of these forget gates is [approximately] zero, the whole gradient dies. If these also tend to be a small number between 0 and 1, the whole thing will vanish, and c_3 won\u2019t make any contributions to the gradient here. This isn\u2019t intrinsically an issue though! Because, when a forget gate is zero, it means that cell is no longer making any contributions past that point. If f_4 is zero, then any y outputs at/past timestep 4 won\u2019t be influenced by c_3 (as well as c_2 and c_1) because we \u201cerased\u201d it from memory. Therefore that particular gradient should be zero. If y_80 is zero, then any outputs at/past timestep 80 won\u2019t be influenced by c_1, c_2,\u00a0\u2026\u00a0, c_79. Same story here. If these forget gates are between 0 and 1, then the influence of our cell decays over time anyways, and our gradients will be very small, so they\u2019ll reflect that. Gers 1999 calls this \u201creleasing resources\u201d. Cell c_3 will still contribute to the overall gradient, though. For example, take this term: Here, we\u2019re looking at y_12 instead of y_100. Chances are that, if you have a sequence of length 100, your 100th cell state isn\u2019t drawing from your 3rd; the forget gate would have been triggered at some point by then. However, the 12th cell state probably will still be drawing from the ones before it. If we decide not to forget in the first 12 timesteps, ie. f_1\u00a0\u2026 f_12 are each not far from 1, then c_3 would have more influence over y_12 and the error that stems from y_12. Thus, the gradient would not vanish and c_3 still contributes to update W_xi, it just doesn\u2019t contribute a gradient where it\u2019s not warranted to (that is, where it doesn\u2019t actually contribute to any activation, because it\u2019s been forgotten). To summarize: one activated forget gate will indeed kill off gradient flow to cell(s), but that is a good thing because the network is learning that that gradient from the future has no benefit and is completely irrelevant to those particular cell(s), since those cells have been forgotten by then. In practice, different cells learn different ranges of context, some short, some long. This is a benefit for LSTMs. So, given a gradient between two cell states in time, when all of these forget gates are [approximately] equal to 1, the gradient signal will remain stable, because we\u2019re multiplying by 1 at each timestep\u200a\u2014\u200aeffectively, not multiplying by anything at all. In such a case, our gradient flow diagram would look like this: It\u2019s\u2026 it\u2019s beautiful! The gradient will have literally zero interactions or disturbances, and will just flow through like it\u2019s driving 150 mph on an empty countryside America highway. The beauty of CECs is that they\u2019re always like this. But, let\u2019s get back to reality. LSTMs aren\u2019t CECs. One disadvantage of these forget gates chaining together is that it could block learning. That is, when we set out to train our LSTM, the forget gates have not been learned; we have to learn them while we learn everything else. So, if they all start around 0, no gradients will flow through our cell states when we perform BPTT, and learning won\u2019t happen at all. The obvious solution is to set the forget gate bias to a very large value when training, so it starts at 1 instead of 0 (because y = 1 is to the far right of the sigmoid function, so adding to the input will ensure ~1 will be the output). In early stages of training, the forget gates equalling/approximating 1 will result in learning not being blocked. So many papers do this and mention it explicitly such that this forget gate bias could even be considered a hyperparameter. By introducing forget gates, we stray from CECs and thus the guarantee that our gradients will never ever vanish. But, again, we do it for good reason. And when gradients vanish it\u2019s because we chose to forget that cell\u200a\u2014\u200aso it\u2019s not necessarily a bad thing. We just need to make sure the forget gates don\u2019t block learning in initial stages of training. We can try computing some more derivatives, just for fun! Let\u2019s sub in real values for the timesteps, backprop across more than one timestep, and do it for a gate this time. We\u2019ll expand c_4 and express it in terms of our gates only. In the process, each c_t, except c_1, will collapse into a few interactions between the f, i, and g gate: Now, let\u2019s get the derivative of c_4 with respect to one of the earliest possible gates, like g_2. In the expression above, this turns out to just be the coefficient of g_2: We experience the same neatness here as with the cell state backprop! It makes complete sense that the gradient would be i_2 \u2299 f_3 \u2299 f_4, since i_2 controls what influence g_2 has over c_2, f_3 controls what influence c_2 has on c_3, and f_4 controls what influence c_3 has over c_4. Notice the chaining up of the forget gates \ud83d\udc7b; everything about the carousels I just talked about\u200a\u2014\u200aand what they imply about vanishing gradients\u200a\u2014\u200aapplies here. I\u2019ll leave it up to you to derive something similar for the other gates. And that\u2019s it! That\u2019s why LSTMs rock their socks off when it comes to keeping their gradients in check,. Here\u2019s a neat GIF showing a visual representation of the gradients that exist at each timestep, starting from timestep 128 and going all the way to the first, during backprop. More noise represents greater values: As you can see, the vanilla RNN\u2019s gradients die off way quicker than the LSTM\u2019s. The RNN is almost immediate in comparison. LSTMs seem like a super highway indeed, although it does seem that they do vanish. In this diagram, it can be suggested that the gradients perhaps die for the LSTM eventually because we chose to forget early cell states; again, this depends on the application at hand, and is learnable. (I\u2019m not sure if this GIF uses truncated backprop, so that could be another thing. In general, I don\u2019t know the context of this GIF.) Also, part of the gradient signal definitely vanishes\u2014it\u2019s the signals that pass through the f/i/g gates that we looked at earlier and obfuscated from the cell state\u2192cell state derivative. We showed they would vanish because of tanh/sigmoid derivatives; initially, these signals will make a fairly significant contribution, but over time they\u2019ll get smaller and smaller. That\u2019s the explanation for this GIF. Exploding gradients is still an issue, though. Recall that when we have a bunch of gradient terms added together, if some of them vanish it doesn\u2019t mean the whole thing will vanish (versus if they were multiplied together). However, if some of the gradients explode, the whole thing explodes; x + 0 = x, but x + \u221e = \u221e. If cell states become unstable and grow too much in some rare scenario, then our gradients could explode. In such a case we\u2019d need to implement gradient clipping, which is where we choose some arbitrary threshold that gradients cannot be larger than; so, grad = min(grad, clip_threshold). This would enable the LSTM to deal with such cases without essentially collapsing. Many successful LSTM applications use gradient clipping. Usually, though, exploding gradients are avoided because sooner or later the forget gate in the carousel is triggered and we reset the memory. There are variants of LSTMs. People have tried modifying the model, like computing the hidden state without using tanh activation (so h_t = o \u2299 c_t) or ditching the i input gate and only using g, since that would still satisfy the -1 to 1 range. The results didn\u2019t change by much. In fact, some researchers even applied evolutionary algorithms to spawn and test a bunch of variants on the standard LSTM equations. Most of the good ones just worked roughly the same. This highlights an issue with LSTMs\u200a\u2014\u200athey are definitely fairly handwavy. We use them because their architecture allows us to prevent gradients from vanishing such that we can learn long-term dependencies, but there\u2019s not much theoretical or empirical backing for them. ANNs and RNNs make sense in that they\u2019re biologically inspired and that they\u2019re essentially just deep composite functions that have parameters we can optimize. LSTMs stray so far from statistical methods and introduce complex concepts/architectures that work but aren\u2019t necessarily justified from the get-go. Fully understanding why LSTMs work so well and coming up with better/simpler architectures is a hot topic of research right now. There are also other variants of RNNs, similar to LSTMs, like GRUs (Gated Recurrent Units). LSTM is still the king, but GRUs in particular have grown in popularity and are seen in many recent, well-respected research papers. It\u2019s a must learn next to LSTMs, but this article would get too bloated with it. TL;DR: GRUs have a less complex architecture than LSTMs but achieves similar results, and they can control the flow of information without requiring a memory cell. And, they\u2019re fairly new. (See, told you \u201ccoming up with better/simpler architectures is a hot topic of research right now\u201d is true!) Yay RNNs! Wanna see a couple cool things? Practical applications were talked about in the first section, and the next section will walk through more technical applications of RNNs ie. in recent research papers. This section, instead, contains some fun things RNNs have done or produced that\u2019ll ease yourself from the hell that was actually understanding them. As simply as we try to convey things on this blog, LSTMs be LSTMs. Sidenote: now, don\u2019t be frightened by \u201cRNNs\u201d. Do be frightened by \u201cvanilla RNNs\u201d, in most contexts, but we use RNN as an umbrella term, and it most often refers to an LSTM or a similar variant like GRU. Many if not all of these are taken from Andrej Karpathy\u2019s CS231n lecture, or his blog post on the same subject: The Unreasonable Effectiveness of Recurrent Neural NetworksMusings of a Computer Scientist.karpathy.github.io You should most certainly visit either his blog post or lecture for info on exactly how these experiments were conducted, and for more interpretation on their results. Taking a look at the \u2018Visualizing the predictions and the \u201cneuron\u201d firings in the RNN\u2019 section would also be helpful to gain more insight and intuition on how RNNs work and learn over time. A recurrent neural network generated this body of text, after it \u201cread\u201d a bunch of Shakespeare: Similarly, Karpathy gave an LSTM a lot of Paul Graham\u2019s startup advice and life wisdom to read, and it produced this: \u201cThe surprised in investors weren\u2019t going to raise money. I\u2019m not the company with the time there are all interesting quickly, don\u2019t have to get off the same programmers. There\u2019s a super-angel round fundraising, why do you can do. If you have a different physical investment are become in people who reduced in a startup with the way to argument the acquirer could see them just that you\u2019re also the founders will part of users\u2019 affords that and an alternation to the idea. [2] Don\u2019t work at first member to see the way kids will seem in advance of a bad successful startup. And if you have to act the big company\u00a0too.\u201d A lot of relevant terminology, but it doesn\u2019t really\u2026 come together \ud83d\ude16. An LSTM can even generate valid XML, after reading Wikipedia!: <page>  <title>Antichrist</title>  <id>865</id>  <revision>    <id>15900676</id>    <timestamp>2002-08-03T18:14:12Z</timestamp>    <contributor>      <username>Paris</username>      <id>23</id>    </contributor>    <minor />    <comment>Automated conversion</comment>    <text xml:space=\"preserve\">#REDIRECT [[Christianity]]</text>  </revision></page> After a similar LSTM read the raw Latex version of an advanced algebra textbook, it came up with this\u200a\u2014\u200aput frankly\u200a\u2014\u200afancy looking bogus. Let\u2019s be real, you could definitely believe this was actual math \ud83d\ude1c: An LSTM also read the Linux source code, and tried to write some code of its own: /* * Increment the size file of the new incorrect UI_FILTER group information * of the size generatively. */static int indicate_policy(void){  int error;  if (fd == MARN_EPT) {    /*     * The kernel blank will coeld it to userspace.     */    if (ss->segment < mem_total)      unblock_graph_and_set_blocked();    else      ret = 1;    goto bail;  }  segaddr = in_SB(in.addr);  selector = seg / 16;  setup_works = true;  for (i = 0; i < blocks; i++) {    seq = buf[i++];    bpf = bd->bd.next + i * search;    if (fd) {      current = blocked;    }  }  rw->name = \"Getjbbregs\";  bprm_self_clearl(&iv->version);  regs->new = blocks[(BPF_STATS << info->historidac)] | PFMR_CLOBATHINC_SECONDS << 12;  return segtable;} SUPERINTELLIGENCE MUCH\u203d SELF-RECURSIVE IMPROVEMENT MUCH\u203d THE END OF THE UNIVERSE MUCH\u203d Nope. Just some code doesn\u2019t compile or make any sense. It even has its own bogus comments! Generating music? Easy! A fun watch: A more informative watch: Something even cooler and\u2026 creepier (seriously, the results after the first couple iterations of training are so unsettling): In Practice So we\u2019ve seen how RNNs work in theory; now where do they fit in in practice? As it turns out, recurrent neural networks can do a whole lot. I\u2019ll try to cover a few of the important, significant, and interesting uses that have cropped up over the last few years. Bidirectional Recurrent Neural\u00a0Networks The Problem: Giving the network access to a sequence of vectors is fine and dandy, but what if we want our output at time t to be conditioned on an input vector that comes at a later timestep? Take the example of speech recognition, where our input vectors are some kind of audio features at time t and the output is the predicted phoneme at that time. In our traditional RNN architecture, the output at time t is conditioned only on input vectors 1..t, but as it turns out future information might be useful too. The sounds at time step t+1 (and maybe t+2, t+3,\u00a0\u2026) are likely part of the same phoneme, and therefore could help us make more accurate predictions. But our network won\u2019t have access to them until we already output a prediction at time t. That\u2019s bad. The Solution: We essentially \u201cdouble up\u201d each RNN neuron into two independent neurons\u200a\u2014\u200aa \u201cforward\u201d neuron and a \u201cbackward\u201d neuron. The forward neuron is the same as a regular RNN neuron, which gets inputs 0..T sequentially, updating its internal state and outputting some value at each time step along the way. The backward neuron follows the same general principle, but it sees the input vectors in reverse order. We\u2019ll look at an example to make sense of all this. This is a typical recurrent neural network: at each timestep, the hidden state is updated based on the latest\u00a0input. This is a bidirectional recurrent neural network. There are two neurons: one that takes inputs like normal, and one that takes them in reverse. Their output is combined to produce one\u00a0output. Let\u2019s walk through this timestep-by-timestep. At t=0, our vanilla RNN cell takes the input, updates its hidden state, and outputs a value. Now let\u2019s look at the BiRNN: the \u201cforward\u201d half of our BiRNN neuron does exactly the same thing, but the \u201cbackward\u201d half looks through all of our inputs\u200a\u2014\u200ain reverse order, t=T..0\u200a\u2014\u200aand updates its hidden state with each one. Then when we get to the t=0 input vector, it updates its hidden state one last time and outputs a final value. We then take this final output value and combine it with the \u201cforward\u201d half (\u201ccombine\u201d is pretty loosely-defined, usually just by concatenation or addition). Moving on to t=1, our \u201cforward\u201d part reads in the next input, updates state, and outputs another value. Combined with the second-to-last output of our \u201cbackward\u201d counterpart, and we have the second output of our BiRNN neuron. Rinse and repeat. And that\u2019s the general idea. Neat, right? BiRNNs (and their more adept cousin, BiLSTMs) are used all over the place. Maybe we\u2019ll see them popping up in some of the other case studies that we\u2019ll be looking at. Autoencoders Remember when we talked about autoencoders? Turns out we can use RNNs there too! Let\u2019s refresh: what is an autoencoder? Put simply, it\u2019s a clever way of tricking a neural network to learn a useful representation of some data. Let\u2019s say we have a dataset of images of faces, and we want to compress the thousands of numbers representing RGB values of pixels down into a 500-dimensional latent vector. We construct a network as such, where the middle layer has 500 neurons: https://en.wikipedia.org/wiki/Autoencoder#/media/File:Autoencoder_structure.png \u2026and train it to reproduce the input in the output. Let\u2019s explore this idea a little further. Imagine that data is flowing through our network, starting with the input layer and through each subsequent layer. We can view each layer as performing a transformation, converting our input to another vector, and then that vector into another, until we get our output. If we train our network to reproduce the input, that means that each intermediate vector must still represent the same information as the input, in some form or another. Essentially, the activations of each layer are a new representation of our input vector. If our network trains well, we can convert a 10,000-dimensional vector of pixel values into a 500-dimensional vector of image features which can be converted back into a 10,000-dimensional vector of pixel values that approximates what the input would have been. Let\u2019s make this a tad more concrete. We have two functions, f and g. f is our encoder, mapping from an n-long vector to an m-long vector. (n is the size of our input, m is the size of our latent representation.) g is our decoder, which maps back from an m-long vector to an n-long vector. In the normal autoencoder setting, both f and g are neural networks trained jointly (or different parts of a single network, same thing really) to reconstruct x. So, where do RNNs fit in? Let\u2019s say our inputs are now sequences of vectors instead of a single vector. We can use a similar concept, with both the encoder and decoder represented using an RNN. Here\u2019s how it works: we feed our input sequence into the encoder RNN. With each input vector of the sequence, this encoder updates its internal state. Eventually, once it has seen the entire input, we have some final network internal state which represents our entire input sequence. Neat! Now, we make the hidden state of our decoder RNN the initial hidden state of our encoder, and ask it to spit out a sequence. Ideally, it spits out something close to what the initial sequence was. Going back to our math-y definitions, we see that it basically fits in to the same framework, except we have q n-long vectors going into f and coming out of g. So q n-long vectors go in to f, and a single m-long vector comes out. We then give this m-long vector back to g, which spits out q n-long vectors. That was a lot of letters, but you get the idea (I hope). Like much of deep learning, the concept itself is pretty simple, but the implications are pretty cool. We can take any sequence\u200a\u2014\u200aa variable-length sequence, mind you\u200a\u2014\u200aand convert it into a fixed-size vector. And then convert that back to a variable-length sequence. It turns out this model is actually incredibly powerful, so let\u2019s take a look at one particularly useful (and successful) application: machine translation. Neural Machine Translation Let\u2019s take these ideas we just learned about sequence-to-sequence (or seq2seq, for short) RNNs and apply them to machine translation. We throw in a sequence of words in one language, and it outputs a sequence of words in another. Simple enough, right? The model we\u2019re going to look at specifically is Google\u2019s implementation of NMT. You can read all the gory details in their paper, but for now why don\u2019t I give you the watered-down version. At it\u2019s core, the GNMT architecture is just another seq2seq model. We have an encoder, consisting of 8 LSTM layers with skip connections (the first layer is bidirectional). We also have a decoder, once again containing 8 LSTM layers with skip connections. (A skip connection in a neural network is a connection which skips a layer and connects to the next available layer.) The decoder network outputs a probability distribution of words (well, sort of\u200a\u2014\u200awe\u2019ll talk more about that later), which we sample from to get our [translated] sentence. \ud83c\udf89 Here\u2019s a scary diagram from the paper: https://arxiv.org/abs/1609.08144 But there are a few other aspects to the GNMT that are important to note (there\u2019s actually lots of interesting stuff going on in this architecture, so I really recommend you do read the paper). Let\u2019s turn our attention to the center of the above diagram. This is a critical part of the GNMT architecture (and GNMT is certainly not the first to use attention) which allows the decoder to focus on certain parts of the encoder\u2019s output as it produces output. Specifically, the GNMT architecture differs from the traditional seq2seq model in that our encoder does not produce a single fixed-width vector (the final hidden state) representing the entire output. Instead, we actually look at the output from each time step, and each time step gives us some latent representation. While decoding, we combine all of these hidden vectors into one context vector using something called soft attention. https://arxiv.org/abs/1609.08144 More concretely, that works like this (at every decoder time step). We first look at the output of the first decoder layer from the last time step. Following the notation from the paper, we\u2019ll call that yi-1. We also have a series of encoder outputs, x1\u2026xM, one for each encoder timestep. For each encoder timestep, we give our special attention function yi-1 and xt and get back a single fixed-size vector st, which we then run through a softmax. So, we\u2019ve converted our encoder information from that timestep (and some decoder information) into a single attention vector\u200a\u2014\u200athis attention vector tells us which parts of the encoder output we should look at more closely. We multiply this attention vector by our encoder output xt, which has the effect of \u201cfocusing\u201d more on certain values and less on others. Finally, we take the sum of those \u201cfocused\u201d vectors over each encoder timestep to produce our attention context for this timestep ai, which is fed to every decoder layer. Oh yeah, that attention function? That\u2019s just yet another neural network. Attention mechanisms like this one are pretty common in many deep learning architectures. This is an example of soft attention: we learn a distribution over our inputs and compute a weighted sum. This process is fully-differentiated, so we can use standard backpropogation to figure out how to train our attention model. Another possible mechanism is called hard attention, in which we select just one of the possible inputs and \u201cfocus\u201d solely on that input. This process is not differentiable, so we need to use some other algorithm (usually some kind of reinforcement learning) to train a hard attention algorithm. GNMT combines all kinds of other cool ideas to achieve state-of-the-art results, including a wordpiece model which segments words into smaller \u201cwordpieces\u201d to help translate rarer words and neat parallelization techniques that let them train this monstrosity of an architecture in reasonable time. A few months ago, Google put their GNMT model into production. Cutting-edge research is being implemented in the real world at an incredible rapid pace within the field of machine learning, and this is just one of countless examples. Long-Term Recurrent Convolutional Networks (Not to be confused with LCRNs.) The Problem: We have a sequence of images that we need to make predictions for. CNNs are good at processing images, RNNs are good at processing sequences\u2026how do we put the two together? The Solution: The solution proposed in this paper is as straightforward as you would expect: take your image, extract features using a CNN, and feed this feature vector to your LSTM. https://arxiv.org/abs/1411.4389 That\u2019s really all there is to it, and the reason it works is because (as we\u2019ve seen before) CNNs are incredibly adept at converting raw pixel data to a more meaningful representation. This saves the LSTMs the problem of parsing through the pixels to figure out what\u2019s going on in the image and allows the LSTM weights to focus on converting a vector of image features into some meaningful sequence (say, a caption). It\u2019s the same reason that using a word embedding is often preferred to a one-hot vector when feeding in words to an NLP model: the more meaningful your representation is, the easier it is to make further predictions with it. Image Captioning (To be confused with LCRNs!) So there has been a lot of really impressive work on image captioning lately, but I wanna give a special shout-out to this 2015 paper from Karpathy et al. It was one of the first ML papers I had ever read, and really got me excited about the field. And, it uses RNNs, so that\u2019s cool too. The idea behind image captioning is kind of self-explanatory, but I\u2019ll explain it anyway. You give the model an image, it gives you a caption. Which is kind of insane if you think about it\u200a\u2014\u200aa computer can go from pixels to interpreting what it\u2019s seeing, and from that generate real and grammatical sentences to explain what it sees. I still can\u2019t really believe stuff like this actually works, but somehow it does. The model described in this specific paper combines our old pal convolutional neural networks with our newly-discovered RNNs. Step 1 is to pass our image through a convolutional neural network and extract some features from the last fully-connected layer. This lets us convert our pixel representation of the image into something that\u2019s hopefully a bit more meaningful. We take this image feature vector and use it to initialize the hidden state of our RNN. This is where it gets cool. We feed our network a start token, and it gives us a word (more accurately, a distribution of words, which we sample to get the first word of our caption). We feed this word back as the next input, and sample another word from the output. And again, and again, and again, until we finally sample an end token and have a complete caption. It\u2019s not strictly necessary to feed the word that we sampled back to the network, but that\u2019s pretty common practice to help the network condition its output on the previous word (the hidden state is critical for this too, of course). The results from this particular paper were pretty cool, you can see some of the results here. Neural Machine Translation, Again Yes, NMTs are just that cool that I need to talk about them again. The Problem: With our good ol\u2019 GNMT architecture, we can train a massive model to convert from language A to language B. That\u2019s great\u200a\u2014\u200aexcept, if we support more than a hundred languages, we need to train more than 10,000 different language-pair models, each of which can take months to converge. That\u2019s no good, and it\u2019s the reason that when Google put GNMT in production, they only did so for eight language-pairs (still a monumental achievement). But\u2026what if we didn\u2019t need to train a separate model for each language pair? What if we could train one model for all the language pairs\u200a\u2014\u200aimpossible, right? The Solution: Apparently it\u2019s not impossible, and to make things even crazier, we can use the original GNMT architecture without modification. The only real change is that we prepend a special language token to the beginning of each sequence telling it what language to translate to. (We also use one shared wordpiece model for all language, instead of one per language pair.) So we\u2019ve condensed tens of thousands of NMT models into a single model that is able to translate to and from any language it was trained on. The paper elaborates on the implications and benefits of this more than I will, but to summarize: Expanding on that last point some more: the authors of the paper even found evidence of an interlingua, or an intermediate representation that is shared by multiple languages. Being able to learn an interlingua is the ideal end goal to create a fully generalized multilingual NMT: we learn an encoder/decoder to convert to/from the interlingua for each language, and we immediately know how to translate to and from that language. We aren\u2019t quite there yet, but this is a major step in that direction. Creating a larger multilingual NMT model and giving it even more data could be all it takes to achieve new state-of-the-art translation results. So, yeah RNNs are pretty awesome. There are new RNN papers published literally every day and it\u2019s impossible to cover everything\u200a\u2014\u200aif you think I missed something important, definitely let me know. (From Rohan: Except Neural Turing Machines and Learning to Learn. Those are dope, we know it, and we\u2019re going to be covering them soon!) Building a Vanilla Recurrent Neural\u00a0Network Let\u2019s get practical for a minute and see how we can build one of these things in practice. We\u2019ll stick with char-rnn (the single layer character level language model we talked about much earlier) with raw numpy so we can see the nitty-gritty details, but if you\u2019re using one of these in practice there are much better solutions! For out-of-the-box functional deep learning models Keras is the de facto framework that people seem to use. For more creative models and all kinds of other fancy stuff I\u2019m a fan of the newly-released PyTorch, or the \u201colder\u201d TensorFlow. I\u2019m going to walk us through this implementation line by line so we can see exactly what\u2019s going on. It\u2019s really well-commented, so feel free to peruse it on your own too. Afterwards, I challenge you to code an LSTM! import numpy as np Well, duh. data = open(\u2018input.txt\u2019, \u2018r\u2019).read()chars = list(set(data))data_size, vocab_size = len(data), len(chars)print \u2018data has %d characters, %d unique.\u2019 % (data_size, vocab_size)char_to_ix = { ch:i for i,ch in enumerate(chars) }ix_to_char = { i:ch for i,ch in enumerate(chars) } We load in our data and get a list of all of the characters that appear in it. We set up two dictionaries: one mapping characters to an index, and one for the reverse. We\u2019ll use this when converting characters to/from a one-hot encoding later on. hidden_size = 100seq_length = 25learning_rate = 1e-1 Typical hyperparam stuff. Our RNN layer will have a hidden size of 100, and we\u2019ll train our network on batches of 25 characters at a time. Since we\u2019ll be training our network with BPTT, we need to make sure the sequences are sufficiently short that we can unroll the network all the way and keep everything in memory. Finally, set the learning rate to\u00a0.1. Wxh = np.random.randn(hidden_size, vocab_size)*0.01 # input to hiddenWhh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hiddenWhy = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to outputbh = np.zeros((hidden_size, 1)) # hidden biasby = np.zeros((vocab_size, 1)) # output bias We set up our parameters\u200a\u2014\u200anote that this is just a typical RNN, no fancy LSTM cells. We have weight matrices for updating our hidden state with each input, updating our hidden state with each timestep, and producing an output (and biases for our hidden state + output). We could be doing some fancy weight initialization here, but some normally-distributed randomness is sufficient for breaking symmetry. Now let\u2019s talk loss function. We start by computing the forward pass, then computing the backward pass, just like with any neural network. xs, hs, ys, ps = {}, {}, {}, {}hs[-1] = np.copy(hprev)loss = 0 We start off by just setting up some variables to store our one-hot inputs, hidden states, outputs, and softmax probabilities. for t in xrange(len(inputs)): Go through each timestep, and for each timestep\u2026 xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representationxs[t][inputs[t]] = 1 Convert our input character at this timestep to a one-hot vector. hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state Update our hidden state. We saw this formula already\u200a\u2014\u200ause our Wxh and Whh matrices to update our hidden state based on the last state and our input, and add a bias. ys[t] = np.dot(Why, hs[t]) + by Compute our output\u2026 ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars \u2026and convert it to a probability distribution with a softmax. loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss) Accumulate the loss for this time step as the negative log of the predicted probability. Ideally, we would have a probability of 1 for the actual next character. If it is 1, the loss is 0, log(1) = 0. As the predicted probability approaches 0, the loss approaches inf, because log(0) = -inf. That\u2019s it for the forward pass (not bad, right? Boiled down, it\u2019s like six lines of code. Piece of cake). dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why) dbh, dby = np.zeros_like(bh), np.zeros_like(by) dhnext = np.zeros_like(hs[0]) Setting up some variables for our backward pass\u200a\u2014\u200athe gradients of our weight matrices, the gradients for our biases, and the gradients from the next timestep (we\u2019ll see how that works in a bit). for t in reversed(xrange(len(inputs))): Go through our sequence in reverse as we back up the gradients. dy = np.copy(ps[t])\u00a0dy[targets[t]] -= 1 # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here First, get the gradient of the output, dy. As it turns out, the gradient of the cross-entropy loss is really as copying over the distribution and subtracting 1 from the correct class. Remember backpropogation? When we have a weighted sum, the gradient of the weights is just the corresponding value that it is being multiplied by, because the other terms drop out and that one weight is treated as a constant. So, computing the gradient of our Why matrix is super simple: just multiply the gradient of loss w.r.t. the output (dy) by the derivative of the output w.r.t. Why (which is just the hidden state at our given timestep), and we get the derivative of the loss w.r.t. Why. dWhy += np.dot(dy, hs[t].T) Like the other gradients (except dy, of course) we accumulate these gradients over all timesteps and apply them at the end. dby += dy The derivative of loss w.r.t. output (dy) multiplied by the derivative of our output w.r.t. the bias (which is 1) gives us the derivative of our output w.r.t. the bias. So far so good. dh = np.dot(Why.T, dy) + dhnext # backprop into h We compute dL/dh using the chain rule, and accumulate it over all timesteps (hence + dhnext). We\u2019ll need this for the next step. dhraw = (1\u200a\u2014\u200ahs[t] * hs[t]) * dh # backprop through tanh nonlinearity This computes the derivative of the np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) line from earlier. dbh += dhraw Which is also our bh derivative, for the same reason that the by derivative was just dy. dWxh += np.dot(dhraw, xs[t].T)dWhh += np.dot(dhraw, hs[t-1].T) We accumulate our weight gradients. dhnext = np.dot(Whh.T, dhraw) And finally, store dh for this timestep so we can use it for the previous one. for dparam in [dWxh, dWhh, dWhy, dbh, dby]:np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients Last but not least, a little gradient clipping so we don\u2019t get no exploding gradients. return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1] And then return all the gradients so we can apply an optimizer step. And that\u2019s it for the backprop code; not too bad, right? def sample(h, seed_ix, n): This method is used for sampling a generated sequence from the network, starting with state h, first letter seed_ix, with length n. x = np.zeros((vocab_size, 1))\u00a0x[seed_ix] = 1 Set up our one-hot encoded input vector based on the seed character. ixes = [] And an array to keep track of our sequence. for t in xrange(n): To generate each character in our sequence\u2026 h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh) Update our hidden state! We saw this formula in the last function, too. y = np.dot(Why, h) + byp = np.exp(y) / np.sum(np.exp(y)) Generate our output and run it through a softmax. Again, straight from the last function. ix = np.random.choice(range(vocab_size), p=p.ravel()) Sample from our output distribution using some numpy magic. x = np.zeros((vocab_size, 1))x[ix] = 1ixes.append(ix) Convert the sampled value into a one-hot encoding and append it to the array. return ixes \u2026and of course, return the final sequence when we\u2019re done. n, p = 0, 0 n is the number of training iterations we\u2019ve done. p is the index into our training data for where we are now. mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why) Set up memory variables for the Adagrad algorithm (out of scope of this post, maybe next time\u200a\u2014\u200ait\u2019s just a variant on gradient descent). while True: Training loop. if p+seq_length+1 >= len(data) or n == 0: This is a little check to see if we need to reset our memory because we\u2019re starting back at the beginning of our data. hprev = np.zeros((hidden_size,1)) # reset RNN memory \u2026and if we are, reset the memory. p = 0 And reset the data pointer. inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]] We grab a seq_length-long piece of the data as our input to the network. At each timestep, we want to predict the next character; this means that our \u201ctargets\u201d will be the next character for each input. We get a snippet of the input the same length as the input, but offset by 1, for the target. if n % 100 == 0:sample_ix = sample(hprev, inputs[0], 200)txt = \u2018\u2019.join(ix_to_char[ix] for ix in sample_ix)print \u2018\u200a\u2014\u200a\u2014 \\n %s \\n\u200a\u2014\u200a\u2014 \u2018 % (txt, ) Here we just print to the terminal a sample every 100 training steps so we can see how its doing. Ideally, this will print out gibberish the first few times, before gradually printing out more and more reasonable language. loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev) Do a forward pass, backward pass, and get the gradients. smooth_loss = smooth_loss * 0.999 + loss * 0.001 Adagrad stuff. if n % 100 == 0: print \u2018iter %d, loss: %f\u2019 % (n, smooth_loss) # print progress Keep up with progress. for param, dparam, mem in zip([Wxh, Whh, Why, bh, by], [dWxh, dWhh, dWhy, dbh, dby], [mWxh, mWhh, mWhy, mbh, mby]):mem += dparam * dparamparam += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update More Adagrad. We should really do an article on optimization algorithms. p += seq_length # move data pointern += 1 # iteration counter Annnddd finally, we update our data pointer and iteration counter. And that\u2019s it. We have an RNN. Neat-o. Reminder: your challenge is to code an LSTM\u2026 and TensorFlow doesn\u2019t count! Conclusion Wow. That was a lot. If you came in knowing nothing or very little about recurrent neural nets, you sure as hell know a lot now. And you don\u2019t just know about something cool; you know about something very important\u200a\u2014\u200asomething that can equip you to read and understand some of the most prominent and hottest recent research papers in machine learning. Something this article didn\u2019t do so good at was making sure the calculus and derivatives were in the context of operating on vectors (because, remember, RNNs/LSTMs operate over vectors). In many cases the derivatives were in the 1-D context. It\u2019s not something you need to worry about, but you might want to look into. We\u2019re finally at the point where we can focus our energies on this blog towards cooler stuff including hot research papers like Neural Turing Machines or Learning to Learn, case studies eg. AlphaGo, other parts of machine learning and artificial intelligence (I\u2019m, Rohan, personally looking forward to optimization!), or different algorithms like GANs. There\u2019s very little compulsory content or \u201cgroundwork\u201d we need to cover anymore. So, now, we\u2019re officially onto the cool stuff. That\u2019s right. A Year Of AI is officially\u2026 cool. ", "child": "1972_1\t1972_2\t1972_3\t1972_4\t1972_5\t1972_6\t1972_71972_1\t1972_2\t1972_3\t1972_4\t1972_5\t1972_6\t1972_71972_1\t1972_2\t1972_3\t1972_4\t1972_5\t1972_6\t1972_71972_1\t1972_2\t1972_3\t1972_4\t1972_5\t1972_6\t1972_71972_1\t1972_2\t1972_3\t1972_4\t1972_5\t1972_6\t1972_71972_1\t1972_2\t1972_3\t1972_4\t1972_5\t1972_6\t1972_71972_1\t1972_2\t1972_3\t1972_4\t1972_5\t1972_6\t1972_7"}