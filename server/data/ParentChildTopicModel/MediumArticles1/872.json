{"name": "872", "parent": "", "title": "Everything about Self Driving Cars Explained for Non-Engineers", "sentences": [{"b259": "Everything about Self Driving Cars Explained for Non-Engineers"}, {"9c93": "I promise you won\u2019t have to use either Google or a dictionary while reading this. In this post I will teach you the core concepts about everything from \u201cdeep learning\u201d to \u201ccomputer vision\u201d, using dead simple English."}, {"0e08": "You probably already know what self driving cars are and that they are considered the dope shit these days, so if you don\u2019t mind I\u2019m going to skip any high school essay-ish introduction.\u00a0:)"}, {"80dd": "But I\u2019m not skipping my own introduction: Hi I\u2019m Aman, I\u2019m an engineer, and I have a low tolerance for unnecessarily \u201csophisticated\u201d talk. I write essays on Medium to make hard things simple. Simplicity is underrated."}, {"80ee": "Self driving cars (also called autonomous cars) work on the combination of 3 cool fields of technology. Here\u2019s a brief introduction to each of them, and then we will go into depth. By the end of this essay, you will know enough about all these technologies to be able to hold an intelligent conversation with an engineer or investor in these fields. Things like \u201cartificial neural networks\u201d won\u2019t sound like magic spells or sci-fi film words to you anymore."}, {"2cb0": "By the way, if you\u2019re experienced in this field please understand beforehand that some of these explanations are simplified for the interest of the larger audience, but are still quite accurate in meaning. No need for nitpicking over word usage or subtle differences. Thanks!"}, {"6004": "Computer Vision (ooooooohhhhh sounds so cool): The technology which allows the car to \u201csee\u201d its surroundings. These are the eyes and ears of the car. Basically we use good old cameras, which are the most important (a simple 2 megapixel camera can work fine), radars, which are second-most important (they throw radio waves around, and just like ultrasound you detect the waves that bounce off objects and come back), and lasers, which are cool to have but they are pretty expensive nowadays, and they don\u2019t work when its raining or foggy (also called \u201clidar\u201d, again you can say they\u2019re like radar but give a little better picture quality, and lasers can go pretty far so you have greater range of view). The lasers are usually placed in a spinning wheel on top of the car so they spin around very fast, looking at the environment around them. Here you can see a Lidar placed on top of the Google car:"}, {"c644": "Check out the Lidar sensor on top. It\u2019s a spinning laser\u00a0beam."}, {"f347": "Deep Learning: It is a technology that allows the car to make driving decisions on its own, based on information it gathered through the computer vision stuff described above. This is what trains the \u2018brain\u2019 of the car. We will go into detail about this in a minute."}, {"25c9": "By combining the two even on a basic level, you can do some pretty interesting things. Here\u2019s a project I made, detecting lane lines and other cars on the road using only a camera feed."}, {"d24f": "Robotics: You can see everything, and you can think and make decisions. But if your brain\u2019s decisions (eg: lift the left leg) can\u2019t reach the muscles of the leg, your leg won\u2019t move and you won\u2019t be able to walk. Similarly, if your car has a \u2018brain\u2019 (=a computer with deep learning software), the computer needs to connect with your car\u2019s parts to be able to control the car. Put simply, these connections and related functions make up \u2018robotics\u2019. It allows you to take the software brain\u2019s decisions and use machinery to actually turn the steering, press and release the throttle, brakes etc."}, {"cf70": "Navigation: Even after being given all the above, ultimately you still need to figure out \u201cwhere\u201d you are on the planet and the directions for where you want to go. There are several aspects to this, like GPS (your good old navigation device, which takes location information from satellites), and stored maps, etc."}, {"7ed4": "So the car controls its steering and brakes etc based on the decisions made by its brain, and these decisions are based on the information received through cameras and radars and lasers and the directions it receives from the navigation programs. This completes the whole system of a self-driving car."}, {"1f68": "Tidbit (feel free to skip)Self driving cars come in many \u201clevels\u201d, from Level 1 to Level 5, based on how independent the car is and how little human assistance it needs while driving. Oh and there\u2019s also Level 0 for cars, which needs the driver to control everything like you\u2019ve been doing it all along."}, {"990f": "Level 5 means the car will be 100% self-driving. It will not have a steering or brakes, because it\u2019s not meant to be driven by people. These cars don\u2019t exist yet, and cutting-edge cars are still at Level 3 and at most Level 4."}, {"945c": "Deep Learning Explained for Chimpanzees like\u00a0Myself"}, {"3f8e": "Let\u2019s say you were a wildlife safari enthusiast, and I was your super-idiot friend. I\u2019m going for safari in Africa next week. And you give me some advice: \u201cAman, stay away from the fucking elephants.\u201d"}, {"5188": "And I ask you back, \u201cWhat\u2019s an elephant?\u201d"}, {"be65": "You will most likely say, \u201cYou stupid fuck, elephants are... okay never mind, here\u2019s a photograph of an elephant, this is what it looks like. Stay away from them.\u201d"}, {"d2cb": "And then I go off to safari."}, {"8ada": "Next week you get to know that I still managed to run into an elephant and almost ended up getting killed. You ask me what happened."}, {"20c4": "I reply, \u201cI don\u2019t know, I did see this huge animal but it didn\u2019t look anything like the photo you showed me, so I thought it was safe to play with and I went ahead and pulled the little wagging thing. Here\u2019s the photograph of the animal I took before that\u2026\u201d"}, {"ea66": "You:\u00a0\u2026\u2026\u2026."}, {"1e0a": "You: \u201cOkay, Aman. I\u2019m sorry, my bad. I expected too much from your brain. Let me give you a \u201ccheat code\u201d which you need to follow when you\u2019re on the safari next time. If you see anything that looks brown-ish from all angles, seems to have four leathery legs like pillars, large flapping ears, and a thick long nose coming out of its face like a big tube, and is fat and bigger than you are, then that\u2019s an elephant and you need to stay away.\u201d"}, {"18aa": "Next month I go back to safari again and I don\u2019t run into any elephants this time, because your \u201ccheat code\u201d works well."}, {"cfff": "How did you come up with that \u201ccheat code\u201d? It\u2019s because you\u2019ve already seen an elephant from all different sides, and you picked some features of an elephant which remain pretty same regardless of which angle you view the elephant from. So you had lots of data about elephants to think about, and that helped you to form a mental picture of the most obvious signs of an elephant, and gave them to me as a cheat code. Realize that I don\u2019t have to really \u201cknow\u201d exactly what an elephant is, I just have a cheat code that helps me recognize an elephant. But that cheat code works almost as well as knowing what elephants are!"}, {"bf83": "But why wasn\u2019t it okay to just show me one photograph (the one you showed earlier) and assume it was enough for me to get the idea? Because I (being an idiot of course) took that photograph as the \u201choly truth\u201d\u200a\u2014\u200aI assumed that every elephant will look *exactly* the same as that photograph, and will be a near perfect match."}, {"e929": "Deep Learning works in a VERY similar way."}, {"f27d": "Here\u2019s the basis of deep learning: artificial neural networks, which I\u2019ll explain now. They are also called deep neural networks."}, {"4ed3": "First, I assume you know a little bit of math from high school. You know what a matrix is, right? And that you can multiply a matrix with some other matrix? Here\u2019s a refresher:"}, {"6f17": "Remember what a matrix is."}, {"d234": "An artificial neural network (ANN) is some really fancy stuff but I\u2019ll take you on the journey with baby steps. You see, the human brain is made up of a network of many cells called \u201cneurons\u201d, which is the inspiration for ANNs. This is what it looks like on paper:"}, {"7b2c": "An ANN is a magic box, which takes in an input and gives out an output. For example, suppose you wanted a magic ANN which takes in a photograph and can tell if the photograph is of an elephant or not. You put a photo into the box, and out comes an answer \u201cyes\u201d or \u201cno\u201d."}, {"6344": "Or, you put in a photograph of the road ahead and you want the ANN to tell you if you should slow down or speed up. And the answer comes out, \u201cspeed up!\u201d"}, {"5b51": "How does it happen? How can you create this ANN?"}, {"382b": "Put simply, an ANN is like a simple version of a human brain. First you train it with data. When you give data to an ANN, it creates a \u201ccheat code\u201d which helps it make decisions next time. (I\u2019ll give a detailed example in a second) This \u201ccheat code\u201d is called \u201cweights\u201d of the ANN. You saw that \u2018matrix\u2019 earlier? You can say that the first matrix [x y] matrix is an input, and the second matrix [u w] matrix is the set of weights or cheat codes. When you multiply the input with the weights, you get an answer."}, {"dec4": "The ANN can be of many different varieties, and based on how you design it, can either give a \u201cyes or no\u201d answer, or, for example, it could give a specific number, or a list of different numbers, etc. You can choose what kind of output it will give and what kind of input it will receive, and how large and complex it will be, which makes it extremely versatile. You can build neural networks that take videos as inputs, voice samples, images, or text paragraphs, etc. Of course, all these are converted into numbers on a computer automatically, and there will be a huge matrix of \u201cweights\u201d which will be multiplied to that input, generating an output which is your answer."}, {"9ce9": "But I\u2019m sure you still don\u2019t understand completely. How do you \u201ctrain\u201d the ANN to give you meaningful outputs?"}, {"f6fb": "Let\u2019s say you want to train an ANN to recognize elephants."}, {"b16b": "So you create a fresh ANN which takes a certain type of input and gives out a certain type of output. At first, this ANN doesn\u2019t really know anything about the problem it\u2019s trying to solve. It\u2019s just like me from the original elephant example\u200a\u2014\u200abefore you showed me the first photograph of an elephant, I had no idea what an elephant even was. If you asked me right there and then to look for an elephant in a photograph, I\u2019d probably have picked something randomly. So you can say that initially, I had some random bullshit cheat code right? But the moment you show me ONE elephant and tell me that it is an elephant, suddenly I adjust my random cheat code and now I have some idea of what elephants are. My cheat code is not random anymore, and if I saw an elephant from the front I\u2019d probably pick the right answer! So I have updated my cheat code when I was given more information. And the more different photographs of elephants I have, the better my cheat code becomes at giving the right answer. I still don\u2019t know \u201cwhat\u201d an elephant is, but I\u2019ve seen enough correlation in different photographs to recognize the most special features of an elephant and stay away from them."}, {"3c87": "Similarly, when you first create an ANN, you start by giving it a random set of weights (=cheat code) to start with. Then you show it an input image (photograph of an elephant), and also tell it the output you expect (answer \u201cyes\u201d). The beauty of ANNs is that if you give it an example input and its corresponding output, then it will adjust its cheat code so that it can repeat that correct answer next time. It will change the numbers in that matrix of weights the more data you give to it, to be able to match your answers. This is why, you can start with a random set of weights and with time the ANN will adjust them so that they work well as a cheat code! Cool right? That\u2019s how the ANN \u201clearns\u201d. The more data you give it, the more it adjusts its weights, and the more accurate it becomes."}, {"e7d0": "As I explained earlier already, how this happens is through matrix multiplication. The matrix of weights for this ANN will have values such that if you multiply it with your input image (the image will be converted into numbers), you can get an answer."}, {"02c9": "Rough matrix representation (I made this diagram myself)\u00a0:)"}, {"a46e": "But that\u2019s not enough. Remember that the ANN is not smart\u200a\u2014\u200ait works on cheat codes, after all. If you only gave it elephant photographs to look at, and keep saying \u201cyes\u201d for every photo, what do you think it will do?"}, {"8bc0": "It will be lazy and assume that every photograph is an elephant! It will adjust its weights (cheat code) in such a way that regardless of the photo it is given, it will always output a \u201cyes\u201d. This is called \u201cunderfitting\u201d, which basically means you made the mistake of thinking that your idiot friend is smart. Your ANN has become biased towards a particular answer. To prevent this, you need to also feed it counter-examples which have the answer \u201cno\u201d. So you also mix in lots of example photos which are not of elephants, but rather lions and cows and human beings and squirrels and giant turtles and sea horses and\u2026 (sorry, I got carried away) and now the ANN is forced to adjust its cheat code more carefully. Now you\u2019re increasing its accuracy and making it more reliable! Or, you can also have \u201coverfitting\u201d, when the ANN\u2019s cheat code is still lazy but it\u2019s now so super smart that it begins to memorize the entire training data you give it! This often happens when your ANN is very large and deep, so in a way it just begins to store all the information you give it. It becomes super good at answering on your training data but it fails when given a new example that it hasn\u2019t encountered before. It\u2019s no longer working like a cheat code, but rather it has become like a dictionary of images. You can prevent this by reducing the size of your model (by randomly dropping some of the weights), or by increasing the variety of data. The former is called \u201cdropout\u201d (pretty straightforward), and the latter is called \u201cbalancing the dataset\u201d."}, {"c7f4": "You have also learned your lesson from the safari example earlier, and now you will be careful to show it other photographs of elephants from many different angles so that it doesn\u2019t make the same mistake that your idiot friend made. This is why researchers/engineers often \u201caugment\u201d their training data sets to include mirror reflections, and brighter/darker versions of the same images, etc, to increase the variety of their data. This helps make sure that the ANN can become as accurate as it can. You want the cheat code to be so super \u201crobust\u201d that it works as well as real humans, or better."}, {"fb10": "By the end of the training, with enough balanced data and augmentation etc, you will have a trained ANN (also called trained model) which can recognize elephants in images with reasonable accuracy, and also know when not to recognize an elephant."}, {"5e87": "\u201cOverfitting\u201d, the word you encountered just two paragraphs ago (=network becoming too smart and acting like it has just memorized your data), is a very common term in the Deep Learning lingo and you now know what it means. (Underfitting is a less troubling problem because it\u2019s fairly obvious to diagnose and easy to tackle.) Someone might say, \u201coh man my model isn\u2019t working so well on new data, I guess it is overfitting\u201d and you\u2019ll say, \u201cdid you try to augment the training data?\u201d and you will instantly own the night. The other person might even assume you\u2019re a deep learning engineer yourself! (If they do, tell them \u201cOh no, I just follow Aman on Medium\u201d. When they ask who I am, please pretend and say \u201coh you don\u2019t know? He\u2019s the coolest guy in this space. Here\u2019s the link\u2026\u201d)."}, {"0ea9": "The process of adjusting weights (updating the cheat code based on data) involves something called backpropagation. While training, whenever you show the ANN an example input (let\u2019s say a kitten) and then tell it the answer (\u201cno it\u2019s not an elephant!\u201d), it will first try to use its current cheat code to come up with its own answer. If the answer is RIGHT, then it doesn\u2019t need to adjust its cheat code, right? The lazy ANN will update its cheat code only when it makes a mistake. So only if the ANN\u2019s answer was WRONG, it will have to adjust its weights (=cheat code) by a tiny bit and test the weights again. This adjustment algorithm is called BACKPROPAGATION. The \u201cmistake\u201d made by the ANN sends ripples through the matrix of weights, changing many of their values. So you can say that the error propagates back through the network. Again, this is why you can always create the ANN with random weights initially, and over time it will adjust them on its own."}, {"d53d": "Put simply, if you take a cheat code to an exam, and get some of your answers wrong, then you will likely upgrade your cheat codes for the next exam in that subject to have higher accuracy. The process of editing/rewriting your cheat code after every exam is called backpropagation."}, {"d955": "Now scroll up to the graph image of a \u201cneural network\u201d I pasted above. You see that thing called the \u201chidden layer\u201d? Well, the ANN is made of layers of neurons, these neurons carry the weights as values. They are called \u201chidden layers\u201d because you don\u2019t need to know the exact weights in the network! You can print the values out, for sure\u200a\u2014\u200abut it\u2019s useless to look at a matrix which can run into millions or billions of small numbers. It\u2019s a cheat code, and it updates itself based on the inputs you give it, and that\u2019s all you need to know."}, {"9069": "This is why deep learning is often considered a \u201cblack box\u201d system. If you\u2019ve ever programmed before in any language, you are used to writing down explicit instructions for the program for everything. But here, you can\u2019t see the underlying \u2018cheat code\u2019 the computer created for itself to use in place of a written algorithm."}, {"1510": "Coming back to self-driving cars, let\u2019s say you took photographs from different cameras around the car at a particular millisecond, and took all the data collected from the radar and lidar (=lasers) too, and combined them together in a list, and used that list as the input for your car\u2019s ANN. And the output you expect from the network is a long list of the steering angle, the throttle/braking value, whether to light up the headlights or not, whether to honk or not, etc."}, {"3efd": "So here\u2019s how you will \u201ctrain\u201d the car\u2019s brain. First you will simply drive the car normally by yourself, but keep collecting the input data from sensors and cameras etc. There is also equipment which measures, for each millisecond of data, the steering angle and throttle pressure etc that you chose. Then once you get back home, you can start \u2018training\u2019 the ANN on all that data you collected while driving. The ANN will update its cheat code to take your computer vision input and try to mimic your driving decisions as closely as possible. This is known as behavioural cloning, and it is what most car companies are doing nowadays\u200a\u2014\u200acollecting driving data, and making their cars \u2018practice\u2019. After every training trip, the car will become better and better at making decisions."}, {"3ba9": "Everything else being similar, driving data is the biggest winning factor in the race to develop self-driving cars."}, {"948e": "Now\u200a\u2014\u200ayou know what \u201cDeep Learning\u201d is, and what it does. You know what deep neural networks are (= ANNs), and how they work (weights = cheat codes, backpropagation = adjusting the weights based on a given example test). You also know that you should have a large and balanced dataset so that the network doesn\u2019t overfit (get lazy or too smart in setting its cheat codes)."}, {"3684": "Congratulations! I kid you not, this is more of an achievement than you think!"}, {"23ca": "Computer Vision"}, {"98e6": "Remember this scene from The Terminator?"}, {"d9c9": "Schwarzenegger enters a bar naked, and looks for someone to take clothes from. This must be what comes to your mind when you think \u201ccomputer vision\u201d, and you are partially right."}, {"4fca": "Self driving cars can also see the world like that, but they are still nowhere near the sophistication you can see there. The major purpose of computer vision techniques is to process the camera images to detect lane lines, track other vehicles and pedestrians, look for any bumps or holes in the road, measure the distance between the car and other objects etc."}, {"67c8": "There\u2019s no one technique or underlying technology for computer vision\u200a\u2014\u200arather, often it comes down to simple image processing. You are limited only by your imagination and ability to design complex algorithms, and if you\u2019re good at geometry that is another plus. For example, a popular way of processing the camera images involves looking at how quickly the colors change in horizontal and vertical directions. That is called a \u201cgradient\u201d and it can be used to find edges. Here\u2019s an example:"}, {"d1bd": "These are all created by using simple techniques using a popular library of computer vision functions, called OpenCV. You should know about OpenCV, because it is the most popular library for this purpose. Every robotics or deep learning guy knows about it."}, {"48b8": "Taking the above example further, the images you see are simply a representation of ones and zeros on a rectangle. The ones are white, and the zeros are black! You can select the pixels in any one region of the picture and play around with it. This is how I detected lane lines and came up with my project output as seen at the beginning of this blog post."}, {"b9a5": "Another common technique used in computer vision to detect objects is called stereo vision. Don\u2019t be thrown off by the seemingly complex name, stereo vision simply means looking at something with more than one eye (you do it all the time). When you see something with two eyes, you have a better estimate of how far it is and what shape it has. Similarly, cars use more than one camera on either side and combine the images to see the world more realistically."}, {"46be": "What about adverse conditions like snow, rain, etc when it\u2019s difficult to see the road? Well, let\u2019s not forget that the other two sensors (radar and lasers). Even when any two sensors don\u2019t work so well, the system is designed such that third one should still be quite reliable (but it\u2019s an ongoing field of research and we\u2019re not perfect yet). Moreover, if the car\u2019s neural networks have already been trained to drive in such conditions, the car should have some idea of how to make decisions. Again, as I said earlier, the foremost priority is about collecting enough data. The more you train, the better the car gets. Period."}, {"3d8f": "I won\u2019t go into more detail about computer vision, as it is a very broad field. Just know that it\u2019s not sci-fi, and there\u2019s nothing \u201cmagical\u201d about it. It\u2019s just about using your imagination to play with images. There are other techniques, which you can read about in this very short and simple page:"}, {"f828": "NOVA | The Great Robot Race | What Robots See (non-Flash) | PBS(Printable) In this slide show, learn about some of the world's largest and most ambitious darpa detection experiments.www.pbs.org"}, {"ee2c": "Robotics and Navigation"}, {"546e": "Actually, I\u2019m not going to spend too much time here. Robotics is pretty straightforward\u200a\u2014\u200ayou just need to know about something called an actuator."}, {"e887": "An actuator is a device which takes an electrical signal as input, and converts it into a physical action. It\u2019s not too complicated, often it just has a motor inside it that rotates by a certain angle based on the value of the signal it receives. Actuators come in all shapes and sizes, there will be one in the steering wheel, one for the throttle, brakes, gears, the engine etc. You get the idea."}, {"ae5f": "For Navigation, apart from the GPS/maps/computer vision technologies, you should also know about a really cool technique called \u201cdead reckoning\u201d. It involves calculating your current position based on your speed and distance travelled and knowing the history of all the turns you made up until the present moment, etc. You remember the sucky Sherlock Holmes movie with Robert Downey Jr? There\u2019s a scene in which he gets kidnapped and taken inside a horse carriage and blindfolded, but after they arrive at the destination he magically knows exactly where he is. Do watch the first few minutes of this scene and you\u2019ll understand what \u201cdead reckoning\u201d is. The system works pretty much like Sherlock Holmes."}, {"05bc": "Alright! Now you know should have a very good idea about how self-driving cars work. Here is what we have\u00a0learned:"}, {"f760": "So\u2026 do you feel more dangerous and awesome now than 20 minutes ago?\u00a0:)"}, {"ef72": "If yes, let me know and I\u2019ll go to sleep feeling good about helping someone."}, {"b52b": "Stay awesome, and do leave your email here if you\u2019d like to stay in touch!"}], "content": "Everything about Self Driving Cars Explained for Non-Engineers I promise you won\u2019t have to use either Google or a dictionary while reading this. In this post I will teach you the core concepts about everything from \u201cdeep learning\u201d to \u201ccomputer vision\u201d, using dead simple English. You probably already know what self driving cars are and that they are considered the dope shit these days, so if you don\u2019t mind I\u2019m going to skip any high school essay-ish introduction.\u00a0:) But I\u2019m not skipping my own introduction: Hi I\u2019m Aman, I\u2019m an engineer, and I have a low tolerance for unnecessarily \u201csophisticated\u201d talk. I write essays on Medium to make hard things simple. Simplicity is underrated. Self driving cars (also called autonomous cars) work on the combination of 3 cool fields of technology. Here\u2019s a brief introduction to each of them, and then we will go into depth. By the end of this essay, you will know enough about all these technologies to be able to hold an intelligent conversation with an engineer or investor in these fields. Things like \u201cartificial neural networks\u201d won\u2019t sound like magic spells or sci-fi film words to you anymore. By the way, if you\u2019re experienced in this field please understand beforehand that some of these explanations are simplified for the interest of the larger audience, but are still quite accurate in meaning. No need for nitpicking over word usage or subtle differences. Thanks! Computer Vision (ooooooohhhhh sounds so cool): The technology which allows the car to \u201csee\u201d its surroundings. These are the eyes and ears of the car. Basically we use good old cameras, which are the most important (a simple 2 megapixel camera can work fine), radars, which are second-most important (they throw radio waves around, and just like ultrasound you detect the waves that bounce off objects and come back), and lasers, which are cool to have but they are pretty expensive nowadays, and they don\u2019t work when its raining or foggy (also called \u201clidar\u201d, again you can say they\u2019re like radar but give a little better picture quality, and lasers can go pretty far so you have greater range of view). The lasers are usually placed in a spinning wheel on top of the car so they spin around very fast, looking at the environment around them. Here you can see a Lidar placed on top of the Google car: Check out the Lidar sensor on top. It\u2019s a spinning laser\u00a0beam. Deep Learning: It is a technology that allows the car to make driving decisions on its own, based on information it gathered through the computer vision stuff described above. This is what trains the \u2018brain\u2019 of the car. We will go into detail about this in a minute. By combining the two even on a basic level, you can do some pretty interesting things. Here\u2019s a project I made, detecting lane lines and other cars on the road using only a camera feed. Robotics: You can see everything, and you can think and make decisions. But if your brain\u2019s decisions (eg: lift the left leg) can\u2019t reach the muscles of the leg, your leg won\u2019t move and you won\u2019t be able to walk. Similarly, if your car has a \u2018brain\u2019 (=a computer with deep learning software), the computer needs to connect with your car\u2019s parts to be able to control the car. Put simply, these connections and related functions make up \u2018robotics\u2019. It allows you to take the software brain\u2019s decisions and use machinery to actually turn the steering, press and release the throttle, brakes etc. Navigation: Even after being given all the above, ultimately you still need to figure out \u201cwhere\u201d you are on the planet and the directions for where you want to go. There are several aspects to this, like GPS (your good old navigation device, which takes location information from satellites), and stored maps, etc. So the car controls its steering and brakes etc based on the decisions made by its brain, and these decisions are based on the information received through cameras and radars and lasers and the directions it receives from the navigation programs. This completes the whole system of a self-driving car. Tidbit (feel free to skip)Self driving cars come in many \u201clevels\u201d, from Level 1 to Level 5, based on how independent the car is and how little human assistance it needs while driving. Oh and there\u2019s also Level 0 for cars, which needs the driver to control everything like you\u2019ve been doing it all along. Level 5 means the car will be 100% self-driving. It will not have a steering or brakes, because it\u2019s not meant to be driven by people. These cars don\u2019t exist yet, and cutting-edge cars are still at Level 3 and at most Level 4. Deep Learning Explained for Chimpanzees like\u00a0Myself Let\u2019s say you were a wildlife safari enthusiast, and I was your super-idiot friend. I\u2019m going for safari in Africa next week. And you give me some advice: \u201cAman, stay away from the fucking elephants.\u201d And I ask you back, \u201cWhat\u2019s an elephant?\u201d You will most likely say, \u201cYou stupid fuck, elephants are... okay never mind, here\u2019s a photograph of an elephant, this is what it looks like. Stay away from them.\u201d And then I go off to safari. Next week you get to know that I still managed to run into an elephant and almost ended up getting killed. You ask me what happened. I reply, \u201cI don\u2019t know, I did see this huge animal but it didn\u2019t look anything like the photo you showed me, so I thought it was safe to play with and I went ahead and pulled the little wagging thing. Here\u2019s the photograph of the animal I took before that\u2026\u201d You:\u00a0\u2026\u2026\u2026. You: \u201cOkay, Aman. I\u2019m sorry, my bad. I expected too much from your brain. Let me give you a \u201ccheat code\u201d which you need to follow when you\u2019re on the safari next time. If you see anything that looks brown-ish from all angles, seems to have four leathery legs like pillars, large flapping ears, and a thick long nose coming out of its face like a big tube, and is fat and bigger than you are, then that\u2019s an elephant and you need to stay away.\u201d Next month I go back to safari again and I don\u2019t run into any elephants this time, because your \u201ccheat code\u201d works well. How did you come up with that \u201ccheat code\u201d? It\u2019s because you\u2019ve already seen an elephant from all different sides, and you picked some features of an elephant which remain pretty same regardless of which angle you view the elephant from. So you had lots of data about elephants to think about, and that helped you to form a mental picture of the most obvious signs of an elephant, and gave them to me as a cheat code. Realize that I don\u2019t have to really \u201cknow\u201d exactly what an elephant is, I just have a cheat code that helps me recognize an elephant. But that cheat code works almost as well as knowing what elephants are! But why wasn\u2019t it okay to just show me one photograph (the one you showed earlier) and assume it was enough for me to get the idea? Because I (being an idiot of course) took that photograph as the \u201choly truth\u201d\u200a\u2014\u200aI assumed that every elephant will look *exactly* the same as that photograph, and will be a near perfect match. Deep Learning works in a VERY similar way. Here\u2019s the basis of deep learning: artificial neural networks, which I\u2019ll explain now. They are also called deep neural networks. First, I assume you know a little bit of math from high school. You know what a matrix is, right? And that you can multiply a matrix with some other matrix? Here\u2019s a refresher: Remember what a matrix is. An artificial neural network (ANN) is some really fancy stuff but I\u2019ll take you on the journey with baby steps. You see, the human brain is made up of a network of many cells called \u201cneurons\u201d, which is the inspiration for ANNs. This is what it looks like on paper: An ANN is a magic box, which takes in an input and gives out an output. For example, suppose you wanted a magic ANN which takes in a photograph and can tell if the photograph is of an elephant or not. You put a photo into the box, and out comes an answer \u201cyes\u201d or \u201cno\u201d. Or, you put in a photograph of the road ahead and you want the ANN to tell you if you should slow down or speed up. And the answer comes out, \u201cspeed up!\u201d How does it happen? How can you create this ANN? Put simply, an ANN is like a simple version of a human brain. First you train it with data. When you give data to an ANN, it creates a \u201ccheat code\u201d which helps it make decisions next time. (I\u2019ll give a detailed example in a second) This \u201ccheat code\u201d is called \u201cweights\u201d of the ANN. You saw that \u2018matrix\u2019 earlier? You can say that the first matrix [x y] matrix is an input, and the second matrix [u w] matrix is the set of weights or cheat codes. When you multiply the input with the weights, you get an answer. The ANN can be of many different varieties, and based on how you design it, can either give a \u201cyes or no\u201d answer, or, for example, it could give a specific number, or a list of different numbers, etc. You can choose what kind of output it will give and what kind of input it will receive, and how large and complex it will be, which makes it extremely versatile. You can build neural networks that take videos as inputs, voice samples, images, or text paragraphs, etc. Of course, all these are converted into numbers on a computer automatically, and there will be a huge matrix of \u201cweights\u201d which will be multiplied to that input, generating an output which is your answer. But I\u2019m sure you still don\u2019t understand completely. How do you \u201ctrain\u201d the ANN to give you meaningful outputs? Let\u2019s say you want to train an ANN to recognize elephants. So you create a fresh ANN which takes a certain type of input and gives out a certain type of output. At first, this ANN doesn\u2019t really know anything about the problem it\u2019s trying to solve. It\u2019s just like me from the original elephant example\u200a\u2014\u200abefore you showed me the first photograph of an elephant, I had no idea what an elephant even was. If you asked me right there and then to look for an elephant in a photograph, I\u2019d probably have picked something randomly. So you can say that initially, I had some random bullshit cheat code right? But the moment you show me ONE elephant and tell me that it is an elephant, suddenly I adjust my random cheat code and now I have some idea of what elephants are. My cheat code is not random anymore, and if I saw an elephant from the front I\u2019d probably pick the right answer! So I have updated my cheat code when I was given more information. And the more different photographs of elephants I have, the better my cheat code becomes at giving the right answer. I still don\u2019t know \u201cwhat\u201d an elephant is, but I\u2019ve seen enough correlation in different photographs to recognize the most special features of an elephant and stay away from them. Similarly, when you first create an ANN, you start by giving it a random set of weights (=cheat code) to start with. Then you show it an input image (photograph of an elephant), and also tell it the output you expect (answer \u201cyes\u201d). The beauty of ANNs is that if you give it an example input and its corresponding output, then it will adjust its cheat code so that it can repeat that correct answer next time. It will change the numbers in that matrix of weights the more data you give to it, to be able to match your answers. This is why, you can start with a random set of weights and with time the ANN will adjust them so that they work well as a cheat code! Cool right? That\u2019s how the ANN \u201clearns\u201d. The more data you give it, the more it adjusts its weights, and the more accurate it becomes. As I explained earlier already, how this happens is through matrix multiplication. The matrix of weights for this ANN will have values such that if you multiply it with your input image (the image will be converted into numbers), you can get an answer. Rough matrix representation (I made this diagram myself)\u00a0:) But that\u2019s not enough. Remember that the ANN is not smart\u200a\u2014\u200ait works on cheat codes, after all. If you only gave it elephant photographs to look at, and keep saying \u201cyes\u201d for every photo, what do you think it will do? It will be lazy and assume that every photograph is an elephant! It will adjust its weights (cheat code) in such a way that regardless of the photo it is given, it will always output a \u201cyes\u201d. This is called \u201cunderfitting\u201d, which basically means you made the mistake of thinking that your idiot friend is smart. Your ANN has become biased towards a particular answer. To prevent this, you need to also feed it counter-examples which have the answer \u201cno\u201d. So you also mix in lots of example photos which are not of elephants, but rather lions and cows and human beings and squirrels and giant turtles and sea horses and\u2026 (sorry, I got carried away) and now the ANN is forced to adjust its cheat code more carefully. Now you\u2019re increasing its accuracy and making it more reliable! Or, you can also have \u201coverfitting\u201d, when the ANN\u2019s cheat code is still lazy but it\u2019s now so super smart that it begins to memorize the entire training data you give it! This often happens when your ANN is very large and deep, so in a way it just begins to store all the information you give it. It becomes super good at answering on your training data but it fails when given a new example that it hasn\u2019t encountered before. It\u2019s no longer working like a cheat code, but rather it has become like a dictionary of images. You can prevent this by reducing the size of your model (by randomly dropping some of the weights), or by increasing the variety of data. The former is called \u201cdropout\u201d (pretty straightforward), and the latter is called \u201cbalancing the dataset\u201d. You have also learned your lesson from the safari example earlier, and now you will be careful to show it other photographs of elephants from many different angles so that it doesn\u2019t make the same mistake that your idiot friend made. This is why researchers/engineers often \u201caugment\u201d their training data sets to include mirror reflections, and brighter/darker versions of the same images, etc, to increase the variety of their data. This helps make sure that the ANN can become as accurate as it can. You want the cheat code to be so super \u201crobust\u201d that it works as well as real humans, or better. By the end of the training, with enough balanced data and augmentation etc, you will have a trained ANN (also called trained model) which can recognize elephants in images with reasonable accuracy, and also know when not to recognize an elephant. \u201cOverfitting\u201d, the word you encountered just two paragraphs ago (=network becoming too smart and acting like it has just memorized your data), is a very common term in the Deep Learning lingo and you now know what it means. (Underfitting is a less troubling problem because it\u2019s fairly obvious to diagnose and easy to tackle.) Someone might say, \u201coh man my model isn\u2019t working so well on new data, I guess it is overfitting\u201d and you\u2019ll say, \u201cdid you try to augment the training data?\u201d and you will instantly own the night. The other person might even assume you\u2019re a deep learning engineer yourself! (If they do, tell them \u201cOh no, I just follow Aman on Medium\u201d. When they ask who I am, please pretend and say \u201coh you don\u2019t know? He\u2019s the coolest guy in this space. Here\u2019s the link\u2026\u201d). The process of adjusting weights (updating the cheat code based on data) involves something called backpropagation. While training, whenever you show the ANN an example input (let\u2019s say a kitten) and then tell it the answer (\u201cno it\u2019s not an elephant!\u201d), it will first try to use its current cheat code to come up with its own answer. If the answer is RIGHT, then it doesn\u2019t need to adjust its cheat code, right? The lazy ANN will update its cheat code only when it makes a mistake. So only if the ANN\u2019s answer was WRONG, it will have to adjust its weights (=cheat code) by a tiny bit and test the weights again. This adjustment algorithm is called BACKPROPAGATION. The \u201cmistake\u201d made by the ANN sends ripples through the matrix of weights, changing many of their values. So you can say that the error propagates back through the network. Again, this is why you can always create the ANN with random weights initially, and over time it will adjust them on its own. Put simply, if you take a cheat code to an exam, and get some of your answers wrong, then you will likely upgrade your cheat codes for the next exam in that subject to have higher accuracy. The process of editing/rewriting your cheat code after every exam is called backpropagation. Now scroll up to the graph image of a \u201cneural network\u201d I pasted above. You see that thing called the \u201chidden layer\u201d? Well, the ANN is made of layers of neurons, these neurons carry the weights as values. They are called \u201chidden layers\u201d because you don\u2019t need to know the exact weights in the network! You can print the values out, for sure\u200a\u2014\u200abut it\u2019s useless to look at a matrix which can run into millions or billions of small numbers. It\u2019s a cheat code, and it updates itself based on the inputs you give it, and that\u2019s all you need to know. This is why deep learning is often considered a \u201cblack box\u201d system. If you\u2019ve ever programmed before in any language, you are used to writing down explicit instructions for the program for everything. But here, you can\u2019t see the underlying \u2018cheat code\u2019 the computer created for itself to use in place of a written algorithm. Coming back to self-driving cars, let\u2019s say you took photographs from different cameras around the car at a particular millisecond, and took all the data collected from the radar and lidar (=lasers) too, and combined them together in a list, and used that list as the input for your car\u2019s ANN. And the output you expect from the network is a long list of the steering angle, the throttle/braking value, whether to light up the headlights or not, whether to honk or not, etc. So here\u2019s how you will \u201ctrain\u201d the car\u2019s brain. First you will simply drive the car normally by yourself, but keep collecting the input data from sensors and cameras etc. There is also equipment which measures, for each millisecond of data, the steering angle and throttle pressure etc that you chose. Then once you get back home, you can start \u2018training\u2019 the ANN on all that data you collected while driving. The ANN will update its cheat code to take your computer vision input and try to mimic your driving decisions as closely as possible. This is known as behavioural cloning, and it is what most car companies are doing nowadays\u200a\u2014\u200acollecting driving data, and making their cars \u2018practice\u2019. After every training trip, the car will become better and better at making decisions. Everything else being similar, driving data is the biggest winning factor in the race to develop self-driving cars. Now\u200a\u2014\u200ayou know what \u201cDeep Learning\u201d is, and what it does. You know what deep neural networks are (= ANNs), and how they work (weights = cheat codes, backpropagation = adjusting the weights based on a given example test). You also know that you should have a large and balanced dataset so that the network doesn\u2019t overfit (get lazy or too smart in setting its cheat codes). Congratulations! I kid you not, this is more of an achievement than you think! Computer Vision Remember this scene from The Terminator? Schwarzenegger enters a bar naked, and looks for someone to take clothes from. This must be what comes to your mind when you think \u201ccomputer vision\u201d, and you are partially right. Self driving cars can also see the world like that, but they are still nowhere near the sophistication you can see there. The major purpose of computer vision techniques is to process the camera images to detect lane lines, track other vehicles and pedestrians, look for any bumps or holes in the road, measure the distance between the car and other objects etc. There\u2019s no one technique or underlying technology for computer vision\u200a\u2014\u200arather, often it comes down to simple image processing. You are limited only by your imagination and ability to design complex algorithms, and if you\u2019re good at geometry that is another plus. For example, a popular way of processing the camera images involves looking at how quickly the colors change in horizontal and vertical directions. That is called a \u201cgradient\u201d and it can be used to find edges. Here\u2019s an example: These are all created by using simple techniques using a popular library of computer vision functions, called OpenCV. You should know about OpenCV, because it is the most popular library for this purpose. Every robotics or deep learning guy knows about it. Taking the above example further, the images you see are simply a representation of ones and zeros on a rectangle. The ones are white, and the zeros are black! You can select the pixels in any one region of the picture and play around with it. This is how I detected lane lines and came up with my project output as seen at the beginning of this blog post. Another common technique used in computer vision to detect objects is called stereo vision. Don\u2019t be thrown off by the seemingly complex name, stereo vision simply means looking at something with more than one eye (you do it all the time). When you see something with two eyes, you have a better estimate of how far it is and what shape it has. Similarly, cars use more than one camera on either side and combine the images to see the world more realistically. What about adverse conditions like snow, rain, etc when it\u2019s difficult to see the road? Well, let\u2019s not forget that the other two sensors (radar and lasers). Even when any two sensors don\u2019t work so well, the system is designed such that third one should still be quite reliable (but it\u2019s an ongoing field of research and we\u2019re not perfect yet). Moreover, if the car\u2019s neural networks have already been trained to drive in such conditions, the car should have some idea of how to make decisions. Again, as I said earlier, the foremost priority is about collecting enough data. The more you train, the better the car gets. Period. I won\u2019t go into more detail about computer vision, as it is a very broad field. Just know that it\u2019s not sci-fi, and there\u2019s nothing \u201cmagical\u201d about it. It\u2019s just about using your imagination to play with images. There are other techniques, which you can read about in this very short and simple page: NOVA | The Great Robot Race | What Robots See (non-Flash) | PBS(Printable) In this slide show, learn about some of the world's largest and most ambitious darpa detection experiments.www.pbs.org Robotics and Navigation Actually, I\u2019m not going to spend too much time here. Robotics is pretty straightforward\u200a\u2014\u200ayou just need to know about something called an actuator. An actuator is a device which takes an electrical signal as input, and converts it into a physical action. It\u2019s not too complicated, often it just has a motor inside it that rotates by a certain angle based on the value of the signal it receives. Actuators come in all shapes and sizes, there will be one in the steering wheel, one for the throttle, brakes, gears, the engine etc. You get the idea. For Navigation, apart from the GPS/maps/computer vision technologies, you should also know about a really cool technique called \u201cdead reckoning\u201d. It involves calculating your current position based on your speed and distance travelled and knowing the history of all the turns you made up until the present moment, etc. You remember the sucky Sherlock Holmes movie with Robert Downey Jr? There\u2019s a scene in which he gets kidnapped and taken inside a horse carriage and blindfolded, but after they arrive at the destination he magically knows exactly where he is. Do watch the first few minutes of this scene and you\u2019ll understand what \u201cdead reckoning\u201d is. The system works pretty much like Sherlock Holmes. Alright! Now you know should have a very good idea about how self-driving cars work. Here is what we have\u00a0learned: So\u2026 do you feel more dangerous and awesome now than 20 minutes ago?\u00a0:) If yes, let me know and I\u2019ll go to sleep feeling good about helping someone. Stay awesome, and do leave your email here if you\u2019d like to stay in touch! ", "child": "872_1\t872_2\t872_3\t872_4\t872_5\t872_6\t872_7\t872_8872_1\t872_2\t872_3\t872_4\t872_5\t872_6\t872_7\t872_8872_1\t872_2\t872_3\t872_4\t872_5\t872_6\t872_7\t872_8872_1\t872_2\t872_3\t872_4\t872_5\t872_6\t872_7\t872_8"}