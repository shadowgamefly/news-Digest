{"content": "Hi Tim, thanks for this. As a working computational journalist, I\u2019m one of the people who\u2019s been advocating for two ideas: 1) Facebook and other platforms should probably do something about the fake news issue, 2) humans will probably need to be involved in the process on an ongoing basis. I appreciate the way you\u2019ve broken down the process of determining that the article was false, and in fact your work here is very similar to what journalists do every day (see for example the BBC\u2019s verification handbook.) I also agree you can go a long way towards automating this process by using certain quality signals, e.g. domain history, citations. But I\u2019d argue we should expect that human judgments of authority will be necessary for the foreseeable future, both in terms of algorithm design and in terms of day-to-day editorial operations. For example, you can fairly easily algorithmically check whether something has been debunked in Snopes, but you first need the human judgement that Snopes is reputable\u200a\u2014\u200aa point which is disputed. Even Google News has a hand-curated list of reputable \u201cnews sources.\u201d Facebook used to have such a list, but discarded it after claims of bias, after which their Trending stories got spammy. And even then, referring to reputable sources just outsources the problems and the costs to traditional media organizations\u200a\u2014\u200awho may not be able to do this job effectively due to lack of capacity, incentives, or visibility into what people are sharing. I am certainly not arguing that only humans can do verification. I do agree it\u2019s possible to build a system that flags suspicious stories using the sorts of lightweight signals you mention. But I would guess that there are too many edge cases and too many verification processes that are hard to automate. You write \u201cchecking sources is something that computers are much better at doing than humans\u201d but I\u2019m not at all sure this is true. Can you say why you believe that? For a start, there are a huge number of possible authoritative sources\u200a\u2014\u200afrom FBI crime statistics to the Mayor\u2019s office\u200a\u2014\u200aand not all of them are online. Journalists still have to email or even (shudder!) call people far more often than you might think. Or consider, as you suggest, the much more constrained problem of checking whether a citation really supports the original post\u2019s argument. And look at the complexity of the reasoning and research process you went through to figure out what the bogus maps represented. All of this seems like hard AI to me. So it\u2019s not clear to me how much really can be automated\u200a\u2014\u200aor to put it another way, how much leverage you can get out of the minimal amount of ongoing editorial work. But I strongly suspect the answer lies in combining automation and humans in an intelligent way. I have in mind a system that catches suspicious stories and flags them for human editorial review. After all, we already have tens of thousands of people flagging offensive posts. But first our platforms would have to decide they cared about the truth. Google bit this bullet many years ago with its Panda system and other upgrades. Facebook still seems extraordinarily reluctant, and I think I understand why: any attempt to determine truth opens them up to charges of bias. To which all I can say is: welcome to being the media. ", "child": "", "name": "737_21", "parent": "737", "title": ""}